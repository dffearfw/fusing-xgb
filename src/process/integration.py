import logging
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import traceback

logger = logging.getLogger("DataIntegrator")


class DataIntegrator:
    def __init__(self, output_dir, secure_processor=None):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger("DataIntegrator")
        self.source_data = {}
        self.db_conn = None
        self.db_fields_config = {
            'sweï¼ˆmmï¼‰': 'swe',
            'Altitude(m)': 'altitude',

            # 'snowDepth(mm)': 'snow_depth',
            # 'snowDensity(g/cm3)': 'snow_density'
        }

    def connect_database(self):
        """è¿æ¥ç«™ç‚¹æ•°æ®åº“"""
        try:
            from src.process.config import config
            db_path = config.get_station_db_path()
            self.db_conn = sqlite3.connect(db_path)
            self.logger.info("æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            return False

    def add_source(self, name, file_path):
        """æ·»åŠ æ•°æ®æº - å®Œæ•´ç‰ˆæœ¬"""
        try:
            path = Path(file_path)

            if not path.exists():
                self.logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                return False

            # è¯»å–æ–‡ä»¶
            if path.suffix.lower() == '.parquet':
                df = pd.read_parquet(path)
            elif path.suffix.lower() in ['.xlsx', '.xls']:
                try:
                    df = pd.read_excel(path, engine='openpyxl')
                except Exception as e:
                    self.logger.warning(f"openpyxlå¼•æ“å¤±è´¥: {str(e)}ï¼Œå°è¯•xlrd")
                    try:
                        df = pd.read_excel(path, engine='xlrd')
                    except Exception as e:
                        self.logger.error(f"æ‰€æœ‰Excelå¼•æ“éƒ½å¤±è´¥: {str(e)}")
                        return False
            else:
                try:
                    df = pd.read_csv(path)
                except UnicodeDecodeError:
                    try:
                        df = pd.read_csv(path, encoding='latin1')
                    except:
                        try:
                            df = pd.read_csv(path, encoding='gbk')
                        except:
                            self.logger.error("CSVæ–‡ä»¶è¯»å–å¤±è´¥")
                            return False

            # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
            df = self._standardize_date_format(df, name)

            # æ ‡å‡†åŒ–åˆ—å
            df = self._standardize_column_names(df, name)

            # æ•°æ®éªŒè¯
            validation_result = self._validate_dataframe(df, name)
            if not validation_result['valid']:
                self.logger.warning(f"æ•°æ®æº {name} éªŒè¯è­¦å‘Š: {validation_result['message']}")

            # è®°å½•åæ ‡ä¿¡æ¯
            if self._has_coordinate_info(df):
                coord_columns = self._get_coordinate_columns(df)
                stations_with_coords = df['station_id'].nunique()
                self.logger.info(f"æ•°æ®æº {name} åŒ…å«åæ ‡ä¿¡æ¯: {coord_columns}, æ¶‰åŠ {stations_with_coords} ä¸ªç«™ç‚¹")

            self.source_data[name] = df
            self.logger.info(f"æ·»åŠ  {name}: {len(df)} è¡Œ, åˆ—: {list(df.columns)}")
            return True

        except Exception as e:
            self.logger.error(f"æ·»åŠ  {name} å¤±è´¥: {e}")
            return False

    def get_station_statistics(self):
        """ç»Ÿè®¡æ•°æ®åº“ä¸­çš„ç«™ç‚¹ä¿¡æ¯

        Returns:
            dict: åŒ…å«ç«™ç‚¹ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        try:
            if not self.db_conn:
                if not self.connect_database():
                    self.logger.error("æ— æ³•è¿æ¥æ•°æ®åº“")
                    return None

            # æŸ¥è¯¢æ‰€æœ‰ä¸é‡å¤çš„ç«™ç‚¹ID
            query = "SELECT DISTINCT station_ID FROM stations"
            df_stations = pd.read_sql_query(query, self.db_conn)

            if df_stations.empty:
                self.logger.warning("æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ç«™ç‚¹æ•°æ®")
                return None

            total_stations = len(df_stations)

            # æŸ¥è¯¢æ¯ä¸ªç«™ç‚¹çš„æ•°æ®è®°å½•æ•°
            count_query = """
            SELECT station_ID, COUNT(*) as record_count 
            FROM stations 
            GROUP BY station_ID
            ORDER BY record_count DESC
            """
            df_counts = pd.read_sql_query(count_query, self.db_conn)

            # æŸ¥è¯¢æ¯ä¸ªç«™ç‚¹çš„SWEæ•°æ®è¦†ç›–æƒ…å†µ
            swe_query = """
            SELECT station_ID, 
                   COUNT(*) as total_records,
                   COUNT(sweï¼ˆmmï¼‰) as swe_records,
                   ROUND(COUNT(sweï¼ˆmmï¼‰) * 100.0 / COUNT(*), 2) as swe_coverage_rate
            FROM stations 
            GROUP BY station_ID
            ORDER BY swe_coverage_rate DESC
            """
            df_swe_coverage = pd.read_sql_query(swe_query, self.db_conn)

            # æŸ¥è¯¢æ—¶é—´èŒƒå›´
            time_query = """
            SELECT MIN(time) as start_time, MAX(time) as end_time
            FROM stations
            """
            df_time_range = pd.read_sql_query(time_query, self.db_conn)

            # æ„å»ºç»Ÿè®¡ç»“æœ
            statistics = {
                'total_stations': total_stations,
                'station_ids': df_stations['station_ID'].tolist(),
                'records_per_station': df_counts.set_index('station_ID')['record_count'].to_dict(),
                'swe_coverage': df_swe_coverage.set_index('station_ID').to_dict('index'),
                'time_range': {
                    'start': df_time_range['start_time'].iloc[0],
                    'end': df_time_range['end_time'].iloc[0]
                },
                'summary': {
                    'avg_records_per_station': df_counts['record_count'].mean(),
                    'max_records_per_station': df_counts['record_count'].max(),
                    'min_records_per_station': df_counts['record_count'].min(),
                    'avg_swe_coverage': df_swe_coverage['swe_coverage_rate'].mean(),
                    'stations_with_swe': len(df_swe_coverage[df_swe_coverage['swe_coverage_rate'] > 0])
                }
            }

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“Š æ•°æ®åº“ç«™ç‚¹ç»Ÿè®¡ä¿¡æ¯")
            self.logger.info("=" * 60)
            self.logger.info(f"æ€»ç«™ç‚¹æ•°é‡: {total_stations}")
            self.logger.info(f"æ—¶é—´èŒƒå›´: {statistics['time_range']['start']} åˆ° {statistics['time_range']['end']}")
            self.logger.info(f"å¹³å‡æ¯ç«™è®°å½•æ•°: {statistics['summary']['avg_records_per_station']:.1f}")
            self.logger.info(f"æœ€å¤§è®°å½•æ•°ç«™ç‚¹: {statistics['summary']['max_records_per_station']}")
            self.logger.info(f"æœ€å°è®°å½•æ•°ç«™ç‚¹: {statistics['summary']['min_records_per_station']}")
            self.logger.info(f"æœ‰SWEæ•°æ®çš„ç«™ç‚¹æ•°: {statistics['summary']['stations_with_swe']}")
            self.logger.info(f"å¹³å‡SWEè¦†ç›–ç‡: {statistics['summary']['avg_swe_coverage']:.1f}%")

            # æ˜¾ç¤ºå‰10ä¸ªç«™ç‚¹IDç¤ºä¾‹
            sample_stations = statistics['station_ids'][:10]
            self.logger.info(f"ç«™ç‚¹IDç¤ºä¾‹ (å‰10ä¸ª): {sample_stations}")

            return statistics

        except Exception as e:
            self.logger.error(f"ç»Ÿè®¡ç«™ç‚¹ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None

    def export_station_statistics(self, output_dir=None):
        """å¯¼å‡ºç«™ç‚¹ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶

        Args:
            output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„

        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            statistics = self.get_station_statistics()
            if statistics is None:
                return None

            if output_dir is None:
                output_dir = self.output_dir

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            Path(output_dir).mkdir(parents=True, exist_ok=True)

            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = Path(output_dir) / f"station_statistics_{timestamp}.xlsx"

            # åˆ›å»ºExcelå†™å…¥å™¨
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                # 1. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
                summary_data = {
                    'ç»Ÿè®¡é¡¹': [
                        'æ€»ç«™ç‚¹æ•°é‡',
                        'æ—¶é—´èŒƒå›´å¼€å§‹',
                        'æ—¶é—´èŒƒå›´ç»“æŸ',
                        'å¹³å‡æ¯ç«™è®°å½•æ•°',
                        'æœ€å¤§è®°å½•æ•°',
                        'æœ€å°è®°å½•æ•°',
                        'æœ‰SWEæ•°æ®çš„ç«™ç‚¹æ•°',
                        'å¹³å‡SWEè¦†ç›–ç‡(%)'
                    ],
                    'æ•°å€¼': [
                        statistics['total_stations'],
                        statistics['time_range']['start'],
                        statistics['time_range']['end'],
                        f"{statistics['summary']['avg_records_per_station']:.1f}",
                        statistics['summary']['max_records_per_station'],
                        statistics['summary']['min_records_per_station'],
                        statistics['summary']['stations_with_swe'],
                        f"{statistics['summary']['avg_swe_coverage']:.1f}"
                    ]
                }
                df_summary = pd.DataFrame(summary_data)
                df_summary.to_excel(writer, sheet_name='åŸºæœ¬ç»Ÿè®¡', index=False)

                # 2. æ‰€æœ‰ç«™ç‚¹åˆ—è¡¨
                df_stations = pd.DataFrame({
                    'station_id': statistics['station_ids']
                })
                df_stations.to_excel(writer, sheet_name='æ‰€æœ‰ç«™ç‚¹', index=False)

                # 3. ç«™ç‚¹è®°å½•æ•°ç»Ÿè®¡
                records_data = []
                for station_id, count in statistics['records_per_station'].items():
                    records_data.append({
                        'station_id': station_id,
                        'record_count': count
                    })
                df_records = pd.DataFrame(records_data).sort_values('record_count', ascending=False)
                df_records.to_excel(writer, sheet_name='ç«™ç‚¹è®°å½•æ•°', index=False)

                # 4. SWEè¦†ç›–ç‡ç»Ÿè®¡
                swe_data = []
                for station_id, coverage in statistics['swe_coverage'].items():
                    swe_data.append({
                        'station_id': station_id,
                        'total_records': coverage['total_records'],
                        'swe_records': coverage['swe_records'],
                        'swe_coverage_rate': coverage['swe_coverage_rate']
                    })
                df_swe = pd.DataFrame(swe_data).sort_values('swe_coverage_rate', ascending=False)
                df_swe.to_excel(writer, sheet_name='SWEè¦†ç›–ç‡', index=False)

            self.logger.info(f"âœ… ç«™ç‚¹ç»Ÿè®¡ä¿¡æ¯å·²å¯¼å‡º: {output_path}")
            return str(output_path)

        except Exception as e:
            self.logger.error(f"å¯¼å‡ºç«™ç‚¹ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None

    def _standardize_date_format(self, df, source_name):
        """ç»Ÿä¸€æ—¥æœŸæ ¼å¼ä¸ºå­—ç¬¦ä¸² YYYY-MM-DD"""
        try:
            if 'date' not in df.columns:
                return df

            df_copy = df.copy()

            # è½¬æ¢ä¸ºç»Ÿä¸€çš„å­—ç¬¦ä¸²æ ¼å¼
            if 'datetime' in str(df_copy['date'].dtype):
                df_copy['date'] = df_copy['date'].dt.strftime('%Y-%m-%d')
            else:
                try:
                    df_copy['date'] = pd.to_datetime(df_copy['date']).dt.strftime('%Y-%m-%d')
                except Exception as e:
                    try:
                        df_copy['date'] = df_copy['date'].str.replace('/', '-')
                        df_copy['date'] = pd.to_datetime(df_copy['date']).dt.strftime('%Y-%m-%d')
                    except:
                        self.logger.warning(f"{source_name}: æ— æ³•æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼")

            return df_copy

        except Exception as e:
            self.logger.error(f"æ ‡å‡†åŒ– {source_name} æ—¥æœŸæ ¼å¼å¤±è´¥: {str(e)}")
            return df

    def _standardize_column_names(self, df, source_name):
        """æ ‡å‡†åŒ–åˆ—å"""
        try:
            df_copy = df.copy()
            column_mapping = {}

            # ç«™ç‚¹IDæ ‡å‡†åŒ–
            if 'station_ID' in df_copy.columns and 'station_id' not in df_copy.columns:
                column_mapping['station_ID'] = 'station_id'
            if 'Station_ID' in df_copy.columns and 'station_id' not in df_copy.columns:
                column_mapping['Station_ID'] = 'station_id'

            # åæ ‡æ ‡å‡†åŒ–
            coord_mapping = {
                'Longitude': 'longitude', 'Lon': 'longitude',
                'Latitude': 'latitude', 'Lat': 'latitude'
            }

            for old_name, new_name in coord_mapping.items():
                if old_name in df_copy.columns and new_name not in df_copy.columns:
                    column_mapping[old_name] = new_name

            if column_mapping:
                df_copy = df_copy.rename(columns=column_mapping)
                self.logger.debug(f"{source_name}: é‡å‘½ååˆ— {list(column_mapping.keys())}")

            return df_copy

        except Exception as e:
            self.logger.error(f"æ ‡å‡†åŒ– {source_name} åˆ—åå¤±è´¥: {str(e)}")
            return df

    def _validate_dataframe(self, df, source_name):
        """éªŒè¯DataFrameç»“æ„"""
        validation = {'valid': True, 'message': ''}

        if 'station_id' not in df.columns:
            validation['valid'] = False
            validation['message'] = "ç¼ºå°‘station_idåˆ—"
            return validation

        numeric_cols = df.select_dtypes(include=['number']).columns
        value_cols = [col for col in numeric_cols if col not in ['station_id', 'date', 'year', 'month', 'day']]

        if not value_cols:
            validation['valid'] = False
            validation['message'] = "æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—"
            return validation

        station_count = df['station_id'].nunique()
        if station_count == 0:
            validation['valid'] = False
            validation['message'] = "æ²¡æœ‰æœ‰æ•ˆçš„ç«™ç‚¹æ•°æ®"
            return validation

        if 'date' in df.columns:
            date_count = df['date'].nunique()
            validation['message'] = f"åŠ¨æ€æ•°æ®æºï¼Œ{station_count}ä¸ªç«™ç‚¹ï¼Œ{date_count}ä¸ªæ—¶é—´ç‚¹"
        else:
            validation['message'] = f"é™æ€æ•°æ®æºï¼Œ{station_count}ä¸ªç«™ç‚¹"

        return validation

    def _has_coordinate_info(self, df):
        """æ£€æŸ¥æ•°æ®æ¡†æ˜¯å¦åŒ…å«åæ ‡ä¿¡æ¯"""
        coordinate_columns = ['longitude', 'latitude', 'Longitude', 'Latitude', 'lon', 'lat', 'Lon', 'Lat']
        return any(col in df.columns for col in coordinate_columns)

    def _get_coordinate_columns(self, df):
        """è·å–æ•°æ®æ¡†ä¸­çš„åæ ‡åˆ—å"""
        coordinate_mapping = {
            'longitude': ['longitude', 'Longitude', 'lon', 'Lon'],
            'latitude': ['latitude', 'Latitude', 'lat', 'Lat']
        }

        found_columns = []
        for standard_name, possible_names in coordinate_mapping.items():
            for name in possible_names:
                if name in df.columns:
                    found_columns.append(name)
                    break

        return found_columns

    def get_database_data(self, station_ids, start_date, end_date):
        """ç»Ÿä¸€è·å–æ•°æ®åº“ä¸­é…ç½®çš„æ‰€æœ‰å­—æ®µ"""
        try:
            if not self.db_conn:
                if not self.connect_database():
                    return pd.DataFrame()

            if not self.db_fields_config:
                self.logger.warning("æ²¡æœ‰é…ç½®æ•°æ®åº“å­—æ®µ")
                return pd.DataFrame()

            # æ„å»ºæŸ¥è¯¢
            field_selects = []
            for db_field in self.db_fields_config.keys():
                field_selects.append(f'"{db_field}"')

            query = f"""
            SELECT station_ID, time, {', '.join(field_selects)}
            FROM stations 
            WHERE time BETWEEN ? AND ?
            """

            # è½¬æ¢æ—¥æœŸæ ¼å¼
            start_db = int(start_date.strftime('%Y%m%d'))
            end_db = int(end_date.strftime('%Y%m%d'))

            df = pd.read_sql_query(query, self.db_conn, params=[start_db, end_db])

            if not df.empty:
                # å¤„ç†æ•°æ®
                df['date'] = df['time'].apply(
                    lambda x: datetime.strptime(str(int(x)), '%Y%m%d').strftime('%Y-%m-%d')
                )
                df['station_id'] = df['station_ID'].astype(str)

                # é‡å‘½ååˆ—
                result_df = df[['station_id', 'date'] + list(self.db_fields_config.keys())]
                result_df = result_df.rename(columns=self.db_fields_config)

                # è¿‡æ»¤ç«™ç‚¹
                if station_ids:
                    result_df = result_df[result_df['station_id'].isin(station_ids)]

                self.logger.info(f"âœ… ä»æ•°æ®åº“è·å– {len(self.db_fields_config)} ä¸ªå­—æ®µ: {list(self.db_fields_config.values())}")

                # ç»Ÿè®¡æ¯ä¸ªå­—æ®µçš„æœ‰æ•ˆæ•°æ®é‡
                for field in self.db_fields_config.values():
                    if field in result_df.columns:
                        valid_count = result_df[field].notna().sum()
                        self.logger.info(f"  {field}: {valid_count} æ¡æœ‰æ•ˆè®°å½•")

                return result_df
            else:
                self.logger.warning("æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            self.logger.error(f"è·å–æ•°æ®åº“æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def _create_correct_wide_table(self):
        """åˆ›å»ºæ­£ç¡®çš„å®½è¡¨ - ä¿ç•™å¹¶é›†ä¸»ç´¢å¼•ï¼Œå¢å¼ºé™æ€æ•°æ®æ”¯æŒä¸å…¼å®¹å¤„ç†"""
        if not self.source_data:
            return pd.DataFrame()

        self.logger.info("åˆ›å»ºæ­£ç¡®å®½è¡¨...")

        try:
            # 1. è®°å½•å¹¶æ ‡å‡†åŒ–æ¯ä¸ªæºçš„station_idæ ¼å¼ï¼ˆä¸ä¿®æ”¹åŸå§‹å­˜å‚¨ï¼‰
            self.logger.info("=== æ•°æ®æºè¯¦ç»†ä¿¡æ¯ ===")
            for source_name, source_df in self.source_data.items():
                if source_df is None:
                    continue
                try:
                    self.source_data[source_name] = self._standardize_station_id_format(source_df, source_name)
                except Exception:
                    # è‹¥å¤±è´¥ï¼Œç»§ç»­ä½†è®°å½•
                    self.logger.warning(f"{source_name}: station_id æ ‡å‡†åŒ–å¤±è´¥ï¼Œç»§ç»­å¤„ç†")

                record_count = len(self.source_data[source_name])
                station_count = self.source_data[source_name]['station_id'].nunique() if 'station_id' in \
                                                                                         self.source_data[
                                                                                             source_name].columns else 0
                date_count = self.source_data[source_name]['date'].nunique() if 'date' in self.source_data[
                    source_name].columns else "N/A"
                self.logger.info(f"{source_name}: {record_count} è®°å½•, {station_count} ç«™ç‚¹, {date_count} æ—¥æœŸ")

                if 'station_id' in self.source_data[source_name].columns:
                    sample_stations = self.source_data[source_name]['station_id'].unique()[:5]
                    self.logger.info(f"  {source_name} ç«™ç‚¹ç¤ºä¾‹: {sample_stations.tolist()}")

            # 2. åˆ†ç±»æ•°æ®æºï¼ˆä¿æŒä½ åŸæœ‰åˆ†ç±»é€»è¾‘ï¼Œä½†ä¿ç•™åŸå§‹ df ä»¥ä¾¿åç»­å¤æ‚å¤„ç†ï¼‰
            static_dfs = []
            yearly_dfs = []
            dynamic_dfs = []  # æ ¼å¼: (source_name, df_wide) å…¶ä¸­ df_wide å·²ç»æ˜¯ (station_id, date, value)
            dynamic_raw = []  # åŸå§‹åŠ¨æ€ DataFrameï¼ˆä¾›GLDAS/ç‰©å€™ç­‰ç‰¹æ®Šå¤„ç†ï¼‰
            phenology_dfs = []
            gldas_dfs = []
            coordinate_dfs = []

            all_station_ids = set()
            all_dates = set()

            for source_name, source_df in self.source_data.items():
                if source_df is None:
                    continue

                # æ”¶é›†ç«™ç‚¹ä¸æ—¥æœŸç”¨äºåç»­æ•°æ®åº“è¯·æ±‚
                if 'station_id' in source_df.columns:
                    all_station_ids.update(source_df['station_id'].astype(str).unique())
                if 'date' in source_df.columns:
                    # ç»Ÿä¸€æ—¥æœŸä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…ç±»å‹ä¸ä¸€è‡´å¯¼è‡´ min/max å‡ºé”™
                    try:
                        dates = pd.to_datetime(source_df['date'], errors='coerce').dropna().dt.strftime(
                            '%Y-%m-%d').unique()
                        all_dates.update(dates)
                    except Exception:
                        pass

                # åˆ†ç±»
                if self._is_gldas_data(source_name, source_df):
                    gldas_dfs.append((source_name, source_df))
                    dynamic_raw.append((source_name, source_df))
                elif self._is_snow_phenology_data(source_name, source_df):
                    phenology_dfs.append((source_name, source_df))
                    dynamic_raw.append((source_name, source_df))
                elif self._is_terrain_features(source_name, source_df) or source_name == 'landuse':
                    static_dfs.append((source_name, source_df))
                    self.logger.info(f"è¯†åˆ«ä¸ºé™æ€æ•°æ®æº: {source_name}")
                elif self._is_yearly_data(source_name, source_df):
                    yearly_dfs.append((source_name, source_df))
                elif 'date' in source_df.columns and 'station_id' in source_df.columns:
                    # åŠ¨æ€æ•°æ®æºï¼šé€‰ç¬¬ä¸€ä¸ªæ•°å€¼åˆ—ä½œä¸ºä»£è¡¨å€¼å¹¶åšå®½è¡¨åˆ—
                    numeric_cols = source_df.select_dtypes(include=['number']).columns
                    value_cols = [col for col in numeric_cols if
                                  col not in ['station_id', 'date', 'year', 'month', 'day', 'dataset_type']]
                    if value_cols:
                        value_col = value_cols[0]
                        source_wide = source_df[['station_id', 'date', value_col]].copy()
                        source_wide = source_wide.rename(columns={value_col: source_name})
                        source_wide = source_wide.drop_duplicates(['station_id', 'date'])
                        # è§„èŒƒåŒ–æ—¥æœŸæ ¼å¼
                        source_wide['date'] = pd.to_datetime(source_wide['date'], errors='coerce').dt.strftime(
                            '%Y-%m-%d')
                        source_wide['station_id'] = source_wide['station_id'].astype(str)
                        dynamic_dfs.append((source_name, source_wide))
                        dynamic_raw.append((source_name, source_df))
                elif 'station_id' in source_df.columns:
                    static_dfs.append((source_name, source_df))

                # åæ ‡è®°å½•
                if self._has_coordinate_info(source_df):
                    coordinate_dfs.append((source_name, source_df))

            self.logger.info(
                f"æ•°æ®æºåˆ†ç±»ç»“æœ: åŠ¨æ€={len(dynamic_dfs)} é™æ€={len(static_dfs)} å¹´åº¦={len(yearly_dfs)} GLDAS={len(gldas_dfs)} ç‰©å€™={len(phenology_dfs)}")

            # 3. ä»æ•°æ®åº“è·å– SWE å’Œ æµ·æ‹”ï¼ˆå½“å­˜åœ¨ç«™ç‚¹/æ—¥æœŸé›†åˆæ—¶ï¼‰
            swe_df = pd.DataFrame()
            altitude_df = pd.DataFrame()
            if all_station_ids and all_dates:
                start_date = min(all_dates) if all_dates else None
                end_date = max(all_dates) if all_dates else None

                try:
                    start_dt = pd.to_datetime(start_date)
                    end_dt = pd.to_datetime(end_date)
                except Exception:
                    start_dt = None
                    end_dt = None

                if start_dt is not None and end_dt is not None:
                    self.logger.info(f"è·å–æ•°æ®åº“æ•°æ®: {len(all_station_ids)} ä¸ªç«™ç‚¹, {len(all_dates)} ä¸ªæ—¥æœŸ")
                    swe_df = self.get_swe_from_database(list(all_station_ids), start_dt, end_dt)
                    altitude_df = self.get_altitude_from_database(list(all_station_ids))

                    # è§„èŒƒåŒ–è¿”å›è¡¨
                    if not swe_df.empty:
                        swe_df['date'] = pd.to_datetime(swe_df['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                        swe_df['station_id'] = swe_df['station_id'].astype(str)
                    if not altitude_df.empty:
                        altitude_df['station_id'] = altitude_df['station_id'].astype(str)

                    self.logger.info(f"æ•°æ®åº“æ•°æ®è·å–ç»“æœ: SWE {len(swe_df)} è¡Œ, æµ·æ‹” {len(altitude_df)} è¡Œ")
                else:
                    self.logger.warning("æ— æ³•ç¡®å®šæ—¥æœŸèŒƒå›´ï¼Œè·³è¿‡æ•°æ®åº“ SWE/æµ·æ‹”è·å–")

            # 4. æ„å»º master indexï¼šåŠ¨æ€æ•°æ®é›†çš„ (station_id,date) å¹¶é›† + SWE çš„ (station_id,date)
            pairs = []
            for name, df in dynamic_dfs:
                if {'station_id', 'date'}.issubset(df.columns):
                    tmp = df[['station_id', 'date']].drop_duplicates()
                    pairs.append(tmp)
            if not swe_df.empty and {'station_id', 'date'}.issubset(swe_df.columns):
                pairs.append(swe_df[['station_id', 'date']].drop_duplicates())

            if not pairs:
                # å¦‚æœæ²¡æœ‰ä»»ä½•åŠ¨æ€å¯¹ï¼Œé€€å›å¤„ç†çº¯é™æ€æƒ…å†µ
                self.logger.warning("æ²¡æœ‰åŠ¨æ€æ•°æ®å¯¹ (station_id,date)ã€‚ä½¿ç”¨çº¯é™æ€æµç¨‹")
                return self._handle_static_only_case(static_dfs, yearly_dfs, phenology_dfs, gldas_dfs)

            master_index = pd.concat(pairs, ignore_index=True).drop_duplicates().reset_index(drop=True)
            master_index['station_id'] = master_index['station_id'].astype(str)
            master_index['date'] = pd.to_datetime(master_index['date'], errors='coerce').dt.strftime('%Y-%m-%d')
            self.logger.info(f"ä¸»ç´¢å¼•æ„å»ºå®Œæˆ (å¹¶é›†): {len(master_index)} è¡Œ")

            # 5. å°†é™æ€æ•°æ®åˆå¹¶ä¸º single static_combinedï¼ˆä¿ç•™æ‰€æœ‰ç‰¹å¾åˆ—ï¼‰
            static_combined = None
            if static_dfs:
                static_combined = self._combine_all_static_data(static_dfs)  # ä½ å·²æœ‰å‡½æ•°ï¼Œä¼šåšæ¸…ç†
                if 'station_id' in static_combined.columns:
                    static_combined['station_id'] = static_combined['station_id'].astype(str)
                else:
                    # é˜²å¾¡ï¼šè‹¥é™æ€è¡¨ç¼º station_idï¼Œåˆ™ä¸¢å¼ƒé™æ€åˆå¹¶
                    self.logger.warning("é™æ€åˆå¹¶ç»“æœç¼ºå°‘ station_id åˆ—ï¼Œè·³è¿‡é™æ€åˆå¹¶")
                    static_combined = None

            # 6. ä»¥ master_index ä¸ºåŸºç¡€ï¼ŒæŠŠé™æ€è¡¨æŒ‰ station_id å·¦è¿æ¥ï¼ˆè¿™æ ·é™æ€åˆ—ä¼šè¢«å¤åˆ¶åˆ°æ¯ä¸ªæ—¶é—´ç‚¹ï¼‰
            final_wide = master_index.copy()
            if static_combined is not None and not static_combined.empty:
                before_static_merge = len(final_wide)
                final_wide = final_wide.merge(static_combined, on='station_id', how='left')
                self.logger.info(f"é™æ€æ•°æ®åˆå¹¶åˆ°ä¸»ç´¢å¼•: {before_static_merge} -> {len(final_wide)} è¡Œ")

            # 7ï¸âƒ£ åˆå¹¶å„åŠ¨æ€æ•°æ®æºï¼ˆæŒ‰ station_id + date å·¦è¿æ¥ï¼‰
            for name, df in dynamic_dfs:
                # ğŸŸ¢ ç‰¹æ®Šæƒ…å†µï¼šåªè¦ gldas çš„ value åˆ—
                if name.lower() == "gldas":
                    if "value" in df.columns:
                        df = df[["station_id", "date", "value"]].rename(columns={"value": "gldas_value"})
                        final_wide = final_wide.merge(df, on=["station_id", "date"], how="left")
                        self.logger.info(f"å·²åˆå¹¶åŠ¨æ€æº [{name}]ï¼ˆä»… value åˆ—ï¼‰ï¼Œå½“å‰è¡Œæ•°: {len(final_wide)}")
                    else:
                        self.logger.warning(f"åŠ¨æ€æº [{name}] æœªæ‰¾åˆ° 'value' åˆ—ï¼Œè·³è¿‡")
                    continue

                # ğŸŸ¡ æ™®é€šæ•°æ®æºï¼ˆå…¨éƒ¨å­—æ®µï¼‰
                feature_cols = [c for c in df.columns if c not in ["station_id", "date"]]
                if not feature_cols:
                    continue

                df_prefixed = df[["station_id", "date"] + feature_cols].copy()
                rename_map = {col: f"{name}_{col}" for col in feature_cols}
                df_prefixed = df_prefixed.rename(columns=rename_map)
                final_wide = final_wide.merge(df_prefixed, on=["station_id", "date"], how="left")
                self.logger.info(f"å·²åˆå¹¶åŠ¨æ€æº [{name}]ï¼Œå½“å‰è¡Œæ•°: {len(final_wide)}")

            # 8. åˆå¹¶æ•°æ®åº“ SWEï¼ˆç¡®ä¿å®Œæ•´è¡Œè¢«ä¿ç•™ â€”â€” master_index å·²åŒ…å« SWE çš„å¹¶é›†ï¼‰
            if not swe_df.empty:
                swe_merge_cols = [c for c in swe_df.columns if c not in ['station_id', 'date']]
                if 'swe' not in swe_df.columns and swe_merge_cols:
                    # å°è¯•å°†ç¬¬ä¸€ä¸ªå¯èƒ½åŒ…å«çš„sweåˆ—é‡å‘½åä¸º 'swe'
                    possible_swe = [c for c in swe_merge_cols if 'swe' in c.lower()]
                    if possible_swe:
                        swe_df = swe_df.rename(columns={possible_swe[0]: 'swe'})
                        swe_merge_cols = [c for c in swe_df.columns if c not in ['station_id', 'date']]

                if 'swe' in swe_df.columns:
                    before_count = len(final_wide)
                    final_wide = final_wide.merge(swe_df[['station_id', 'date', 'swe']], on=['station_id', 'date'],
                                                  how='left')
                    after_count = len(final_wide)
                    swe_valid_count = final_wide['swe'].notna().sum()
                    self.logger.info(f"åˆå¹¶æ•°æ®åº“SWEæ•°æ®: {before_count} -> {after_count} è¡Œ, æœ‰æ•ˆswe: {swe_valid_count}")
                else:
                    self.logger.warning("SWE æ•°æ®å­˜åœ¨ä½†æœªè¯†åˆ«åˆ°sweåˆ—ï¼Œè·³è¿‡è‡ªåŠ¨åˆå¹¶ï¼Œè¯·ç¡®è®¤åˆ—å")

            # 9. åˆå¹¶æ•°æ®åº“æµ·æ‹”ï¼ˆé™æ€ï¼ŒæŒ‰ station_idï¼‰
            if not altitude_df.empty:
                altitude_df = altitude_df.rename(columns={col: col for col in altitude_df.columns})  # æ— æ“ä½œï¼Œä»…ç¡®ä¿åˆ—åœ¨dfä¸­
                if 'station_id' in altitude_df.columns and 'altitude' in altitude_df.columns:
                    before_count = len(final_wide)
                    final_wide = final_wide.merge(altitude_df[['station_id', 'altitude']].drop_duplicates('station_id'),
                                                  on='station_id', how='left')
                    after_count = len(final_wide)
                    altitude_valid_count = final_wide[
                        'altitude'].notna().sum() if 'altitude' in final_wide.columns else 0
                    self.logger.info(f"åˆå¹¶æ•°æ®åº“æµ·æ‹”æ•°æ®: {before_count} -> {after_count} è¡Œ, æœ‰æ•ˆæµ·æ‹”: {altitude_valid_count}")
                else:
                    self.logger.warning("æµ·æ‹”è¡¨æœªåŒ…å« station_id æˆ– altitude åˆ—ï¼Œè·³è¿‡æµ·æ‹”åˆå¹¶")

            # 10. GLDAS / ç‰©å€™ / å¹´åº¦ ç‰¹æ®Šå¤„ç†ï¼ˆæŠŠåŸå§‹ raw df æŒ‰éœ€åˆå¹¶æˆ–åšé¢å¤–å¤„ç†ï¼‰
            # ä¿æŒä½ åŸæœ‰çš„å¤„ç†é’©å­ï¼šå¦‚æœä½ éœ€è¦ç‰¹å®šåˆ—çš„å¤„ç†é€»è¾‘ï¼Œè¯·åœ¨è¿™é‡Œæ‰©å±•
            if gldas_dfs:
                for g_name, g_df in gldas_dfs:
                    self.logger.info(f"å¤„ç† GLDAS æº {g_name}: {len(g_df)} è¡Œ (åˆå¹¶åˆ° final_wide)")
                    # å°è¯•æŠŠ gldas çš„å…³é”®å˜é‡ï¼ˆè‹¥å­˜åœ¨ station_id,dateï¼‰åˆå¹¶
                    if {'station_id', 'date'}.issubset(g_df.columns):
                        gdf = g_df.copy()
                        gdf['station_id'] = gdf['station_id'].astype(str)
                        gdf['date'] = pd.to_datetime(gdf['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                        # å–æ•°å€¼åˆ—å¹¶åŠ å‰ç¼€å†åˆå¹¶
                        num_cols = [c for c in gdf.select_dtypes(include=['number']).columns if
                                    c not in ['station_id', 'date']]
                        if num_cols:
                            g_pref = gdf[['station_id', 'date'] + num_cols].drop_duplicates(['station_id', 'date'])
                            # é‡å‘½åé¿å…å†²çª
                            rename_map = {c: f"{g_name}_{c}" for c in num_cols}
                            g_pref = g_pref.rename(columns=rename_map)
                            final_wide = final_wide.merge(g_pref, on=['station_id', 'date'], how='left')
                            self.logger.info(f"  åˆå¹¶ GLDAS ç‰¹å¾: {len(num_cols)} åˆ—")
                    else:
                        self.logger.warning(f"  GLDAS æº {g_name} ç¼ºå°‘ station_id/dateï¼Œè·³è¿‡è‡ªåŠ¨åˆå¹¶")

            if phenology_dfs:
                for p_name, p_df in phenology_dfs:
                    self.logger.info(f"å¤„ç†ç‰©å€™æº {p_name}: {len(p_df)} è¡Œ")
                    if {'station_id', 'date'}.issubset(p_df.columns):
                        pcopy = p_df.copy()
                        pcopy['station_id'] = pcopy['station_id'].astype(str)
                        pcopy['date'] = pd.to_datetime(pcopy['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                        # åˆå¹¶å¯èƒ½çš„ç‰©å€™æ ‡è¯†åˆ—ï¼ˆéæ—¶é—´ç»´åº¦ï¼‰
                        cols = [c for c in pcopy.columns if c not in ['station_id', 'date']]
                        if cols:
                            p_pref = pcopy[['station_id', 'date'] + cols].drop_duplicates(['station_id', 'date'])
                            rename_map = {c: f"{p_name}_{c}" for c in cols}
                            p_pref = p_pref.rename(columns=rename_map)
                            final_wide = final_wide.merge(p_pref, on=['station_id', 'date'], how='left')
                            self.logger.info(f"  åˆå¹¶ç‰©å€™ç‰¹å¾: {len(cols)} åˆ—")

            if yearly_dfs:
                for y_name, y_df in yearly_dfs:
                    self.logger.info(f"å¤„ç†å¹´åº¦æº {y_name}: {len(y_df)} è¡Œ")
                    # å¹´åº¦æ•°æ®é€šå¸¸æŒ‰ station_id åˆå¹¶ï¼ˆæˆ–æŒ‰ station_id + yearï¼‰
                    if 'station_id' in y_df.columns:
                        ycopy = y_df.copy()
                        ycopy['station_id'] = ycopy['station_id'].astype(str)
                        # å»æ‰æ—¶é—´ç›¸å…³å­—æ®µå†æŒ‰ station_id åˆå¹¶ï¼ˆè‹¥æœ‰å¹´åº¦ç‰¹å¾ï¼‰
                        cols = [c for c in ycopy.columns if c not in ['station_id', 'date', 'year', 'month']]
                        if cols:
                            y_pref = ycopy[['station_id'] + cols].drop_duplicates('station_id')
                            # é‡å‘½åé¿å…å†²çª
                            rename_map = {c: f"{y_name}_{c}" for c in cols}
                            y_pref = y_pref.rename(columns=rename_map)
                            final_wide = final_wide.merge(y_pref, on='station_id', how='left')
                            self.logger.info(f"  åˆå¹¶å¹´åº¦ç‰¹å¾: {len(cols)} åˆ—")

            # 11. ç¡®ä¿åæ ‡ä¿¡æ¯å®Œæ•´ï¼ˆå°è¯•ä»å·²æœ‰ coordinate_dfs è¡¥å…¨ï¼‰
            final_wide = self._ensure_complete_coordinates(final_wide)
            # å¦‚æœä»ç„¶ç¼ºå¤±ï¼Œå°è¯•ç”¨æ‰€æœ‰æºè¡¥å……
            final_wide = self._supplement_coordinates_from_all_sources(final_wide)

            # 12. æ’åºã€é‡ç½®ç´¢å¼•
            if 'date' in final_wide.columns:
                # ç¡®ä¿ date ä¸ºå­—ç¬¦ä¸² YYYY-MM-DDï¼ˆä¾¿äºå¯¼å‡ºï¼‰
                final_wide['date'] = pd.to_datetime(final_wide['date'], errors='coerce').dt.strftime('%Y-%m-%d')
            final_wide = final_wide.sort_values(['station_id', 'date']).reset_index(drop=True)

            # 13. æœ€ç»ˆéªŒè¯ä¸æ—¥å¿—è¾“å‡º
            self._validate_final_wide_table(final_wide)
            self.logger.info(f"âœ… å®½è¡¨åˆ›å»ºå®Œæˆ: {final_wide.shape}")

            return final_wide

        except Exception as e:
            self.logger.error(f"åˆ›å»ºå®½è¡¨å¤±è´¥: {e}")
            self.logger.debug(traceback.format_exc())
            return pd.DataFrame()

    def validate_static_data_merge(self, final_wide, static_source_name):
        """éªŒè¯é™æ€æ•°æ®æ˜¯å¦æ­£ç¡®åˆå¹¶åˆ°æ‰€æœ‰æ—¶é—´ç‚¹"""
        self.logger.info(f"=== éªŒè¯é™æ€æ•°æ® {static_source_name} åˆå¹¶ ===")

        # æ£€æŸ¥é™æ€ç‰¹å¾åˆ—
        static_features = [col for col in final_wide.columns if
                           static_source_name in col.lower() or 'landuse' in col.lower()]

        if not static_features:
            self.logger.warning(f"æ²¡æœ‰æ‰¾åˆ° {static_source_name} ç›¸å…³çš„ç‰¹å¾åˆ—")
            return

        for feature in static_features:
            # æ£€æŸ¥æ¯ä¸ªç«™ç‚¹çš„è¯¥ç‰¹å¾åœ¨æ‰€æœ‰æ—¶é—´ç‚¹æ˜¯å¦ä¸€è‡´
            station_consistency = final_wide.groupby('station_id')[feature].apply(
                lambda x: x.nunique() == 1 if x.notna().any() else True
            )

            inconsistent_stations = station_consistency[~station_consistency].index.tolist()

            if inconsistent_stations:
                self.logger.error(f"ç‰¹å¾ {feature} åœ¨ä»¥ä¸‹ç«™ç‚¹çš„ä¸åŒæ—¶é—´ç‚¹å€¼ä¸ä¸€è‡´: {inconsistent_stations[:5]}")
            else:
                self.logger.info(f"ç‰¹å¾ {feature} åœ¨æ‰€æœ‰ç«™ç‚¹çš„æ‰€æœ‰æ—¶é—´ç‚¹å€¼ä¸€è‡´")

            # æ£€æŸ¥å¡«å……ç‡
            filled_count = final_wide[feature].notna().sum()
            total_count = len(final_wide)
            fill_rate = (filled_count / total_count) * 100

            self.logger.info(f"ç‰¹å¾ {feature} å¡«å……ç‡: {filled_count}/{total_count} ({fill_rate:.1f}%)")

            # å¦‚æœå¡«å……ç‡ä½ï¼Œæ˜¾ç¤ºä¸€äº›ç¼ºå¤±çš„ç«™ç‚¹
            if fill_rate < 90:
                missing_stations = final_wide[final_wide[feature].isna()]['station_id'].unique()
                self.logger.warning(f"ç‰¹å¾ {feature} ç¼ºå¤±çš„ç«™ç‚¹ç¤ºä¾‹: {missing_stations[:10]}")

    def _standardize_station_id_format(self, df, source_name):
        """ç»Ÿä¸€station_idä¸ºå­—ç¬¦ä¸²æ ¼å¼"""
        try:
            if 'station_id' not in df.columns:
                return df

            df_copy = df.copy()

            # æ£€æŸ¥å½“å‰station_idçš„æ•°æ®ç±»å‹
            station_id_dtype = str(df_copy['station_id'].dtype)
            self.logger.debug(f"{source_name} station_idç±»å‹: {station_id_dtype}")

            # è½¬æ¢ä¸ºç»Ÿä¸€çš„å­—ç¬¦ä¸²æ ¼å¼
            if station_id_dtype != 'object':
                df_copy['station_id'] = df_copy['station_id'].astype(str)
                self.logger.debug(f"{source_name}: è½¬æ¢station_idåˆ°å­—ç¬¦ä¸²æ ¼å¼")

            return df_copy

        except Exception as e:
            self.logger.error(f"æ ‡å‡†åŒ– {source_name} station_idæ ¼å¼å¤±è´¥: {str(e)}")
            return df

    def _add_coordinate_info(self, df, coordinate_dfs):
        """æ·»åŠ åæ ‡ä¿¡æ¯åˆ°æ•°æ®æ¡†"""
        try:
            if not coordinate_dfs:
                return df

            # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ…å«åæ ‡ä¿¡æ¯çš„æ•°æ®æº
            coord_source_name, coord_df = coordinate_dfs[0]
            self.logger.info(f"ä» {coord_source_name} æå–åæ ‡ä¿¡æ¯")

            coord_columns = self._get_coordinate_columns(coord_df)
            if coord_columns:
                # æ ‡å‡†åŒ–åæ ‡åˆ—å
                coord_mapping = {}
                for col in coord_columns:
                    if col.lower() in ['longitude', 'lon']:
                        coord_mapping[col] = 'longitude'
                    elif col.lower() in ['latitude', 'lat']:
                        coord_mapping[col] = 'latitude'

                coord_df_standardized = coord_df.rename(columns=coord_mapping)
                coord_mapping_df = coord_df_standardized[['station_id', 'longitude', 'latitude']].drop_duplicates(
                    'station_id')

                # åˆå¹¶åæ ‡ä¿¡æ¯
                before_coord_count = df[['longitude', 'latitude']].notna().all(
                    axis=1).sum() if 'longitude' in df.columns and 'latitude' in df.columns else 0
                df = df.merge(coord_mapping_df, on='station_id', how='left', suffixes=('', '_dup'))

                # å¤„ç†é‡å¤åˆ—
                for col in ['longitude', 'latitude']:
                    if f'{col}_dup' in df.columns:
                        # å¦‚æœåŸåˆ—ä¸ºç©ºï¼Œä½¿ç”¨æ–°åˆ—çš„å€¼å¡«å……
                        mask = df[col].isna() & df[f'{col}_dup'].notna()
                        df.loc[mask, col] = df.loc[mask, f'{col}_dup']
                        df = df.drop(f'{col}_dup', axis=1)

                after_coord_count = df[['longitude', 'latitude']].notna().all(axis=1).sum()
                self.logger.info(f"åæ ‡ä¿¡æ¯æ·»åŠ : {before_coord_count} -> {after_coord_count} å®Œæ•´åæ ‡è®°å½•")

            return df

        except Exception as e:
            self.logger.error(f"æ·»åŠ åæ ‡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return df

    def emergency_fix(self, issue_type='all'):
        """åº”æ€¥æ•°æ®ä¿®å¤ - å¢å¼ºç‰ˆæœ¬ï¼Œä¸“é—¨å¤„ç†æ•°æ®ç±»å‹é—®é¢˜"""
        try:
            fix_count = 0

            if not self.source_data:
                self.logger.warning("æ²¡æœ‰æ•°æ®æºå¯ä¿®å¤")
                return fix_count

            # å¼ºåˆ¶æ ‡å‡†åŒ–æ‰€æœ‰æ•°æ®æºçš„station_idæ ¼å¼
            self.logger.info("=== å¼ºåˆ¶æ ‡å‡†åŒ–æ‰€æœ‰station_idæ ¼å¼ ===")
            for name, df in self.source_data.items():
                if 'station_id' in df.columns:
                    original_type = str(df['station_id'].dtype)
                    fixed_df = self._standardize_station_id_format(df, f"ä¿®å¤-{name}")
                    new_type = str(fixed_df['station_id'].dtype)

                    if original_type != new_type:
                        self.source_data[name] = fixed_df
                        fix_count += 1
                        self.logger.info(f"âœ… ä¿®å¤ {name}: station_id {original_type} -> {new_type}")

            # å¤„ç†é‡å¤æ•°æ®
            if issue_type in ['all', 'duplicates']:
                fix_count += self._fix_duplicates()

            # å¤„ç†åˆ—åé—®é¢˜
            if issue_type in ['all', 'column_names']:
                fix_count += self._fix_column_names()

            self.logger.info(f"ğŸ¯ åº”æ€¥ä¿®å¤å®Œæˆ: æ€»å…±æ‰§è¡Œäº† {fix_count} ä¸ªä¿®å¤")
            return fix_count

        except Exception as e:
            self.logger.error(f"åº”æ€¥ä¿®å¤å¤±è´¥: {e}")
            return 0

    def _fix_duplicates(self):
        """ä¿®å¤é‡å¤æ•°æ®"""
        fix_count = 0
        for name, df in self.source_data.items():
            original_count = len(df)
            df_clean = df.drop_duplicates()
            removed_count = original_count - len(df_clean)
            if removed_count > 0:
                self.source_data[name] = df_clean
                fix_count += removed_count
                self.logger.info(f"âœ… å»é‡ {name}: ç§»é™¤ {removed_count} ä¸ªé‡å¤è¡Œ")
        return fix_count

    def _fix_column_names(self):
        """ä¿®å¤åˆ—åé—®é¢˜"""
        fix_count = 0
        for name, df in self.source_data.items():
            column_mapping = {}
            if 'station_ID' in df.columns and 'station_id' not in df.columns:
                column_mapping['station_ID'] = 'station_id'
            if 'Station_ID' in df.columns and 'station_id' not in df.columns:
                column_mapping['Station_ID'] = 'station_id'

            if column_mapping:
                df_renamed = df.rename(columns=column_mapping)
                self.source_data[name] = df_renamed
                fix_count += len(column_mapping)
                self.logger.info(f"âœ… åˆ—åä¿®å¤ {name}: {list(column_mapping.keys())}")
        return fix_count

    def _ensure_complete_coordinates(self, df):
        """ç¡®ä¿åæ ‡ä¿¡æ¯å®Œæ•´"""
        try:
            # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘åæ ‡ä¿¡æ¯
            if 'longitude' not in df.columns or 'latitude' not in df.columns:
                self.logger.info("å°è¯•ä»æ‰€æœ‰æ•°æ®æºè¡¥å……åæ ‡ä¿¡æ¯")
                df = self._supplement_coordinates_from_all_sources(df)

            # ç»Ÿè®¡åæ ‡å®Œæ•´æ€§
            if 'longitude' in df.columns and 'latitude' in df.columns:
                complete_coords = df[['longitude', 'latitude']].notna().all(axis=1).sum()
                total_records = len(df)
                completeness = (complete_coords / total_records) * 100
                self.logger.info(f"åæ ‡å®Œæ•´æ€§: {complete_coords}/{total_records} ({completeness:.1f}%)")

            return df

        except Exception as e:
            self.logger.error(f"ç¡®ä¿åæ ‡å®Œæ•´å¤±è´¥: {str(e)}")
            return df

    def _supplement_coordinates_from_all_sources(self, df):
        """ä»æ‰€æœ‰æ•°æ®æºè¡¥å……åæ ‡ä¿¡æ¯"""
        try:
            all_coordinates = {}

            for source_name, source_df in self.source_data.items():
                if self._has_coordinate_info(source_df):
                    coord_columns = self._get_coordinate_columns(source_df)
                    if len(coord_columns) >= 2:
                        coord_info = source_df[['station_id'] + coord_columns].drop_duplicates('station_id')

                        for _, row in coord_info.iterrows():
                            station_id = row['station_id']
                            if station_id not in all_coordinates:
                                # ç¡®å®šç»åº¦å’Œçº¬åº¦åˆ—
                                lon_col = next((col for col in coord_columns if col.lower() in ['longitude', 'lon']),
                                               None)
                                lat_col = next((col for col in coord_columns if col.lower() in ['latitude', 'lat']),
                                               None)

                                if lon_col and lat_col and pd.notna(row[lon_col]) and pd.notna(row[lat_col]):
                                    all_coordinates[station_id] = {
                                        'longitude': float(row[lon_col]),
                                        'latitude': float(row[lat_col])
                                    }

            # å°†åæ ‡ä¿¡æ¯æ·»åŠ åˆ°æ•°æ®æ¡†
            if all_coordinates:
                coord_df = pd.DataFrame([
                    {'station_id': sid, 'longitude': info['longitude'], 'latitude': info['latitude']}
                    for sid, info in all_coordinates.items()
                ])

                if 'longitude' in df.columns and 'latitude' in df.columns:
                    # æ›´æ–°ç°æœ‰çš„ç©ºå€¼åæ ‡
                    for station_id, coords in all_coordinates.items():
                        mask = (df['station_id'] == station_id) & (df['longitude'].isna() | df['latitude'].isna())
                        df.loc[mask, 'longitude'] = coords['longitude']
                        df.loc[mask, 'latitude'] = coords['latitude']
                else:
                    # æ·»åŠ æ–°çš„åæ ‡åˆ—
                    df = df.merge(coord_df, on='station_id', how='left')

                self.logger.info(f"ä» {len(all_coordinates)} ä¸ªæ•°æ®æºè¡¥å……äº†åæ ‡ä¿¡æ¯")

            return df

        except Exception as e:
            self.logger.error(f"è¡¥å……åæ ‡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return df

    def _get_hydrological_year_from_str(self, date_str):
        """ä»æ—¥æœŸå­—ç¬¦ä¸²è®¡ç®—æ°´æ–‡å¹´"""
        try:
            date = pd.to_datetime(date_str)
            if date.month >= 9 or (date.month == 9 and date.day >= 1):
                return date.year
            else:
                return date.year - 1
        except:
            return None

    def _is_gldas_data(self, source_name, df):
        """åˆ¤æ–­æ˜¯å¦ä¸ºGLDASæ•°æ®"""
        gldas_keywords = ['gldas', 'seasonal_doy']
        if any(keyword in source_name.lower() for keyword in gldas_keywords):
            return True

        gldas_columns = ['seasonal_doy_Da', 'seasonal_doy_Db', 'seasonal_doy_Dc', 'seasonal_doy_Dd']
        return any(col in df.columns for col in gldas_columns)

    def _is_snow_phenology_data(self, source_name, df):
        """åˆ¤æ–­æ˜¯å¦ä¸ºç§¯é›ªç‰©å€™æ•°æ®"""
        snow_keywords = ['snow_phenology', 'snow_start', 'snow_end', 'phenology', 'scp']
        if any(keyword in source_name.lower() for keyword in snow_keywords):
            return True

        return 'dataset_type' in df.columns and 'hydrological_year' in df.columns

    def _is_terrain_features(self, source_name, df):
        """åˆ¤æ–­æ˜¯å¦ä¸ºåœ°å½¢ç‰¹å¾æ•°æ®"""
        if 'terrain' in source_name.lower() or 'feature' in source_name.lower():
            return True

        numeric_cols = df.select_dtypes(include=['number']).columns
        feature_cols = [col for col in numeric_cols if col not in ['station_id', 'date', 'longitude', 'latitude']]

        return len(feature_cols) > 1 and 'date' not in df.columns

    def _is_yearly_data(self, source_name, df):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¹´åº¦æ•°æ®"""
        if 'landcover' in source_name.lower():
            return True

        if 'station_id' in df.columns:
            station_counts = df.groupby('station_id').size()
            return station_counts.mean() < 5

        return False

    def _is_static_data_source(self, source_name, source_df):
        """åˆ¤æ–­æ˜¯å¦ä¸ºé™æ€æ•°æ®æº"""
        # é€šè¿‡åç§°åˆ¤æ–­
        static_keywords = ['landuse', 'terrain', 'static', 'elevation', 'altitude']
        if any(keyword in source_name.lower() for keyword in static_keywords):
            return True

        # é€šè¿‡æ•°æ®ç‰¹å¾åˆ¤æ–­
        if 'date' not in source_df.columns:
            return True

        # å¦‚æœåªæœ‰å¾ˆå°‘çš„æ—¶é—´ç‚¹ï¼Œä¹Ÿå¯èƒ½æ˜¯é™æ€æ•°æ®
        if 'date' in source_df.columns:
            unique_dates = source_df['date'].nunique()
            if unique_dates <= 1:
                return True

        return False

    def _merge_static_data_properly(self, base_df, static_df, source_name):
        """æ­£ç¡®åˆå¹¶é™æ€æ•°æ®ï¼ˆç¡®ä¿æ‰€æœ‰è®°å½•éƒ½æœ‰å€¼ï¼‰"""
        try:
            # ç¡®ä¿é™æ€æ•°æ®æœ‰station_idåˆ—
            if 'station_id' not in static_df.columns:
                self.logger.error(f"é™æ€æ•°æ® {source_name} ç¼ºå°‘station_idåˆ—")
                return base_df

            # è·å–é™æ€æ•°æ®çš„ç‰¹å¾åˆ—
            exclude_cols = ['station_id', 'date', 'longitude', 'latitude', 'processing_time']
            feature_cols = [col for col in static_df.columns if col not in exclude_cols]

            if not feature_cols:
                self.logger.warning(f"é™æ€æ•°æ® {source_name} æ²¡æœ‰ç‰¹å¾åˆ—")
                return base_df

            # å‡†å¤‡é™æ€æ•°æ®ï¼ˆå»é‡ï¼Œæ¯ä¸ªç«™ç‚¹ä¸€æ¡è®°å½•ï¼‰
            static_clean = static_df[['station_id'] + feature_cols].drop_duplicates('station_id')

            self.logger.info(f"é™æ€æ•°æ® {source_name}: {len(static_clean)} ä¸ªç«™ç‚¹çš„ {len(feature_cols)} ä¸ªç‰¹å¾")

            # åˆå¹¶åˆ°åŸºç¡€æ•°æ®
            before_merge = base_df.shape
            merged_df = base_df.merge(static_clean, on='station_id', how='left')
            after_merge = merged_df.shape

            # ç»Ÿè®¡å¡«å……æƒ…å†µ
            for feature in feature_cols:
                filled_count = merged_df[feature].notna().sum()
                fill_rate = (filled_count / len(merged_df)) * 100
                self.logger.info(f"  {feature}: {filled_count}/{len(merged_df)} è®°å½•æœ‰å€¼ ({fill_rate:.1f}%)")

            self.logger.info(f"é™æ€æ•°æ® {source_name} åˆå¹¶: {before_merge} -> {after_merge}")

            return merged_df

        except Exception as e:
            self.logger.error(f"åˆå¹¶é™æ€æ•°æ® {source_name} å¤±è´¥: {str(e)}")
            return base_df

    def add_hydrological_year(self, df):
        """æ·»åŠ æ°´æ–‡å¹´åˆ—

        æ°´æ–‡å¹´å®šä¹‰ï¼š9æœˆ1æ—¥å¼€å§‹åˆ°æ¬¡å¹´8æœˆ31æ—¥
        ä¾‹å¦‚ï¼š2023å¹´9æœˆ1æ—¥åˆ°2024å¹´8æœˆ31æ—¥å±äº2024æ°´æ–‡å¹´

        Args:
            df: åŒ…å«dateåˆ—çš„æ•°æ®æ¡†

        Returns:
            DataFrame: æ·»åŠ äº†hydrological_yearåˆ—çš„æ•°æ®æ¡†
        """
        try:
            if 'date' not in df.columns:
                self.logger.warning("æ•°æ®æ¡†ä¸­æ²¡æœ‰dateåˆ—ï¼Œæ— æ³•è®¡ç®—æ°´æ–‡å¹´")
                return df

            df_processed = df.copy()
            df_processed['date'] = pd.to_datetime(df_processed['date'])

            # è®¡ç®—æ°´æ–‡å¹´
            def calculate_hydrological_year(date):
                """è®¡ç®—æ°´æ–‡å¹´"""
                try:
                    if pd.isna(date):
                        return np.nan

                    year = date.year
                    month = date.month

                    # 9æœˆåˆ°12æœˆï¼šå±äºä¸‹ä¸€å¹´åº¦çš„æ°´æ–‡å¹´
                    if month >= 9:
                        return year + 1
                    # 1æœˆåˆ°8æœˆï¼šå±äºå½“å‰å¹´åº¦çš„æ°´æ–‡å¹´
                    else:
                        return year

                except Exception as e:
                    self.logger.debug(f"è®¡ç®—æ°´æ–‡å¹´å¤±è´¥: {e}")
                    return np.nan

            # åº”ç”¨è®¡ç®—
            df_processed['hydrological_year'] = df_processed['date'].apply(calculate_hydrological_year)

            # ç»Ÿè®¡æ°´æ–‡å¹´åˆ†å¸ƒ
            if 'hydrological_year' in df_processed.columns:
                hydrological_year_stats = df_processed['hydrological_year'].value_counts().sort_index()
                self.logger.info("æ°´æ–‡å¹´åˆ†å¸ƒ:")
                for hy_year, count in hydrological_year_stats.items():
                    if pd.notna(hy_year):
                        self.logger.info(f"  {int(hy_year)}æ°´æ–‡å¹´: {count} æ¡è®°å½•")

            return df_processed

        except Exception as e:
            self.logger.error(f"æ·»åŠ æ°´æ–‡å¹´å¤±è´¥: {str(e)}")
            return df

    def add_hydrological_doy(self, df):
        """æ·»åŠ æ°´æ–‡å¹´DOYåˆ—

        æ°´æ–‡å¹´ä»9æœˆ1æ—¥å¼€å§‹ï¼š
        - 9æœˆ1æ—¥åˆ°12æœˆ31æ—¥ï¼šæ°´æ–‡å¹´DOY = è‡ªç„¶å¹´DOY - 243 + 1
        - 1æœˆ1æ—¥åˆ°8æœˆ31æ—¥ï¼šæ°´æ–‡å¹´DOY = è‡ªç„¶å¹´DOY + 122

        Args:
            df: åŒ…å«dateå’Œdoyåˆ—çš„æ•°æ®æ¡†

        Returns:
            DataFrame: æ·»åŠ äº†hydrological_doyåˆ—çš„æ•°æ®æ¡†
        """
        try:
            if 'date' not in df.columns:
                self.logger.warning("æ•°æ®æ¡†ä¸­æ²¡æœ‰dateåˆ—ï¼Œæ— æ³•è®¡ç®—æ°´æ–‡å¹´DOY")
                return df

            if 'doy' not in df.columns:
                self.logger.warning("æ•°æ®æ¡†ä¸­æ²¡æœ‰doyåˆ—ï¼Œæ— æ³•è®¡ç®—æ°´æ–‡å¹´DOY")
                return df

            df_processed = df.copy()

            # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
            df_processed['date'] = pd.to_datetime(df_processed['date'])

            # æå–æœˆä»½
            df_processed['month'] = df_processed['date'].dt.month

            # è®¡ç®—æ°´æ–‡å¹´DOY
            def calculate_hydrological_doy(row):
                """è®¡ç®—æ°´æ–‡å¹´DOY"""
                try:
                    doy = row['doy']
                    month = row['month']

                    if pd.isna(doy) or pd.isna(month):
                        return np.nan

                    # 9æœˆåˆ°12æœˆï¼ˆæ°´æ–‡å¹´ä¸è‡ªç„¶å¹´ç›¸åŒï¼Œä½†é‡æ–°ç¼–å·ä»1å¼€å§‹ï¼‰
                    if month >= 9:
                        # 9æœˆ1æ—¥æ˜¯è‡ªç„¶å¹´DOY 244ï¼Œå¯¹åº”æ°´æ–‡å¹´DOY 1
                        hydrological_doy = doy - 243
                    else:
                        # 1æœˆåˆ°8æœˆï¼šå±äºä¸Šä¸€ä¸ªæ°´æ–‡å¹´çš„ååŠéƒ¨åˆ†
                        # 1æœˆ1æ—¥æ˜¯è‡ªç„¶å¹´DOY 1ï¼Œå¯¹åº”æ°´æ–‡å¹´DOY 122
                        hydrological_doy = doy + 122

                    # ç¡®ä¿DOYåœ¨æœ‰æ•ˆèŒƒå›´å†…ï¼ˆ1-366ï¼‰
                    if hydrological_doy < 1:
                        return 1
                    elif hydrological_doy > 366:
                        return 366
                    else:
                        return int(hydrological_doy)

                except Exception as e:
                    self.logger.debug(f"è®¡ç®—æ°´æ–‡å¹´DOYå¤±è´¥: {e}")
                    return np.nan

            # åº”ç”¨è®¡ç®—
            df_processed['hydrological_doy'] = df_processed.apply(calculate_hydrological_doy, axis=1)

            # ç»Ÿè®¡è®¡ç®—æƒ…å†µ
            valid_hydrological_doy = df_processed['hydrological_doy'].notna().sum()
            total_records = len(df_processed)

            self.logger.info(f"âœ… æ°´æ–‡å¹´DOYè®¡ç®—å®Œæˆ: {valid_hydrological_doy}/{total_records} æœ‰æ•ˆè®°å½•")

            # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
            sample_records = df_processed[['date', 'doy', 'hydrological_doy']].head(5)
            self.logger.info("æ°´æ–‡å¹´DOYè®¡ç®—ç¤ºä¾‹:")
            for _, row in sample_records.iterrows():
                if pd.notna(row['hydrological_doy']):
                    self.logger.info(f"  æ—¥æœŸ: {row['date'].strftime('%Y-%m-%d')}, "
                                     f"è‡ªç„¶å¹´DOY: {row['doy']}, "
                                     f"æ°´æ–‡å¹´DOY: {row['hydrological_doy']}")

            return df_processed

        except Exception as e:
            self.logger.error(f"æ·»åŠ æ°´æ–‡å¹´DOYå¤±è´¥: {str(e)}")
            return df

    def validate_hydrological_calculation(self, df):
        """éªŒè¯æ°´æ–‡å¹´å’Œæ°´æ–‡å¹´DOYè®¡ç®—æ˜¯å¦æ­£ç¡®

        Args:
            df: åŒ…å«æ°´æ–‡å¹´ç›¸å…³åˆ—çš„æ•°æ®æ¡†
        """
        try:
            if 'date' not in df.columns or 'doy' not in df.columns:
                return

            # æµ‹è¯•å‡ ä¸ªå…³é”®æ—¥æœŸ
            test_dates = [
                ('2013-09-01', 244, 1, 2014),  # æ°´æ–‡å¹´å¼€å§‹
                ('2013-12-31', 365, 122, 2014),  # æ°´æ–‡å¹´ä¸ŠåŠå¹´ç»“æŸ
                ('2014-01-01', 1, 123, 2014),  # æ°´æ–‡å¹´ä¸‹åŠå¹´å¼€å§‹
                ('2014-08-31', 243, 366, 2014),  # æ°´æ–‡å¹´ç»“æŸ
            ]

            self.logger.info("æ°´æ–‡å¹´è®¡ç®—éªŒè¯:")
            for test_date, expected_doy, expected_hydro_doy, expected_hydro_year in test_dates:
                test_row = df[df['date'] == test_date]
                if not test_row.empty:
                    actual_hydro_doy = test_row['hydrological_doy'].iloc[
                        0] if 'hydrological_doy' in test_row.columns else 'N/A'
                    actual_hydro_year = test_row['hydrological_year'].iloc[
                        0] if 'hydrological_year' in test_row.columns else 'N/A'

                    hydro_doy_status = "âœ…" if actual_hydro_doy == expected_hydro_doy else "âŒ"
                    hydro_year_status = "âœ…" if actual_hydro_year == expected_hydro_year else "âŒ"

                    self.logger.info(f"  {hydro_doy_status}{hydro_year_status} {test_date}: "
                                     f"DOYæœŸæœ›{expected_hydro_doy}å®é™…{actual_hydro_doy}, "
                                     f"æ°´æ–‡å¹´æœŸæœ›{expected_hydro_year}å®é™…{actual_hydro_year}")
                else:
                    self.logger.warning(f"  æµ‹è¯•æ—¥æœŸ {test_date} åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨")

        except Exception as e:
            self.logger.warning(f"éªŒè¯æ°´æ–‡å¹´è®¡ç®—å¤±è´¥: {e}")

    def _handle_static_only_case(self, static_dfs, yearly_dfs, phenology_dfs, gldas_dfs):
        """å¤„ç†åªæœ‰é™æ€æ•°æ®çš„æƒ…å†µ"""
        try:
            self.logger.info("å¤„ç†çº¯é™æ€æ•°æ®æƒ…å†µ")

            all_dfs = []

            for name, df in static_dfs + yearly_dfs + phenology_dfs + gldas_dfs:
                df_copy = df.copy()
                if 'date' not in df_copy.columns:
                    df_copy['date'] = datetime.now().strftime('%Y-%m-%d')
                all_dfs.append(df_copy)

            if not all_dfs:
                return pd.DataFrame()

            final_df = all_dfs[0]
            for i in range(1, len(all_dfs)):
                common_keys = ['station_id', 'date']
                available_keys = [key for key in common_keys if key in final_df.columns and key in all_dfs[i].columns]

                if available_keys:
                    final_df = final_df.merge(all_dfs[i], on=available_keys, how='outer')

            self.logger.info(f"çº¯é™æ€æ•°æ®åˆå¹¶å®Œæˆ: {final_df.shape}")
            return final_df

        except Exception as e:
            self.logger.error(f"å¤„ç†çº¯é™æ€æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def _validate_final_wide_table(self, df):
        """éªŒè¯æœ€ç»ˆå®½è¡¨æ•°æ® - ä¿®å¤æ—¥æœŸéªŒè¯"""
        self.logger.info("=== æœ€ç»ˆå®½è¡¨éªŒè¯ ===")
        self.logger.info(f"æ€»è¡Œæ•°: {len(df)}")
        self.logger.info(f"æ€»åˆ—æ•°: {len(df.columns)}")

        # æ£€æŸ¥æ—¥æœŸåˆ—
        if 'date' in df.columns:
            # æ£€æŸ¥æ—¥æœŸèŒƒå›´
            date_range = f"{df['date'].min()} åˆ° {df['date'].max()}"
            unique_dates = df['date'].nunique()
            self.logger.info(f"æ—¥æœŸèŒƒå›´: {date_range}")
            self.logger.info(f"å”¯ä¸€æ—¥æœŸæ•°: {unique_dates}")

            # æ£€æŸ¥æ˜¯å¦æœ‰ä»Šå¤©çš„æ—¥æœŸï¼ˆä¸åº”è¯¥æœ‰ï¼‰
            today = datetime.now().strftime('%Y-%m-%d')
            today_records = df[df['date'] == today]
            if len(today_records) > 0:
                self.logger.error(f"âŒ é”™è¯¯: å‘ç° {len(today_records)} æ¡è®°å½•çš„æ—¥æœŸæ˜¯ä»Šå¤©({today})")
                self.logger.error("è¿™è¡¨ç¤ºé™æ€æ•°æ®çš„æ—¶é—´åˆ—æ²¡æœ‰è¢«æ­£ç¡®æ¸…ç†")
            else:
                self.logger.info("âœ… æ—¥æœŸéªŒè¯é€šè¿‡: æ²¡æœ‰ä»Šå¤©çš„æ—¥æœŸ")

        # æ£€æŸ¥é™æ€æ•°æ®åˆ—
        static_keywords = ['landuse', 'terrain', 'elevation', 'altitude']
        static_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in static_keywords)]

        if static_cols:
            self.logger.info("é™æ€æ•°æ®åˆ—éªŒè¯:")
            for col in static_cols:
                filled_count = df[col].notna().sum()
                fill_rate = (filled_count / len(df)) * 100
                unique_values = df[col].nunique()
                self.logger.info(f"  {col}: {filled_count}/{len(df)} æœ‰å€¼ ({fill_rate:.1f}%), {unique_values} ä¸ªå”¯ä¸€å€¼")

        # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®ç”¨äºéªŒè¯
        if len(df) > 0:
            self.logger.info("å‰3è¡Œæ•°æ®éªŒè¯:")
            sample_cols = ['station_id', 'date']
            # æ·»åŠ ä¸€äº›é™æ€ç‰¹å¾åˆ—
            for col in static_cols[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé™æ€åˆ—
                if col in df.columns:
                    sample_cols.append(col)

            sample_df = df[sample_cols].head(3)
            for _, row in sample_df.iterrows():
                values = []
                for col in sample_cols:
                    if col not in ['station_id', 'date'] and pd.notna(row[col]):
                        values.append(f"{col}: {row[col]}")
                value_str = ", ".join(values) if values else "æ— é™æ€æ•°æ®"
                self.logger.info(f"  ç«™ç‚¹ {row['station_id']} | æ—¥æœŸ {row['date']} | {value_str}")

    def save_master_excel(self, format_type='wide'):
        """ä¿å­˜ä¸»Excelæ–‡ä»¶"""
        if not self.source_data:
            self.logger.error("æ²¡æœ‰æ•°æ®æºå¯æ•´åˆ")
            return None

        try:
            # å…ˆæ‰§è¡Œæ•°æ®ä¿®å¤
            fix_count = self.emergency_fix()
            self.logger.info(f"æ•°æ®ä¿®å¤å®Œæˆ: {fix_count} ä¸ªé—®é¢˜å·²ä¿®å¤")

            # åˆ›å»ºå®½è¡¨
            if format_type == 'wide':
                final_df = self._create_correct_wide_table()
            else:
                final_df = self.get_combined_data()

            if final_df is None or final_df.empty:
                self.logger.error("æœ€ç»ˆæ•°æ®ä¸ºç©ºæˆ–ä¸ºNone")
                return None



            # ç¡®ä¿åŒ…å«æ°´æ–‡å¹´ä¿¡æ¯ï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
            if 'date' in final_df.columns and 'doy' in final_df.columns:
                if 'hydrological_doy' not in final_df.columns:
                    self.logger.info("æ·»åŠ æ°´æ–‡å¹´DOYè®¡ç®—...")
                    final_df = self.add_hydrological_doy(final_df)
                if 'hydrological_year' not in final_df.columns:
                    self.logger.info("æ·»åŠ æ°´æ–‡å¹´è®¡ç®—...")
                    final_df = self.add_hydrological_year(final_df)

            # æ•°æ®å®Œæ•´æ€§éªŒè¯ï¼ˆåŒ…å«å¹´ä»½æœˆä»½éªŒè¯ï¼‰
            final_df = self._validate_data_integrity(final_df)

            # é¢å¤–çš„æ—¶é—´åˆ†å¸ƒåˆ†æ
            if 'date' in final_df.columns:
                time_analysis = self.analyze_time_distribution(final_df)
                if time_analysis:
                    self.logger.info("âœ… æ—¶é—´åˆ†å¸ƒåˆ†æå®Œæˆ")

            if final_df.empty:
                self.logger.error("éªŒè¯åæ•°æ®ä¸ºç©º")
                return None

            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = self.output_dir / f"integrated_data_{timestamp}.xlsx"

            # ä¿å­˜Excelæ–‡ä»¶
            final_df.to_excel(output_path, index=False)
            self.logger.info(f"âœ… æ•´åˆæ•°æ®ä¿å­˜æˆåŠŸ: {output_path}")

            return str(output_path)

        except Exception as e:
            self.logger.error(f"ä¿å­˜å¤±è´¥: {e}")
            self.logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None

    def extract_time_info(self, output_dir=None):
        """æå–æ¯æ¡è®°å½•çš„å¹´æœˆä¿¡æ¯å¹¶ç»Ÿè®¡

        Args:
            output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„

        Returns:
            dict: åŒ…å«æ—¶é—´ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        try:
            if not self.db_conn:
                if not self.connect_database():
                    self.logger.error("æ— æ³•è¿æ¥æ•°æ®åº“")
                    return None

            # æŸ¥è¯¢æ‰€æœ‰è®°å½•çš„æ—¶é—´ä¿¡æ¯
            query = """
            SELECT station_ID, time, sweï¼ˆmmï¼‰
            FROM stations
            ORDER BY station_ID, time
            """
            df = pd.read_sql_query(query, self.db_conn)

            if df.empty:
                self.logger.warning("æ•°æ®åº“ä¸­æœªæ‰¾åˆ°è®°å½•")
                return None

            # è½¬æ¢æ—¶é—´æ ¼å¼å¹¶æå–å¹´æœˆ
            df_processed = df.copy()

            # å°†æ—¶é—´åˆ—è½¬æ¢ä¸ºdatetimeæ ¼å¼
            df_processed['time'] = pd.to_datetime(df_processed['time'], format='%Y%m%d', errors='coerce')

            # æå–å¹´æœˆä¿¡æ¯
            df_processed['year'] = df_processed['time'].dt.year
            df_processed['month'] = df_processed['time'].dt.month
            df_processed['year_month'] = df_processed['time'].dt.to_period('M')

            # ç»Ÿè®¡ä¿¡æ¯
            total_records = len(df_processed)
            valid_time_records = df_processed['time'].notna().sum()

            # å¹´ä»½ç»Ÿè®¡
            year_stats = df_processed['year'].value_counts().sort_index()

            # æœˆä»½ç»Ÿè®¡
            month_stats = df_processed['month'].value_counts().sort_index()

            # å¹´æœˆç»„åˆç»Ÿè®¡
            year_month_stats = df_processed['year_month'].value_counts().sort_index()

            # å„ç«™ç‚¹çš„æ—¶é—´è¦†ç›–ç»Ÿè®¡
            station_time_stats = df_processed.groupby('station_ID').agg({
                'time': ['min', 'max', 'count'],
                'year': ['min', 'max', 'nunique'],
                'month': 'nunique'
            }).round(2)

            # é‡å‘½ååˆ—
            station_time_stats.columns = [
                'first_record', 'last_record', 'total_records',
                'min_year', 'max_year', 'unique_years', 'unique_months'
            ]

            # SWEæ•°æ®çš„æ—¶é—´åˆ†å¸ƒ
            swe_time_stats = df_processed[df_processed['sweï¼ˆmmï¼‰'].notna()].groupby('year_month').agg({
                'station_ID': 'nunique',
                'sweï¼ˆmmï¼‰': 'count'
            }).rename(columns={'station_ID': 'stations_with_swe', 'sweï¼ˆmmï¼‰': 'swe_records'})

            # æ„å»ºç»Ÿè®¡ç»“æœ
            time_statistics = {
                'total_records': total_records,
                'valid_time_records': valid_time_records,
                'time_validity_rate': (valid_time_records / total_records) * 100,
                'year_distribution': year_stats.to_dict(),
                'month_distribution': month_stats.to_dict(),
                'year_month_distribution': year_month_stats.to_dict(),
                'station_time_coverage': station_time_stats.to_dict('index'),
                'swe_time_distribution': swe_time_stats.to_dict('index'),
                'time_range': {
                    'overall_start': df_processed['time'].min(),
                    'overall_end': df_processed['time'].max(),
                    'overall_years': f"{df_processed['year'].min()} - {df_processed['year'].max()}",
                    'total_months': df_processed['year_month'].nunique()
                },
                'data_samples': df_processed.head(1000).to_dict('records')  # ä¿å­˜å‰1000æ¡è®°å½•ä½œä¸ºæ ·æœ¬
            }

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“… æ•°æ®åº“æ—¶é—´ä¿¡æ¯ç»Ÿè®¡")
            self.logger.info("=" * 60)
            self.logger.info(f"æ€»è®°å½•æ•°: {total_records}")
            self.logger.info(f"æœ‰æ•ˆæ—¶é—´è®°å½•: {valid_time_records} ({time_statistics['time_validity_rate']:.1f}%)")
            self.logger.info(
                f"æ—¶é—´èŒƒå›´: {time_statistics['time_range']['overall_start']} åˆ° {time_statistics['time_range']['overall_end']}")
            self.logger.info(f"è¦†ç›–å¹´ä»½: {time_statistics['time_range']['overall_years']}")
            self.logger.info(f"æ€»æœˆä»½æ•°: {time_statistics['time_range']['total_months']}")

            self.logger.info(f"\nå¹´ä»½åˆ†å¸ƒ:")
            for year, count in year_stats.items():
                self.logger.info(f"  {year}: {count} æ¡è®°å½•")

            self.logger.info(f"\næœˆä»½åˆ†å¸ƒ:")
            month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                           'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
            for month, count in month_stats.items():
                month_name = month_names[month - 1] if 1 <= month <= 12 else f"Month{month}"
                self.logger.info(f"  {month_name}: {count} æ¡è®°å½•")

            # å¯¼å‡ºåˆ°æ–‡ä»¶
            if output_dir:
                self._export_time_statistics(time_statistics, output_dir)

            return time_statistics

        except Exception as e:
            self.logger.error(f"æå–æ—¶é—´ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None

    def _build_master_from_union(self, dynamic_dfs, swe_df=None, static_combined=None):
        """
        ä½¿ç”¨æ‰€æœ‰åŠ¨æ€æ•°æ®æºä¸æ•°æ®åº“SWEçš„ (station_id, date) å¹¶é›†æ„å»ºä¸»æ¡†æ¶ï¼Œ
        ç„¶åæŠŠ static_combinedï¼ˆé™æ€ç‰¹å¾ï¼‰å’Œå„åŠ¨æ€æº left merge ä¸Šå»ã€‚
        - dynamic_dfs: list of (source_name, df) where df has station_id & date
        - swe_df: DataFrame with station_id & date (optional)
        - static_combined: DataFrame with station_id and static features (optional)
        Returns: final_wide DataFrame whose rows == union of all station_id/date pairs
        """
        try:
            # 1. æ”¶é›†æ‰€æœ‰ (station_id, date) å¯¹
            pairs = []
            for name, df in dynamic_dfs:
                if 'station_id' in df.columns and 'date' in df.columns:
                    tmp = df[['station_id', 'date']].drop_duplicates()
                    pairs.append(tmp)

            if swe_df is not None and not swe_df.empty and 'station_id' in swe_df.columns and 'date' in swe_df.columns:
                pairs.append(swe_df[['station_id', 'date']].drop_duplicates())

            if not pairs:
                # æ²¡æœ‰åŠ¨æ€æˆ–sweæ•°æ®ï¼Œè¿”å›ç©º
                return pd.DataFrame()

            # åˆå¹¶æ‰€æœ‰ pairs çš„å¹¶é›†
            master_index_df = pd.concat(pairs, ignore_index=True).drop_duplicates().reset_index(drop=True)

            # è§„èŒƒåŒ–ç±»å‹ä¸æ’åºï¼ˆå¯é€‰ï¼‰
            master_index_df['station_id'] = master_index_df['station_id'].astype(str)
            master_index_df['date'] = pd.to_datetime(master_index_df['date']).dt.strftime('%Y-%m-%d')

            # 2. ä»¥ master_index_df ä¸ºåŸºç¡€ï¼Œå…ˆåˆå¹¶é™æ€ç‰¹å¾ï¼ˆæŒ‰ station_idï¼‰
            master = master_index_df.copy()
            if static_combined is not None and not static_combined.empty:
                # ç¡®ä¿é™æ€è¡¨ station_id ä¸ºå­—ç¬¦ä¸²
                static_combined = static_combined.copy()
                if 'station_id' in static_combined.columns:
                    static_combined['station_id'] = static_combined['station_id'].astype(str)
                    # å»é™¤é‡å¤ station_id ä¿è¯ä¸€æ¡é™æ€è®°å½•
                    static_one = static_combined.drop_duplicates(subset=['station_id'])
                    master = master.merge(static_one, on='station_id', how='left')
                else:
                    self.logger.warning("_build_master_from_union: static_combined ç¼ºå°‘ station_id åˆ—ï¼Œè·³è¿‡é™æ€åˆå¹¶")

            # 3. æŠŠæ¯ä¸ªåŠ¨æ€æºçš„ç‰¹å¾æŒ‰ (station_id, date) å·¦è¿æ¥åˆ° master
            for name, df in dynamic_dfs:
                df_copy = df.copy()
                df_copy['station_id'] = df_copy['station_id'].astype(str)
                if 'date' in df_copy.columns:
                    # ç»Ÿä¸€æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DD å­—ç¬¦ä¸²ï¼Œé˜²æ­¢åŒ¹é…å¤±è´¥
                    df_copy['date'] = pd.to_datetime(df_copy['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                # å–å‡ºé id/date çš„åˆ—ä½œä¸ºç‰¹å¾
                feature_cols = [c for c in df_copy.columns if c not in ['station_id', 'date']]
                if not feature_cols:
                    continue
                # ä¸ºé¿å…åˆ—åå†²çªï¼Œç»™åˆ—åŠ ä¸Šå‰ç¼€ï¼ˆæºåï¼‰
                prefixed = df_copy[['station_id', 'date'] + feature_cols].copy()
                # å¦‚æœæºåæ˜¯å¯è¯»çš„ï¼Œä¿ç•™ï¼›å¦åˆ™ä½¿ç”¨ name
                new_col_mapping = {col: f"{name}_{col}" for col in feature_cols}
                prefixed = prefixed.rename(columns=new_col_mapping)
                # å»é‡ä»¥é˜²æ­¢é‡å¤ (station_id, date, feature...)
                prefixed = prefixed.drop_duplicates(subset=['station_id', 'date'])
                master = master.merge(prefixed, on=['station_id', 'date'], how='left')

            # 4. è‹¥æä¾›äº† swe_dfï¼Œä½†ä½ å¸Œæœ›ä¿ç•™å…¶åŸå§‹åˆ—å 'swe'ï¼Œåˆ™åˆå¹¶å¹¶é‡å‘½å
            if swe_df is not None and not swe_df.empty:
                swe_copy = swe_df.copy()
                swe_copy['station_id'] = swe_copy['station_id'].astype(str)
                swe_copy['date'] = pd.to_datetime(swe_copy['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                # å‡å®š swe_df å·²ç»è¢«é‡å‘½åä¸º 'swe' åˆ—ï¼›å¦‚æœä¸æ˜¯ï¼Œè¯·åœ¨è°ƒç”¨å‰é‡å‘½å
                if 'swe' not in swe_copy.columns:
                    # å°è¯•æŸ¥æ‰¾å¯èƒ½çš„sweåˆ—
                    possible = [c for c in swe_copy.columns if 'swe' in c.lower()]
                    if possible:
                        swe_copy = swe_copy.rename(columns={possible[0]: 'swe'})
                swe_copy = swe_copy[['station_id', 'date', 'swe']].drop_duplicates(subset=['station_id', 'date'])
                master = master.merge(swe_copy, on=['station_id', 'date'], how='left')

            # 5. æ’åºå¹¶è¿”å›
            master = master.sort_values(['station_id', 'date']).reset_index(drop=True)
            self.logger.info(f"_build_master_from_union: master è¡Œæ•°={len(master)} (ç”±åŠ¨æ€æº+SWEå¹¶é›†æ„å»º)")
            return master

        except Exception as e:
            self.logger.error(f"_build_master_from_union å¤±è´¥: {e}")
            self.logger.debug(traceback.format_exc())
            return pd.DataFrame()

    def _export_time_statistics(self, time_statistics, output_dir):
        """å¯¼å‡ºæ—¶é—´ç»Ÿè®¡ä¿¡æ¯åˆ°Excelæ–‡ä»¶

        Args:
            time_statistics (dict): æ—¶é—´ç»Ÿè®¡ä¿¡æ¯
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        try:
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = Path(output_dir) / f"time_statistics_{timestamp}.xlsx"

            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                # 1. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
                summary_data = {
                    'ç»Ÿè®¡é¡¹': [
                        'æ€»è®°å½•æ•°',
                        'æœ‰æ•ˆæ—¶é—´è®°å½•æ•°',
                        'æ—¶é—´æœ‰æ•ˆæ€§ç‡(%)',
                        'æ—¶é—´èŒƒå›´å¼€å§‹',
                        'æ—¶é—´èŒƒå›´ç»“æŸ',
                        'è¦†ç›–å¹´ä»½èŒƒå›´',
                        'æ€»æœˆä»½æ•°'
                    ],
                    'æ•°å€¼': [
                        time_statistics['total_records'],
                        time_statistics['valid_time_records'],
                        f"{time_statistics['time_validity_rate']:.2f}",
                        time_statistics['time_range']['overall_start'],
                        time_statistics['time_range']['overall_end'],
                        time_statistics['time_range']['overall_years'],
                        time_statistics['time_range']['total_months']
                    ]
                }
                df_summary = pd.DataFrame(summary_data)
                df_summary.to_excel(writer, sheet_name='åŸºæœ¬ç»Ÿè®¡', index=False)

                # 2. å¹´ä»½åˆ†å¸ƒ
                df_years = pd.DataFrame({
                    'year': list(time_statistics['year_distribution'].keys()),
                    'record_count': list(time_statistics['year_distribution'].values())
                })
                df_years.to_excel(writer, sheet_name='å¹´ä»½åˆ†å¸ƒ', index=False)

                # 3. æœˆä»½åˆ†å¸ƒ
                month_names = ['ä¸€æœˆ', 'äºŒæœˆ', 'ä¸‰æœˆ', 'å››æœˆ', 'äº”æœˆ', 'å…­æœˆ',
                               'ä¸ƒæœˆ', 'å…«æœˆ', 'ä¹æœˆ', 'åæœˆ', 'åä¸€æœˆ', 'åäºŒæœˆ']
                df_months = pd.DataFrame({
                    'month': list(time_statistics['month_distribution'].keys()),
                    'month_name': [month_names[m - 1] if 1 <= m <= 12 else f"æœˆä»½{m}"
                                   for m in time_statistics['month_distribution'].keys()],
                    'record_count': list(time_statistics['month_distribution'].values())
                })
                df_months.to_excel(writer, sheet_name='æœˆä»½åˆ†å¸ƒ', index=False)

                # 4. å¹´æœˆç»„åˆåˆ†å¸ƒ
                df_year_month = pd.DataFrame({
                    'year_month': [str(ym) for ym in time_statistics['year_month_distribution'].keys()],
                    'record_count': list(time_statistics['year_month_distribution'].values())
                })
                df_year_month.to_excel(writer, sheet_name='å¹´æœˆåˆ†å¸ƒ', index=False)

                # 5. ç«™ç‚¹æ—¶é—´è¦†ç›–ç»Ÿè®¡
                station_data = []
                for station_id, stats in time_statistics['station_time_coverage'].items():
                    station_data.append({
                        'station_id': station_id,
                        'first_record': stats['first_record'],
                        'last_record': stats['last_record'],
                        'total_records': stats['total_records'],
                        'min_year': stats['min_year'],
                        'max_year': stats['max_year'],
                        'unique_years': stats['unique_years'],
                        'unique_months': stats['unique_months']
                    })
                df_stations = pd.DataFrame(station_data)
                df_stations.to_excel(writer, sheet_name='ç«™ç‚¹æ—¶é—´è¦†ç›–', index=False)

                # 6. SWEæ•°æ®æ—¶é—´åˆ†å¸ƒ
                swe_data = []
                for ym, stats in time_statistics['swe_time_distribution'].items():
                    swe_data.append({
                        'year_month': str(ym),
                        'stations_with_swe': stats['stations_with_swe'],
                        'swe_records': stats['swe_records']
                    })
                df_swe = pd.DataFrame(swe_data)
                df_swe.to_excel(writer, sheet_name='SWEæ—¶é—´åˆ†å¸ƒ', index=False)

                # 7. æ•°æ®æ ·æœ¬ï¼ˆå‰1000æ¡ï¼‰
                df_samples = pd.DataFrame(time_statistics['data_samples'])
                df_samples.to_excel(writer, sheet_name='æ•°æ®æ ·æœ¬', index=False)

            self.logger.info(f"âœ… æ—¶é—´ç»Ÿè®¡ä¿¡æ¯å·²å¯¼å‡º: {output_path}")
            return str(output_path)

        except Exception as e:
            self.logger.error(f"å¯¼å‡ºæ—¶é—´ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None

    def _validate_data_integrity(self, df):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼ŒåŒ…æ‹¬å¹´ä»½å’Œæœˆä»½ä¿¡æ¯

        Args:
            df: è¦éªŒè¯çš„DataFrame

        Returns:
            DataFrame: éªŒè¯åçš„æ•°æ®
        """
        if df.empty:
            return df

        self.logger.info("=== æ•°æ®æœ‰æ•ˆæ€§éªŒè¯ ===")

        # éªŒè¯å„ä¸ªç‰¹å¾åˆ—
        data_cols = [col for col in df.columns if col not in ['station_id', 'date', 'data_source']]

        for col in data_cols:
            valid_count = df[col].notna().sum()
            total_count = len(df)
            fill_rate = (valid_count / total_count) * 100 if total_count > 0 else 0
            self.logger.info(f"{col}: {valid_count}/{total_count} æœ‰æ•ˆè®°å½• ({fill_rate:.1f}%)")

        # éªŒè¯æ—¥æœŸç›¸å…³åˆ—
        if 'date' in df.columns:
            # éªŒè¯æ—¥æœŸæ ¼å¼
            try:
                df['date'] = pd.to_datetime(df['date'])
                valid_dates = df['date'].notna().sum()
                self.logger.info(f"date: {valid_dates}/{len(df)} æœ‰æ•ˆæ—¥æœŸ ({valid_dates / len(df) * 100:.1f}%)")

                # æå–å¹¶éªŒè¯å¹´ä»½å’Œæœˆä»½
                df['year'] = df['date'].dt.year
                df['month'] = df['date'].dt.month

                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ°´æ–‡å¹´DOY
                if 'hydrological_doy' in df.columns:
                    hydro_doy_valid = df['hydrological_doy'].notna().sum()
                    self.logger.info(
                        f"hydrological_doy: {hydro_doy_valid}/{len(df)} æœ‰æ•ˆè®°å½• ({hydro_doy_valid / len(df) * 100:.1f}%)")

                    # æ˜¾ç¤ºæ°´æ–‡å¹´DOYç»Ÿè®¡
                    if hydro_doy_valid > 0:
                        hydro_doy_stats = df['hydrological_doy'].describe()
                        self.logger.info(
                            f"æ°´æ–‡å¹´DOYç»Ÿè®¡: èŒƒå›´={hydro_doy_stats['min']:.0f}-{hydro_doy_stats['max']:.0f}, å‡å€¼={hydro_doy_stats['mean']:.1f}")
                else:
                    self.logger.warning("æ•°æ®ä¸­ç¼ºå°‘ hydrological_doy åˆ—")

                # ç»Ÿè®¡å¹´ä»½ä¿¡æ¯
                year_stats = df['year'].value_counts().sort_index()
                self.logger.info("å¹´ä»½åˆ†å¸ƒ:")
                for year, count in year_stats.items():
                    self.logger.info(f"  {year}: {count} æ¡è®°å½•")

                # ç»Ÿè®¡æœˆä»½ä¿¡æ¯
                month_stats = df['month'].value_counts().sort_index()
                month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
                self.logger.info("æœˆä»½åˆ†å¸ƒ:")
                for month, count in month_stats.items():
                    month_name = month_names[month - 1] if 1 <= month <= 12 else f"Month{month}"
                    self.logger.info(f"  {month_name}: {count} æ¡è®°å½•")

                # ç»Ÿè®¡æ—¶é—´èŒƒå›´
                start_date = df['date'].min()
                end_date = df['date'].max()
                total_years = df['year'].nunique()
                total_months = df['month'].nunique()

                self.logger.info(f"æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
                self.logger.info(f"è¦†ç›–å¹´ä»½æ•°: {total_years}")
                self.logger.info(f"è¦†ç›–æœˆä»½æ•°: {total_months}")

            except Exception as e:
                self.logger.warning(f"æ—¥æœŸå¤„ç†å¤±è´¥: {e}")

        # è¯†åˆ«å¯èƒ½çš„é—®é¢˜è®°å½•
        if data_cols:
            empty_records = df[data_cols].isna().all(axis=1)
            if empty_records.any():
                empty_count = empty_records.sum()
                self.logger.warning(f"å‘ç° {empty_count} æ¡å…¨ç©ºè®°å½•")

        return df

    def analyze_time_distribution(self, df):
        """åˆ†ææ•°æ®çš„æ—¶é—´åˆ†å¸ƒï¼ˆå¹´ä»½å’Œæœˆä»½ï¼‰

        Args:
            df: åŒ…å«dateåˆ—çš„æ•°æ®æ¡†

        Returns:
            dict: æ—¶é—´åˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯
        """
        try:
            if 'date' not in df.columns:
                self.logger.warning("æ•°æ®æ¡†ä¸­æ²¡æœ‰dateåˆ—")
                return None

            # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
            df_analysis = df.copy()
            df_analysis['date'] = pd.to_datetime(df_analysis['date'])

            # æå–å¹´æœˆä¿¡æ¯
            df_analysis['year'] = df_analysis['date'].dt.year
            df_analysis['month'] = df_analysis['date'].dt.month
            df_analysis['year_month'] = df_analysis['date'].dt.to_period('M')

            # åŸºæœ¬ç»Ÿè®¡
            total_records = len(df_analysis)
            valid_dates = df_analysis['date'].notna().sum()

            # å¹´ä»½ç»Ÿè®¡
            year_stats = df_analysis['year'].value_counts().sort_index()

            # æœˆä»½ç»Ÿè®¡
            month_stats = df_analysis['month'].value_counts().sort_index()

            # å¹´æœˆç»„åˆç»Ÿè®¡
            year_month_stats = df_analysis['year_month'].value_counts().sort_index()

            # å„ç«™ç‚¹çš„å¹´ä»½è¦†ç›–
            station_year_stats = df_analysis.groupby('station_id')['year'].agg(['min', 'max', 'nunique']).round()
            station_year_stats = station_year_stats.rename(columns={
                'min': 'first_year',
                'max': 'last_year',
                'nunique': 'years_covered'
            })

            # æ„å»ºç»“æœ
            time_analysis = {
                'total_records': total_records,
                'valid_dates': valid_dates,
                'date_validity_rate': (valid_dates / total_records) * 100,
                'year_distribution': year_stats.to_dict(),
                'month_distribution': month_stats.to_dict(),
                'year_month_distribution': year_month_stats.to_dict(),
                'station_year_coverage': station_year_stats.to_dict('index'),
                'time_range': {
                    'start_date': df_analysis['date'].min(),
                    'end_date': df_analysis['date'].max(),
                    'total_years': df_analysis['year'].nunique(),
                    'total_months': df_analysis['month'].nunique(),
                    'year_range': f"{df_analysis['year'].min()} - {df_analysis['year'].max()}"
                }
            }

            # è¾“å‡ºè¯¦ç»†ç»Ÿè®¡
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“… æ•°æ®æ—¶é—´åˆ†å¸ƒåˆ†æ")
            self.logger.info("=" * 60)
            self.logger.info(f"æ€»è®°å½•æ•°: {total_records}")
            self.logger.info(f"æœ‰æ•ˆæ—¥æœŸè®°å½•: {valid_dates} ({time_analysis['date_validity_rate']:.1f}%)")
            self.logger.info(
                f"æ—¶é—´èŒƒå›´: {time_analysis['time_range']['start_date']} åˆ° {time_analysis['time_range']['end_date']}")
            self.logger.info(f"è¦†ç›–å¹´ä»½: {time_analysis['time_range']['year_range']}")
            self.logger.info(f"æ€»å¹´ä»½æ•°: {time_analysis['time_range']['total_years']}")
            self.logger.info(f"æ€»æœˆä»½æ•°: {time_analysis['time_range']['total_months']}")

            self.logger.info(f"\nğŸ“ˆ å¹´ä»½åˆ†å¸ƒ:")
            for year, count in year_stats.items():
                percentage = (count / total_records) * 100
                self.logger.info(f"  {year}: {count} æ¡è®°å½• ({percentage:.1f}%)")

            self.logger.info(f"\nğŸ“… æœˆä»½åˆ†å¸ƒ:")
            month_names = ['ä¸€æœˆ', 'äºŒæœˆ', 'ä¸‰æœˆ', 'å››æœˆ', 'äº”æœˆ', 'å…­æœˆ',
                           'ä¸ƒæœˆ', 'å…«æœˆ', 'ä¹æœˆ', 'åæœˆ', 'åä¸€æœˆ', 'åäºŒæœˆ']
            for month, count in month_stats.items():
                month_name = month_names[month - 1] if 1 <= month <= 12 else f"æœˆä»½{month}"
                percentage = (count / total_records) * 100
                self.logger.info(f"  {month_name}: {count} æ¡è®°å½• ({percentage:.1f}%)")

            return time_analysis

        except Exception as e:
            self.logger.error(f"åˆ†ææ—¶é—´åˆ†å¸ƒå¤±è´¥: {str(e)}")
            return None

    def get_comprehensive_statistics(self, output_dir=None):
        """è·å–ç»¼åˆç»Ÿè®¡ä¿¡æ¯ï¼ˆç«™ç‚¹+æ—¶é—´ï¼‰

        Args:
            output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„

        Returns:
            dict: ç»¼åˆç»Ÿè®¡ä¿¡æ¯
        """
        try:
            self.logger.info("ğŸ“Š å¼€å§‹ç»¼åˆç»Ÿè®¡åˆ†æ...")

            # è·å–ç«™ç‚¹ç»Ÿè®¡
            station_stats = self.get_station_statistics()
            # è·å–æ—¶é—´ç»Ÿè®¡
            time_stats = self.extract_time_info()

            if not station_stats or not time_stats:
                self.logger.error("æ— æ³•è·å–å®Œæ•´çš„ç»Ÿè®¡ä¿¡æ¯")
                return None

            # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
            comprehensive_stats = {
                'station_statistics': station_stats,
                'time_statistics': time_stats,
                'summary': {
                    'total_stations': station_stats['total_stations'],
                    'total_records': time_stats['total_records'],
                    'time_range': time_stats['time_range']['overall_years'],
                    'avg_records_per_station': station_stats['summary']['avg_records_per_station'],
                    'swe_coverage_rate': station_stats['summary']['avg_swe_coverage']
                }
            }

            # è¾“å‡ºç»¼åˆæ‘˜è¦
            self.logger.info("=" * 60)
            self.logger.info("ğŸ“ˆ ç»¼åˆç»Ÿè®¡æ‘˜è¦")
            self.logger.info("=" * 60)
            self.logger.info(f"æ€»ç«™ç‚¹æ•°: {comprehensive_stats['summary']['total_stations']}")
            self.logger.info(f"æ€»è®°å½•æ•°: {comprehensive_stats['summary']['total_records']}")
            self.logger.info(f"æ—¶é—´èŒƒå›´: {comprehensive_stats['summary']['time_range']}")
            self.logger.info(f"å¹³å‡æ¯ç«™è®°å½•: {comprehensive_stats['summary']['avg_records_per_station']:.1f}")
            self.logger.info(f"SWEè¦†ç›–ç‡: {comprehensive_stats['summary']['swe_coverage_rate']:.1f}%")

            # å¯¼å‡ºç»¼åˆç»Ÿè®¡
            if output_dir:
                self._export_comprehensive_statistics(comprehensive_stats, output_dir)

            return comprehensive_stats

        except Exception as e:
            self.logger.error(f"è·å–ç»¼åˆç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None

    def get_swe_from_database(self, station_ids, start_date, end_date):
        """ä»æ•°æ®åº“è·å–SWEæ•°æ®å’Œæµ·æ‹”é«˜åº¦ - å¢å¼ºç‰ˆæœ¬"""
        try:
            if not self.db_conn:
                if not self.connect_database():
                    return pd.DataFrame()

            # æ ¹æ®å®é™…æ•°æ®åº“åˆ—åè°ƒæ•´
            swe_column = 'sweï¼ˆmmï¼‰'
            station_id_column = 'station_ID'
            time_column = 'time'
            altitude_column = 'Altitude(m)'  # æ–°å¢æµ·æ‹”åˆ—

            self.logger.info(f"è·å–æ•°æ®åº“æ•°æ® - SWEåˆ—: '{swe_column}', æµ·æ‹”åˆ—: '{altitude_column}'")

            # æ„å»ºæŸ¥è¯¢ - åŒæ—¶è·å–SWEå’Œæµ·æ‹”æ•°æ®
            query = f"""
            SELECT {station_id_column}, {time_column}, "{swe_column}", "{altitude_column}"
            FROM stations 
            WHERE "{swe_column}" IS NOT NULL 
                AND {time_column} BETWEEN ? AND ?
            """

            # è½¬æ¢æ—¥æœŸæ ¼å¼ä¸ºæ•°æ®åº“ä¸­çš„æ ¼å¼ (YYYYMMDD)
            start_db = int(start_date.strftime('%Y%m%d'))
            end_db = int(end_date.strftime('%Y%m%d'))

            params = [start_db, end_db]
            df = pd.read_sql_query(query, self.db_conn, params=params)

            if not df.empty:
                # è½¬æ¢æ•°æ®åº“æ ¼å¼çš„æ—¥æœŸ (YYYYMMDD -> YYYY-MM-DD)
                df['date'] = df[time_column].apply(
                    lambda x: datetime.strptime(str(int(x)), '%Y%m%d').strftime('%Y-%m-%d')
                )

                # æ ‡å‡†åŒ–ç«™ç‚¹IDåˆ—å
                df['station_id'] = df[station_id_column].astype(str)

                # é€‰æ‹©éœ€è¦çš„åˆ—ï¼ŒåŒ…æ‹¬æµ·æ‹”
                result_df = df[['station_id', 'date', swe_column, altitude_column]].copy()
                result_df = result_df.rename(columns={
                    swe_column: 'swe',
                    altitude_column: 'altitude'
                })

                # ç«™ç‚¹åŒ¹é…è°ƒè¯•
                needed_stations = set(station_ids)
                available_stations = set(result_df['station_id'].unique())
                matched_stations = needed_stations & available_stations

                self.logger.info(f"ç«™ç‚¹åŒ¹é…è¯¦æƒ…:")
                self.logger.info(f"  æ‰€éœ€ç«™ç‚¹æ•°é‡: {len(needed_stations)}")
                self.logger.info(f"  æ•°æ®åº“æœ‰æ•°æ®çš„ç«™ç‚¹æ•°é‡: {len(available_stations)}")
                self.logger.info(f"  åŒ¹é…çš„ç«™ç‚¹æ•°é‡: {len(matched_stations)}")

                if matched_stations:
                    result_df = result_df[result_df['station_id'].isin(matched_stations)]

                    # æ•°æ®ç»Ÿè®¡
                    swe_stats = result_df['swe'].describe()
                    altitude_stats = result_df['altitude'].describe()
                    unique_dates = result_df['date'].nunique()

                    self.logger.info(f"âœ… æˆåŠŸè·å–æ•°æ®åº“æ•°æ®: {len(result_df)} æ¡è®°å½•")
                    self.logger.info(f"  SWEæ•°æ®è¦†ç›– {unique_dates} ä¸ªæ—¥æœŸ")
                    self.logger.info(
                        f"  SWEç»Ÿè®¡ - å‡å€¼: {swe_stats['mean']:.2f}, èŒƒå›´: {swe_stats['min']:.2f}-{swe_stats['max']:.2f}")
                    self.logger.info(
                        f"  æµ·æ‹”ç»Ÿè®¡ - å‡å€¼: {altitude_stats['mean']:.2f}, èŒƒå›´: {altitude_stats['min']:.2f}-{altitude_stats['max']:.2f}")

                    return result_df
                else:
                    self.logger.warning("æ²¡æœ‰åŒ¹é…çš„ç«™ç‚¹ID")
                    return pd.DataFrame()
            else:
                self.logger.warning("æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            self.logger.error(f"è·å–æ•°æ®åº“æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def get_static_altitude_data(self, station_ids):
        """è·å–ç«™ç‚¹çš„é™æ€æµ·æ‹”æ•°æ®ï¼ˆæ¯ä¸ªç«™ç‚¹ä¸€ä¸ªæµ·æ‹”å€¼ï¼‰"""
        try:
            if not self.db_conn:
                if not self.connect_database():
                    return pd.DataFrame()

            altitude_column = 'Altitude(m)'
            station_id_column = 'station_ID'

            # è·å–æ¯ä¸ªç«™ç‚¹çš„å¹³å‡æµ·æ‹”ï¼ˆé™æ€ç‰¹å¾ï¼‰
            query = f"""
            SELECT {station_id_column}, AVG("{altitude_column}") as altitude
            FROM stations 
            WHERE "{altitude_column}" IS NOT NULL
            GROUP BY {station_id_column}
            """

            df = pd.read_sql_query(query, self.db_conn)

            if not df.empty:
                df['station_id'] = df[station_id_column].astype(str)
                result_df = df[['station_id', 'altitude']].drop_duplicates('station_id')

                # è¿‡æ»¤éœ€è¦çš„ç«™ç‚¹
                if station_ids:
                    result_df = result_df[result_df['station_id'].isin(station_ids)]

                self.logger.info(f"è·å–é™æ€æµ·æ‹”æ•°æ®: {len(result_df)} ä¸ªç«™ç‚¹")
                return result_df
            else:
                self.logger.warning("æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æµ·æ‹”æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            self.logger.error(f"è·å–é™æ€æµ·æ‹”æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def _combine_all_static_data(self, static_dfs):
        """åˆå¹¶æ‰€æœ‰é™æ€æ•°æ®æº"""
        self.logger.info("=== åˆå¹¶æ‰€æœ‰é™æ€æ•°æ®æº ===")

        static_combined = None
        for name, static_df in static_dfs:
            self.logger.info(f"å¤„ç†é™æ€æ•°æ®æº: {name}, è®°å½•æ•°: {len(static_df)}")

            # æ¸…ç†é™æ€æ•°æ®
            static_df_clean = self._clean_static_data(static_df, name)

            if static_combined is None:
                static_combined = static_df_clean
            else:
                before_cols = len(static_combined.columns)
                static_combined = static_combined.merge(
                    static_df_clean,
                    on='station_id',
                    how='outer'
                )
                after_cols = len(static_combined.columns)
                self.logger.info(f"åˆå¹¶é™æ€æ•°æ®æº {name}: æ·»åŠ  {after_cols - before_cols} åˆ—")

        self.logger.info(f"é™æ€æ•°æ®åˆå¹¶å®Œæˆ: {len(static_combined)} æ¡è®°å½•")
        self.logger.info(f"é™æ€æ•°æ®åˆ—: {list(static_combined.columns)}")

        return static_combined

    def _clean_static_data(self, static_df, source_name):
        """æ¸…ç†é™æ€æ•°æ®ï¼Œç¡®ä¿æ²¡æœ‰æ—¶é—´åˆ—"""
        static_df_clean = static_df.copy()

        # å½»åº•ç§»é™¤æ‰€æœ‰æ—¶é—´ç›¸å…³åˆ—
        time_columns = ['date', 'year', 'month', 'processing_year', 'data_year',
                        'processing_time', 'data_version', 'source_file']
        columns_removed = []
        for col in time_columns:
            if col in static_df_clean.columns:
                static_df_clean = static_df_clean.drop(col, axis=1)
                columns_removed.append(col)

        if columns_removed:
            self.logger.info(f"ä» {source_name} ç§»é™¤æ—¶é—´åˆ—: {columns_removed}")

        # å»é‡ï¼Œç¡®ä¿æ¯ä¸ªç«™ç‚¹åªæœ‰ä¸€æ¡è®°å½•
        before_dedup = len(static_df_clean)
        static_df_clean = static_df_clean.drop_duplicates(subset=['station_id'])
        after_dedup = len(static_df_clean)
        if before_dedup != after_dedup:
            self.logger.info(f"{source_name} å»é‡: {before_dedup} -> {after_dedup}")

        # åªä¿ç•™å¿…è¦çš„åˆ—
        keep_columns = ['station_id']
        feature_columns = [col for col in static_df_clean.columns
                           if col not in ['station_id', 'longitude', 'latitude', 'Longitude', 'Latitude']]
        keep_columns.extend(feature_columns)

        return static_df_clean[keep_columns]

    def _validate_static_merge(self, final_wide, static_combined):
        """éªŒè¯é™æ€æ•°æ®åˆå¹¶ç»“æœ"""
        self.logger.info("=== éªŒè¯é™æ€æ•°æ®åˆå¹¶ ===")

        static_features = [col for col in static_combined.columns if col != 'station_id']

        for feature in static_features:
            if feature in final_wide.columns:
                # æ£€æŸ¥å¡«å……ç‡
                filled_count = final_wide[feature].notna().sum()
                total_count = len(final_wide)
                fill_rate = (filled_count / total_count) * 100

                # æ£€æŸ¥æ¯ä¸ªç«™ç‚¹çš„æ‰€æœ‰æ—¶é—´ç‚¹æ˜¯å¦éƒ½æœ‰æ•°æ®
                if 'date' in final_wide.columns:
                    station_coverage = final_wide.groupby('station_id').apply(
                        lambda x: x[feature].notna().all()
                    )
                    fully_covered = station_coverage.sum()
                    total_stations = len(station_coverage)

                    self.logger.info(f"{feature}: {filled_count}/{total_count} è®°å½•æœ‰å€¼ ({fill_rate:.1f}%)")
                    self.logger.info(f"  {fully_covered}/{total_stations} ä¸ªç«™ç‚¹çš„æ‰€æœ‰æ—¶é—´ç‚¹éƒ½æœ‰æ•°æ®")

                    if fill_rate < 95:
                        missing_stations = final_wide[final_wide[feature].isna()]['station_id'].unique()
                        self.logger.warning(f"  ç¼ºå¤±æ•°æ®çš„ç«™ç‚¹ç¤ºä¾‹: {missing_stations[:5]}")

    def get_altitude_from_database(self, station_ids):
        """ä»æ•°æ®åº“è·å–æµ·æ‹”æ•°æ®"""
        try:
            if not self.db_conn:
                if not self.connect_database():
                    return pd.DataFrame()

            altitude_column = 'Altitude(m)'
            self.logger.info(f"è·å–æµ·æ‹”æ•°æ®ï¼Œä½¿ç”¨åˆ—: '{altitude_column}'")

            # è·å–æ¯ä¸ªç«™ç‚¹çš„æµ·æ‹”æ•°æ®ï¼ˆå–å¹³å‡å€¼ï¼‰
            query = f"""
            SELECT station_ID, AVG("{altitude_column}") as altitude
            FROM stations 
            WHERE "{altitude_column}" IS NOT NULL
            GROUP BY station_ID
            """

            df = pd.read_sql_query(query, self.db_conn)

            if not df.empty:
                df['station_id'] = df['station_ID'].astype(str)
                df = df[['station_id', 'altitude']]

                # è¿‡æ»¤éœ€è¦çš„ç«™ç‚¹
                if station_ids:
                    df = df[df['station_id'].isin(station_ids)]

                self.logger.info(f"âœ… æˆåŠŸè·å–æµ·æ‹”æ•°æ®: {len(df)} ä¸ªç«™ç‚¹")

                # æ˜¾ç¤ºæµ·æ‹”ç»Ÿè®¡
                altitude_stats = df['altitude'].describe()
                self.logger.info(
                    f"  æµ·æ‹”ç»Ÿè®¡ - å‡å€¼: {altitude_stats['mean']:.2f}, èŒƒå›´: {altitude_stats['min']:.2f}-{altitude_stats['max']:.2f}")

                return df
            else:
                self.logger.warning("æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æµ·æ‹”æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            self.logger.error(f"è·å–æµ·æ‹”æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def _query_swe_batch(self, station_ids, swe_column, start_db, end_db):
        """åˆ†æ‰¹æŸ¥è¯¢SWEæ•°æ®"""
        try:
            placeholders = ','.join(['?'] * len(station_ids))
            query = f"""
            SELECT station_ID, time, {swe_column} 
            FROM stations 
            WHERE station_ID IN ({placeholders}) AND time BETWEEN ? AND ?
            """
            params = station_ids + [start_db, end_db]

            df = pd.read_sql_query(query, self.db_conn, params=params)
            return df

        except Exception as e:
            self.logger.error(f"åˆ†æ‰¹æŸ¥è¯¢SWEæ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def check_database_structure(self):
        """æ£€æŸ¥æ•°æ®åº“è¡¨ç»“æ„ - è¯¦ç»†ç‰ˆæœ¬"""
        try:
            if not self.db_conn:
                if not self.connect_database():
                    return None

            # è·å–è¡¨ç»“æ„ä¿¡æ¯
            tables_query = "SELECT name FROM sqlite_master WHERE type='table';"
            tables = pd.read_sql_query(tables_query, self.db_conn)
            self.logger.info(f"æ•°æ®åº“ä¸­çš„è¡¨: {tables['name'].tolist()}")

            # æ£€æŸ¥stationsè¡¨çš„åˆ—
            if 'stations' in tables['name'].values:
                columns_query = "PRAGMA table_info(stations);"
                columns = pd.read_sql_query(columns_query, self.db_conn)
                column_names = columns['name'].tolist()
                self.logger.info(f"stationsè¡¨çš„å®Œæ•´åˆ—ä¿¡æ¯:")
                for _, row in columns.iterrows():
                    self.logger.info(f"  - {row['name']} ({row['type']})")
                return column_names
            else:
                self.logger.error("stationsè¡¨ä¸å­˜åœ¨")
                return None

        except Exception as e:
            self.logger.error(f"æ£€æŸ¥æ•°æ®åº“ç»“æ„å¤±è´¥: {str(e)}")
            return None

    def get_combined_data(self):
        """åˆå¹¶æ•°æ®"""
        if not self.source_data:
            return pd.DataFrame()

        dfs = []
        for name, df in self.source_data.items():
            df_copy = df.copy()
            df_copy['data_source'] = name
            dfs.append(df_copy)

        return pd.concat(dfs, ignore_index=True)

    def force_fix_static_data(self, final_wide, static_dfs):
        """å¼ºåˆ¶ä¿®å¤é™æ€æ•°æ®åˆå¹¶é—®é¢˜"""
        self.logger.info("=== å¼ºåˆ¶ä¿®å¤é™æ€æ•°æ®åˆå¹¶ ===")

        # åˆå¹¶æ‰€æœ‰é™æ€æ•°æ®
        static_combined = self._combine_all_static_data(static_dfs)

        # è·å–åŠ¨æ€æ•°æ®ä¸­çš„æ‰€æœ‰ç«™ç‚¹
        dynamic_stations = set(final_wide['station_id'].unique())
        static_stations = set(static_combined['station_id'].unique())

        self.logger.info(f"åŠ¨æ€æ•°æ®ç«™ç‚¹: {len(dynamic_stations)}")
        self.logger.info(f"é™æ€æ•°æ®ç«™ç‚¹: {len(static_stations)}")

        # ä¸ºæ¯ä¸ªåŠ¨æ€æ•°æ®ç«™ç‚¹ç¡®ä¿æœ‰é™æ€æ•°æ®
        for station_id in dynamic_stations:
            if station_id not in static_stations:
                self.logger.warning(f"ç«™ç‚¹ {station_id} ç¼ºå°‘é™æ€æ•°æ®ï¼Œåˆ›å»ºç©ºè®°å½•")
                # åˆ›å»ºç©ºè®°å½•
                empty_record = {'station_id': station_id}
                for col in static_combined.columns:
                    if col != 'station_id':
                        empty_record[col] = np.nan
                # æ·»åŠ åˆ°é™æ€æ•°æ®
                static_combined = pd.concat([static_combined, pd.DataFrame([empty_record])], ignore_index=True)

        # é‡æ–°åˆå¹¶
        final_fixed = final_wide.merge(static_combined, on='station_id', how='left')

        self.logger.info(f"å¼ºåˆ¶ä¿®å¤å®Œæˆ: {len(final_wide)} -> {len(final_fixed)} è¡Œ")
        return final_fixed

    def analyze_station_matching(self, final_wide, static_combined):
        """åˆ†æç«™ç‚¹åŒ¹é…æƒ…å†µ"""
        self.logger.info("=== ç«™ç‚¹åŒ¹é…åˆ†æ ===")

        # è·å–æ‰€æœ‰ç«™ç‚¹
        dynamic_stations = set(final_wide['station_id'].unique())
        static_stations = set(static_combined['station_id'].unique())

        self.logger.info(f"åŠ¨æ€æ•°æ®ç«™ç‚¹æ€»æ•°: {len(dynamic_stations)}")
        self.logger.info(f"é™æ€æ•°æ®ç«™ç‚¹æ€»æ•°: {len(static_stations)}")

        # åˆ†æåŒ¹é…æƒ…å†µ
        common_stations = dynamic_stations & static_stations
        only_dynamic_stations = dynamic_stations - static_stations
        only_static_stations = static_stations - dynamic_stations

        self.logger.info(f"å…±åŒç«™ç‚¹: {len(common_stations)}")
        self.logger.info(f"ä»…åŠ¨æ€æ•°æ®ç«™ç‚¹: {len(only_dynamic_stations)}")
        self.logger.info(f"ä»…é™æ€æ•°æ®ç«™ç‚¹: {len(only_static_stations)}")

        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
        if only_dynamic_stations:
            self.logger.warning(f"åŠ¨æ€æ•°æ®ä¸­ç¼ºå°‘é™æ€æ•°æ®çš„ç«™ç‚¹ç¤ºä¾‹: {list(only_dynamic_stations)[:10]}")

        if only_static_stations:
            self.logger.warning(f"é™æ€æ•°æ®ä¸­å¤šä½™çš„ç«™ç‚¹ç¤ºä¾‹: {list(only_static_stations)[:10]}")

        return {
            'common_stations': common_stations,
            'only_dynamic_stations': only_dynamic_stations,
            'only_static_stations': only_static_stations
        }

    def generate_report(self):
        """ç”Ÿæˆæ•°æ®æ•´åˆæŠ¥å‘Š"""
        try:
            report_lines = []
            report_lines.append("=== æ•°æ®æ•´åˆæŠ¥å‘Š ===")
            report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report_lines.append("")

            # æ•°æ®æºç»Ÿè®¡
            report_lines.append("=== æ•°æ®æºç»Ÿè®¡ ===")
            for name, df in self.source_data.items():
                station_count = df['station_id'].nunique() if 'station_id' in df.columns else 0
                date_count = df['date'].nunique() if 'date' in df.columns else "N/A"
                record_count = len(df)

                report_lines.append(f"{name}:")
                report_lines.append(f"  - è®°å½•æ•°: {record_count}")
                report_lines.append(f"  - ç«™ç‚¹æ•°: {station_count}")
                report_lines.append(f"  - æ—¥æœŸæ•°: {date_count}")

                # æ£€æŸ¥åæ ‡ä¿¡æ¯
                if self._has_coordinate_info(df):
                    coord_columns = self._get_coordinate_columns(df)
                    report_lines.append(f"  - åæ ‡ä¿¡æ¯: {coord_columns}")

                report_lines.append("")

            # ç‰¹å¾åˆ—ç»Ÿè®¡
            report_lines.append("=== ç‰¹å¾åˆ—ç»Ÿè®¡ ===")
            all_columns = set()
            for name, df in self.source_data.items():
                all_columns.update(df.columns)

            # æ’é™¤IDå’Œæ—¥æœŸåˆ—
            feature_columns = [col for col in all_columns if col not in ['station_id', 'date', 'data_source']]
            report_lines.append(f"æ€»ç‰¹å¾åˆ—æ•°: {len(feature_columns)}")
            report_lines.append("ç‰¹å¾åˆ—åˆ—è¡¨:")
            for col in sorted(feature_columns):
                report_lines.append(f"  - {col}")

            return "\n".join(report_lines)

        except Exception as e:
            self.logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"

    def clear_data(self):
        """æ¸…ç©ºæ•°æ®"""
        self.source_data.clear()
        self.logger.info("æ•°æ®å·²æ¸…ç©º")

    def close(self):
        """å…³é—­èµ„æº"""
        try:
            if self.db_conn:
                self.db_conn.close()
                self.logger.debug("æ•°æ®åº“è¿æ¥å·²å…³é—­")
        except Exception as e:
            self.logger.warning(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {str(e)}")

    def __del__(self):
        """ææ„å‡½æ•°"""
        self.close()