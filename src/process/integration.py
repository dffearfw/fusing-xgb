import logging
import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import traceback

logger = logging.getLogger("DataIntegrator")


class DataIntegrator:
    def __init__(self, output_dir, secure_processor=None):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger("DataIntegrator")
        self.source_data = {}
        self.db_conn = None
        self.db_fields_config = {
            'sweï¼ˆmmï¼‰': 'swe',
            'Altitude(m)': 'altitude',

            # 'snowDepth(mm)': 'snow_depth',
            # 'snowDensity(g/cm3)': 'snow_density'
        }

    def connect_database(self):
        """è¿æ¥ç«™ç‚¹æ•°æ®åº“"""
        try:
            from src.process.config import config
            db_path = config.get_station_db_path()
            self.db_conn = sqlite3.connect(db_path)
            self.logger.info("æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            return False

    def add_source(self, name, file_path):
        """æ·»åŠ æ•°æ®æº - å®Œæ•´ç‰ˆæœ¬"""
        try:
            path = Path(file_path)

            if not path.exists():
                self.logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                return False

            # è¯»å–æ–‡ä»¶
            if path.suffix.lower() == '.parquet':
                df = pd.read_parquet(path)
            elif path.suffix.lower() in ['.xlsx', '.xls']:
                try:
                    df = pd.read_excel(path, engine='openpyxl')
                except Exception as e:
                    self.logger.warning(f"openpyxlå¼•æ“å¤±è´¥: {str(e)}ï¼Œå°è¯•xlrd")
                    try:
                        df = pd.read_excel(path, engine='xlrd')
                    except Exception as e:
                        self.logger.error(f"æ‰€æœ‰Excelå¼•æ“éƒ½å¤±è´¥: {str(e)}")
                        return False
            else:
                try:
                    df = pd.read_csv(path)
                except UnicodeDecodeError:
                    try:
                        df = pd.read_csv(path, encoding='latin1')
                    except:
                        try:
                            df = pd.read_csv(path, encoding='gbk')
                        except:
                            self.logger.error("CSVæ–‡ä»¶è¯»å–å¤±è´¥")
                            return False

            # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
            df = self._standardize_date_format(df, name)

            # æ ‡å‡†åŒ–åˆ—å
            df = self._standardize_column_names(df, name)

            # æ•°æ®éªŒè¯
            validation_result = self._validate_dataframe(df, name)
            if not validation_result['valid']:
                self.logger.warning(f"æ•°æ®æº {name} éªŒè¯è­¦å‘Š: {validation_result['message']}")

            # è®°å½•åæ ‡ä¿¡æ¯
            if self._has_coordinate_info(df):
                coord_columns = self._get_coordinate_columns(df)
                stations_with_coords = df['station_id'].nunique()
                self.logger.info(f"æ•°æ®æº {name} åŒ…å«åæ ‡ä¿¡æ¯: {coord_columns}, æ¶‰åŠ {stations_with_coords} ä¸ªç«™ç‚¹")

            self.source_data[name] = df
            self.logger.info(f"æ·»åŠ  {name}: {len(df)} è¡Œ, åˆ—: {list(df.columns)}")
            return True

        except Exception as e:
            self.logger.error(f"æ·»åŠ  {name} å¤±è´¥: {e}")
            return False

    def _standardize_date_format(self, df, source_name):
        """ç»Ÿä¸€æ—¥æœŸæ ¼å¼ä¸ºå­—ç¬¦ä¸² YYYY-MM-DD"""
        try:
            if 'date' not in df.columns:
                return df

            df_copy = df.copy()

            # è½¬æ¢ä¸ºç»Ÿä¸€çš„å­—ç¬¦ä¸²æ ¼å¼
            if 'datetime' in str(df_copy['date'].dtype):
                df_copy['date'] = df_copy['date'].dt.strftime('%Y-%m-%d')
            else:
                try:
                    df_copy['date'] = pd.to_datetime(df_copy['date']).dt.strftime('%Y-%m-%d')
                except Exception as e:
                    try:
                        df_copy['date'] = df_copy['date'].str.replace('/', '-')
                        df_copy['date'] = pd.to_datetime(df_copy['date']).dt.strftime('%Y-%m-%d')
                    except:
                        self.logger.warning(f"{source_name}: æ— æ³•æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼")

            return df_copy

        except Exception as e:
            self.logger.error(f"æ ‡å‡†åŒ– {source_name} æ—¥æœŸæ ¼å¼å¤±è´¥: {str(e)}")
            return df

    def _standardize_column_names(self, df, source_name):
        """æ ‡å‡†åŒ–åˆ—å"""
        try:
            df_copy = df.copy()
            column_mapping = {}

            # ç«™ç‚¹IDæ ‡å‡†åŒ–
            if 'station_ID' in df_copy.columns and 'station_id' not in df_copy.columns:
                column_mapping['station_ID'] = 'station_id'
            if 'Station_ID' in df_copy.columns and 'station_id' not in df_copy.columns:
                column_mapping['Station_ID'] = 'station_id'

            # åæ ‡æ ‡å‡†åŒ–
            coord_mapping = {
                'Longitude': 'longitude', 'Lon': 'longitude',
                'Latitude': 'latitude', 'Lat': 'latitude'
            }

            for old_name, new_name in coord_mapping.items():
                if old_name in df_copy.columns and new_name not in df_copy.columns:
                    column_mapping[old_name] = new_name

            if column_mapping:
                df_copy = df_copy.rename(columns=column_mapping)
                self.logger.debug(f"{source_name}: é‡å‘½ååˆ— {list(column_mapping.keys())}")

            return df_copy

        except Exception as e:
            self.logger.error(f"æ ‡å‡†åŒ– {source_name} åˆ—åå¤±è´¥: {str(e)}")
            return df

    def _validate_dataframe(self, df, source_name):
        """éªŒè¯DataFrameç»“æ„"""
        validation = {'valid': True, 'message': ''}

        if 'station_id' not in df.columns:
            validation['valid'] = False
            validation['message'] = "ç¼ºå°‘station_idåˆ—"
            return validation

        numeric_cols = df.select_dtypes(include=['number']).columns
        value_cols = [col for col in numeric_cols if col not in ['station_id', 'date', 'year', 'month', 'day']]

        if not value_cols:
            validation['valid'] = False
            validation['message'] = "æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—"
            return validation

        station_count = df['station_id'].nunique()
        if station_count == 0:
            validation['valid'] = False
            validation['message'] = "æ²¡æœ‰æœ‰æ•ˆçš„ç«™ç‚¹æ•°æ®"
            return validation

        if 'date' in df.columns:
            date_count = df['date'].nunique()
            validation['message'] = f"åŠ¨æ€æ•°æ®æºï¼Œ{station_count}ä¸ªç«™ç‚¹ï¼Œ{date_count}ä¸ªæ—¶é—´ç‚¹"
        else:
            validation['message'] = f"é™æ€æ•°æ®æºï¼Œ{station_count}ä¸ªç«™ç‚¹"

        return validation

    def _has_coordinate_info(self, df):
        """æ£€æŸ¥æ•°æ®æ¡†æ˜¯å¦åŒ…å«åæ ‡ä¿¡æ¯"""
        coordinate_columns = ['longitude', 'latitude', 'Longitude', 'Latitude', 'lon', 'lat', 'Lon', 'Lat']
        return any(col in df.columns for col in coordinate_columns)

    def _get_coordinate_columns(self, df):
        """è·å–æ•°æ®æ¡†ä¸­çš„åæ ‡åˆ—å"""
        coordinate_mapping = {
            'longitude': ['longitude', 'Longitude', 'lon', 'Lon'],
            'latitude': ['latitude', 'Latitude', 'lat', 'Lat']
        }

        found_columns = []
        for standard_name, possible_names in coordinate_mapping.items():
            for name in possible_names:
                if name in df.columns:
                    found_columns.append(name)
                    break

        return found_columns

    def get_database_data(self, station_ids, start_date, end_date):
        """ç»Ÿä¸€è·å–æ•°æ®åº“ä¸­é…ç½®çš„æ‰€æœ‰å­—æ®µ"""
        try:
            if not self.db_conn:
                if not self.connect_database():
                    return pd.DataFrame()

            if not self.db_fields_config:
                self.logger.warning("æ²¡æœ‰é…ç½®æ•°æ®åº“å­—æ®µ")
                return pd.DataFrame()

            # æ„å»ºæŸ¥è¯¢
            field_selects = []
            for db_field in self.db_fields_config.keys():
                field_selects.append(f'"{db_field}"')

            query = f"""
            SELECT station_ID, time, {', '.join(field_selects)}
            FROM stations 
            WHERE time BETWEEN ? AND ?
            """

            # è½¬æ¢æ—¥æœŸæ ¼å¼
            start_db = int(start_date.strftime('%Y%m%d'))
            end_db = int(end_date.strftime('%Y%m%d'))

            df = pd.read_sql_query(query, self.db_conn, params=[start_db, end_db])

            if not df.empty:
                # å¤„ç†æ•°æ®
                df['date'] = df['time'].apply(
                    lambda x: datetime.strptime(str(int(x)), '%Y%m%d').strftime('%Y-%m-%d')
                )
                df['station_id'] = df['station_ID'].astype(str)

                # é‡å‘½ååˆ—
                result_df = df[['station_id', 'date'] + list(self.db_fields_config.keys())]
                result_df = result_df.rename(columns=self.db_fields_config)

                # è¿‡æ»¤ç«™ç‚¹
                if station_ids:
                    result_df = result_df[result_df['station_id'].isin(station_ids)]

                self.logger.info(f"âœ… ä»æ•°æ®åº“è·å– {len(self.db_fields_config)} ä¸ªå­—æ®µ: {list(self.db_fields_config.values())}")

                # ç»Ÿè®¡æ¯ä¸ªå­—æ®µçš„æœ‰æ•ˆæ•°æ®é‡
                for field in self.db_fields_config.values():
                    if field in result_df.columns:
                        valid_count = result_df[field].notna().sum()
                        self.logger.info(f"  {field}: {valid_count} æ¡æœ‰æ•ˆè®°å½•")

                return result_df
            else:
                self.logger.warning("æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            self.logger.error(f"è·å–æ•°æ®åº“æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def _create_correct_wide_table(self):
        """åˆ›å»ºæ­£ç¡®çš„å®½è¡¨ - è¯¦ç»†è°ƒè¯•ç‰ˆæœ¬"""
        if not self.source_data:
            return pd.DataFrame()

        self.logger.info("åˆ›å»ºæ­£ç¡®å®½è¡¨...")

        try:
            # 1. åˆ†ç¦»ä¸åŒç±»å‹çš„æ•°æ®æº
            static_dfs = []
            yearly_dfs = []
            dynamic_dfs = []
            phenology_dfs = []
            gldas_dfs = []
            coordinate_dfs = []

            # æ”¶é›†æ‰€æœ‰ç«™ç‚¹å’Œæ—¥æœŸèŒƒå›´
            all_station_ids = set()
            all_dates = set()

            for source_name, source_df in self.source_data.items():
                source_df = self._standardize_station_id_format(source_df, source_name)

            for source_name, source_df in self.source_data.items():
                # æ”¶é›†ç«™ç‚¹ID
                if 'station_id' in source_df.columns:
                    all_station_ids.update(source_df['station_id'].unique())

                # æ”¶é›†æ—¥æœŸ
                if 'date' in source_df.columns:
                    all_dates.update(source_df['date'].unique())

                # åˆ†ç±»æ•°æ®æº
                if self._is_gldas_data(source_name, source_df):
                    gldas_dfs.append((source_name, source_df))
                elif self._is_snow_phenology_data(source_name, source_df):
                    phenology_dfs.append((source_name, source_df))
                elif self._is_terrain_features(source_name, source_df):
                    static_dfs.append((source_name, source_df))
                elif self._is_yearly_data(source_name, source_df):
                    yearly_dfs.append((source_name, source_df))
                elif 'date' in source_df.columns and 'station_id' in source_df.columns:
                    # åŠ¨æ€æ•°æ®æº
                    numeric_cols = source_df.select_dtypes(include=['number']).columns
                    value_cols = [col for col in numeric_cols if
                                  col not in ['station_id', 'date', 'year', 'month', 'day', 'dataset_type']]
                    if value_cols:
                        value_col = value_cols[0]
                        source_wide = source_df[['station_id', 'date', value_col]].copy()
                        source_wide = source_wide.rename(columns={value_col: source_name})
                        source_wide = source_wide.drop_duplicates(['station_id', 'date'])
                        dynamic_dfs.append((source_name, source_wide))
                elif 'station_id' in source_df.columns:
                    static_dfs.append((source_name, source_df))

                # è®°å½•åæ ‡ä¿¡æ¯
                if self._has_coordinate_info(source_df):
                    coordinate_dfs.append((source_name, source_df))

            self.logger.info(f"æ•°æ®æºåˆ†ç±»ç»“æœ:")
            self.logger.info(f"  åŠ¨æ€æ•°æ®æº: {len(dynamic_dfs)} ä¸ª")
            self.logger.info(f"  é™æ€æ•°æ®æº: {len(static_dfs)} ä¸ª")
            self.logger.info(f"  å¹´åº¦æ•°æ®æº: {len(yearly_dfs)} ä¸ª")
            self.logger.info(f"  æ€»ç«™ç‚¹æ•°: {len(all_station_ids)}")
            self.logger.info(f"  æ€»æ—¥æœŸæ•°: {len(all_dates)}")

            # 2. ä»æ•°æ®åº“è·å–SWEæ•°æ®å’Œæµ·æ‹”æ•°æ®
            swe_df = pd.DataFrame()
            altitude_df = pd.DataFrame()
            if all_station_ids and all_dates:
                start_date = min(all_dates) if all_dates else datetime(2013, 1, 1)
                end_date = max(all_dates) if all_dates else datetime(2018, 12, 31)

                self.logger.info(f"è·å–æ•°æ®åº“æ•°æ®: {len(all_station_ids)} ä¸ªç«™ç‚¹, {len(all_dates)} ä¸ªæ—¥æœŸ")
                swe_df = self.get_swe_from_database(list(all_station_ids),
                                                    pd.to_datetime(start_date),
                                                    pd.to_datetime(end_date))

                # è·å–æµ·æ‹”æ•°æ®
                altitude_df = self.get_altitude_from_database(list(all_station_ids))

                self.logger.info(f"æ•°æ®åº“æ•°æ®è·å–ç»“æœ:")
                self.logger.info(f"  SWEæ•°æ®: {len(swe_df)} è¡Œ")
                self.logger.info(f"  æµ·æ‹”æ•°æ®: {len(altitude_df)} è¡Œ")

            # 3. åˆ›å»ºåŸºç¡€æ¡†æ¶
            final_wide = pd.DataFrame()

            if dynamic_dfs:
                first_dynamic_name, final_wide = dynamic_dfs[0]
                self.logger.info(f"ä»¥åŠ¨æ€æ•°æ®æº {first_dynamic_name} ä¸ºåŸºç¡€æ¡†æ¶: {len(final_wide)} è¡Œ")
                self.logger.info(f"åŸºç¡€æ¡†æ¶åˆ—: {list(final_wide.columns)}")

                # åˆå¹¶å…¶ä»–åŠ¨æ€æ•°æ®æº
                for i in range(1, len(dynamic_dfs)):
                    name, next_df = dynamic_dfs[i]
                    before_count = len(final_wide)
                    next_df = self._standardize_station_id_format(next_df, name)
                    final_wide = final_wide.merge(next_df, on=['station_id', 'date'], how='left')
                    after_count = len(final_wide)
                    self.logger.info(f"åˆå¹¶åŠ¨æ€æ•°æ®æº {name}: {before_count} -> {after_count} è¡Œ")
            else:
                self.logger.warning("æ²¡æœ‰åŠ¨æ€æ•°æ®æºï¼Œä½¿ç”¨é™æ€æ•°æ®å¤„ç†")
                return self._handle_static_only_case(static_dfs, yearly_dfs, phenology_dfs, gldas_dfs)

            # 4. åˆå¹¶æ•°æ®åº“SWEæ•°æ®
            if not swe_df.empty:
                self.logger.info(f"åˆå¹¶SWEæ•°æ®å‰ - final_wideå½¢çŠ¶: {final_wide.shape}")
                self.logger.info(f"SWEæ•°æ®å½¢çŠ¶: {swe_df.shape}")

                before_count = len(final_wide)
                final_wide = final_wide.merge(swe_df, on=['station_id', 'date'], how='left')
                after_count = len(final_wide)
                swe_valid_count = final_wide['swe'].notna().sum()
                self.logger.info(f"åˆå¹¶æ•°æ®åº“SWEæ•°æ®: {before_count} -> {after_count} è¡Œ, æœ‰æ•ˆå€¼: {swe_valid_count}")

            # 5. åˆå¹¶æ•°æ®åº“æµ·æ‹”æ•°æ® - ä½¿ç”¨suffixeså‚æ•°
            if not altitude_df.empty:
                self.logger.info(f"åˆå¹¶æµ·æ‹”æ•°æ®...")
                self.logger.info(f"  åˆå¹¶å‰final_wideåˆ—: {list(final_wide.columns)}")
                self.logger.info(f"  æµ·æ‹”æ•°æ®åˆ—: {list(altitude_df.columns)}")

                before_count = len(final_wide)

                # ä½¿ç”¨suffixeså‚æ•°æ˜ç¡®æŒ‡å®šåç¼€
                final_wide = final_wide.merge(
                    altitude_df,
                    on=['station_id'],
                    how='left',
                    suffixes=('', '_to_drop')  # åŸè¡¨ä¸åŠ åç¼€ï¼Œæ–°è¡¨åŠ _to_dropåç¼€
                )

                after_count = len(final_wide)

                # æ£€æŸ¥åˆå¹¶åçš„åˆ—
                self.logger.info(f"  åˆå¹¶åfinal_wideåˆ—: {list(final_wide.columns)}")

                # å¤„ç†å¸¦åç¼€çš„åˆ— - ä¿ç•™æ–°åˆå¹¶çš„æ•°æ®ï¼Œåˆ é™¤æ—§æ•°æ®
                columns_to_drop = []
                columns_to_rename = {}

                for col in final_wide.columns:
                    if col.endswith('_to_drop'):
                        base_col = col[:-8]  # å»æ‰ '_to_drop' åç¼€
                        # å¦‚æœåŸè¡¨ä¸­æœ‰åŒååˆ—ï¼Œåˆ é™¤åŸè¡¨çš„åˆ—ï¼Œé‡å‘½åæ–°åˆ—
                        if base_col in final_wide.columns:
                            columns_to_drop.append(base_col)  # åˆ é™¤åŸè¡¨çš„åˆ—
                            columns_to_rename[col] = base_col  # é‡å‘½åæ–°åˆ—ä¸ºåŸå
                        else:
                            columns_to_rename[col] = base_col  # ç›´æ¥é‡å‘½å

                # æ‰§è¡Œåˆ é™¤å’Œé‡å‘½å
                if columns_to_drop:
                    final_wide = final_wide.drop(columns=columns_to_drop)
                    self.logger.info(f"  åˆ é™¤åŸåˆ—: {columns_to_drop}")

                if columns_to_rename:
                    final_wide = final_wide.rename(columns=columns_to_rename)
                    self.logger.info(f"  é‡å‘½ååˆ—: {columns_to_rename}")

                altitude_valid_count = final_wide['altitude'].notna().sum() if 'altitude' in final_wide.columns else 0
                self.logger.info(f"åˆå¹¶æ•°æ®åº“æµ·æ‹”æ•°æ®: {before_count} -> {after_count} è¡Œ, æœ‰æ•ˆå€¼: {altitude_valid_count}")
                self.logger.info(f"  æœ€ç»ˆåˆ—: {list(final_wide.columns)}")

            # 6. å¤„ç†GLDASæ•°æ®
            if gldas_dfs:
                for gldas_name, gldas_df in gldas_dfs:
                    self.logger.info(f"å¤„ç†GLDASæ•°æ® {gldas_name}: {len(gldas_df)} è¡Œ")

                    feature_columns = ['station_id', 'date']
                    target_columns = ['doy', 'seasonal_doy_Da', 'seasonal_doy_Db',
                                      'seasonal_doy_Dc', 'seasonal_doy_Dd', 'gldas']

                    available_columns = []
                    for col in target_columns:
                        if col in gldas_df.columns:
                            available_columns.append(col)
                            feature_columns.append(col)

                    if available_columns:
                        gldas_wide = gldas_df[feature_columns].copy()
                        gldas_wide = gldas_wide.drop_duplicates(['station_id', 'date'])

                        before_count = len(final_wide)
                        final_wide = final_wide.merge(gldas_wide, on=['station_id', 'date'], how='left')
                        after_count = len(final_wide)

                        for col in available_columns:
                            valid_count = final_wide[col].notna().sum()
                            self.logger.info(f"GLDASç‰¹å¾ {col}: {valid_count} æœ‰æ•ˆè®°å½•")

                        self.logger.info(f"åˆå¹¶GLDASæ•°æ®: {before_count} -> {after_count} è¡Œ")

              # 7. å¤„ç†ç§¯é›ªç‰©å€™æ•°æ®
            if phenology_dfs:
                for phenology_name, phenology_df in phenology_dfs:
                    self.logger.info(f"å¤„ç†ç§¯é›ªç‰©å€™æ•°æ® {phenology_name}: {len(phenology_df)} è¡Œ")

                    if 'dataset_type' in phenology_df.columns and 'hydrological_year' in phenology_df.columns:
                        value_col = 'day_of_year' if 'day_of_year' in phenology_df.columns else 'value'

                        if value_col in phenology_df.columns:
                            # åˆ†ç¦»åˆæ—¥å’Œç»ˆæ—¥æ•°æ®
                            start_data = phenology_df[phenology_df['dataset_type'] == 'start'][
                                ['station_id', 'hydrological_year', value_col]].copy()
                            start_data = start_data.rename(columns={value_col: f'{phenology_name}_start'})
                            start_data = start_data.drop_duplicates(['station_id', 'hydrological_year'])

                            end_data = phenology_df[phenology_df['dataset_type'] == 'end'][
                                ['station_id', 'hydrological_year', value_col]].copy()
                            end_data = end_data.rename(columns={value_col: f'{phenology_name}_end'})
                            end_data = end_data.drop_duplicates(['station_id', 'hydrological_year'])

                            # æ·»åŠ æ°´æ–‡å¹´åˆ—
                            final_wide = final_wide.copy()
                            final_wide['hydrological_year'] = final_wide['date'].apply(
                                lambda x: self._get_hydrological_year_from_str(x)
                            )

                            # åˆå¹¶åˆæ—¥æ•°æ®
                            if not start_data.empty:
                                before_count = final_wide[
                                    f'{phenology_name}_start'].notna().sum() if f'{phenology_name}_start' in final_wide.columns else 0
                                final_wide = final_wide.merge(start_data, on=['station_id', 'hydrological_year'],
                                                              how='left')
                                after_count = final_wide[f'{phenology_name}_start'].notna().sum()
                                self.logger.info(f"åˆå¹¶åˆæ—¥æ•°æ®: {before_count} -> {after_count} æœ‰æ•ˆè®°å½•")

                            # åˆå¹¶ç»ˆæ—¥æ•°æ®
                            if not end_data.empty:
                                before_count = final_wide[
                                    f'{phenology_name}_end'].notna().sum() if f'{phenology_name}_end' in final_wide.columns else 0
                                final_wide = final_wide.merge(end_data, on=['station_id', 'hydrological_year'],
                                                              how='left')
                                after_count = final_wide[f'{phenology_name}_end'].notna().sum()
                                self.logger.info(f"åˆå¹¶ç»ˆæ—¥æ•°æ®: {before_count} -> {after_count} æœ‰æ•ˆè®°å½•")

                            # ç§»é™¤ä¸´æ—¶åˆ—
                            final_wide = final_wide.drop('hydrological_year', axis=1)

            # 8. å¤„ç†å¹´åº¦æ•°æ®
            if yearly_dfs:
                for yearly_name, yearly_df in yearly_dfs:
                    self.logger.info(f"å¤„ç†å¹´åº¦æ•°æ® {yearly_name}")

                    if 'year' not in yearly_df.columns:
                        if 'date' in yearly_df.columns:
                            yearly_df = yearly_df.copy()
                            yearly_df['year'] = pd.to_datetime(yearly_df['date']).dt.year
                        else:
                            self.logger.warning(f"å¹´åº¦æ•°æ® {yearly_name} ç¼ºå°‘å¹´ä»½ä¿¡æ¯ï¼Œè·³è¿‡")
                            continue

                    numeric_cols = yearly_df.select_dtypes(include=['number']).columns
                    value_cols = [col for col in numeric_cols if col not in ['station_id', 'date', 'year']]

                    if not value_cols:
                        self.logger.warning(f"å¹´åº¦æ•°æ® {yearly_name} æ²¡æœ‰æ•°å€¼åˆ—ï¼Œè·³è¿‡")
                        continue

                    value_col = value_cols[0]
                    yearly_mapping = yearly_df.set_index(['station_id', 'year'])[value_col].to_dict()

                    final_wide = final_wide.copy()
                    if 'year' not in final_wide.columns:
                        final_wide['year'] = pd.to_datetime(final_wide['date']).dt.year

                    def get_yearly_value(row):
                        key = (row['station_id'], row['year'])
                        return yearly_mapping.get(key)

                    final_wide[yearly_name] = final_wide.apply(get_yearly_value, axis=1)

                    filled_count = final_wide[yearly_name].notna().sum()
                    self.logger.info(f"å¹´åº¦æ•°æ® {yearly_name}: å¡«å……äº† {filled_count} æ¡è®°å½•")

                    final_wide = final_wide.drop('year', axis=1)

            # 9. åˆå¹¶æ‰€æœ‰é™æ€æ•°æ®æº
            if static_dfs:
                static_combined = static_dfs[0][1]

                for i in range(1, len(static_dfs)):
                    name, static_df = static_dfs[i]
                    before_cols = len(static_combined.columns)
                    static_combined = static_combined.merge(static_df, on='station_id', how='outer')
                    after_cols = len(static_combined.columns)
                    added_cols = after_cols - before_cols
                    self.logger.info(f"åˆå¹¶é™æ€æ•°æ®æº {name}: æ·»åŠ  {added_cols} åˆ—")

                before_count = len(final_wide)
                final_wide = final_wide.merge(static_combined, on='station_id', how='left')
                after_count = len(final_wide)
                self.logger.info(f"åˆå¹¶æ‰€æœ‰é™æ€æ•°æ®: {before_count} -> {after_count} è¡Œ")

            # 10. ç¡®ä¿åæ ‡ä¿¡æ¯å®Œæ•´
            final_wide = self._ensure_complete_coordinates(final_wide)

            # 11. æ’åºå’Œæ•´ç†
            final_wide = final_wide.sort_values(['station_id', 'date']).reset_index(drop=True)

            # æœ€ç»ˆæ•°æ®éªŒè¯
            self._validate_final_wide_table(final_wide)

            self.logger.info(f"âœ… å®½è¡¨åˆ›å»ºå®Œæˆ: {final_wide.shape}")
            return final_wide

        except Exception as e:
            self.logger.error(f"åˆ›å»ºå®½è¡¨å¤±è´¥: {e}")
            import traceback
            self.logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return pd.DataFrame()

    def _standardize_station_id_format(self, df, source_name):
        """ç»Ÿä¸€station_idä¸ºå­—ç¬¦ä¸²æ ¼å¼"""
        try:
            if 'station_id' not in df.columns:
                return df

            df_copy = df.copy()

            # æ£€æŸ¥å½“å‰station_idçš„æ•°æ®ç±»å‹
            station_id_dtype = str(df_copy['station_id'].dtype)
            self.logger.debug(f"{source_name} station_idç±»å‹: {station_id_dtype}")

            # è½¬æ¢ä¸ºç»Ÿä¸€çš„å­—ç¬¦ä¸²æ ¼å¼
            if station_id_dtype != 'object':
                df_copy['station_id'] = df_copy['station_id'].astype(str)
                self.logger.debug(f"{source_name}: è½¬æ¢station_idåˆ°å­—ç¬¦ä¸²æ ¼å¼")

            return df_copy

        except Exception as e:
            self.logger.error(f"æ ‡å‡†åŒ– {source_name} station_idæ ¼å¼å¤±è´¥: {str(e)}")
            return df

    def _add_coordinate_info(self, df, coordinate_dfs):
        """æ·»åŠ åæ ‡ä¿¡æ¯åˆ°æ•°æ®æ¡†"""
        try:
            if not coordinate_dfs:
                return df

            # ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ…å«åæ ‡ä¿¡æ¯çš„æ•°æ®æº
            coord_source_name, coord_df = coordinate_dfs[0]
            self.logger.info(f"ä» {coord_source_name} æå–åæ ‡ä¿¡æ¯")

            coord_columns = self._get_coordinate_columns(coord_df)
            if coord_columns:
                # æ ‡å‡†åŒ–åæ ‡åˆ—å
                coord_mapping = {}
                for col in coord_columns:
                    if col.lower() in ['longitude', 'lon']:
                        coord_mapping[col] = 'longitude'
                    elif col.lower() in ['latitude', 'lat']:
                        coord_mapping[col] = 'latitude'

                coord_df_standardized = coord_df.rename(columns=coord_mapping)
                coord_mapping_df = coord_df_standardized[['station_id', 'longitude', 'latitude']].drop_duplicates(
                    'station_id')

                # åˆå¹¶åæ ‡ä¿¡æ¯
                before_coord_count = df[['longitude', 'latitude']].notna().all(
                    axis=1).sum() if 'longitude' in df.columns and 'latitude' in df.columns else 0
                df = df.merge(coord_mapping_df, on='station_id', how='left', suffixes=('', '_dup'))

                # å¤„ç†é‡å¤åˆ—
                for col in ['longitude', 'latitude']:
                    if f'{col}_dup' in df.columns:
                        # å¦‚æœåŸåˆ—ä¸ºç©ºï¼Œä½¿ç”¨æ–°åˆ—çš„å€¼å¡«å……
                        mask = df[col].isna() & df[f'{col}_dup'].notna()
                        df.loc[mask, col] = df.loc[mask, f'{col}_dup']
                        df = df.drop(f'{col}_dup', axis=1)

                after_coord_count = df[['longitude', 'latitude']].notna().all(axis=1).sum()
                self.logger.info(f"åæ ‡ä¿¡æ¯æ·»åŠ : {before_coord_count} -> {after_coord_count} å®Œæ•´åæ ‡è®°å½•")

            return df

        except Exception as e:
            self.logger.error(f"æ·»åŠ åæ ‡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return df

    def emergency_fix(self, issue_type='all'):
        """åº”æ€¥æ•°æ®ä¿®å¤ - å¢å¼ºç‰ˆæœ¬ï¼Œä¸“é—¨å¤„ç†æ•°æ®ç±»å‹é—®é¢˜"""
        try:
            fix_count = 0

            if not self.source_data:
                self.logger.warning("æ²¡æœ‰æ•°æ®æºå¯ä¿®å¤")
                return fix_count

            # å¼ºåˆ¶æ ‡å‡†åŒ–æ‰€æœ‰æ•°æ®æºçš„station_idæ ¼å¼
            self.logger.info("=== å¼ºåˆ¶æ ‡å‡†åŒ–æ‰€æœ‰station_idæ ¼å¼ ===")
            for name, df in self.source_data.items():
                if 'station_id' in df.columns:
                    original_type = str(df['station_id'].dtype)
                    fixed_df = self._standardize_station_id_format(df, f"ä¿®å¤-{name}")
                    new_type = str(fixed_df['station_id'].dtype)

                    if original_type != new_type:
                        self.source_data[name] = fixed_df
                        fix_count += 1
                        self.logger.info(f"âœ… ä¿®å¤ {name}: station_id {original_type} -> {new_type}")

            # å¤„ç†é‡å¤æ•°æ®
            if issue_type in ['all', 'duplicates']:
                fix_count += self._fix_duplicates()

            # å¤„ç†åˆ—åé—®é¢˜
            if issue_type in ['all', 'column_names']:
                fix_count += self._fix_column_names()

            self.logger.info(f"ğŸ¯ åº”æ€¥ä¿®å¤å®Œæˆ: æ€»å…±æ‰§è¡Œäº† {fix_count} ä¸ªä¿®å¤")
            return fix_count

        except Exception as e:
            self.logger.error(f"åº”æ€¥ä¿®å¤å¤±è´¥: {e}")
            return 0

    def _fix_duplicates(self):
        """ä¿®å¤é‡å¤æ•°æ®"""
        fix_count = 0
        for name, df in self.source_data.items():
            original_count = len(df)
            df_clean = df.drop_duplicates()
            removed_count = original_count - len(df_clean)
            if removed_count > 0:
                self.source_data[name] = df_clean
                fix_count += removed_count
                self.logger.info(f"âœ… å»é‡ {name}: ç§»é™¤ {removed_count} ä¸ªé‡å¤è¡Œ")
        return fix_count

    def _fix_column_names(self):
        """ä¿®å¤åˆ—åé—®é¢˜"""
        fix_count = 0
        for name, df in self.source_data.items():
            column_mapping = {}
            if 'station_ID' in df.columns and 'station_id' not in df.columns:
                column_mapping['station_ID'] = 'station_id'
            if 'Station_ID' in df.columns and 'station_id' not in df.columns:
                column_mapping['Station_ID'] = 'station_id'

            if column_mapping:
                df_renamed = df.rename(columns=column_mapping)
                self.source_data[name] = df_renamed
                fix_count += len(column_mapping)
                self.logger.info(f"âœ… åˆ—åä¿®å¤ {name}: {list(column_mapping.keys())}")
        return fix_count

    def _ensure_complete_coordinates(self, df):
        """ç¡®ä¿åæ ‡ä¿¡æ¯å®Œæ•´"""
        try:
            # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘åæ ‡ä¿¡æ¯
            if 'longitude' not in df.columns or 'latitude' not in df.columns:
                self.logger.info("å°è¯•ä»æ‰€æœ‰æ•°æ®æºè¡¥å……åæ ‡ä¿¡æ¯")
                df = self._supplement_coordinates_from_all_sources(df)

            # ç»Ÿè®¡åæ ‡å®Œæ•´æ€§
            if 'longitude' in df.columns and 'latitude' in df.columns:
                complete_coords = df[['longitude', 'latitude']].notna().all(axis=1).sum()
                total_records = len(df)
                completeness = (complete_coords / total_records) * 100
                self.logger.info(f"åæ ‡å®Œæ•´æ€§: {complete_coords}/{total_records} ({completeness:.1f}%)")

            return df

        except Exception as e:
            self.logger.error(f"ç¡®ä¿åæ ‡å®Œæ•´å¤±è´¥: {str(e)}")
            return df

    def _supplement_coordinates_from_all_sources(self, df):
        """ä»æ‰€æœ‰æ•°æ®æºè¡¥å……åæ ‡ä¿¡æ¯"""
        try:
            all_coordinates = {}

            for source_name, source_df in self.source_data.items():
                if self._has_coordinate_info(source_df):
                    coord_columns = self._get_coordinate_columns(source_df)
                    if len(coord_columns) >= 2:
                        coord_info = source_df[['station_id'] + coord_columns].drop_duplicates('station_id')

                        for _, row in coord_info.iterrows():
                            station_id = row['station_id']
                            if station_id not in all_coordinates:
                                # ç¡®å®šç»åº¦å’Œçº¬åº¦åˆ—
                                lon_col = next((col for col in coord_columns if col.lower() in ['longitude', 'lon']),
                                               None)
                                lat_col = next((col for col in coord_columns if col.lower() in ['latitude', 'lat']),
                                               None)

                                if lon_col and lat_col and pd.notna(row[lon_col]) and pd.notna(row[lat_col]):
                                    all_coordinates[station_id] = {
                                        'longitude': float(row[lon_col]),
                                        'latitude': float(row[lat_col])
                                    }

            # å°†åæ ‡ä¿¡æ¯æ·»åŠ åˆ°æ•°æ®æ¡†
            if all_coordinates:
                coord_df = pd.DataFrame([
                    {'station_id': sid, 'longitude': info['longitude'], 'latitude': info['latitude']}
                    for sid, info in all_coordinates.items()
                ])

                if 'longitude' in df.columns and 'latitude' in df.columns:
                    # æ›´æ–°ç°æœ‰çš„ç©ºå€¼åæ ‡
                    for station_id, coords in all_coordinates.items():
                        mask = (df['station_id'] == station_id) & (df['longitude'].isna() | df['latitude'].isna())
                        df.loc[mask, 'longitude'] = coords['longitude']
                        df.loc[mask, 'latitude'] = coords['latitude']
                else:
                    # æ·»åŠ æ–°çš„åæ ‡åˆ—
                    df = df.merge(coord_df, on='station_id', how='left')

                self.logger.info(f"ä» {len(all_coordinates)} ä¸ªæ•°æ®æºè¡¥å……äº†åæ ‡ä¿¡æ¯")

            return df

        except Exception as e:
            self.logger.error(f"è¡¥å……åæ ‡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return df

    def _get_hydrological_year_from_str(self, date_str):
        """ä»æ—¥æœŸå­—ç¬¦ä¸²è®¡ç®—æ°´æ–‡å¹´"""
        try:
            date = pd.to_datetime(date_str)
            if date.month >= 9 or (date.month == 9 and date.day >= 1):
                return date.year
            else:
                return date.year - 1
        except:
            return None

    def _is_gldas_data(self, source_name, df):
        """åˆ¤æ–­æ˜¯å¦ä¸ºGLDASæ•°æ®"""
        gldas_keywords = ['gldas', 'seasonal_doy']
        if any(keyword in source_name.lower() for keyword in gldas_keywords):
            return True

        gldas_columns = ['seasonal_doy_Da', 'seasonal_doy_Db', 'seasonal_doy_Dc', 'seasonal_doy_Dd']
        return any(col in df.columns for col in gldas_columns)

    def _is_snow_phenology_data(self, source_name, df):
        """åˆ¤æ–­æ˜¯å¦ä¸ºç§¯é›ªç‰©å€™æ•°æ®"""
        snow_keywords = ['snow_phenology', 'snow_start', 'snow_end', 'phenology', 'scp']
        if any(keyword in source_name.lower() for keyword in snow_keywords):
            return True

        return 'dataset_type' in df.columns and 'hydrological_year' in df.columns

    def _is_terrain_features(self, source_name, df):
        """åˆ¤æ–­æ˜¯å¦ä¸ºåœ°å½¢ç‰¹å¾æ•°æ®"""
        if 'terrain' in source_name.lower() or 'feature' in source_name.lower():
            return True

        numeric_cols = df.select_dtypes(include=['number']).columns
        feature_cols = [col for col in numeric_cols if col not in ['station_id', 'date', 'longitude', 'latitude']]

        return len(feature_cols) > 1 and 'date' not in df.columns

    def _is_yearly_data(self, source_name, df):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¹´åº¦æ•°æ®"""
        if 'landcover' in source_name.lower():
            return True

        if 'station_id' in df.columns:
            station_counts = df.groupby('station_id').size()
            return station_counts.mean() < 5

        return False

    def _handle_static_only_case(self, static_dfs, yearly_dfs, phenology_dfs, gldas_dfs):
        """å¤„ç†åªæœ‰é™æ€æ•°æ®çš„æƒ…å†µ"""
        try:
            self.logger.info("å¤„ç†çº¯é™æ€æ•°æ®æƒ…å†µ")

            all_dfs = []

            for name, df in static_dfs + yearly_dfs + phenology_dfs + gldas_dfs:
                df_copy = df.copy()
                if 'date' not in df_copy.columns:
                    df_copy['date'] = datetime.now().strftime('%Y-%m-%d')
                all_dfs.append(df_copy)

            if not all_dfs:
                return pd.DataFrame()

            final_df = all_dfs[0]
            for i in range(1, len(all_dfs)):
                common_keys = ['station_id', 'date']
                available_keys = [key for key in common_keys if key in final_df.columns and key in all_dfs[i].columns]

                if available_keys:
                    final_df = final_df.merge(all_dfs[i], on=available_keys, how='outer')

            self.logger.info(f"çº¯é™æ€æ•°æ®åˆå¹¶å®Œæˆ: {final_df.shape}")
            return final_df

        except Exception as e:
            self.logger.error(f"å¤„ç†çº¯é™æ€æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def _validate_final_wide_table(self, df):
        """éªŒè¯æœ€ç»ˆå®½è¡¨æ•°æ® - åŒ…å«æµ·æ‹”æ•°æ®"""
        self.logger.info("=== æœ€ç»ˆå®½è¡¨éªŒè¯ ===")
        self.logger.info(f"æ€»è¡Œæ•°: {len(df)}")
        self.logger.info(f"æ€»åˆ—æ•°: {len(df.columns)}")

        # æ£€æŸ¥åæ ‡ä¿¡æ¯
        if 'longitude' in df.columns and 'latitude' in df.columns:
            coord_count = df[['longitude', 'latitude']].notna().all(axis=1).sum()
            coord_percentage = (coord_count / len(df)) * 100
            self.logger.info(f"åæ ‡ä¿¡æ¯å®Œæ•´æ€§: {coord_count}/{len(df)} ({coord_percentage:.1f}%)")

        # æ£€æŸ¥SWEæ•°æ®
        if 'swe' in df.columns:
            swe_count = df['swe'].notna().sum()
            swe_percentage = (swe_count / len(df)) * 100
            self.logger.info(f"SWEæ•°æ®å®Œæ•´æ€§: {swe_count}/{len(df)} ({swe_percentage:.1f}%)")

        # æ£€æŸ¥æµ·æ‹”æ•°æ®
        if 'altitude' in df.columns:
            altitude_count = df['altitude'].notna().sum()
            altitude_percentage = (altitude_count / len(df)) * 100
            altitude_stats = df['altitude'].describe()
            self.logger.info(f"æµ·æ‹”æ•°æ®å®Œæ•´æ€§: {altitude_count}/{len(df)} ({altitude_percentage:.1f}%)")
            self.logger.info(
                f"æµ·æ‹”ç»Ÿè®¡ - å‡å€¼: {altitude_stats['mean']:.2f}, èŒƒå›´: {altitude_stats['min']:.2f}-{altitude_stats['max']:.2f}")

        # æ˜¾ç¤ºå‰5è¡Œæ•°æ®ç¤ºä¾‹
        if len(df) > 0:
            self.logger.info("å‰5è¡Œæ•°æ®ç¤ºä¾‹:")
            sample_cols = ['station_id', 'date']
            if 'longitude' in df.columns and 'latitude' in df.columns:
                sample_cols.extend(['longitude', 'latitude'])
            if 'swe' in df.columns:
                sample_cols.append('swe')
            if 'altitude' in df.columns:
                sample_cols.append('altitude')

            # æ·»åŠ å…¶ä»–æ•°æ®åˆ—
            other_cols = [col for col in df.columns if col not in sample_cols]
            sample_cols.extend(other_cols[:5])  # æ˜¾ç¤ºå‰5ä¸ªå…¶ä»–åˆ—

            sample_df = df[sample_cols].head(5)
            for _, row in sample_df.iterrows():
                values = []
                for col in sample_cols:
                    if col not in ['station_id', 'date'] and pd.notna(row[col]):
                        values.append(f"{col}: {row[col]}")
                value_str = ", ".join(values) if values else "æ— æ•°æ®"
                self.logger.info(f"  ç«™ç‚¹ {row['station_id']} | æ—¥æœŸ {row['date']} | {value_str}")

    def save_master_excel(self, format_type='wide'):
        """ä¿å­˜ä¸»Excelæ–‡ä»¶"""
        if not self.source_data:
            self.logger.error("æ²¡æœ‰æ•°æ®æºå¯æ•´åˆ")
            return None

        try:
            # å…ˆæ‰§è¡Œæ•°æ®ä¿®å¤
            fix_count = self.emergency_fix()
            self.logger.info(f"æ•°æ®ä¿®å¤å®Œæˆ: {fix_count} ä¸ªé—®é¢˜å·²ä¿®å¤")

            # åˆ›å»ºå®½è¡¨
            if format_type == 'wide':
                final_df = self._create_correct_wide_table()
            else:
                final_df = self.get_combined_data()

            if final_df is None or final_df.empty:
                self.logger.error("æœ€ç»ˆæ•°æ®ä¸ºç©ºæˆ–ä¸ºNone")
                return None

            # æ•°æ®å®Œæ•´æ€§éªŒè¯
            final_df = self._validate_data_integrity(final_df)

            if final_df.empty:
                self.logger.error("éªŒè¯åæ•°æ®ä¸ºç©º")
                return None

            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = self.output_dir / f"integrated_data_{timestamp}.xlsx"

            # ä¿å­˜Excelæ–‡ä»¶
            final_df.to_excel(output_path, index=False)
            self.logger.info(f"âœ… æ•´åˆæ•°æ®ä¿å­˜æˆåŠŸ: {output_path}")

            return str(output_path)

        except Exception as e:
            self.logger.error(f"ä¿å­˜å¤±è´¥: {e}")
            self.logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None

    def get_swe_from_database(self, station_ids, start_date, end_date):
        """ä»æ•°æ®åº“è·å–SWEæ•°æ®å’Œæµ·æ‹”é«˜åº¦ - å¢å¼ºç‰ˆæœ¬"""
        try:
            if not self.db_conn:
                if not self.connect_database():
                    return pd.DataFrame()

            # æ ¹æ®å®é™…æ•°æ®åº“åˆ—åè°ƒæ•´
            swe_column = 'sweï¼ˆmmï¼‰'
            station_id_column = 'station_ID'
            time_column = 'time'
            altitude_column = 'Altitude(m)'  # æ–°å¢æµ·æ‹”åˆ—

            self.logger.info(f"è·å–æ•°æ®åº“æ•°æ® - SWEåˆ—: '{swe_column}', æµ·æ‹”åˆ—: '{altitude_column}'")

            # æ„å»ºæŸ¥è¯¢ - åŒæ—¶è·å–SWEå’Œæµ·æ‹”æ•°æ®
            query = f"""
            SELECT {station_id_column}, {time_column}, "{swe_column}", "{altitude_column}"
            FROM stations 
            WHERE "{swe_column}" IS NOT NULL 
                AND {time_column} BETWEEN ? AND ?
            """

            # è½¬æ¢æ—¥æœŸæ ¼å¼ä¸ºæ•°æ®åº“ä¸­çš„æ ¼å¼ (YYYYMMDD)
            start_db = int(start_date.strftime('%Y%m%d'))
            end_db = int(end_date.strftime('%Y%m%d'))

            params = [start_db, end_db]
            df = pd.read_sql_query(query, self.db_conn, params=params)

            if not df.empty:
                # è½¬æ¢æ•°æ®åº“æ ¼å¼çš„æ—¥æœŸ (YYYYMMDD -> YYYY-MM-DD)
                df['date'] = df[time_column].apply(
                    lambda x: datetime.strptime(str(int(x)), '%Y%m%d').strftime('%Y-%m-%d')
                )

                # æ ‡å‡†åŒ–ç«™ç‚¹IDåˆ—å
                df['station_id'] = df[station_id_column].astype(str)

                # é€‰æ‹©éœ€è¦çš„åˆ—ï¼ŒåŒ…æ‹¬æµ·æ‹”
                result_df = df[['station_id', 'date', swe_column, altitude_column]].copy()
                result_df = result_df.rename(columns={
                    swe_column: 'swe',
                    altitude_column: 'altitude'
                })

                # ç«™ç‚¹åŒ¹é…è°ƒè¯•
                needed_stations = set(station_ids)
                available_stations = set(result_df['station_id'].unique())
                matched_stations = needed_stations & available_stations

                self.logger.info(f"ç«™ç‚¹åŒ¹é…è¯¦æƒ…:")
                self.logger.info(f"  æ‰€éœ€ç«™ç‚¹æ•°é‡: {len(needed_stations)}")
                self.logger.info(f"  æ•°æ®åº“æœ‰æ•°æ®çš„ç«™ç‚¹æ•°é‡: {len(available_stations)}")
                self.logger.info(f"  åŒ¹é…çš„ç«™ç‚¹æ•°é‡: {len(matched_stations)}")

                if matched_stations:
                    result_df = result_df[result_df['station_id'].isin(matched_stations)]

                    # æ•°æ®ç»Ÿè®¡
                    swe_stats = result_df['swe'].describe()
                    altitude_stats = result_df['altitude'].describe()
                    unique_dates = result_df['date'].nunique()

                    self.logger.info(f"âœ… æˆåŠŸè·å–æ•°æ®åº“æ•°æ®: {len(result_df)} æ¡è®°å½•")
                    self.logger.info(f"  SWEæ•°æ®è¦†ç›– {unique_dates} ä¸ªæ—¥æœŸ")
                    self.logger.info(
                        f"  SWEç»Ÿè®¡ - å‡å€¼: {swe_stats['mean']:.2f}, èŒƒå›´: {swe_stats['min']:.2f}-{swe_stats['max']:.2f}")
                    self.logger.info(
                        f"  æµ·æ‹”ç»Ÿè®¡ - å‡å€¼: {altitude_stats['mean']:.2f}, èŒƒå›´: {altitude_stats['min']:.2f}-{altitude_stats['max']:.2f}")

                    return result_df
                else:
                    self.logger.warning("æ²¡æœ‰åŒ¹é…çš„ç«™ç‚¹ID")
                    return pd.DataFrame()
            else:
                self.logger.warning("æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            self.logger.error(f"è·å–æ•°æ®åº“æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def get_static_altitude_data(self, station_ids):
        """è·å–ç«™ç‚¹çš„é™æ€æµ·æ‹”æ•°æ®ï¼ˆæ¯ä¸ªç«™ç‚¹ä¸€ä¸ªæµ·æ‹”å€¼ï¼‰"""
        try:
            if not self.db_conn:
                if not self.connect_database():
                    return pd.DataFrame()

            altitude_column = 'Altitude(m)'
            station_id_column = 'station_ID'

            # è·å–æ¯ä¸ªç«™ç‚¹çš„å¹³å‡æµ·æ‹”ï¼ˆé™æ€ç‰¹å¾ï¼‰
            query = f"""
            SELECT {station_id_column}, AVG("{altitude_column}") as altitude
            FROM stations 
            WHERE "{altitude_column}" IS NOT NULL
            GROUP BY {station_id_column}
            """

            df = pd.read_sql_query(query, self.db_conn)

            if not df.empty:
                df['station_id'] = df[station_id_column].astype(str)
                result_df = df[['station_id', 'altitude']].drop_duplicates('station_id')

                # è¿‡æ»¤éœ€è¦çš„ç«™ç‚¹
                if station_ids:
                    result_df = result_df[result_df['station_id'].isin(station_ids)]

                self.logger.info(f"è·å–é™æ€æµ·æ‹”æ•°æ®: {len(result_df)} ä¸ªç«™ç‚¹")
                return result_df
            else:
                self.logger.warning("æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æµ·æ‹”æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            self.logger.error(f"è·å–é™æ€æµ·æ‹”æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def get_altitude_from_database(self, station_ids):
        """ä»æ•°æ®åº“è·å–æµ·æ‹”æ•°æ®"""
        try:
            if not self.db_conn:
                if not self.connect_database():
                    return pd.DataFrame()

            altitude_column = 'Altitude(m)'
            self.logger.info(f"è·å–æµ·æ‹”æ•°æ®ï¼Œä½¿ç”¨åˆ—: '{altitude_column}'")

            # è·å–æ¯ä¸ªç«™ç‚¹çš„æµ·æ‹”æ•°æ®ï¼ˆå–å¹³å‡å€¼ï¼‰
            query = f"""
            SELECT station_ID, AVG("{altitude_column}") as altitude
            FROM stations 
            WHERE "{altitude_column}" IS NOT NULL
            GROUP BY station_ID
            """

            df = pd.read_sql_query(query, self.db_conn)

            if not df.empty:
                df['station_id'] = df['station_ID'].astype(str)
                df = df[['station_id', 'altitude']]

                # è¿‡æ»¤éœ€è¦çš„ç«™ç‚¹
                if station_ids:
                    df = df[df['station_id'].isin(station_ids)]

                self.logger.info(f"âœ… æˆåŠŸè·å–æµ·æ‹”æ•°æ®: {len(df)} ä¸ªç«™ç‚¹")

                # æ˜¾ç¤ºæµ·æ‹”ç»Ÿè®¡
                altitude_stats = df['altitude'].describe()
                self.logger.info(
                    f"  æµ·æ‹”ç»Ÿè®¡ - å‡å€¼: {altitude_stats['mean']:.2f}, èŒƒå›´: {altitude_stats['min']:.2f}-{altitude_stats['max']:.2f}")

                return df
            else:
                self.logger.warning("æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æµ·æ‹”æ•°æ®")
                return pd.DataFrame()

        except Exception as e:
            self.logger.error(f"è·å–æµ·æ‹”æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def _query_swe_batch(self, station_ids, swe_column, start_db, end_db):
        """åˆ†æ‰¹æŸ¥è¯¢SWEæ•°æ®"""
        try:
            placeholders = ','.join(['?'] * len(station_ids))
            query = f"""
            SELECT station_ID, time, {swe_column} 
            FROM stations 
            WHERE station_ID IN ({placeholders}) AND time BETWEEN ? AND ?
            """
            params = station_ids + [start_db, end_db]

            df = pd.read_sql_query(query, self.db_conn, params=params)
            return df

        except Exception as e:
            self.logger.error(f"åˆ†æ‰¹æŸ¥è¯¢SWEæ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def check_database_structure(self):
        """æ£€æŸ¥æ•°æ®åº“è¡¨ç»“æ„ - è¯¦ç»†ç‰ˆæœ¬"""
        try:
            if not self.db_conn:
                if not self.connect_database():
                    return None

            # è·å–è¡¨ç»“æ„ä¿¡æ¯
            tables_query = "SELECT name FROM sqlite_master WHERE type='table';"
            tables = pd.read_sql_query(tables_query, self.db_conn)
            self.logger.info(f"æ•°æ®åº“ä¸­çš„è¡¨: {tables['name'].tolist()}")

            # æ£€æŸ¥stationsè¡¨çš„åˆ—
            if 'stations' in tables['name'].values:
                columns_query = "PRAGMA table_info(stations);"
                columns = pd.read_sql_query(columns_query, self.db_conn)
                column_names = columns['name'].tolist()
                self.logger.info(f"stationsè¡¨çš„å®Œæ•´åˆ—ä¿¡æ¯:")
                for _, row in columns.iterrows():
                    self.logger.info(f"  - {row['name']} ({row['type']})")
                return column_names
            else:
                self.logger.error("stationsè¡¨ä¸å­˜åœ¨")
                return None

        except Exception as e:
            self.logger.error(f"æ£€æŸ¥æ•°æ®åº“ç»“æ„å¤±è´¥: {str(e)}")
            return None

    def get_combined_data(self):
        """åˆå¹¶æ•°æ®"""
        if not self.source_data:
            return pd.DataFrame()

        dfs = []
        for name, df in self.source_data.items():
            df_copy = df.copy()
            df_copy['data_source'] = name
            dfs.append(df_copy)

        return pd.concat(dfs, ignore_index=True)

    def _validate_data_integrity(self, df):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼Œç¡®ä¿æ²¡æœ‰è™šå‡çš„ç«™ç‚¹-æ—¥æœŸç»„åˆ"""
        if df.empty:
            return df

        # è·å–æ•°æ®åˆ—
        data_cols = [col for col in df.columns if col not in ['station_id', 'date', 'data_source']]

        if not data_cols:
            return df

        # ç»Ÿè®¡æ¯ä¸ªæ•°æ®æºçš„æœ‰æ•ˆè®°å½•æ•°
        self.logger.info("=== æ•°æ®æœ‰æ•ˆæ€§éªŒè¯ ===")
        for col in data_cols:
            valid_count = df[col].notna().sum()
            total_count = len(df)
            fill_rate = (valid_count / total_count) * 100 if total_count > 0 else 0
            self.logger.info(f"{col}: {valid_count}/{total_count} æœ‰æ•ˆè®°å½• ({fill_rate:.1f}%)")

        # è¯†åˆ«å¯èƒ½çš„é—®é¢˜è®°å½•
        empty_records = df[data_cols].isna().all(axis=1)
        if empty_records.any():
            empty_count = empty_records.sum()
            self.logger.warning(f"å‘ç° {empty_count} æ¡å…¨ç©ºè®°å½•")

            # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
            empty_examples = df[empty_records][['station_id', 'date']].head(5)
            self.logger.info("å…¨ç©ºè®°å½•ç¤ºä¾‹:")
            for _, row in empty_examples.iterrows():
                self.logger.info(f"  ç«™ç‚¹ {row['station_id']}, æ—¥æœŸ {row['date']}")

        return df

    def generate_report(self):
        """ç”Ÿæˆæ•°æ®æ•´åˆæŠ¥å‘Š"""
        try:
            report_lines = []
            report_lines.append("=== æ•°æ®æ•´åˆæŠ¥å‘Š ===")
            report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report_lines.append("")

            # æ•°æ®æºç»Ÿè®¡
            report_lines.append("=== æ•°æ®æºç»Ÿè®¡ ===")
            for name, df in self.source_data.items():
                station_count = df['station_id'].nunique() if 'station_id' in df.columns else 0
                date_count = df['date'].nunique() if 'date' in df.columns else "N/A"
                record_count = len(df)

                report_lines.append(f"{name}:")
                report_lines.append(f"  - è®°å½•æ•°: {record_count}")
                report_lines.append(f"  - ç«™ç‚¹æ•°: {station_count}")
                report_lines.append(f"  - æ—¥æœŸæ•°: {date_count}")

                # æ£€æŸ¥åæ ‡ä¿¡æ¯
                if self._has_coordinate_info(df):
                    coord_columns = self._get_coordinate_columns(df)
                    report_lines.append(f"  - åæ ‡ä¿¡æ¯: {coord_columns}")

                report_lines.append("")

            # ç‰¹å¾åˆ—ç»Ÿè®¡
            report_lines.append("=== ç‰¹å¾åˆ—ç»Ÿè®¡ ===")
            all_columns = set()
            for name, df in self.source_data.items():
                all_columns.update(df.columns)

            # æ’é™¤IDå’Œæ—¥æœŸåˆ—
            feature_columns = [col for col in all_columns if col not in ['station_id', 'date', 'data_source']]
            report_lines.append(f"æ€»ç‰¹å¾åˆ—æ•°: {len(feature_columns)}")
            report_lines.append("ç‰¹å¾åˆ—åˆ—è¡¨:")
            for col in sorted(feature_columns):
                report_lines.append(f"  - {col}")

            return "\n".join(report_lines)

        except Exception as e:
            self.logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
            return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"

    def clear_data(self):
        """æ¸…ç©ºæ•°æ®"""
        self.source_data.clear()
        self.logger.info("æ•°æ®å·²æ¸…ç©º")

    def close(self):
        """å…³é—­èµ„æº"""
        try:
            if self.db_conn:
                self.db_conn.close()
                self.logger.debug("æ•°æ®åº“è¿æ¥å·²å…³é—­")
        except Exception as e:
            self.logger.warning(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {str(e)}")

    def __del__(self):
        """ææ„å‡½æ•°"""
        self.close()