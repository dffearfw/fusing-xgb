import traceback
import pandas as pd
import logging
from pathlib import Path

from datetime import datetime


class DataIntegrator:
    def __init__(self, output_dir, secure_processor=None):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger("DataIntegrator")
        self.source_data = {}

    def add_source(self, name, file_path):
        """æ·»åŠ æ•°æ®æº - å¢å¼ºç‰ˆæœ¬"""
        try:
            path = Path(file_path)
            if path.suffix.lower() == '.parquet':
                df = pd.read_parquet(path)
            else:
                df = pd.read_csv(path)

            # æ•°æ®éªŒè¯
            validation_result = self._validate_dataframe(df, name)
            if not validation_result['valid']:
                self.logger.warning(f"æ•°æ®æº {name} éªŒè¯è­¦å‘Š: {validation_result['message']}")
                # ç»§ç»­å¤„ç†ï¼Œä½†è®°å½•è­¦å‘Š

            self.source_data[name] = df
            self.logger.info(f"æ·»åŠ  {name}: {len(df)} è¡Œ, åˆ—: {list(df.columns)}")
            return True

        except Exception as e:
            self.logger.error(f"æ·»åŠ  {name} å¤±è´¥: {e}")
            return False

    def _validate_dataframe(self, df, source_name):
        """éªŒè¯DataFrameç»“æ„"""
        validation = {'valid': True, 'message': ''}

        # æ£€æŸ¥å¿…éœ€åˆ—
        if 'station_id' not in df.columns:
            validation['valid'] = False
            validation['message'] = "ç¼ºå°‘station_idåˆ—"
            return validation

        # æ£€æŸ¥æ•°æ®ç±»å‹
        numeric_cols = df.select_dtypes(include=['number']).columns
        value_cols = [col for col in numeric_cols if col not in ['station_id', 'date', 'year', 'month', 'day']]

        if not value_cols:
            validation['valid'] = False
            validation['message'] = "æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—"
            return validation

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        station_count = df['station_id'].nunique()
        if station_count == 0:
            validation['valid'] = False
            validation['message'] = "æ²¡æœ‰æœ‰æ•ˆçš„ç«™ç‚¹æ•°æ®"
            return validation

        # åˆ¤æ–­æ•°æ®æºç±»å‹
        if 'date' in df.columns:
            date_count = df['date'].nunique()
            validation['message'] = f"åŠ¨æ€æ•°æ®æºï¼Œ{station_count}ä¸ªç«™ç‚¹ï¼Œ{date_count}ä¸ªæ—¶é—´ç‚¹"
        else:
            validation['message'] = f"é™æ€æ•°æ®æºï¼Œ{station_count}ä¸ªç«™ç‚¹"

        return validation

    def save_master_excel(self, format_type='wide'):
        """ä¿å­˜ä¸»Excelæ–‡ä»¶ - ä½¿ç”¨ä¿®å¤çš„å®½è¡¨ç‰ˆæœ¬"""
        if not self.source_data:
            self.logger.error("æ²¡æœ‰æ•°æ®æºå¯æ•´åˆ")
            return None

        try:
            # å…ˆæ‰§è¡Œæ•°æ®ä¿®å¤
            fix_count = self.emergency_fix()
            if fix_count is None:
                fix_count = 0

            self.logger.info(f"æ•°æ®ä¿®å¤å®Œæˆ: {fix_count} ä¸ªé—®é¢˜å·²ä¿®å¤")

            # åˆ›å»ºä¿®å¤åçš„å®½è¡¨
            if format_type == 'wide':
                final_df = self._create_correct_wide_table()  # ä½¿ç”¨ä¿®å¤ç‰ˆæœ¬
                # æˆ–è€…ä½¿ç”¨ç®€å•ä¿®å¤ç‰ˆæœ¬ï¼š
                # final_df = self._create_simple_fixed_wide_table()
            else:
                final_df = self.get_combined_data()

            if final_df is None or final_df.empty:
                self.logger.error("æœ€ç»ˆæ•°æ®ä¸ºç©ºæˆ–ä¸ºNone")
                return None

            # éªŒè¯landcoveræ•°æ®çš„å¡«å……æƒ…å†µ
            if 'landcover' in final_df.columns:
                landcover_filled = final_df['landcover'].notna().sum()
                total_records = len(final_df)
                fill_rate = (landcover_filled / total_records) * 100
                self.logger.info(f"Landcoveræ•°æ®å¡«å……: {landcover_filled}/{total_records} ({fill_rate:.1f}%)")

            # æ•°æ®å®Œæ•´æ€§éªŒè¯
            final_df = self._validate_data_integrity(final_df)

            if final_df.empty:
                self.logger.error("éªŒè¯åæ•°æ®ä¸ºç©º")
                return None

            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = self.output_dir / f"integrated_data_{timestamp}.xlsx"

            # ä¿å­˜Excelæ–‡ä»¶
            final_df.to_excel(output_path, index=False)
            self.logger.info(f"âœ… æ•´åˆæ•°æ®ä¿å­˜æˆåŠŸ: {output_path}")

            # æ•°æ®éªŒè¯
            self._comprehensive_validation(final_df)

            return str(output_path)

        except Exception as e:
            self.logger.error(f"ä¿å­˜å¤±è´¥: {e}")
            self.logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None

    def _create_correct_wide_table(self):
        """åˆ›å»ºæ­£ç¡®çš„å®½è¡¨ - é‡æ–°ç¼–å†™å®Œæ•´ç‰ˆæœ¬"""
        if not self.source_data:
            return pd.DataFrame()

        self.logger.info("åˆ›å»ºæ­£ç¡®å®½è¡¨...")

        try:
            # 1. åˆ†ç¦»ä¸åŒç±»å‹çš„æ•°æ®æº
            static_dfs = []  # åœ°å½¢ç‰¹å¾ç­‰é™æ€æ•°æ®
            yearly_dfs = []  # æ¯å¹´è®°å½•ä¸€æ¬¡çš„æ•°æ®ï¼ˆå¦‚landcoverï¼‰
            dynamic_dfs = []  # æœ‰æ—¶é—´ç»´åº¦çš„åŠ¨æ€æ•°æ®
            phenology_dfs = []  # ç§¯é›ªç‰©å€™æ•°æ®

            for source_name, source_df in self.source_data.items():
                # è¯†åˆ«ç§¯é›ªç‰©å€™æ•°æ®
                if self._is_snow_phenology_data(source_name, source_df):
                    phenology_dfs.append((source_name, source_df))
                    self.logger.info(f"è¯†åˆ«ä¸ºç§¯é›ªç‰©å€™æ•°æ® {source_name}: {len(source_df)} è¡Œ")

                # æ£€æŸ¥æ•°æ®æºç±»å‹
                elif self._is_terrain_features(source_name, source_df):
                    static_dfs.append((source_name, source_df))
                    self.logger.info(f"è¯†åˆ«ä¸ºåœ°å½¢ç‰¹å¾æ•°æ® {source_name}")

                elif self._is_yearly_data(source_name, source_df):
                    # æ¯å¹´è®°å½•ä¸€æ¬¡çš„æ•°æ®ï¼ˆå¦‚landcoverï¼‰
                    yearly_dfs.append((source_name, source_df))
                    self.logger.info(f"è¯†åˆ«ä¸ºå¹´åº¦æ•°æ® {source_name}: {len(source_df)} è¡Œ")

                elif 'date' in source_df.columns and 'station_id' in source_df.columns:
                    # åŠ¨æ€æ•°æ®æº
                    numeric_cols = source_df.select_dtypes(include=['number']).columns
                    value_cols = [col for col in numeric_cols if
                                  col not in ['station_id', 'date', 'year', 'month', 'day', 'dataset_type']]

                    if value_cols:
                        value_col = value_cols[0]
                        source_wide = source_df[['station_id', 'date', value_col]].copy()
                        source_wide = source_wide.rename(columns={value_col: source_name})
                        source_wide = source_wide.drop_duplicates(['station_id', 'date'])
                        dynamic_dfs.append((source_name, source_wide))
                        self.logger.info(f"å‡†å¤‡åŠ¨æ€æ•°æ®æº {source_name}: {len(source_wide)} è¡Œ")

                elif 'station_id' in source_df.columns:
                    static_dfs.append((source_name, source_df))
                    self.logger.info(f"å‡†å¤‡é™æ€æ•°æ®æº {source_name}: {len(source_df)} è¡Œ")
                else:
                    self.logger.warning(f"æ•°æ®æº {source_name} æ ¼å¼ä¸æ”¯æŒ")

            if not dynamic_dfs and not static_dfs and not yearly_dfs and not phenology_dfs:
                self.logger.error("æ²¡æœ‰å¯ç”¨çš„æ•°æ®æº")
                return pd.DataFrame()

            # 2. å¤„ç†åŠ¨æ€æ•°æ®æºï¼ˆåˆ›å»ºåŸºç¡€æ¡†æ¶ï¼‰
            if dynamic_dfs:
                first_dynamic_name, final_wide = dynamic_dfs[0]
                self.logger.info(f"ä»¥åŠ¨æ€æ•°æ®æº {first_dynamic_name} ä¸ºåŸºç¡€æ¡†æ¶: {len(final_wide)} è¡Œ")

                # åˆå¹¶å…¶ä»–åŠ¨æ€æ•°æ®æº
                for i in range(1, len(dynamic_dfs)):
                    name, next_df = dynamic_dfs[i]
                    before_count = len(final_wide)
                    final_wide = final_wide.merge(next_df, on=['station_id', 'date'], how='left')
                    after_count = len(final_wide)
                    self.logger.info(f"åˆå¹¶åŠ¨æ€æ•°æ®æº {name}: {before_count} -> {after_count} è¡Œ")
            else:
                # å¦‚æœæ²¡æœ‰åŠ¨æ€æ•°æ®æºï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                self.logger.info("æ²¡æœ‰åŠ¨æ€æ•°æ®æºï¼Œå¤„ç†çº¯é™æ€æ•°æ®")
                return self._handle_static_only_case(static_dfs, yearly_dfs, phenology_dfs)

            # 3. å¤„ç†ç§¯é›ªç‰©å€™æ•°æ® - ç®€å•ç›´æ¥çš„æ–¹æ³•
            if phenology_dfs:
                for phenology_name, phenology_df in phenology_dfs:
                    self.logger.info(f"å¤„ç†ç§¯é›ªç‰©å€™æ•°æ® {phenology_name}: {len(phenology_df)} è¡Œ")

                    # æ£€æŸ¥æ•°æ®ç»“æ„
                    self.logger.info(f"ç§¯é›ªç‰©å€™æ•°æ®åˆ—: {list(phenology_df.columns)}")

                    # ç¡®ä¿æœ‰å¿…è¦çš„å­—æ®µ
                    if 'dataset_type' not in phenology_df.columns or 'hydrological_year' not in phenology_df.columns:
                        self.logger.warning(f"ç§¯é›ªç‰©å€™æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µï¼Œè·³è¿‡")
                        continue

                    # ä½¿ç”¨day_of_yearä½œä¸ºæ•°å€¼
                    value_col = 'day_of_year' if 'day_of_year' in phenology_df.columns else 'value'
                    if value_col not in phenology_df.columns:
                        self.logger.warning(f"ç§¯é›ªç‰©å€™æ•°æ®æ²¡æœ‰æ•°å€¼å­—æ®µï¼Œè·³è¿‡")
                        continue

                    # åˆ†ç¦»åˆæ—¥å’Œç»ˆæ—¥æ•°æ®
                    start_data = phenology_df[phenology_df['dataset_type'] == 'start'][
                        ['station_id', 'hydrological_year', value_col]].copy()
                    start_data = start_data.rename(columns={value_col: f'{phenology_name}_start'})
                    start_data = start_data.drop_duplicates(['station_id', 'hydrological_year'])

                    end_data = phenology_df[phenology_df['dataset_type'] == 'end'][
                        ['station_id', 'hydrological_year', value_col]].copy()
                    end_data = end_data.rename(columns={value_col: f'{phenology_name}_end'})
                    end_data = end_data.drop_duplicates(['station_id', 'hydrological_year'])

                    self.logger.info(f"åˆæ—¥æ•°æ®: {len(start_data)} æ¡, ç»ˆæ—¥æ•°æ®: {len(end_data)} æ¡")

                    # ä¸ºfinal_wideæ·»åŠ æ°´æ–‡å¹´åˆ—
                    final_wide = final_wide.copy()
                    final_wide['hydrological_year'] = final_wide['date'].apply(
                        lambda x: self._get_hydrological_year_from_str(x)
                    )

                    # åˆå¹¶åˆæ—¥æ•°æ®
                    if not start_data.empty:
                        before_count = final_wide[
                            f'{phenology_name}_start'].notna().sum() if f'{phenology_name}_start' in final_wide.columns else 0
                        final_wide = final_wide.merge(start_data, on=['station_id', 'hydrological_year'], how='left')
                        after_count = final_wide[f'{phenology_name}_start'].notna().sum()
                        self.logger.info(f"åˆå¹¶åˆæ—¥æ•°æ®: {before_count} -> {after_count} æœ‰æ•ˆè®°å½•")

                    # åˆå¹¶ç»ˆæ—¥æ•°æ®
                    if not end_data.empty:
                        before_count = final_wide[
                            f'{phenology_name}_end'].notna().sum() if f'{phenology_name}_end' in final_wide.columns else 0
                        final_wide = final_wide.merge(end_data, on=['station_id', 'hydrological_year'], how='left')
                        after_count = final_wide[f'{phenology_name}_end'].notna().sum()
                        self.logger.info(f"åˆå¹¶ç»ˆæ—¥æ•°æ®: {before_count} -> {after_count} æœ‰æ•ˆè®°å½•")

                    # ç§»é™¤ä¸´æ—¶åˆ—
                    final_wide = final_wide.drop('hydrological_year', axis=1)

            # 4. å¤„ç†å¹´åº¦æ•°æ®ï¼ˆå¦‚landcoverï¼‰
            if yearly_dfs:
                for yearly_name, yearly_df in yearly_dfs:
                    self.logger.info(f"å¤„ç†å¹´åº¦æ•°æ® {yearly_name}")

                    # ç¡®ä¿å¹´åº¦æ•°æ®æœ‰å¹´ä»½ä¿¡æ¯
                    if 'year' not in yearly_df.columns:
                        # ä»dateåˆ—æå–å¹´ä»½
                        if 'date' in yearly_df.columns:
                            yearly_df = yearly_df.copy()
                            yearly_df['year'] = pd.to_datetime(yearly_df['date']).dt.year
                        else:
                            self.logger.warning(f"å¹´åº¦æ•°æ® {yearly_name} ç¼ºå°‘å¹´ä»½ä¿¡æ¯ï¼Œè·³è¿‡")
                            continue

                    # è·å–å¹´åº¦æ•°æ®çš„æ•°å€¼åˆ—
                    numeric_cols = yearly_df.select_dtypes(include=['number']).columns
                    value_cols = [col for col in numeric_cols if col not in ['station_id', 'date', 'year']]

                    if not value_cols:
                        self.logger.warning(f"å¹´åº¦æ•°æ® {yearly_name} æ²¡æœ‰æ•°å€¼åˆ—ï¼Œè·³è¿‡")
                        continue

                    value_col = value_cols[0]

                    # ä¸ºæ¯ä¸ªç«™ç‚¹-å¹´ä»½ç»„åˆåˆ›å»ºæ˜ å°„
                    yearly_mapping = yearly_df.set_index(['station_id', 'year'])[value_col].to_dict()

                    # åœ¨åŸºç¡€æ¡†æ¶ä¸­æ·»åŠ å¹´ä»½åˆ—
                    final_wide = final_wide.copy()
                    if 'year' not in final_wide.columns:
                        final_wide['year'] = pd.to_datetime(final_wide['date']).dt.year

                    # åº”ç”¨å¹´åº¦æ•°æ®åˆ°æ‰€æœ‰è®°å½•
                    def get_yearly_value(row):
                        key = (row['station_id'], row['year'])
                        return yearly_mapping.get(key)

                    final_wide[yearly_name] = final_wide.apply(get_yearly_value, axis=1)

                    # ç»Ÿè®¡å¡«å……æƒ…å†µ
                    filled_count = final_wide[yearly_name].notna().sum()
                    self.logger.info(f"å¹´åº¦æ•°æ® {yearly_name}: å¡«å……äº† {filled_count} æ¡è®°å½•")

                    # ç§»é™¤ä¸´æ—¶åˆ—
                    final_wide = final_wide.drop('year', axis=1)

            # 5. åˆå¹¶æ‰€æœ‰é™æ€æ•°æ®æº
            if static_dfs:
                static_combined = static_dfs[0][1]  # ç¬¬ä¸€ä¸ªé™æ€æ•°æ®æº

                for i in range(1, len(static_dfs)):
                    name, static_df = static_dfs[i]
                    before_cols = len(static_combined.columns)
                    static_combined = static_combined.merge(static_df, on='station_id', how='outer')
                    after_cols = len(static_combined.columns)
                    added_cols = after_cols - before_cols
                    self.logger.info(f"åˆå¹¶é™æ€æ•°æ®æº {name}: æ·»åŠ  {added_cols} åˆ—")

                # å°†é™æ€æ•°æ®åˆå¹¶åˆ°åŠ¨æ€æ¡†æ¶ä¸­
                before_count = len(final_wide)
                final_wide = final_wide.merge(static_combined, on='station_id', how='left')
                after_count = len(final_wide)
                self.logger.info(f"åˆå¹¶æ‰€æœ‰é™æ€æ•°æ®: {before_count} -> {after_count} è¡Œ")

            # 6. æ’åºå’Œæ•´ç†
            final_wide = final_wide.sort_values(['station_id', 'date']).reset_index(drop=True)

            # æœ€ç»ˆæ•°æ®éªŒè¯
            self._validate_final_wide_table(final_wide)

            self.logger.info(f"âœ… å®½è¡¨åˆ›å»ºå®Œæˆ: {final_wide.shape}")
            return final_wide

        except Exception as e:
            self.logger.error(f"åˆ›å»ºå®½è¡¨å¤±è´¥: {e}")
            import traceback
            self.logger.debug(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return pd.DataFrame()

    def _get_hydrological_year_from_str(self, date_str):
        """ä»æ—¥æœŸå­—ç¬¦ä¸²è®¡ç®—æ°´æ–‡å¹´"""
        try:
            date = pd.to_datetime(date_str)
            # 9æœˆ1æ—¥å¼€å§‹çš„æ°´æ–‡å¹´
            if date.month > 9 or (date.month == 9 and date.day >= 1):
                return date.year
            else:
                return date.year - 1
        except:
            return None

    def _is_snow_phenology_data(self, source_name, df):
        """åˆ¤æ–­æ˜¯å¦ä¸ºç§¯é›ªç‰©å€™æ•°æ®"""
        snow_keywords = ['snow_phenology', 'snow_start', 'snow_end', 'phenology', 'scp']
        if any(keyword in source_name.lower() for keyword in snow_keywords):
            return True

        # æ£€æŸ¥æ•°æ®ç»“æ„ç‰¹å¾
        if 'dataset_type' in df.columns and 'hydrological_year' in df.columns:
            return True

        return False

    def _validate_final_wide_table(self, df):
        """éªŒè¯æœ€ç»ˆå®½è¡¨æ•°æ®"""
        self.logger.info("=== æœ€ç»ˆå®½è¡¨éªŒè¯ ===")
        self.logger.info(f"æ€»è¡Œæ•°: {len(df)}")
        self.logger.info(f"æ€»åˆ—æ•°: {len(df.columns)}")
        self.logger.info(f"æ‰€æœ‰åˆ—å: {list(df.columns)}")

        # æ£€æŸ¥ç§¯é›ªç‰©å€™æ•°æ®
        snow_start_cols = [col for col in df.columns if col.endswith('_start')]
        snow_end_cols = [col for col in df.columns if col.endswith('_end')]
        snow_cols = [col for col in df.columns if
                     any(keyword in col.lower() for keyword in ['snow', 'scp', 'phenology'])]

        if snow_start_cols:
            self.logger.info(f"ç§¯é›ªåˆæ—¥ç›¸å…³åˆ—: {snow_start_cols}")
            for col in snow_start_cols:
                valid_count = df[col].notna().sum()
                self.logger.info(f"{col}: {valid_count} è¡Œæœ‰å€¼")

        if snow_end_cols:
            self.logger.info(f"ç§¯é›ªç»ˆæ—¥ç›¸å…³åˆ—: {snow_end_cols}")
            for col in snow_end_cols:
                valid_count = df[col].notna().sum()
                self.logger.info(f"{col}: {valid_count} è¡Œæœ‰å€¼")

        if snow_cols and not snow_start_cols and not snow_end_cols:
            self.logger.info(f"ç§¯é›ªç‰©å€™ç›¸å…³åˆ—: {snow_cols}")
            for col in snow_cols:
                valid_count = df[col].notna().sum()
                self.logger.info(f"{col}: {valid_count} è¡Œæœ‰å€¼")

        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹æ•°æ®
        if len(df) > 0:
            self.logger.info("å‰5è¡Œæ•°æ®ç¤ºä¾‹:")
            sample_cols = ['station_id', 'date']

            # æ·»åŠ ç§¯é›ªç‰©å€™åˆ—
            if snow_start_cols:
                sample_cols.extend(snow_start_cols[:1])
            if snow_end_cols:
                sample_cols.extend(snow_end_cols[:1])
            if snow_cols and not snow_start_cols and not snow_end_cols:
                sample_cols.extend(snow_cols[:1])
        # æ·»åŠ å…¶ä»–æ•°æ®åˆ—ç¤ºä¾‹
        other_cols = [col for col in df.columns if col not in sample_cols and col not in ['station_id', 'date']]
        if other_cols:
            sample_cols.extend(other_cols[:3])  # æ˜¾ç¤ºå‰3ä¸ªå…¶ä»–åˆ—

        sample_df = df[sample_cols].head(5)
        for _, row in sample_df.iterrows():
            values = []
            for col in sample_cols:
                if col not in ['station_id', 'date'] and pd.notna(row[col]):
                    values.append(f"{col}: {row[col]}")
            value_str = ", ".join(values) if values else "æ— æ•°æ®"
            self.logger.info(f"  ç«™ç‚¹ {row['station_id']} | æ—¥æœŸ {row['date']} | {value_str}")

    def _handle_static_only_case(self, static_dfs, yearly_dfs, phenology_dfs):
        """å¤„ç†åªæœ‰é™æ€æ•°æ®çš„æƒ…å†µ - ä¿®å¤ç‰ˆæœ¬"""
        try:
            self.logger.info("å¤„ç†çº¯é™æ€æ•°æ®æƒ…å†µ")

            all_dfs = []

            # å¤„ç†é™æ€æ•°æ®
            for name, df in static_dfs:
                df_copy = df.copy()
                # ä¸ºé™æ€æ•°æ®æ·»åŠ ä¸€ä¸ªè™šæ‹Ÿæ—¥æœŸï¼ˆå¦‚æœéœ€è¦ï¼‰
                if 'date' not in df_copy.columns:
                    df_copy['date'] = datetime.now().strftime('%Y-%m-%d')
                all_dfs.append(df_copy)

            # å¤„ç†å¹´åº¦æ•°æ®
            for name, df in yearly_dfs:
                df_copy = df.copy()
                if 'date' not in df_copy.columns and 'year' in df_copy.columns:
                    # ä¸ºå¹´åº¦æ•°æ®åˆ›å»ºè™šæ‹Ÿæ—¥æœŸ
                    df_copy['date'] = df_copy['year'].astype(str) + '-06-15'  # å¹´ä¸­æ—¥æœŸ
                all_dfs.append(df_copy)

            # å¤„ç†ç§¯é›ªç‰©å€™æ•°æ®
            for name, df in phenology_dfs:
                df_copy = df.copy()
                # ä¸ºç§¯é›ªç‰©å€™æ•°æ®ç¡®ä¿æœ‰æ—¥æœŸ
                if 'date' not in df_copy.columns:
                    df_copy['date'] = datetime.now().strftime('%Y-%m-%d')
                all_dfs.append(df_copy)

            if not all_dfs:
                self.logger.error("æ²¡æœ‰æœ‰æ•ˆæ•°æ®")
                return pd.DataFrame()

            # åˆå¹¶æ‰€æœ‰æ•°æ®
            final_df = all_dfs[0]
            for i in range(1, len(all_dfs)):
                # æ‰¾åˆ°å…±åŒçš„é”®è¿›è¡Œåˆå¹¶
                common_keys = ['station_id', 'date']
                available_keys = [key for key in common_keys if key in final_df.columns and key in all_dfs[i].columns]

                if available_keys:
                    final_df = final_df.merge(all_dfs[i], on=available_keys, how='outer')
                else:
                    # å¦‚æœæ²¡æœ‰å…±åŒé”®ï¼Œå°è¯•åªæŒ‰station_idåˆå¹¶
                    if 'station_id' in final_df.columns and 'station_id' in all_dfs[i].columns:
                        final_df = final_df.merge(all_dfs[i], on='station_id', how='outer')
                    else:
                        self.logger.warning(f"æ— æ³•åˆå¹¶æ•°æ®æºï¼Œæ²¡æœ‰å…±åŒçš„é”®")

            self.logger.info(f"çº¯é™æ€æ•°æ®åˆå¹¶å®Œæˆ: {final_df.shape}")
            return final_df

        except Exception as e:
            self.logger.error(f"å¤„ç†çº¯é™æ€æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def _is_yearly_data(self, source_name, df):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¹´åº¦æ•°æ®ï¼ˆå¦‚landcoverï¼‰"""
        # é€šè¿‡åç§°åˆ¤æ–­
        if 'landcover' in source_name.lower():
            return True

        # é€šè¿‡æ•°æ®ç»“æ„åˆ¤æ–­
        if 'station_id' in df.columns:
            # æ£€æŸ¥æ¯ä¸ªç«™ç‚¹çš„è®°å½•æ•° - å¦‚æœå¾ˆå°‘ï¼Œå¯èƒ½æ˜¯å¹´åº¦æ•°æ®
            station_counts = df.groupby('station_id').size()
            avg_records_per_station = station_counts.mean()

            # å¦‚æœå¹³å‡æ¯ä¸ªç«™ç‚¹è®°å½•æ•°å¾ˆå°‘ï¼ˆæ¯”å¦‚ < 5ï¼‰ï¼Œå¯èƒ½æ˜¯å¹´åº¦æ•°æ®
            if avg_records_per_station < 5:
                return True

        return False

    def _is_terrain_features(self, source_name, df):
        """åˆ¤æ–­æ˜¯å¦ä¸ºåœ°å½¢ç‰¹å¾æ•°æ®"""
        # é€šè¿‡åç§°åˆ¤æ–­
        if 'terrain' in source_name.lower() or 'feature' in source_name.lower():
            return True

        # é€šè¿‡åˆ—ç»“æ„åˆ¤æ–­ï¼ˆåœ°å½¢ç‰¹å¾é€šå¸¸æœ‰å¤šä¸ªç‰¹å¾åˆ—ï¼‰
        numeric_cols = df.select_dtypes(include=['number']).columns
        feature_cols = [col for col in numeric_cols if col not in ['station_id', 'date', 'longitude', 'latitude']]

        # å¦‚æœæœ‰å¤šä¸ªæ•°å€¼åˆ—ä¸”æ²¡æœ‰dateåˆ—ï¼Œå¾ˆå¯èƒ½æ˜¯åœ°å½¢ç‰¹å¾å®½è¡¨
        if len(feature_cols) > 1 and 'date' not in df.columns:
            return True

        return False

    def _validate_final_result(self, df):
        """éªŒè¯æœ€ç»ˆç»“æœ"""
        self.logger.info("=== æœ€ç»ˆç»“æœéªŒè¯ ===")
        self.logger.info(f"æ€»è¡Œæ•°: {len(df)}")
        self.logger.info(f"åˆ—å: {list(df.columns)}")

        # æ£€æŸ¥å„æ•°æ®æºçš„å¡«å……æƒ…å†µ
        data_cols = [col for col in df.columns if col not in ['station_id', 'date']]
        for col in data_cols:
            non_null = df[col].notna().sum()
            self.logger.info(f"{col}: {non_null} è¡Œæœ‰å€¼")

        # æ˜¾ç¤ºæ ·ä¾‹æ•°æ®
        if len(df) > 0:
            self.logger.info("å‰5è¡Œæ•°æ®æ ·ä¾‹:")
            for i in range(min(5, len(df))):
                row = df.iloc[i]
                values = []
                for col in data_cols:
                    if pd.notna(row[col]):
                        values.append(f"{col}: {row[col]}")

                if values:
                    self.logger.info(f"  ç«™ç‚¹ {row['station_id']} | æ—¶é—´ {row['date']} | {', '.join(values)}")

    def _validate_data_integrity(self, df):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼Œç¡®ä¿æ²¡æœ‰è™šå‡çš„ç«™ç‚¹-æ—¥æœŸç»„åˆ"""
        if df.empty:
            return df

        # è·å–æ•°æ®åˆ—
        data_cols = [col for col in df.columns if col not in ['station_id', 'date']]

        if not data_cols:
            return df

        # ç»Ÿè®¡æ¯ä¸ªæ•°æ®æºçš„æœ‰æ•ˆè®°å½•æ•°
        self.logger.info("=== æ•°æ®æœ‰æ•ˆæ€§éªŒè¯ ===")
        for col in data_cols:
            valid_count = df[col].notna().sum()
            total_count = len(df)
            fill_rate = (valid_count / total_count) * 100 if total_count > 0 else 0
            self.logger.info(f"{col}: {valid_count}/{total_count} æœ‰æ•ˆè®°å½• ({fill_rate:.1f}%)")

        # è¯†åˆ«å¯èƒ½çš„é—®é¢˜è®°å½•
        empty_records = df[data_cols].isna().all(axis=1)
        if empty_records.any():
            empty_count = empty_records.sum()
            self.logger.warning(f"å‘ç° {empty_count} æ¡å…¨ç©ºè®°å½•")

            # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
            empty_examples = df[empty_records][['station_id', 'date']].head(5)
            self.logger.info("å…¨ç©ºè®°å½•ç¤ºä¾‹:")
            for _, row in empty_examples.iterrows():
                self.logger.info(f"  ç«™ç‚¹ {row['station_id']}, æ—¥æœŸ {row['date']}")

        return df

    def get_combined_data(self):
        """åˆå¹¶æ•°æ®"""
        if not self.source_data:
            return pd.DataFrame()

        dfs = []
        for name, df in self.source_data.items():
            df_copy = df.copy()
            df_copy['data_source'] = name
            dfs.append(df_copy)

        return pd.concat(dfs, ignore_index=True)

    def emergency_fix(self, issue_type='duplicates'):
        """åº”æ€¥æ•°æ®ä¿®å¤ - æ˜¾ç¤ºæ‰€æœ‰æ•°æ®æºçš„ä¿®å¤æƒ…å†µ"""
        try:
            fix_count = 0

            if not self.source_data:
                self.logger.warning("æ²¡æœ‰æ•°æ®æºå¯ä¿®å¤")
                return fix_count

            # 1. å¤„ç†é‡å¤æ•°æ®
            if issue_type == 'duplicates':
                for name, df in self.source_data.items():
                    if 'station_id' in df.columns:
                        original_count = len(df)

                        # âœ… è®°å½•æ¯ä¸ªæ•°æ®æºçš„åˆå§‹çŠ¶æ€
                        self.logger.info(f"æ£€æŸ¥ {name} æ•°æ®æº: {original_count} è¡Œ")

                        # ç§¯é›ªç‰©å€™æ•°æ®ç‰¹æ®Šå¤„ç†
                        if self._is_snow_phenology_data(name, df):
                            # ç§¯é›ªç‰©å€™æ•°æ®ï¼šæŒ‰ç«™ç‚¹+æ°´æ–‡å¹´+ç±»å‹å»é‡
                            if 'hydrological_year' in df.columns and 'dataset_type' in df.columns:
                                duplicate_mask = df.duplicated(
                                    subset=['station_id', 'hydrological_year', 'dataset_type'],
                                    keep='first'
                                )
                                duplicates_df = df[duplicate_mask]

                                if not duplicates_df.empty:
                                    self.logger.info(f"ğŸ” {name} é‡å¤è®°å½•è¯¦æƒ… (æŒ‰ç«™ç‚¹+æ°´æ–‡å¹´+ç±»å‹):")
                                    sample_duplicates = duplicates_df.head(3)
                                    for idx, row in sample_duplicates.iterrows():
                                        self.logger.info(
                                            f"   ç«™ç‚¹ {row['station_id']}, "
                                            f"æ°´æ–‡å¹´ {row['hydrological_year']}, "
                                            f"ç±»å‹ {row['dataset_type']}, "
                                            f"day_of_year: {row.get('day_of_year', 'N/A')}"
                                        )

                                df_clean = df.drop_duplicates(
                                    subset=['station_id', 'hydrological_year', 'dataset_type'],
                                    keep='first'
                                )
                            else:
                                # å¦‚æœæ²¡æœ‰æ°´æ–‡å¹´ä¿¡æ¯ï¼ŒæŒ‰ç«™ç‚¹+ç±»å‹å»é‡
                                duplicate_mask = df.duplicated(['station_id', 'dataset_type'], keep='first')
                                duplicates_df = df[duplicate_mask]

                                if not duplicates_df.empty:
                                    self.logger.info(f"ğŸ” {name} é‡å¤è®°å½•è¯¦æƒ… (æŒ‰ç«™ç‚¹+ç±»å‹):")
                                    sample_duplicates = duplicates_df.head(3)
                                    for idx, row in sample_duplicates.iterrows():
                                        self.logger.info(
                                            f"   ç«™ç‚¹ {row['station_id']}, "
                                            f"ç±»å‹ {row['dataset_type']}, "
                                            f"day_of_year: {row.get('day_of_year', 'N/A')}"
                                        )

                                df_clean = df.drop_duplicates(['station_id', 'dataset_type'], keep='first')

                        # å…¶ä»–æ•°æ®æº
                        elif 'date' in df.columns:
                            # åŠ¨æ€æ•°æ®ï¼šæŒ‰ç«™ç‚¹+æ—¥æœŸå»é‡
                            duplicate_mask = df.duplicated(['station_id', 'date'], keep='first')
                            duplicates_df = df[duplicate_mask]

                            if not duplicates_df.empty:
                                self.logger.info(f"ğŸ” {name} é‡å¤è®°å½•è¯¦æƒ… (æŒ‰ç«™ç‚¹+æ—¥æœŸ):")
                                sample_duplicates = duplicates_df.head(3)
                                for idx, row in sample_duplicates.iterrows():
                                    # æ˜¾ç¤ºæ›´å¤šå­—æ®µä¿¡æ¯
                                    info_parts = [f"ç«™ç‚¹ {row['station_id']}", f"æ—¥æœŸ {row['date']}"]

                                    # æ·»åŠ ç»çº¬åº¦ä¿¡æ¯
                                    if 'original_lon' in row and 'original_lat' in row:
                                        info_parts.append(f"åæ ‡({row['original_lon']:.3f}, {row['original_lat']:.3f})")

                                    # æ·»åŠ ä¸»è¦æ•°å€¼å­—æ®µ
                                    value_cols = [col for col in ['value', 'day_of_year', 'snow_depth', 'temperature']
                                                  if col in row and pd.notna(row[col])]
                                    for col in value_cols[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ªæ•°å€¼å­—æ®µ
                                        info_parts.append(f"{col}: {row[col]}")

                                    self.logger.info("   " + ", ".join(info_parts))

                            df_clean = df.drop_duplicates(['station_id', 'date'])
                        else:
                            # é™æ€æ•°æ®ï¼šæŒ‰ç«™ç‚¹å»é‡
                            duplicate_mask = df.duplicated(['station_id'], keep='first')
                            duplicates_df = df[duplicate_mask]

                            if not duplicates_df.empty:
                                self.logger.info(f"ğŸ” {name} é‡å¤è®°å½•è¯¦æƒ… (æŒ‰ç«™ç‚¹):")
                                sample_duplicates = duplicates_df.head(3)
                                for idx, row in sample_duplicates.iterrows():
                                    info_parts = [f"ç«™ç‚¹ {row['station_id']}"]

                                    # æ˜¾ç¤ºç‰¹å¾å€¼
                                    feature_cols = [col for col in df.columns
                                                    if col not in ['station_id', 'station_ID', 'Station_ID']
                                                    and pd.api.types.is_numeric_dtype(df[col])]
                                    for col in feature_cols[:3]:  # æ˜¾ç¤ºå‰3ä¸ªç‰¹å¾
                                        if pd.notna(row[col]):
                                            info_parts.append(f"{col}: {row[col]}")

                                    self.logger.info("   " + ", ".join(info_parts))

                            df_clean = df.drop_duplicates(['station_id'])

                        removed_count = original_count - len(df_clean)
                        if removed_count > 0:
                            self.source_data[name] = df_clean
                            fix_count += removed_count
                            self.logger.info(f"âœ… ä¿®å¤ {name}: ç§»é™¤ {removed_count} ä¸ªé‡å¤è¡Œ")
                        else:
                            self.logger.info(f"âœ… {name}: æ— é‡å¤è®°å½•")

                        # 2. å¤„ç†åˆ—åä¸ä¸€è‡´é—®é¢˜
                    elif issue_type == 'column_names':
                        for name, df in self.source_data.items():
                            # æ ‡å‡†åŒ–åˆ—å
                            column_mapping = {}
                            if 'station_ID' in df.columns and 'station_id' not in df.columns:
                                column_mapping['station_ID'] = 'station_id'
                            if 'Station_ID' in df.columns and 'station_id' not in df.columns:
                                column_mapping['Station_ID'] = 'station_id'

                            if column_mapping:
                                df_renamed = df.rename(columns=column_mapping)
                                self.source_data[name] = df_renamed
                                fix_count += len(column_mapping)
                                self.logger.info(f"âœ… ä¿®å¤ {name}: é‡å‘½å {list(column_mapping.keys())}")

                    # 3. å¤„ç†æ•°æ®ç±»å‹é—®é¢˜
                    elif issue_type == 'data_types':
                        for name, df in self.source_data.items():
                            # ç¡®ä¿station_idæ˜¯å­—ç¬¦ä¸²ç±»å‹
                            if 'station_id' in df.columns:
                                if df['station_id'].dtype != 'object':
                                    df_fixed = df.copy()
                                    df_fixed['station_id'] = df_fixed['station_id'].astype(str)
                                    self.source_data[name] = df_fixed
                                    fix_count += 1
                                    self.logger.info(f"âœ… ä¿®å¤ {name}: station_idè½¬æ¢ä¸ºå­—ç¬¦ä¸²ç±»å‹")

                self.logger.info(f"ğŸ¯ åº”æ€¥ä¿®å¤å®Œæˆ: æ€»å…±æ‰§è¡Œäº† {fix_count} ä¸ªä¿®å¤")
                return fix_count

        except Exception as e:
            self.logger.error(f"åº”æ€¥ä¿®å¤å¤±è´¥: {e}")
            return 0

    def generate_report(self):
        return "æŠ¥å‘Šç”Ÿæˆå®Œæˆ"

    def _comprehensive_validation(self, df):
        """å…¨é¢éªŒè¯å®½è¡¨æ•°æ®"""
        self.logger.info("=== å…¨é¢æ•°æ®éªŒè¯ ===")

        # 1. åŸºæœ¬ç»Ÿè®¡
        self.logger.info(f"è¡¨æ ¼å½¢çŠ¶: {df.shape}")
        self.logger.info(f"å”¯ä¸€ç«™ç‚¹æ•°: {df['station_id'].nunique()}")
        self.logger.info(f"æ—¶é—´ç‚¹æ•°: {df['date'].nunique()}")
        self.logger.info(f"æ—¶é—´èŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")

        # 2. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        data_cols = [col for col in df.columns if col not in ['station_id', 'date']]
        self.logger.info("=== å„æ•°æ®æºå¡«å……æƒ…å†µ ===")

        for col in data_cols:
            non_null = df[col].notna().sum()
            fill_rate = (non_null / len(df)) * 100
            if non_null > 0:
                avg_val = df[col].mean()
                min_val = df[col].min()
                max_val = df[col].max()
                self.logger.info(
                    f"{col}: {non_null}è¡Œæœ‰å€¼({fill_rate:.1f}%), èŒƒå›´: {min_val:.2f}-{max_val:.2f}, å¹³å‡: {avg_val:.2f}")
            else:
                self.logger.info(f"{col}: 0è¡Œæœ‰å€¼")

        # 3. æ£€æŸ¥é‡å¤è¡Œ
        duplicates = df.duplicated(['station_id', 'date']).sum()
        self.logger.info(f"é‡å¤è¡Œæ•°: {duplicates}")

        # 4. æŠ½æ ·æ˜¾ç¤ºå…·ä½“æ•°æ®
        self.logger.info("=== æ•°æ®æŠ½æ ·æ˜¾ç¤º ===")

        # æ˜¾ç¤ºå‰5è¡Œ
        self.logger.info("å‰5è¡Œæ•°æ®:")
        for i in range(min(5, len(df))):
            row = df.iloc[i]
            values = []
            for col in data_cols:
                if pd.notna(row[col]):
                    values.append(f"{col}: {row[col]}")
            self.logger.info(f"  è¡Œ{i + 1}: ç«™ç‚¹{row['station_id']}, æ—¶é—´{row['date']}")
            if values:
                self.logger.info(f"      æ•°æ®: {', '.join(values)}")

        # 5. æ£€æŸ¥ç‰¹å®šå·²çŸ¥æ•°æ®ç‚¹
        self.logger.info("=== å·²çŸ¥æ•°æ®ç‚¹éªŒè¯ ===")
        known_points = [
            (50136, '2013-01-05', 'snow_depth', 14.1),
            (54764, '2013-01-02', 'ERA5æ¸©åº¦', 2.67)
        ]

        for station, date, data_type, expected_value in known_points:
            match = df[(df['station_id'] == station) & (df['date'] == date)]
            if not match.empty:
                actual_value = match.iloc[0].get(data_type)
                if pd.notna(actual_value):
                    diff = abs(actual_value - expected_value)
                    status = "âœ“" if diff < 0.1 else "âœ—"
                    self.logger.info(
                        f"{status} ç«™ç‚¹{station} {date} {data_type}: æœŸæœ›{expected_value}, å®é™…{actual_value:.2f}")
                else:
                    self.logger.info(f"âœ— ç«™ç‚¹{station} {date} {data_type}: æœŸæœ›{expected_value}, å®é™…æ— æ•°æ®")
            else:
                self.logger.info(f"âœ— ç«™ç‚¹{station} {date}: æ— è®°å½•")

    def clear_data(self):
        self.source_data.clear()
