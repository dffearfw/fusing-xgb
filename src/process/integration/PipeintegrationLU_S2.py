import pandas as pd
import numpy as np
import logging
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any, List

logger = logging.getLogger("LandUseOneHotPipeline")


class LandUseOneHotPipeline:
    """
    åœŸåœ°åˆ©ç”¨ç±»å‹ç‹¬çƒ­ç¼–ç ç®¡é“
    å°†cnlucc_landuse_codeä»æ ‡ç­¾ç¼–ç è½¬æ¢ä¸ºç‹¬çƒ­ç¼–ç 
    """

    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logger

        self.input_df: Optional[pd.DataFrame] = None
        self.processed_df: Optional[pd.DataFrame] = None
        self.unique_codes: List[int] = []

    def load_data(self, file_path: str) -> bool:
        """åŠ è½½æ•°æ®"""
        try:
            self.input_df = pd.read_excel(file_path)
            self.logger.info(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(self.input_df)}è¡Œ, {len(self.input_df.columns)}åˆ—")

            # æ£€æŸ¥æ˜¯å¦åŒ…å«landuse_codeåˆ—
            if 'cnlucc_landuse_code' not in self.input_df.columns:
                self.logger.error("âŒ æ•°æ®ä¸­æœªæ‰¾åˆ° cnlucc_landuse_code åˆ—")
                return False

            return True

        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False

    def analyze_landuse_distribution(self) -> Dict[str, Any]:
        """åˆ†æåœŸåœ°åˆ©ç”¨ç±»å‹åˆ†å¸ƒ"""
        if self.input_df is None:
            return {}

        try:
            analysis = {}

            # ç»Ÿè®¡åœŸåœ°åˆ©ç”¨ç±»å‹
            landuse_counts = self.input_df['cnlucc_landuse_code'].value_counts().sort_index()
            analysis['landuse_distribution'] = landuse_counts.to_dict()

            # ç»Ÿè®¡ç¼ºå¤±å€¼
            missing_count = self.input_df['cnlucc_landuse_code'].isna().sum()
            analysis['missing_count'] = missing_count
            analysis['missing_percentage'] = (missing_count / len(self.input_df)) * 100

            # è¯†åˆ«æ‰€æœ‰å”¯ä¸€çš„åœŸåœ°åˆ©ç”¨ç±»å‹
            self.unique_codes = sorted(self.input_df['cnlucc_landuse_code'].dropna().unique().astype(int))
            analysis['unique_landuse_codes'] = self.unique_codes
            analysis['num_unique_codes'] = len(self.unique_codes)

            # è¾“å‡ºåˆ†æç»“æœ
            self.logger.info("ğŸ“Š åœŸåœ°åˆ©ç”¨ç±»å‹åˆ†æ:")
            self.logger.info(f"  å”¯ä¸€ç¼–ç æ•°é‡: {analysis['num_unique_codes']}")
            self.logger.info(f"  ç¼ºå¤±å€¼æ•°é‡: {analysis['missing_count']} ({analysis['missing_percentage']:.1f}%)")

            self.logger.info("  åœŸåœ°åˆ©ç”¨ç±»å‹åˆ†å¸ƒ:")
            for code in self.unique_codes:
                count = landuse_counts[code]
                percentage = (count / len(self.input_df)) * 100
                self.logger.info(f"    {code}: {count} æ¡è®°å½• ({percentage:.1f}%)")

            return analysis

        except Exception as e:
            self.logger.error(f"âŒ åˆ†æåœŸåœ°åˆ©ç”¨ç±»å‹å¤±è´¥: {e}")
            return {}

    def create_one_hot_encoding(self, analysis: Dict[str, Any]) -> pd.DataFrame:
        """åˆ›å»ºç‹¬çƒ­ç¼–ç """
        if self.input_df is None:
            return pd.DataFrame()

        try:
            # åˆ›å»ºæ•°æ®å‰¯æœ¬
            self.processed_df = self.input_df.copy()

            self.logger.info(f"ğŸ”„ ä¸º {len(self.unique_codes)} ç§åœŸåœ°åˆ©ç”¨ç±»å‹åˆ›å»ºç‹¬çƒ­ç¼–ç ...")

            # ä¸ºæ¯ä¸ªåœŸåœ°åˆ©ç”¨ç±»å‹åˆ›å»ºç‹¬çƒ­ç¼–ç åˆ—
            for code in self.unique_codes:
                col_name = f"landuse_{code}"
                # åˆ›å»ºäºŒè¿›åˆ¶åˆ—ï¼šå¦‚æœlanduse_codeç­‰äºå½“å‰codeåˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0
                self.processed_df[col_name] = (self.processed_df['cnlucc_landuse_code'] == code).astype(int)

                # ç»Ÿè®¡è¯¥åˆ—çš„1çš„æ•°é‡
                ones_count = self.processed_df[col_name].sum()
                percentage = (ones_count / len(self.processed_df)) * 100

                self.logger.info(f"  âœ… {col_name}: {ones_count} æ¡è®°å½• ({percentage:.1f}%)")

            # æ£€æŸ¥ç‹¬çƒ­ç¼–ç çš„æ­£ç¡®æ€§
            self._validate_one_hot_encoding()

            self.logger.info(f"âœ… ç‹¬çƒ­ç¼–ç å®Œæˆ: æ–°å¢ {len(self.unique_codes)} åˆ—")

            return self.processed_df

        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºç‹¬çƒ­ç¼–ç å¤±è´¥: {e}")
            return pd.DataFrame()

    def _validate_one_hot_encoding(self):
        """éªŒè¯ç‹¬çƒ­ç¼–ç çš„æ­£ç¡®æ€§"""
        try:
            # æ£€æŸ¥æ¯è¡Œæ˜¯å¦åªæœ‰ä¸€ä¸ª1ï¼ˆäº’æ–¥æ€§ï¼‰
            one_hot_cols = [f"landuse_{code}" for code in self.unique_codes]
            row_sums = self.processed_df[one_hot_cols].sum(axis=1)

            # ç»Ÿè®¡æ¯è¡Œ1çš„æ•°é‡
            sum_counts = row_sums.value_counts().sort_index()

            self.logger.info("ğŸ” ç‹¬çƒ­ç¼–ç éªŒè¯:")
            for sum_val, count in sum_counts.items():
                percentage = (count / len(self.processed_df)) * 100
                self.logger.info(f"  æ¯è¡Œ{sum_val}ä¸ª1: {count} è¡Œ ({percentage:.1f}%)")

            # æ£€æŸ¥æ˜¯å¦æœ‰è¡Œæ²¡æœ‰1ï¼ˆç¼ºå¤±å€¼æƒ…å†µï¼‰
            zero_rows = (row_sums == 0).sum()
            if zero_rows > 0:
                self.logger.warning(f"  âš ï¸  {zero_rows} è¡Œæ²¡æœ‰åœŸåœ°åˆ©ç”¨ç±»å‹ç¼–ç ï¼ˆç¼ºå¤±å€¼ï¼‰")

            # æ£€æŸ¥æ˜¯å¦æœ‰è¡Œæœ‰å¤šä¸ª1ï¼ˆç¼–ç é”™è¯¯ï¼‰
            multi_one_rows = (row_sums > 1).sum()
            if multi_one_rows > 0:
                self.logger.error(f"  âŒ {multi_one_rows} è¡Œæœ‰å¤šä¸ªåœŸåœ°åˆ©ç”¨ç±»å‹ç¼–ç ï¼ˆç¼–ç é”™è¯¯ï¼‰")
            else:
                self.logger.info("  âœ… æ‰€æœ‰è¡Œéƒ½æœ‰ä¸”åªæœ‰ä¸€ä¸ªåœŸåœ°åˆ©ç”¨ç±»å‹ç¼–ç ")

        except Exception as e:
            self.logger.error(f"âŒ éªŒè¯ç‹¬çƒ­ç¼–ç å¤±è´¥: {e}")

    def create_landuse_mapping_report(self, analysis: Dict[str, Any]) -> str:
        """ç”ŸæˆåœŸåœ°åˆ©ç”¨ç±»å‹æ˜ å°„æŠ¥å‘Š"""
        report_lines = [
            "=" * 60,
            "åœŸåœ°åˆ©ç”¨ç±»å‹ç‹¬çƒ­ç¼–ç æŠ¥å‘Š",
            "=" * 60,
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            ""
        ]

        report_lines.extend([
            "åŸºæœ¬ç»Ÿè®¡:",
            f"æ€»è®°å½•æ•°: {len(self.input_df)}",
            f"å”¯ä¸€åœŸåœ°åˆ©ç”¨ç¼–ç æ•°: {analysis['num_unique_codes']}",
            f"ç¼ºå¤±å€¼æ•°é‡: {analysis['missing_count']} ({analysis['missing_percentage']:.1f}%)",
            ""
        ])

        # åœŸåœ°åˆ©ç”¨ç±»å‹åˆ†å¸ƒ
        report_lines.append("åœŸåœ°åˆ©ç”¨ç±»å‹åˆ†å¸ƒ:")
        landuse_counts = analysis['landuse_distribution']
        for code in self.unique_codes:
            count = landuse_counts[code]
            percentage = (count / len(self.input_df)) * 100
            report_lines.append(f"  {code}: {count} è®°å½• ({percentage:.1f}%)")

        # ç‹¬çƒ­ç¼–ç åˆ—ä¿¡æ¯
        if self.processed_df is not None:
            report_lines.extend([
                "",
                "ç”Ÿæˆçš„ç‹¬çƒ­ç¼–ç åˆ—:",
            ])
            for code in self.unique_codes:
                col_name = f"landuse_{code}"
                ones_count = self.processed_df[col_name].sum()
                percentage = (ones_count / len(self.processed_df)) * 100
                report_lines.append(f"  {col_name}: {ones_count} ä¸ª1 ({percentage:.1f}%)")

        # æ•°æ®ç¤ºä¾‹ - å®Œå…¨é‡å†™ï¼Œé¿å…åˆ—è®¿é—®é—®é¢˜
        if self.processed_df is not None:
            report_lines.extend([
                "",
                "æ•°æ®ç¤ºä¾‹ (å‰3æ¡):",
            ])

            # åªæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯å’ŒåŸå§‹ç¼–ç 
            sample_data = self.processed_df[['station_id', 'date', 'cnlucc_landuse_code']].head(3)

            for _, row in sample_data.iterrows():
                sample_line = f"  ç«™ç‚¹ {row['station_id']} | æ—¥æœŸ {row['date']} | "
                sample_line += f"åŸç¼–ç : {row['cnlucc_landuse_code']}"
                report_lines.append(sample_line)

            # æ·»åŠ ç‹¬çƒ­ç¼–ç çš„å•ç‹¬è¯´æ˜
            report_lines.extend([
                "",
                "ç‹¬çƒ­ç¼–ç è¯´æ˜:",
                f"- å…±ç”Ÿæˆ {len(self.unique_codes)} ä¸ªç‹¬çƒ­ç¼–ç åˆ—",
                f"- åˆ—åæ ¼å¼: landuse_ç¼–ç  (å¦‚ landuse_10, landuse_20 ç­‰)",
                f"- æ¯ä¸ªè®°å½•åœ¨å¯¹åº”çš„åœŸåœ°åˆ©ç”¨ç±»å‹åˆ—ä¸­å€¼ä¸º1ï¼Œå…¶ä»–ä¸º0",
                f"- æ‰€æœ‰ç‹¬çƒ­ç¼–ç åˆ—å·²æˆåŠŸæ·»åŠ åˆ°æ•°æ®ä¸­"
            ])

        return "\n".join(report_lines)

    def save_processed_data(self, suffix: str = "") -> Optional[str]:
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        if self.processed_df is None:
            self.logger.error("âŒ æ²¡æœ‰å¤„ç†åçš„æ•°æ®å¯ä¿å­˜")
            return None

        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            suffix_str = f"_{suffix}" if suffix else ""
            filename = f"landuse_onehot_data{suffix_str}_{timestamp}.xlsx"
            output_path = self.output_dir / filename

            self.processed_df.to_excel(output_path, index=False)
            self.logger.info(f"ğŸ’¾ å¤„ç†åçš„æ•°æ®ä¿å­˜æˆåŠŸ: {output_path}")

            return str(output_path)

        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return None

    def run_pipeline(self, input_file: str) -> bool:
        """è¿è¡Œå®Œæ•´çš„åœŸåœ°åˆ©ç”¨ç±»å‹ç‹¬çƒ­ç¼–ç ç®¡é“"""
        self.logger.info("ğŸš€ å¯åŠ¨åœŸåœ°åˆ©ç”¨ç±»å‹ç‹¬çƒ­ç¼–ç ç®¡é“")

        # 1. åŠ è½½æ•°æ®
        if not self.load_data(input_file):
            return False

        # 2. åˆ†æåœŸåœ°åˆ©ç”¨ç±»å‹åˆ†å¸ƒ
        analysis = self.analyze_landuse_distribution()
        if not analysis:
            return False

        # 3. åˆ›å»ºç‹¬çƒ­ç¼–ç 
        processed_df = self.create_one_hot_encoding(analysis)
        if processed_df.empty:
            return False

        # 4. ç”ŸæˆæŠ¥å‘Š
        report = self.create_landuse_mapping_report(analysis)
        print(f"\nğŸ“„ åœŸåœ°åˆ©ç”¨ç±»å‹æŠ¥å‘Š:\n{report}")

        # 5. ä¿å­˜ç»“æœ
        output_path = self.save_processed_data("onehot")

        if output_path:
            self.logger.info(f"âœ… ç®¡é“æ‰§è¡Œå®Œæˆ! è¾“å‡ºæ–‡ä»¶: {output_path}")

            # æ˜¾ç¤ºæ€»ç»“ä¿¡æ¯
            one_hot_cols = [f"landuse_{code}" for code in self.unique_codes]
            print(f"\nğŸ‰ å¤„ç†å®Œæˆæ€»ç»“:")
            print(f"  åŸå§‹æ•°æ®: {len(self.input_df)} è¡Œ, {len(self.input_df.columns)} åˆ—")
            print(f"  å¤„ç†åæ•°æ®: {len(self.processed_df)} è¡Œ, {len(self.processed_df.columns)} åˆ—")
            print(f"  æ–°å¢ç‹¬çƒ­ç¼–ç åˆ—: {len(one_hot_cols)} åˆ—")
            print(f"  åœŸåœ°åˆ©ç”¨ç±»å‹æ€»æ•°: {analysis['num_unique_codes']} ç§")
            print(f"  åœŸåœ°åˆ©ç”¨ç¼–ç åˆ—è¡¨: {self.unique_codes}")
            print(f"  è¾“å‡ºæ–‡ä»¶: {output_path}")

            return True
        else:
            return False


def main():
    """ä½¿ç”¨ç¤ºä¾‹"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # åˆ›å»ºç®¡é“å®ä¾‹
    pipeline = LandUseOneHotPipeline("landuse_output")

    # è¿è¡Œç®¡é“
    success = pipeline.run_pipeline("D:/pyworkspace/fusing xgb/src/training/corrected_data.xlsx")  # æ›¿æ¢ä¸ºä½ çš„è¾“å…¥æ–‡ä»¶è·¯å¾„

    if success:
        print("\nğŸ‰ åœŸåœ°åˆ©ç”¨ç±»å‹ç‹¬çƒ­ç¼–ç å¤„ç†å®Œæˆ!")
    else:
        print("\nâŒ å¤„ç†å¤±è´¥!")


if __name__ == "__main__":
    main()
