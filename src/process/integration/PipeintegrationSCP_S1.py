import pandas as pd
import numpy as np
import logging
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any

logger = logging.getLogger("StableDataPipeline")


class StableDataPipeline:
    """
    ç¨³å®šæ•°æ®ç®¡é“ - ç²¾ç¡®è®¡ç®—äºŒåˆ†äºŒè‡³æ—¥
    """

    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logger

        self.reference_df: Optional[pd.DataFrame] = None
        self.integrated_df: Optional[pd.DataFrame] = None
        self.corrected_df: Optional[pd.DataFrame] = None

        self.config = {
            'merge_keys': ['station_id', 'date'],
            'protected_columns': ['scp_start', 'scp_end'],
            'backup_original': True
        }

        # 2012-2018å¹´ç²¾ç¡®çš„äºŒåˆ†äºŒè‡³æ—¥æ—¥æœŸ
        self.solstice_equinox_dates = {
            2012: {
                'autumn_equinox': '2012-09-22',
                'winter_solstice': '2012-12-21',
                'spring_equinox': '2013-03-20',
                'summer_solstice': '2013-06-21'
            },
            2013: {
                'autumn_equinox': '2013-09-22',
                'winter_solstice': '2013-12-21',
                'spring_equinox': '2014-03-20',
                'summer_solstice': '2014-06-21'
            },
            2014: {
                'autumn_equinox': '2014-09-23',
                'winter_solstice': '2014-12-21',
                'spring_equinox': '2015-03-20',
                'summer_solstice': '2015-06-21'
            },
            2015: {
                'autumn_equinox': '2015-09-23',
                'winter_solstice': '2015-12-22',
                'spring_equinox': '2016-03-20',
                'summer_solstice': '2016-06-20'
            },
            2016: {
                'autumn_equinox': '2016-09-22',
                'winter_solstice': '2016-12-21',
                'spring_equinox': '2017-03-20',
                'summer_solstice': '2017-06-21'
            },
            2017: {
                'autumn_equinox': '2017-09-22',
                'winter_solstice': '2017-12-21',
                'spring_equinox': '2018-03-20',
                'summer_solstice': '2018-06-21'
            },
            2018: {
                'autumn_equinox': '2018-09-23',
                'winter_solstice': '2018-12-21',
                'spring_equinox': '2019-03-20',
                'summer_solstice': '2019-06-21'
            }
        }

    def calculate_solstice_equinox_distances(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç²¾ç¡®è®¡ç®—åˆ°äºŒåˆ†äºŒè‡³æ—¥çš„è·ç¦»

        ä½¿ç”¨2012-2018å¹´å®é™…çš„äºŒåˆ†äºŒè‡³æ—¥æ—¥æœŸï¼Œè®¡ç®—æ¯ä¸ªè®°å½•åˆ°å¯¹åº”æ°´æ–‡å¹´å…³é”®æ—¥çš„è·ç¦»
        """
        try:
            df_copy = df.copy()

            # ç¡®ä¿æœ‰æ°´æ–‡å¹´DOYå’Œæ°´æ–‡å¹´
            if 'hydrological_doy' not in df_copy.columns or 'hydrological_year' not in df_copy.columns:
                self.logger.error("âŒ éœ€è¦å…ˆè®¡ç®—æ°´æ–‡å¹´DOYå’Œæ°´æ–‡å¹´")
                return df_copy

            self.logger.info("ğŸŒ ç²¾ç¡®è®¡ç®—åˆ°äºŒåˆ†äºŒè‡³æ—¥çš„è·ç¦»...")

            # ä¸ºæ¯ä¸ªè®°å½•è®¡ç®—åˆ°å››ä¸ªå…³é”®æ—¥çš„è·ç¦»
            for event in ['autumn_equinox', 'winter_solstice', 'spring_equinox', 'summer_solstice']:
                distance_col = f'distance_to_{event}'
                df_copy[distance_col] = np.nan

                # å¯¹æ¯ä¸ªæ°´æ–‡å¹´åˆ†åˆ«å¤„ç†
                for hydro_year in df_copy['hydrological_year'].unique():
                    if pd.isna(hydro_year):
                        continue

                    hydro_year = int(hydro_year)

                    # è·å–è¯¥æ°´æ–‡å¹´çš„å…³é”®æ—¥æ—¥æœŸ
                    if hydro_year in self.solstice_equinox_dates:
                        event_date = self.solstice_equinox_dates[hydro_year][event]

                        # è®¡ç®—å…³é”®æ—¥çš„æ°´æ–‡å¹´DOY
                        event_dt = pd.to_datetime(event_date)
                        event_hydro_doy = self._calculate_hydrological_doy_for_date(event_dt)

                        # è®¡ç®—è¯¥æ°´æ–‡å¹´å†…æ‰€æœ‰è®°å½•åˆ°è¿™ä¸ªå…³é”®æ—¥çš„è·ç¦»
                        mask = df_copy['hydrological_year'] == hydro_year
                        df_copy.loc[mask, distance_col] = abs(df_copy.loc[mask, 'hydrological_doy'] - event_hydro_doy)

            # è®¡ç®—åˆ°æœ€è¿‘çš„å…³é”®æ—¥è·ç¦»
            distances = df_copy[[f'distance_to_{event}' for event in
                                 ['autumn_equinox', 'winter_solstice', 'spring_equinox', 'summer_solstice']]]
            df_copy['distance_to_nearest_solstice_equinox'] = distances.min(axis=1)
            df_copy['nearest_solstice_equinox'] = distances.idxmin(axis=1).str.replace('distance_to_', '')

            # ç»Ÿè®¡ä¿¡æ¯
            self.logger.info("ğŸ“Š äºŒåˆ†äºŒè‡³æ—¥è·ç¦»ç»Ÿè®¡:")
            for event in ['autumn_equinox', 'winter_solstice', 'spring_equinox', 'summer_solstice']:
                col = f'distance_to_{event}'
                valid_count = df_copy[col].notna().sum()
                if valid_count > 0:
                    min_dist = df_copy[col].min()
                    max_dist = df_copy[col].max()
                    mean_dist = df_copy[col].mean()
                    self.logger.info(
                        f"  {event}: {valid_count} è®°å½•, è·ç¦»èŒƒå›´ {min_dist:.0f}-{max_dist:.0f}, å¹³å‡ {mean_dist:.1f}")

            # ç»Ÿè®¡æœ€è¿‘å…³é”®æ—¥åˆ†å¸ƒ
            nearest_counts = df_copy['nearest_solstice_equinox'].value_counts()
            self.logger.info("  æœ€è¿‘å…³é”®æ—¥åˆ†å¸ƒ:")
            for event, count in nearest_counts.items():
                percentage = (count / len(df_copy)) * 100
                self.logger.info(f"    {event}: {count} æ¡è®°å½• ({percentage:.1f}%)")

            return df_copy

        except Exception as e:
            self.logger.error(f"âŒ è®¡ç®—äºŒåˆ†äºŒè‡³æ—¥è·ç¦»å¤±è´¥: {e}")
            return df

    def _calculate_hydrological_doy_for_date(self, date):
        """
        è®¡ç®—æŒ‡å®šæ—¥æœŸçš„æ°´æ–‡å¹´DOY
        """
        try:
            # è®¡ç®—è‡ªç„¶å¹´DOY
            natural_doy = date.dayofyear

            # åˆ¤æ–­æ˜¯å¦ä¸ºé—°å¹´
            is_leap_year = (date.year % 4 == 0 and date.year % 100 != 0) or (date.year % 400 == 0)

            # è®¡ç®—æ°´æ–‡å¹´DOY
            if date.month >= 9:
                if is_leap_year:
                    hydrological_doy = natural_doy - 244
                else:
                    hydrological_doy = natural_doy - 243
            else:
                hydrological_doy = natural_doy + 122

            return hydrological_doy

        except Exception as e:
            self.logger.error(f"âŒ è®¡ç®—æ—¥æœŸ {date} çš„æ°´æ–‡å¹´DOYå¤±è´¥: {e}")
            return np.nan

    def validate_solstice_equinox_calculation(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        éªŒè¯ç²¾ç¡®çš„äºŒåˆ†äºŒè‡³æ—¥è·ç¦»è®¡ç®—
        """
        validation_results = {
            'test_cases': [],
            'summary': {}
        }

        try:
            # æµ‹è¯•å…³é”®æ—¥æœŸï¼ˆä½¿ç”¨å®é™…çš„äºŒåˆ†äºŒè‡³æ—¥ï¼‰
            test_dates = [
                # 2013å¹´å…³é”®æ—¥
                ('2013-09-22', 2014, 1, 'autumn_equinox', 0),  # ç§‹åˆ†
                ('2013-12-21', 2014, 91, 'winter_solstice', 0),  # å†¬è‡³
                ('2014-03-20', 2014, 201, 'spring_equinox', 0),  # æ˜¥åˆ†
                ('2014-06-21', 2014, 294, 'summer_solstice', 0),  # å¤è‡³

                # 2016å¹´å…³é”®æ—¥ï¼ˆé—°å¹´ï¼‰
                ('2016-09-22', 2017, 1, 'autumn_equinox', 0),  # ç§‹åˆ†
                ('2016-12-21', 2017, 91, 'winter_solstice', 0),  # å†¬è‡³
                ('2017-03-20', 2017, 201, 'spring_equinox', 0),  # æ˜¥åˆ†
                ('2017-06-21', 2017, 294, 'summer_solstice', 0),  # å¤è‡³

                # ä¸­é—´æ—¥æœŸæµ‹è¯•
                ('2013-10-01', 2014, 10, 'autumn_equinox', 9),  # ç§‹åˆ†å9å¤©
                ('2014-01-01', 2014, 123, 'winter_solstice', 32),  # å†¬è‡³å32å¤©
                ('2014-04-01', 2014, 213, 'spring_equinox', 12),  # æ˜¥åˆ†å12å¤©
                ('2014-07-01', 2014, 304, 'summer_solstice', 10),  # å¤è‡³å10å¤©
            ]

            self.logger.info("ğŸ” ç²¾ç¡®äºŒåˆ†äºŒè‡³æ—¥è·ç¦»è®¡ç®—éªŒè¯:")

            passed_tests = 0
            total_tests = len(test_dates)

            for test_date, expected_hydro_year, expected_hydro_doy, expected_nearest, expected_distance in test_dates:
                # åœ¨æ•°æ®ä¸­æŸ¥æ‰¾æµ‹è¯•æ—¥æœŸ
                test_row = df[df['date'] == test_date]

                test_result = {
                    'date': test_date,
                    'expected_hydro_year': expected_hydro_year,
                    'expected_hydro_doy': expected_hydro_doy,
                    'expected_nearest': expected_nearest,
                    'expected_distance': expected_distance,
                    'found_in_data': not test_row.empty
                }

                if not test_row.empty:
                    actual_hydro_year = test_row['hydrological_year'].iloc[0]
                    actual_hydro_doy = test_row['hydrological_doy'].iloc[0]
                    actual_nearest = test_row['nearest_solstice_equinox'].iloc[0]
                    actual_distance = test_row['distance_to_nearest_solstice_equinox'].iloc[0]

                    test_result.update({
                        'actual_hydro_year': actual_hydro_year,
                        'actual_hydro_doy': actual_hydro_doy,
                        'actual_nearest': actual_nearest,
                        'actual_distance': actual_distance,
                        'hydro_year_correct': actual_hydro_year == expected_hydro_year,
                        'hydro_doy_correct': actual_hydro_doy == expected_hydro_doy,
                        'nearest_correct': actual_nearest == expected_nearest,
                        'distance_correct': actual_distance == expected_distance
                    })

                    # æ£€æŸ¥æ‰€æœ‰å­—æ®µæ˜¯å¦æ­£ç¡®
                    all_correct = (actual_hydro_year == expected_hydro_year and
                                   actual_hydro_doy == expected_hydro_doy and
                                   actual_nearest == expected_nearest and
                                   actual_distance == expected_distance)

                    status = "âœ…" if all_correct else "âŒ"

                    if all_correct:
                        passed_tests += 1

                    self.logger.info(f"  {status} {test_date}: "
                                     f"æ°´æ–‡å¹´={actual_hydro_year}(æœŸæœ›{expected_hydro_year}), "
                                     f"æ°´æ–‡å¹´DOY={actual_hydro_doy}(æœŸæœ›{expected_hydro_doy}), "
                                     f"æœ€è¿‘={actual_nearest}(æœŸæœ›{expected_nearest}), "
                                     f"è·ç¦»={actual_distance}(æœŸæœ›{expected_distance})")
                else:
                    test_result.update({
                        'actual_hydro_year': None,
                        'actual_hydro_doy': None,
                        'actual_nearest': None,
                        'actual_distance': None,
                        'hydro_year_correct': False,
                        'hydro_doy_correct': False,
                        'nearest_correct': False,
                        'distance_correct': False
                    })

                    self.logger.warning(f"  âš ï¸  æµ‹è¯•æ—¥æœŸ {test_date} åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨")

                validation_results['test_cases'].append(test_result)

            # ç»Ÿè®¡éªŒè¯ç»“æœ
            validation_results['summary'] = {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': total_tests - passed_tests,
                'success_rate': (passed_tests / total_tests) * 100 if total_tests > 0 else 0
            }

            self.logger.info(f"ğŸ“Š ç²¾ç¡®äºŒåˆ†äºŒè‡³æ—¥éªŒè¯ç»“æœ: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡ "
                             f"({validation_results['summary']['success_rate']:.1f}%)")

            return validation_results

        except Exception as e:
            self.logger.error(f"âŒ éªŒè¯äºŒåˆ†äºŒè‡³æ—¥è®¡ç®—å¤±è´¥: {e}")
            return validation_results

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜...
    def calculate_hydrological_doy(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ°´æ–‡å¹´DOY"""
        try:
            df_copy = df.copy()
            df_copy['date'] = pd.to_datetime(df_copy['date'])
            df_copy['natural_doy'] = df_copy['date'].dt.dayofyear
            df_copy['year'] = df_copy['date'].dt.year
            df_copy['month'] = df_copy['date'].dt.month

            def is_leap_year(year):
                return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)

            df_copy['is_leap_year'] = df_copy['year'].apply(is_leap_year)

            def get_hydrological_doy(row):
                try:
                    natural_doy = row['natural_doy']
                    month = row['month']
                    is_leap = row['is_leap_year']

                    if pd.isna(natural_doy) or pd.isna(month):
                        return np.nan

                    if month >= 9:
                        if is_leap:
                            hydrological_doy = natural_doy - 244
                        else:
                            hydrological_doy = natural_doy - 243
                    else:
                        hydrological_doy = natural_doy + 122

                    if hydrological_doy < 1:
                        return 1
                    elif hydrological_doy > 366:
                        return 366
                    else:
                        return int(hydrological_doy)

                except Exception:
                    return np.nan

            df_copy['hydrological_doy'] = df_copy.apply(get_hydrological_doy, axis=1)

            def get_hydrological_year(date):
                try:
                    if pd.isna(date):
                        return np.nan
                    year = date.year
                    month = date.month
                    if month >= 9:
                        return year + 1
                    else:
                        return year
                except Exception:
                    return np.nan

            df_copy['hydrological_year'] = df_copy['date'].apply(get_hydrological_year)

            self.logger.info(f"âœ… æ°´æ–‡å¹´DOYè®¡ç®—å®Œæˆ: {df_copy['hydrological_doy'].notna().sum()}/{len(df_copy)} æœ‰æ•ˆè®°å½•")
            return df_copy

        except Exception as e:
            self.logger.error(f"âŒ è®¡ç®—æ°´æ–‡å¹´DOYå¤±è´¥: {e}")
            return df

    # å…¶ä»–æ–¹æ³•ï¼ˆfind_and_save_duplicates, load_reference_data, load_integrated_data, copy_scp_columnsç­‰ï¼‰
    # ä¿æŒä¸å˜ï¼Œè¿™é‡Œçœç•¥ä»¥èŠ‚çœç©ºé—´...

    def generate_solstice_equinox_report(self) -> str:
        """ç”Ÿæˆç²¾ç¡®çš„äºŒåˆ†äºŒè‡³æ—¥æŠ¥å‘Š"""
        if self.corrected_df is None:
            return "æ— æ•°æ®å¯ç”ŸæˆæŠ¥å‘Š"

        try:
            report_lines = [
                "=" * 60,
                "ç²¾ç¡®äºŒåˆ†äºŒè‡³æ—¥è·ç¦»è®¡ç®—æŠ¥å‘Š",
                "=" * 60,
                f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                ""
            ]

            # æ˜¾ç¤º2012-2018å¹´ç²¾ç¡®çš„äºŒåˆ†äºŒè‡³æ—¥
            report_lines.append("2012-2018å¹´ç²¾ç¡®äºŒåˆ†äºŒè‡³æ—¥æ—¥æœŸ:")
            for year, events in self.solstice_equinox_dates.items():
                report_lines.append(f"  {year}æ°´æ–‡å¹´:")
                for event, date in events.items():
                    # è®¡ç®—è¯¥æ—¥æœŸçš„æ°´æ–‡å¹´DOY
                    event_dt = pd.to_datetime(date)
                    hydro_doy = self._calculate_hydrological_doy_for_date(event_dt)
                    report_lines.append(f"    {event}: {date} (æ°´æ–‡å¹´DOY={hydro_doy})")
            report_lines.append("")

            # æœ€è¿‘å…³é”®æ—¥åˆ†å¸ƒ
            if 'nearest_solstice_equinox' in self.corrected_df.columns:
                nearest_counts = self.corrected_df['nearest_solstice_equinox'].value_counts()
                report_lines.append("æœ€è¿‘å…³é”®æ—¥åˆ†å¸ƒ:")
                for event, count in nearest_counts.items():
                    percentage = (count / len(self.corrected_df)) * 100
                    report_lines.append(f"  - {event}: {count} æ¡è®°å½• ({percentage:.1f}%)")
                report_lines.append("")

            # è·ç¦»ç»Ÿè®¡
            if 'distance_to_nearest_solstice_equinox' in self.corrected_df.columns:
                dist_stats = self.corrected_df['distance_to_nearest_solstice_equinox'].describe()
                report_lines.extend([
                    "åˆ°æœ€è¿‘å…³é”®æ—¥è·ç¦»ç»Ÿè®¡:",
                    f"æœ€å°å€¼: {dist_stats['min']:.0f} å¤©",
                    f"æœ€å¤§å€¼: {dist_stats['max']:.0f} å¤©",
                    f"å¹³å‡å€¼: {dist_stats['mean']:.1f} å¤©",
                    f"æ ‡å‡†å·®: {dist_stats['std']:.1f} å¤©",
                    ""
                ])

            # å„å…³é”®æ—¥è·ç¦»ç»Ÿè®¡
            solstice_cols = [col for col in self.corrected_df.columns if
                             col.startswith('distance_to_') and not col.endswith('nearest')]
            if solstice_cols:
                report_lines.append("å„å…³é”®æ—¥è·ç¦»ç»Ÿè®¡:")
                for col in solstice_cols:
                    event = col.replace('distance_to_', '')
                    event_stats = self.corrected_df[col].describe()
                    valid_count = self.corrected_df[col].notna().sum()
                    report_lines.append(f"  - {event}: {valid_count} æ¡æœ‰æ•ˆè®°å½•")
                    report_lines.append(f"    æœ€å°å€¼: {event_stats['min']:.0f} å¤©")
                    report_lines.append(f"    æœ€å¤§å€¼: {event_stats['max']:.0f} å¤©")
                    report_lines.append(f"    å¹³å‡å€¼: {event_stats['mean']:.1f} å¤©")
                report_lines.append("")

            # æŒ‰æ°´æ–‡å¹´ç»Ÿè®¡
            if 'hydrological_year' in self.corrected_df.columns:
                hydro_years = sorted(self.corrected_df['hydrological_year'].dropna().unique())
                report_lines.append("å„æ°´æ–‡å¹´è¦†ç›–æƒ…å†µ:")
                for hydro_year in hydro_years:
                    year_mask = self.corrected_df['hydrological_year'] == hydro_year
                    year_count = year_mask.sum()
                    if year_count > 0:
                        report_lines.append(f"  {int(hydro_year)}æ°´æ–‡å¹´: {year_count} æ¡è®°å½•")
                report_lines.append("")

            # ç¤ºä¾‹æ•°æ®
            report_lines.append("æ•°æ®ç¤ºä¾‹ (å‰5æ¡):")
            sample_cols = ['station_id', 'date', 'hydrological_year', 'hydrological_doy',
                           'nearest_solstice_equinox', 'distance_to_nearest_solstice_equinox']

            for event in ['autumn_equinox', 'winter_solstice', 'spring_equinox', 'summer_solstice']:
                sample_cols.append(f'distance_to_{event}')

            if 'scp_start' in self.corrected_df.columns:
                sample_cols.extend(['scp_start', 'scp_end'])

            sample_data = self.corrected_df[sample_cols].head()
            for _, row in sample_data.iterrows():
                sample_line = f"  ç«™ç‚¹ {row['station_id']} | æ—¥æœŸ {row['date'].strftime('%Y-%m-%d')} | "
                sample_line += f"æ°´æ–‡å¹´ {int(row['hydrological_year'])} | "
                sample_line += f"æ°´æ–‡å¹´DOY {row['hydrological_doy']} | "
                sample_line += f"æœ€è¿‘ {row['nearest_solstice_equinox']} | "
                sample_line += f"è·ç¦» {row['distance_to_nearest_solstice_equinox']} å¤©"

                # æ˜¾ç¤ºå„å…³é”®æ—¥è·ç¦»
                distances = []
                for event in ['autumn_equinox', 'winter_solstice', 'spring_equinox', 'summer_solstice']:
                    dist = row[f'distance_to_{event}']
                    if pd.notna(dist):
                        distances.append(f"{event[:3]}:{int(dist)}")
                if distances:
                    sample_line += f" | å„è·ç¦»[{', '.join(distances)}]"

                if 'scp_start' in row and pd.notna(row['scp_start']):
                    sample_line += f" | SCP[{int(row['scp_start'])}-{int(row['scp_end'])}]"

                report_lines.append(sample_line)

            return "\n".join(report_lines)

        except Exception as e:
            self.logger.error(f"âŒ ç”ŸæˆäºŒåˆ†äºŒè‡³æ—¥æŠ¥å‘Šå¤±è´¥: {e}")
            return f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}"

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜...
    def find_and_save_duplicates(self):
        """æ‰¾å‡ºå¹¶ä¿å­˜æ‰€æœ‰é‡å¤è®°å½•"""
        if self.reference_df is None or self.integrated_df is None:
            self.logger.error("âŒ æ•°æ®æœªåŠ è½½å®Œæˆ")
            return

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        ref_duplicates = self.reference_df[
            self.reference_df.duplicated(subset=['station_id', 'date'], keep=False)
        ].copy()

        if len(ref_duplicates) > 0:
            ref_duplicates['_duplicate_group'] = ref_duplicates.groupby(['station_id', 'date']).ngroup()
            ref_output_path = self.output_dir / f"reference_duplicates_{timestamp}.xlsx"
            ref_duplicates.to_excel(ref_output_path, index=False)
            self.logger.info(f"ğŸ“ å‚è€ƒæ•°æ®é‡å¤è®°å½•å·²ä¿å­˜: {ref_output_path}")

    def load_reference_data(self, file_path: str) -> bool:
        """åŠ è½½å‚è€ƒæ•°æ®"""
        try:
            self.reference_df = pd.read_excel(file_path)
            self.logger.info(f"âœ… å‚è€ƒæ•°æ®åŠ è½½: {len(self.reference_df)}è¡Œ, {len(self.reference_df.columns)}åˆ—")

            required_cols = self.config['merge_keys'] + self.config['protected_columns']
            missing_cols = [col for col in required_cols if col not in self.reference_df.columns]

            if missing_cols:
                self.logger.error(f"âŒ å‚è€ƒæ•°æ®ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
                return False

            return True

        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½å‚è€ƒæ•°æ®å¤±è´¥: {e}")
            return False

    def load_integrated_data(self, file_path: str) -> bool:
        """åŠ è½½æ•´åˆæ•°æ®"""
        try:
            self.integrated_df = pd.read_excel(file_path)
            self.logger.info(f"âœ… æ•´åˆæ•°æ®åŠ è½½: {len(self.integrated_df)}è¡Œ, {len(self.integrated_df.columns)}åˆ—")

            missing_cols = [col for col in self.config['merge_keys'] if col not in self.integrated_df.columns]
            if missing_cols:
                self.logger.error(f"âŒ æ•´åˆæ•°æ®ç¼ºå°‘å…³é”®åˆ—: {missing_cols}")
                return False

            return True

        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½æ•´åˆæ•°æ®å¤±è´¥: {e}")
            return False

    def copy_scp_columns(self) -> bool:
        """
        å¤åˆ¶SCPåˆ—å¹¶è®¡ç®—ç²¾ç¡®çš„äºŒåˆ†äºŒè‡³æ—¥è·ç¦»
        """
        if self.reference_df is None or self.integrated_df is None:
            self.logger.error("âŒ æ•°æ®æœªåŠ è½½å®Œæˆ")
            return False

        try:
            # å…ˆæ‰¾å‡ºé‡å¤è®°å½•
            self.logger.info("ğŸ” åˆ†æé‡å¤è®°å½•...")
            self.find_and_save_duplicates()

            # è®°å½•åŸå§‹è®°å½•æ•°
            original_integrated_count = len(self.integrated_df)

            # åˆ›å»ºä¿®æ­£æ•°æ®å‰¯æœ¬
            self.corrected_df = self.integrated_df.copy()

            # å‡†å¤‡å‚è€ƒæ•°æ®
            ref_subset = self.reference_df[self.config['merge_keys'] + self.config['protected_columns']].copy()

            self.logger.info("ğŸ”„ å¤„ç†æ•°æ®æ ¼å¼å’Œç©ºå€¼...")

            # å¤„ç†å‚è€ƒæ•°æ®
            ref_subset = ref_subset.dropna(subset=['station_id'])
            ref_subset['station_id'] = (
                ref_subset['station_id']
                    .astype(float)
                    .apply(lambda x: str(int(x)) if pd.notna(x) else None)
            )
            ref_subset = ref_subset.dropna(subset=['station_id'])
            ref_subset['date'] = pd.to_datetime(ref_subset['date']).dt.strftime('%Y-%m-%d')

            # å»é‡
            ref_before_dedup = len(ref_subset)
            ref_subset = ref_subset.drop_duplicates(subset=self.config['merge_keys'])
            if ref_before_dedup != len(ref_subset):
                self.logger.info(f"âœ… å‚è€ƒæ•°æ®å»é‡: {ref_before_dedup} â†’ {len(ref_subset)}")

            # å¤„ç†æ•´åˆæ•°æ®
            self.corrected_df['station_id'] = self.corrected_df['station_id'].astype(str)
            self.corrected_df['date'] = pd.to_datetime(self.corrected_df['date']).dt.strftime('%Y-%m-%d')

            # å»é‡
            int_before_dedup = len(self.corrected_df)
            self.corrected_df = self.corrected_df.drop_duplicates(subset=self.config['merge_keys'])
            if int_before_dedup != len(self.corrected_df):
                self.logger.info(f"âœ… æ•´åˆæ•°æ®å»é‡: {int_before_dedup} â†’ {len(self.corrected_df)}")

            # æ‰§è¡Œåˆå¹¶
            self.logger.info("ğŸ”„ æ‰§è¡Œæ•°æ®åˆå¹¶...")
            merged = self.corrected_df.merge(
                ref_subset,
                on=self.config['merge_keys'],
                how='left',
                suffixes=('', '_ref')
            )

            # æ›´æ–°SCPåˆ—
            if 'scp_start_ref' in merged.columns:
                mask = merged['scp_start_ref'].notna()
                merged.loc[mask, 'scp_start'] = merged.loc[mask, 'scp_start_ref']
                self.logger.info(f"âœ… æ›´æ–°äº† {mask.sum()} æ¡ scp_start è®°å½•")

            if 'scp_end_ref' in merged.columns:
                mask = merged['scp_end_ref'].notna()
                merged.loc[mask, 'scp_end'] = merged.loc[mask, 'scp_end_ref']
                self.logger.info(f"âœ… æ›´æ–°äº† {mask.sum()} æ¡ scp_end è®°å½•")

            # ç§»é™¤ä¸´æ—¶åˆ—
            columns_to_keep = [col for col in merged.columns if not col.endswith('_ref')]
            self.corrected_df = merged[columns_to_keep]

            # è®¡ç®—æ°´æ–‡å¹´DOY
            self.logger.info("ğŸŒŠ è®¡ç®—æ°´æ–‡å¹´DOY...")
            self.corrected_df = self.calculate_hydrological_doy(self.corrected_df)

            # è®¡ç®—ç²¾ç¡®çš„äºŒåˆ†äºŒè‡³æ—¥è·ç¦»
            self.logger.info("ğŸŒ è®¡ç®—ç²¾ç¡®çš„äºŒåˆ†äºŒè‡³æ—¥è·ç¦»...")
            self.corrected_df = self.calculate_solstice_equinox_distances(self.corrected_df)

            # éªŒè¯è®¡ç®—
            self.logger.info("ğŸ” éªŒè¯ç²¾ç¡®è®¡ç®—...")
            solstice_validation = self.validate_solstice_equinox_calculation(self.corrected_df)

            self.logger.info("ğŸ“Š å¤„ç†å®Œæˆ!")
            self.logger.info(f"  åŸå§‹è®°å½•æ•°: {original_integrated_count}")
            self.logger.info(f"  æœ€ç»ˆè®°å½•æ•°: {len(self.corrected_df)}")
            self.logger.info(
                f"  ç²¾ç¡®äºŒåˆ†äºŒè‡³æ—¥éªŒè¯: {solstice_validation['summary']['passed_tests']}/{solstice_validation['summary']['total_tests']} é€šè¿‡")

            return True

        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False

    def validate_data_integrity(self) -> Dict[str, Any]:
        """æ•°æ®å®Œæ•´æ€§éªŒè¯"""
        if self.corrected_df is None:
            return {}

        validation = {
            'basic_stats': {
                'total_rows': len(self.corrected_df),
                'total_columns': len(self.corrected_df.columns),
                'unique_stations': self.corrected_df['station_id'].nunique(),
                'date_range': f"{self.corrected_df['date'].min()} åˆ° {self.corrected_df['date'].max()}"
            },
            'scp_coverage': {},
            'hydrological_info': {},
            'solstice_equinox_info': {},
            'duplicates': self.corrected_df.duplicated(subset=['station_id', 'date']).sum()
        }

        # SCPåˆ—è¦†ç›–ç»Ÿè®¡
        if 'scp_start' in self.corrected_df.columns:
            scp_start_coverage = (self.corrected_df['scp_start'].notna().sum() / len(self.corrected_df)) * 100
            validation['scp_coverage']['scp_start'] = f"{scp_start_coverage:.1f}%"

        if 'scp_end' in self.corrected_df.columns:
            scp_end_coverage = (self.corrected_df['scp_end'].notna().sum() / len(self.corrected_df)) * 100
            validation['scp_coverage']['scp_end'] = f"{scp_end_coverage:.1f}%"

        # æ°´æ–‡å¹´ä¿¡æ¯ç»Ÿè®¡
        if 'hydrological_doy' in self.corrected_df.columns:
            hydro_doy_valid = self.corrected_df['hydrological_doy'].notna().sum()
            hydro_year_valid = self.corrected_df['hydrological_year'].notna().sum()
            leap_year_count = self.corrected_df['is_leap_year'].sum()

            validation['hydrological_info'] = {
                'hydro_doy_coverage': f"{(hydro_doy_valid / len(self.corrected_df)) * 100:.1f}%",
                'hydro_year_coverage': f"{(hydro_year_valid / len(self.corrected_df)) * 100:.1f}%",
                'hydro_year_range': f"{self.corrected_df['hydrological_year'].min():.0f} - {self.corrected_df['hydrological_year'].max():.0f}",
                'hydro_doy_range': f"{self.corrected_df['hydrological_doy'].min():.0f} - {self.corrected_df['hydrological_doy'].max():.0f}",
                'leap_year_records': f"{leap_year_count}/{len(self.corrected_df)} ({(leap_year_count / len(self.corrected_df)) * 100:.1f}%)"
            }

        # äºŒåˆ†äºŒè‡³æ—¥ä¿¡æ¯ç»Ÿè®¡
        solstice_cols = [col for col in self.corrected_df.columns if
                         col.startswith('distance_to_') and not col.endswith('nearest')]
        if solstice_cols:
            nearest_counts = self.corrected_df['nearest_solstice_equinox'].value_counts()
            avg_nearest_distance = self.corrected_df['distance_to_nearest_solstice_equinox'].mean()

            validation['solstice_equinox_info'] = {
                'nearest_distribution': nearest_counts.to_dict(),
                'avg_distance_to_nearest': f"{avg_nearest_distance:.1f}",
                'min_distance_to_nearest': f"{self.corrected_df['distance_to_nearest_solstice_equinox'].min():.0f}",
                'max_distance_to_nearest': f"{self.corrected_df['distance_to_nearest_solstice_equinox'].max():.0f}"
            }

        return validation

    def save_results(self, suffix: str = "") -> Optional[str]:
        """ä¿å­˜ç»“æœ"""
        if self.corrected_df is None:
            self.logger.error("âŒ æ²¡æœ‰ä¿®æ­£æ•°æ®å¯ä¿å­˜")
            return None

        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            suffix_str = f"_{suffix}" if suffix else ""
            filename = f"corrected_data{suffix_str}_{timestamp}.xlsx"
            output_path = self.output_dir / filename

            self.corrected_df.to_excel(output_path, index=False)
            self.logger.info(f"ğŸ’¾ ç»“æœä¿å­˜æˆåŠŸ: {output_path}")

            return str(output_path)

        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return None


def main():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    pipeline = StableDataPipeline("pipeline_output")

    # åŠ è½½æ•°æ®
    if not pipeline.load_reference_data("D:/pyworkspace/fusing xgb/src/training/integrated_data.xlsx"):
        return

    if not pipeline.load_integrated_data("D:/pyworkspace/fusing xgb/src/process/integration/pipeline_output/corrected_data_with_corrected_hydro_doy_20251021_192055.xlsx"):
        return

    # æ‰§è¡Œä¿®å¤
    if pipeline.copy_scp_columns():
        validation = pipeline.validate_data_integrity()

        print("\nğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"  SCPåˆ—è¦†ç›–: {validation['scp_coverage']}")
        print(f"  é‡å¤è®°å½•: {validation['duplicates']}")
        if validation['hydrological_info']:
            print(f"  æ°´æ–‡å¹´ä¿¡æ¯: {validation['hydrological_info']}")
        if validation['solstice_equinox_info']:
            print(f"  äºŒåˆ†äºŒè‡³æ—¥ä¿¡æ¯: {validation['solstice_equinox_info']}")

        # ç”Ÿæˆç²¾ç¡®çš„äºŒåˆ†äºŒè‡³æ—¥æŠ¥å‘Š
        solstice_report = pipeline.generate_solstice_equinox_report()
        print(f"\nğŸŒ ç²¾ç¡®äºŒåˆ†äºŒè‡³æ—¥è¯¦ç»†æŠ¥å‘Š:\n{solstice_report}")

        output_path = pipeline.save_results("with_exact_solstice_distances")

        if output_path:
            print(f"\nâœ… å¤„ç†å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {output_path}")
    else:
        print("\nâŒ å¤„ç†å¤±è´¥ï¼")


if __name__ == "__main__":
    main()