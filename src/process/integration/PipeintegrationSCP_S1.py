import pandas as pd
import numpy as np
import logging
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any

logger = logging.getLogger("StableDataPipeline")


class StableDataPipeline:
    """
    ç¨³å®šæ•°æ®ç®¡é“ - æ‰¾å‡ºé‡å¤è®°å½•
    """

    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logger

        self.reference_df: Optional[pd.DataFrame] = None
        self.integrated_df: Optional[pd.DataFrame] = None
        self.corrected_df: Optional[pd.DataFrame] = None

        self.config = {
            'merge_keys': ['station_id', 'date'],
            'protected_columns': ['scp_start', 'scp_end'],
            'backup_original': True
        }

    def find_and_save_duplicates(self):
        """æ‰¾å‡ºå¹¶ä¿å­˜æ‰€æœ‰é‡å¤è®°å½•"""
        if self.reference_df is None or self.integrated_df is None:
            self.logger.error("âŒ æ•°æ®æœªåŠ è½½å®Œæˆ")
            return

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # 1. æ‰¾å‡ºå‚è€ƒæ•°æ®çš„é‡å¤è®°å½•
        ref_duplicates = self.reference_df[
            self.reference_df.duplicated(subset=['station_id', 'date'], keep=False)
        ].copy()

        if len(ref_duplicates) > 0:
            ref_duplicates['_duplicate_group'] = ref_duplicates.groupby(['station_id', 'date']).ngroup()
            ref_output_path = self.output_dir / f"reference_duplicates_{timestamp}.xlsx"
            ref_duplicates.to_excel(ref_output_path, index=False)
            self.logger.info(f"ğŸ“ å‚è€ƒæ•°æ®é‡å¤è®°å½•å·²ä¿å­˜: {ref_output_path}")

            # æ˜¾ç¤ºé‡å¤ç»Ÿè®¡
            ref_dup_stats = ref_duplicates.groupby(['station_id', 'date']).size().reset_index(name='count')
            self.logger.info("ğŸ” å‚è€ƒæ•°æ®é‡å¤è®°å½•è¯¦æƒ…:")
            for _, row in ref_dup_stats.iterrows():
                self.logger.info(f"  ç«™ç‚¹: {row['station_id']}, æ—¥æœŸ: {row['date']}, é‡å¤æ¬¡æ•°: {row['count']}")

        # 2. æ‰¾å‡ºæ•´åˆæ•°æ®çš„é‡å¤è®°å½•
        int_duplicates = self.integrated_df[
            self.integrated_df.duplicated(subset=['station_id', 'date'], keep=False)
        ].copy()

        if len(int_duplicates) > 0:
            int_duplicates['_duplicate_group'] = int_duplicates.groupby(['station_id', 'date']).ngroup()
            int_output_path = self.output_dir / f"integrated_duplicates_{timestamp}.xlsx"
            int_duplicates.to_excel(int_output_path, index=False)
            self.logger.info(f"ğŸ“ æ•´åˆæ•°æ®é‡å¤è®°å½•å·²ä¿å­˜: {int_output_path}")

            # æ˜¾ç¤ºé‡å¤ç»Ÿè®¡
            int_dup_stats = int_duplicates.groupby(['station_id', 'date']).size().reset_index(name='count')
            self.logger.info("ğŸ” æ•´åˆæ•°æ®é‡å¤è®°å½•è¯¦æƒ…:")
            for _, row in int_dup_stats.iterrows():
                self.logger.info(f"  ç«™ç‚¹: {row['station_id']}, æ—¥æœŸ: {row['date']}, é‡å¤æ¬¡æ•°: {row['count']}")

        # 3. æ‰¾å‡ºåˆå¹¶åäº§ç”Ÿçš„é‡å¤è®°å½•ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if self.corrected_df is not None:
            corrected_duplicates = self.corrected_df[
                self.corrected_df.duplicated(subset=['station_id', 'date'], keep=False)
            ].copy()

            if len(corrected_duplicates) > 0:
                corrected_duplicates['_duplicate_group'] = corrected_duplicates.groupby(['station_id', 'date']).ngroup()
                corrected_output_path = self.output_dir / f"corrected_duplicates_{timestamp}.xlsx"
                corrected_duplicates.to_excel(corrected_output_path, index=False)
                self.logger.info(f"ğŸ“ ä¿®æ­£åæ•°æ®é‡å¤è®°å½•å·²ä¿å­˜: {corrected_output_path}")

        # 4. ç”Ÿæˆé‡å¤è®°å½•åˆ†ææŠ¥å‘Š
        self._generate_duplicate_report(ref_duplicates, int_duplicates, timestamp)

    def _generate_duplicate_report(self, ref_duplicates, int_duplicates, timestamp):
        """ç”Ÿæˆé‡å¤è®°å½•åˆ†ææŠ¥å‘Š"""
        report_lines = [
            "=" * 60,
            "é‡å¤è®°å½•åˆ†ææŠ¥å‘Š",
            "=" * 60,
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            ""
        ]

        # å‚è€ƒæ•°æ®é‡å¤åˆ†æ
        if len(ref_duplicates) > 0:
            ref_dup_stats = ref_duplicates.groupby(['station_id', 'date']).size().reset_index(name='count')
            report_lines.extend([
                "å‚è€ƒæ•°æ®é‡å¤è®°å½•:",
                f"æ€»é‡å¤ç»„æ•°: {len(ref_dup_stats)}",
                f"æ€»é‡å¤è®°å½•æ•°: {len(ref_duplicates)}",
                ""
            ])

            report_lines.append("é‡å¤è®°å½•è¯¦æƒ…:")
            for _, row in ref_dup_stats.iterrows():
                report_lines.append(f"  - ç«™ç‚¹: {row['station_id']}, æ—¥æœŸ: {row['date']}, é‡å¤æ¬¡æ•°: {row['count']}")
        else:
            report_lines.append("å‚è€ƒæ•°æ®: æ— é‡å¤è®°å½•")

        report_lines.append("")

        # æ•´åˆæ•°æ®é‡å¤åˆ†æ
        if len(int_duplicates) > 0:
            int_dup_stats = int_duplicates.groupby(['station_id', 'date']).size().reset_index(name='count')
            report_lines.extend([
                "æ•´åˆæ•°æ®é‡å¤è®°å½•:",
                f"æ€»é‡å¤ç»„æ•°: {len(int_dup_stats)}",
                f"æ€»é‡å¤è®°å½•æ•°: {len(int_duplicates)}",
                ""
            ])

            report_lines.append("é‡å¤è®°å½•è¯¦æƒ…:")
            for _, row in int_dup_stats.iterrows():
                report_lines.append(f"  - ç«™ç‚¹: {row['station_id']}, æ—¥æœŸ: {row['date']}, é‡å¤æ¬¡æ•°: {row['count']}")
        else:
            report_lines.append("æ•´åˆæ•°æ®: æ— é‡å¤è®°å½•")

        # ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / f"duplicate_analysis_report_{timestamp}.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))

        self.logger.info(f"ğŸ“„ é‡å¤è®°å½•åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        # åœ¨æ§åˆ¶å°è¾“å‡ºæ‘˜è¦
        print("\n" + "=" * 50)
        print("é‡å¤è®°å½•æ‘˜è¦")
        print("=" * 50)
        if len(ref_duplicates) > 0:
            print(f"å‚è€ƒæ•°æ®: {len(ref_duplicates)} æ¡é‡å¤è®°å½•")
        if len(int_duplicates) > 0:
            print(f"æ•´åˆæ•°æ®: {len(int_duplicates)} æ¡é‡å¤è®°å½•")

    def load_reference_data(self, file_path: str) -> bool:
        """åŠ è½½å‚è€ƒæ•°æ®"""
        try:
            self.reference_df = pd.read_excel(file_path)
            self.logger.info(f"âœ… å‚è€ƒæ•°æ®åŠ è½½: {len(self.reference_df)}è¡Œ, {len(self.reference_df.columns)}åˆ—")

            required_cols = self.config['merge_keys'] + self.config['protected_columns']
            missing_cols = [col for col in required_cols if col not in self.reference_df.columns]

            if missing_cols:
                self.logger.error(f"âŒ å‚è€ƒæ•°æ®ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
                return False

            return True

        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½å‚è€ƒæ•°æ®å¤±è´¥: {e}")
            return False

    def load_integrated_data(self, file_path: str) -> bool:
        """åŠ è½½æ•´åˆæ•°æ®"""
        try:
            self.integrated_df = pd.read_excel(file_path)
            self.logger.info(f"âœ… æ•´åˆæ•°æ®åŠ è½½: {len(self.integrated_df)}è¡Œ, {len(self.integrated_df.columns)}åˆ—")

            missing_cols = [col for col in self.config['merge_keys'] if col not in self.integrated_df.columns]
            if missing_cols:
                self.logger.error(f"âŒ æ•´åˆæ•°æ®ç¼ºå°‘å…³é”®åˆ—: {missing_cols}")
                return False

            return True

        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½æ•´åˆæ•°æ®å¤±è´¥: {e}")
            return False

    def copy_scp_columns(self) -> bool:
        """
        å¤åˆ¶SCPåˆ—å¹¶æ‰¾å‡ºé‡å¤è®°å½•
        """
        if self.reference_df is None or self.integrated_df is None:
            self.logger.error("âŒ æ•°æ®æœªåŠ è½½å®Œæˆ")
            return False

        try:
            # å…ˆæ‰¾å‡ºé‡å¤è®°å½•
            self.logger.info("ğŸ” å¼€å§‹åˆ†æé‡å¤è®°å½•...")
            self.find_and_save_duplicates()

            # è®°å½•åŸå§‹è®°å½•æ•°
            original_integrated_count = len(self.integrated_df)

            # åˆ›å»ºä¿®æ­£æ•°æ®å‰¯æœ¬
            self.corrected_df = self.integrated_df.copy()

            # å‡†å¤‡å‚è€ƒæ•°æ®
            ref_subset = self.reference_df[self.config['merge_keys'] + self.config['protected_columns']].copy()

            self.logger.info("ğŸ”„ å¤„ç†æ•°æ®æ ¼å¼å’Œç©ºå€¼...")

            # 1. å¤„ç†å‚è€ƒæ•°æ®
            ref_subset = ref_subset.dropna(subset=['station_id'])

            # å®‰å…¨è½¬æ¢
            ref_subset['station_id'] = (
                ref_subset['station_id']
                    .astype(float)
                    .apply(lambda x: str(int(x)) if pd.notna(x) else None)
            )
            ref_subset = ref_subset.dropna(subset=['station_id'])
            ref_subset['date'] = pd.to_datetime(ref_subset['date']).dt.strftime('%Y-%m-%d')

            # å»é‡
            ref_before_dedup = len(ref_subset)
            ref_subset = ref_subset.drop_duplicates(subset=self.config['merge_keys'])
            if ref_before_dedup != len(ref_subset):
                self.logger.info(f"âœ… å‚è€ƒæ•°æ®å»é‡: {ref_before_dedup} â†’ {len(ref_subset)}")

            # 2. å¤„ç†æ•´åˆæ•°æ®
            self.corrected_df['station_id'] = self.corrected_df['station_id'].astype(str)
            self.corrected_df['date'] = pd.to_datetime(self.corrected_df['date']).dt.strftime('%Y-%m-%d')

            # å»é‡
            int_before_dedup = len(self.corrected_df)
            self.corrected_df = self.corrected_df.drop_duplicates(subset=self.config['merge_keys'])
            if int_before_dedup != len(self.corrected_df):
                self.logger.info(f"âœ… æ•´åˆæ•°æ®å»é‡: {int_before_dedup} â†’ {len(self.corrected_df)}")

            # æ‰§è¡Œåˆå¹¶
            self.logger.info("ğŸ”„ æ‰§è¡Œæ•°æ®åˆå¹¶...")
            merged = self.corrected_df.merge(
                ref_subset,
                on=self.config['merge_keys'],
                how='left',
                suffixes=('', '_ref')
            )

            # æ›´æ–°SCPåˆ—
            if 'scp_start_ref' in merged.columns:
                mask = merged['scp_start_ref'].notna()
                merged.loc[mask, 'scp_start'] = merged.loc[mask, 'scp_start_ref']
                self.logger.info(f"âœ… æ›´æ–°äº† {mask.sum()} æ¡ scp_start è®°å½•")

            if 'scp_end_ref' in merged.columns:
                mask = merged['scp_end_ref'].notna()
                merged.loc[mask, 'scp_end'] = merged.loc[mask, 'scp_end_ref']
                self.logger.info(f"âœ… æ›´æ–°äº† {mask.sum()} æ¡ scp_end è®°å½•")

            # ç§»é™¤ä¸´æ—¶åˆ—
            columns_to_keep = [col for col in merged.columns if not col.endswith('_ref')]
            self.corrected_df = merged[columns_to_keep]

            self.logger.info("ğŸ“Š å¤„ç†å®Œæˆ!")
            self.logger.info(f"  åŸå§‹è®°å½•æ•°: {original_integrated_count}")
            self.logger.info(f"  æœ€ç»ˆè®°å½•æ•°: {len(self.corrected_df)}")

            return True

        except Exception as e:
            self.logger.error(f"âŒ å¤åˆ¶SCPåˆ—å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False

    def validate_data_integrity(self) -> Dict[str, Any]:
        """æ•°æ®å®Œæ•´æ€§éªŒè¯"""
        if self.corrected_df is None:
            return {}

        validation = {
            'basic_stats': {
                'total_rows': len(self.corrected_df),
                'total_columns': len(self.corrected_df.columns),
                'unique_stations': self.corrected_df['station_id'].nunique(),
                'date_range': f"{self.corrected_df['date'].min()} åˆ° {self.corrected_df['date'].max()}"
            },
            'scp_coverage': {},
            'duplicates': self.corrected_df.duplicated(subset=['station_id', 'date']).sum()
        }

        if 'scp_start' in self.corrected_df.columns:
            scp_start_coverage = (self.corrected_df['scp_start'].notna().sum() / len(self.corrected_df)) * 100
            validation['scp_coverage']['scp_start'] = f"{scp_start_coverage:.1f}%"

        if 'scp_end' in self.corrected_df.columns:
            scp_end_coverage = (self.corrected_df['scp_end'].notna().sum() / len(self.corrected_df)) * 100
            validation['scp_coverage']['scp_end'] = f"{scp_end_coverage:.1f}%"

        return validation

    def save_results(self, suffix: str = "") -> Optional[str]:
        """ä¿å­˜ç»“æœ"""
        if self.corrected_df is None:
            self.logger.error("âŒ æ²¡æœ‰ä¿®æ­£æ•°æ®å¯ä¿å­˜")
            return None

        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            suffix_str = f"_{suffix}" if suffix else ""
            filename = f"corrected_data{suffix_str}_{timestamp}.xlsx"
            output_path = self.output_dir / filename

            self.corrected_df.to_excel(output_path, index=False)
            self.logger.info(f"ğŸ’¾ ç»“æœä¿å­˜æˆåŠŸ: {output_path}")

            return str(output_path)

        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return None


def main():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    pipeline = StableDataPipeline("pipeline_output")

    # åŠ è½½æ•°æ®
    if not pipeline.load_reference_data("D:/pyworkspace/fusing xgb/src/training/integrated_data.xlsx "):
        return

    if not pipeline.load_integrated_data("D:/pyworkspace/fusing xgb/src/outputs/combined/integrated_data_20251020_200459.xlsx"):
        return

    # æ‰§è¡Œä¿®å¤
    if pipeline.copy_scp_columns():
        validation = pipeline.validate_data_integrity()
        print("\nğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"  SCPåˆ—è¦†ç›–: {validation['scp_coverage']}")
        print(f"  é‡å¤è®°å½•: {validation['duplicates']}")

        output_path = pipeline.save_results("fixed_scp")

        if output_path:
            print(f"\nâœ… å¤„ç†å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {output_path}")
    else:
        print("\nâŒ å¤„ç†å¤±è´¥ï¼")


if __name__ == "__main__":
    main()