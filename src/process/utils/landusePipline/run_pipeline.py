# run_pipeline.py
import argparse
import logging
import sys
import os
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional, Callable
from functools import wraps
from dataclasses import dataclass
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ç°åœ¨å¯ä»¥ç›´æ¥å¯¼å…¥ä½ çš„åŸå§‹æ¨¡å—
try:
    from module.landuse_fixer import LandUseFixer
    from module.landuse_encoder import LandUseEncoder
except ImportError as e:
    print(f"âŒ å¯¼å…¥åŸå§‹æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ landuse_fixer.py å’Œ landuse_encoder.py åœ¨Pythonè·¯å¾„ä¸­")
    sys.exit(1)


# ==================== æ ¸å¿ƒPipelineæ¡†æ¶ ====================

@dataclass
class PipelineContext:
    """Pipelineæ‰§è¡Œä¸Šä¸‹æ–‡"""
    input_file: str
    output_dir: Path
    step_data: Dict[str, Any] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        self.step_data = {}
        self.metadata = {}
        self.output_dir = Path(self.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)


class PipelineStep:
    """Pipelineæ­¥éª¤è£…é¥°å™¨"""
    _registry = {}

    def __init__(self, name: str, version: str = "v1"):
        self.name = name
        self.version = version

    def __call__(self, func: Callable) -> Callable:
        @wraps(func)
        def wrapper(ctx: PipelineContext, *args, **kwargs):
            step_key = f"{self.name}_{self.version}"

            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
            input_file = ctx.step_data.get('previous_output', ctx.input_file)
            if not Path(input_file).exists():
                raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")

            logging.info(f"ğŸ¯ æ‰§è¡Œæ­¥éª¤: {step_key}")

            # æ‰§è¡Œæ­¥éª¤å‡½æ•°
            result = func(ctx, input_file, *args, **kwargs)

            # ä¿å­˜æ­¥éª¤ç»“æœ
            ctx.step_data[step_key] = {
                'input': input_file,
                'output': result,
                'timestamp': pd.Timestamp.now().isoformat()
            }
            ctx.step_data['previous_output'] = result

            logging.info(f"âœ… æ­¥éª¤å®Œæˆ: {step_key} -> {result}")
            return result

        # æ³¨å†Œæ­¥éª¤
        self._registry[f"{self.name}_{self.version}"] = wrapper
        return wrapper

    @classmethod
    def get_step(cls, name: str, version: str = "v1"):
        return cls._registry.get(f"{name}_{version}")


class Pipeline:
    """Pipelineæ‰§è¡Œå™¨"""

    def __init__(self, name: str = "LandUsePipeline"):
        self.name = name
        self.steps: List[Dict] = []
        self.logger = logging.getLogger(name)

    def add_step(self, step_name: str, step_func: Callable, **kwargs):
        """æ·»åŠ å¤„ç†æ­¥éª¤"""
        self.steps.append({
            'name': step_name,
            'func': step_func,
            'kwargs': kwargs
        })
        return self

    def execute(self, input_file: str, output_dir: str = None) -> str:
        """æ‰§è¡ŒPipeline"""
        if output_dir is None:
            output_dir = Path(input_file).parent / "pipeline_output"

        ctx = PipelineContext(input_file, output_dir)

        self.logger.info(f"ğŸš€ å¯åŠ¨Pipeline: {self.name}")
        self.logger.info(f"è¾“å…¥: {input_file}")
        self.logger.info(f"æ­¥éª¤æ•°: {len(self.steps)}")

        try:
            for i, step in enumerate(self.steps, 1):
                self.logger.info(f"\nğŸ“¦ æ­¥éª¤ {i}/{len(self.steps)}: {step['name']}")
                step['func'](ctx, **step['kwargs'])

            final_output = ctx.step_data['previous_output']
            self.logger.info(f"\nğŸ‰ Pipelineå®Œæˆ: {final_output}")

            # ä¿å­˜æ‰§è¡Œæ‘˜è¦
            self._save_execution_summary(ctx)

            return final_output

        except Exception as e:
            self.logger.error(f"âŒ Pipelineå¤±è´¥: {e}")
            raise

    def _save_execution_summary(self, ctx: PipelineContext):
        """ä¿å­˜æ‰§è¡Œæ‘˜è¦"""
        summary = {
            'pipeline_name': self.name,
            'execution_time': pd.Timestamp.now().isoformat(),
            'input_file': ctx.input_file,
            'output_dir': str(ctx.output_dir),
            'steps': ctx.step_data
        }

        summary_file = ctx.output_dir / "execution_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        self.logger.info(f"ğŸ“Š æ‰§è¡Œæ‘˜è¦: {summary_file}")


# ==================== Pipelineæ­¥éª¤å®šä¹‰ ====================

@PipelineStep(name="data_fixer", version="v1")
def fix_landuse_data(ctx: PipelineContext, input_file: str,
                     output_suffix: str = "_fixed") -> str:
    """ä¿®å¤åœŸåœ°åˆ©ç”¨æ•°æ®"""
    fixer = LandUseFixer()
    output_file = ctx.output_dir / f"{Path(input_file).stem}{output_suffix}.xlsx"

    result = fixer.fix_landuse_data(input_file, str(output_file))
    if not result:
        raise ValueError("æ•°æ®ä¿®å¤å¤±è´¥")

    return result


@PipelineStep(name="feature_encoder", version="hashing")
def encode_with_hashing(ctx: PipelineContext, input_file: str,
                        landuse_col: str = "landuse_code",
                        n_features: int = 12) -> str:
    """ç‰¹å¾å“ˆå¸Œç¼–ç """
    encoder = LandUseEncoder()
    df = pd.read_excel(input_file)

    df_encoded = encoder.feature_hashing_encoding(
        df, landuse_col=landuse_col, n_features=n_features
    )

    output_file = ctx.output_dir / f"{Path(input_file).stem}_hashed.xlsx"
    df_encoded.to_excel(output_file, index=False)

    return str(output_file)


@PipelineStep(name="feature_encoder", version="composite")
def encode_composite(ctx: PipelineContext, input_file: str,
                     methods: List[str] = None) -> str:
    """å¤åˆç¼–ç ç­–ç•¥"""
    encoder = LandUseEncoder()
    df = pd.read_excel(input_file)

    methods = methods or ['hashing', 'frequency']

    for method in methods:
        if method == 'hashing':
            df = encoder.feature_hashing_encoding(df, n_features=10)
        elif method == 'frequency':
            df = encoder.frequency_encoding(df)
        elif method == 'target':
            if 'swe' in df.columns:
                df = encoder.target_encoding(df, target_col='swe')

    output_file = ctx.output_dir / f"{Path(input_file).stem}_composite_encoded.xlsx"
    df.to_excel(output_file, index=False)

    return str(output_file)


@PipelineStep(name="data_analyzer", version="basic")
def analyze_data(ctx: PipelineContext, input_file: str) -> str:
    """æ•°æ®åˆ†ææ­¥éª¤"""
    fixer = LandUseFixer()
    encoder = LandUseEncoder()

    # åˆ†æåŸå§‹æ•°æ®
    fixer.analyze_data(input_file)

    df = pd.read_excel(input_file)
    if 'landuse_code' in df.columns:
        encoder.compare_encoding_schemes(df)

    # è¿”å›åŸæ–‡ä»¶ï¼Œä¸ä¿®æ”¹æ•°æ®
    return input_file


# ==================== Pipelineæ„å»ºå™¨ ====================

class LandUsePipelineBuilder:
    """Pipelineæ„å»ºå™¨ - æµç•…æ¥å£"""

    def __init__(self):
        self.pipeline = Pipeline()

    def fix_data(self, version: str = "v1", **kwargs) -> 'LandUsePipelineBuilder':
        """æ·»åŠ æ•°æ®ä¿®å¤æ­¥éª¤"""
        step_func = PipelineStep.get_step("data_fixer", version)
        if not step_func:
            raise ValueError(f"æœªçŸ¥çš„æ•°æ®ä¿®å¤ç‰ˆæœ¬: {version}")

        self.pipeline.add_step(f"data_fixing_{version}", step_func, **kwargs)
        return self

    def encode_features(self, method: str = "hashing", **kwargs) -> 'LandUsePipelineBuilder':
        """æ·»åŠ ç‰¹å¾ç¼–ç æ­¥éª¤"""
        step_func = PipelineStep.get_step("feature_encoder", method)
        if not step_func:
            raise ValueError(f"æœªçŸ¥çš„ç¼–ç æ–¹æ³•: {method}")

        self.pipeline.add_step(f"feature_encoding_{method}", step_func, **kwargs)
        return self

    def analyze(self) -> 'LandUsePipelineBuilder':
        """æ·»åŠ åˆ†ææ­¥éª¤"""
        step_func = PipelineStep.get_step("data_analyzer", "basic")
        self.pipeline.add_step("data_analysis", step_func)
        return self

    def build(self) -> Pipeline:
        """æ„å»ºPipeline"""
        return self.pipeline


# é¢„å®šä¹‰Pipelineé…æ–¹
class PipelineRecipes:
    """é¢„å®šä¹‰çš„Pipelineé…æ–¹"""

    @staticmethod
    def basic_clean_encode() -> Pipeline:
        """åŸºç¡€æ¸…æ´—+ç¼–ç """
        return (LandUsePipelineBuilder()
                .fix_data("v1")
                .encode_features("hashing", n_features=10)
                .build())

    @staticmethod
    def advanced_analysis() -> Pipeline:
        """é«˜çº§åˆ†ææµç¨‹"""
        return (LandUsePipelineBuilder()
                .analyze()
                .fix_data("v1")
                .encode_features("composite", methods=['hashing', 'frequency'])
                .build())

    @staticmethod
    def quick_fix() -> Pipeline:
        """å¿«é€Ÿä¿®å¤"""
        return (LandUsePipelineBuilder()
                .fix_data("v1")
                .build())


# ==================== ä¸»ç¨‹åº ====================

def setup_logging(verbose: bool = False):
    """è®¾ç½®æ—¥å¿—"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('pipeline.log', encoding='utf-8')
        ]
    )


def main():
    parser = argparse.ArgumentParser(description='åœŸåœ°åˆ©ç”¨æ•°æ®å¤„ç†Pipeline')
    parser.add_argument('input_file', help='è¾“å…¥Excelæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output-dir', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--recipe', choices=['basic', 'advanced', 'quick'],
                        default='basic', help='é¢„å®šä¹‰é…æ–¹')
    parser.add_argument('--custom', action='store_true', help='è‡ªå®šä¹‰æµç¨‹')
    parser.add_argument('-v', '--verbose', action='store_true', help='è¯¦ç»†æ—¥å¿—')

    args = parser.parse_args()
    setup_logging(args.verbose)

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.input_file):
        logging.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        return 1

    try:
        if args.custom:
            # è‡ªå®šä¹‰æµç¨‹
            pipeline = (LandUsePipelineBuilder()
                        .fix_data("v1")
                        .encode_features("hashing", n_features=12)
                        .analyze()
                        .build())
        else:
            # ä½¿ç”¨é¢„å®šä¹‰é…æ–¹
            recipes = {
                'basic': PipelineRecipes.basic_clean_encode,
                'advanced': PipelineRecipes.advanced_analysis,
                'quick': PipelineRecipes.quick_fix
            }
            pipeline = recipes[args.recipe]()

        # æ‰§è¡ŒPipeline
        result = pipeline.execute(args.input_file, args.output_dir)
        print(f"ğŸ‰ Pipelineæ‰§è¡ŒæˆåŠŸ!")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {result}")

    except Exception as e:
        logging.error(f"Pipelineæ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        logging.debug(traceback.format_exc())
        return 1

    return 0


if __name__ == "__main__":
    exit(main())