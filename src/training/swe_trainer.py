import logging
import pandas as pd
import numpy as np
import xgboost as xgb
from matplotlib import pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
from scipy.stats import pearsonr
from sklearn.model_selection import LeaveOneGroupOut
import os
import joblib
import json
from datetime import datetime
import seaborn as sns
from scipy import stats

logger = logging.getLogger("SWEXGBoostTrainer")


class SWEXGBoostTrainer:
    """SWE XGBoostè®­ç»ƒå™¨ - é›†æˆæ•°æ®é¢„å¤„ç†ã€äº¤å‰éªŒè¯å’Œæ¨¡å‹è®­ç»ƒ"""

    # é»˜è®¤XGBoostå‚æ•°
    DEFAULT_PARAMS = {
        'n_estimators': 60,
        'learning_rate': 0.17,
        'max_depth': 5,
        'min_child_weight': 5,
        'gamma': 0,
        'subsample': 0.8,
        'colsample_bytree': 0.5,
        'reg_alpha': 0.05,
        'random_state': 42,
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse'
    }

    def __init__(self, params=None):
        """åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            params (dict, optional): XGBoostå‚æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å‚æ•°
        """
        self.logger = logger
        self.model = None
        self.feature_columns = None
        self.target_column = 'swe'

        # æ›´æ–°å‚æ•°
        self.params = self.DEFAULT_PARAMS.copy()
        if params:
            self.params.update(params)

        self.logger.info(f"åˆå§‹åŒ–SWE XGBoostè®­ç»ƒå™¨")
        self.logger.info(f"æ¨¡å‹å‚æ•°: {self.params}")

    def validate_data(self, df):
        """éªŒè¯è¾“å…¥æ•°æ®

        Args:
            df (pd.DataFrame): è¾“å…¥æ•°æ®

        Raises:
            ValueError: æ•°æ®éªŒè¯å¤±è´¥æ—¶æŠ›å‡º
        """
        self.logger.info("éªŒè¯è¾“å…¥æ•°æ®...")

        # æ£€æŸ¥DataFrameç±»å‹
        if not isinstance(df, pd.DataFrame):
            raise ValueError("è¾“å…¥æ•°æ®å¿…é¡»æ˜¯pandas DataFrame")

        # æ£€æŸ¥å¿…è¦åˆ—
        required_columns = ['station_id', 'date', self.target_column]
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")

        # æ£€æŸ¥ç›®æ ‡å˜é‡
        swe_na_count = df[self.target_column].isna().sum()
        swe_total_count = len(df)
        if swe_na_count == swe_total_count:
            raise ValueError(f"{self.target_column}åˆ—å…¨éƒ¨ä¸ºç©ºå€¼")

        self.logger.info(f"SWEæ•°æ®å®Œæ•´æ€§: {swe_total_count - swe_na_count}/{swe_total_count} æœ‰æ•ˆ")

        # æ£€æŸ¥æ ·æœ¬æ•°é‡
        if len(df) < 10:
            raise ValueError(f"æ ·æœ¬æ•°é‡å¤ªå°‘ ({len(df)})ï¼Œè‡³å°‘éœ€è¦10ä¸ªæ ·æœ¬")

        # æ£€æŸ¥ç«™ç‚¹æ•°é‡
        station_count = df['station_id'].nunique()
        if station_count < 2:
            raise ValueError(f"ç«™ç‚¹æ•°é‡å¤ªå°‘ ({station_count})ï¼Œè‡³å°‘éœ€è¦2ä¸ªç«™ç‚¹")

        self.logger.info(f"âœ… æ•°æ®éªŒè¯é€šè¿‡: {len(df)} è¡Œ, {len(df.columns)} åˆ—, {station_count} ä¸ªç«™ç‚¹")

    def preprocess_data(self, df):
        """æ•°æ®é¢„å¤„ç†"""
        self.logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")

        # éªŒè¯æ•°æ®
        self.validate_data(df)

        # åˆ›å»ºæ•°æ®å‰¯æœ¬
        df_clean = df.copy()

        # åˆ é™¤ç›®æ ‡å˜é‡ä¸ºç©ºçš„æ ·æœ¬
        initial_count = len(df_clean)
        df_clean = df_clean.dropna(subset=[self.target_column]).copy()
        removed_count = initial_count - len(df_clean)

        if removed_count > 0:
            self.logger.info(f"åˆ é™¤ {removed_count} ä¸ªSWEä¸ºç©ºå€¼çš„æ ·æœ¬")
        self.logger.info(f"å‰©ä½™æœ‰æ•ˆæ ·æœ¬: {len(df_clean)} è¡Œ")

        # å¤„ç†landuseå“ˆå¸Œç‰¹å¾ - åˆå¹¶ä¸ºå•ä¸ªå‘é‡ç‰¹å¾
        df_clean = self._process_landuse_features(df_clean)

        # ç¡®å®šç‰¹å¾åˆ—ï¼ˆæ’é™¤station_id, date, sweã€åŸå§‹çš„landuse_hashåˆ—å’Œhydrological_doyï¼‰
        exclude_columns = ['station_id', 'date', self.target_column, 'hydrological_doy']

        # æ’é™¤åŸå§‹çš„landuse_hashåˆ—
        original_landuse_columns = [col for col in df_clean.columns if col.startswith('landuse_hash_')]
        exclude_columns.extend(original_landuse_columns)

        self.feature_columns = [col for col in df_clean.columns if col not in exclude_columns]

        if not self.feature_columns:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç‰¹å¾åˆ—")

        self.logger.info(f"ä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")
        self.logger.info(f"ç‰¹å¾åˆ—è¡¨: {self.feature_columns}")

        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = df_clean[self.feature_columns].copy()
        y = df_clean[self.target_column].copy()

        # å¤„ç†ç‰¹å¾ç¼ºå¤±å€¼
        X_processed = self._handle_missing_values(X)

        # é‡è¦ï¼šç¡®ä¿Xå’Œyçš„é•¿åº¦ä¸€è‡´
        if len(X_processed) != len(y):
            self.logger.warning(f"ç‰¹å¾å’Œç›®æ ‡å˜é‡é•¿åº¦ä¸ä¸€è‡´: X={len(X_processed)}, y={len(y)}")
            # æ‰¾åˆ°å…±åŒçš„ç´¢å¼•
            common_idx = X_processed.index.intersection(y.index)
            X_processed = X_processed.loc[common_idx]
            y = y.loc[common_idx]
            self.logger.info(f"å¯¹é½å: X={len(X_processed)}, y={len(y)}")

        # å‡†å¤‡åˆ†ç»„ä¿¡æ¯
        df_clean['year'] = pd.to_datetime(df_clean['date']).dt.year
        # ç¡®ä¿åˆ†ç»„ä¿¡æ¯ä¸å¤„ç†åçš„æ•°æ®å¯¹é½
        station_groups = df_clean.loc[X_processed.index, 'station_id'].values
        year_groups = df_clean.loc[X_processed.index, 'year'].values

        # æœ€ç»ˆæ£€æŸ¥
        if len(X_processed) != len(y) or len(X_processed) != len(station_groups):
            raise ValueError(f"æ•°æ®é•¿åº¦ä¸ä¸€è‡´: X={len(X_processed)}, y={len(y)}, station_groups={len(station_groups)}")

        # å‡†å¤‡ç‰¹å¾ç”¨äºè®­ç»ƒï¼ˆå¤„ç†landuseå‘é‡ç‰¹å¾ï¼‰
        X_final = self._prepare_features_for_training(X_processed)

        # ç»Ÿè®¡ä¿¡æ¯
        station_count = len(np.unique(station_groups))
        year_count = len(np.unique(year_groups))
        swe_mean = y.mean()
        swe_std = y.std()

        self.logger.info("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        self.logger.info(f"  ğŸ“Š æ ·æœ¬æ•°: {len(X_final)}")
        self.logger.info(f"  ğŸ”§ ç‰¹å¾æ•°: {X_final.shape[1]}")
        self.logger.info(f"  ğŸ“ ç«™ç‚¹æ•°: {station_count}")
        self.logger.info(f"  ğŸ“… å¹´ä»½æ•°: {year_count}")
        self.logger.info(f"  â„ï¸  SWEç»Ÿè®¡: å‡å€¼={swe_mean:.2f}mm, æ ‡å‡†å·®={swe_std:.2f}mm")

        return X_final, y.values, station_groups, year_groups

    def _process_landuse_features(self, df):
        """å¤„ç†landuseå“ˆå¸Œç‰¹å¾ï¼Œå°†å¤šä¸ªå“ˆå¸Œåˆ—åˆå¹¶ä¸ºå•ä¸ªå‘é‡ç‰¹å¾

        Args:
            df (pd.DataFrame): åŸå§‹æ•°æ®

        Returns:
            pd.DataFrame: å¤„ç†åçš„æ•°æ®
        """
        self.logger.info("å¤„ç†landuseå“ˆå¸Œç‰¹å¾...")

        # æ‰¾å‡ºæ‰€æœ‰çš„landuse_hashåˆ—
        landuse_columns = [col for col in df.columns if col.startswith('landuse_hash_')]

        if not landuse_columns:
            self.logger.warning("æœªæ‰¾åˆ°landuse_hashç‰¹å¾åˆ—")
            return df

        self.logger.info(f"æ‰¾åˆ° {len(landuse_columns)} ä¸ªlanduseå“ˆå¸Œç‰¹å¾")

        # æ£€æŸ¥landuseåˆ—çš„æ•°æ®ç±»å‹
        for col in landuse_columns:
            unique_count = df[col].nunique()
            na_count = df[col].isna().sum()
            dtype = df[col].dtype
            self.logger.debug(f"  {col}: ç±»å‹={dtype}, å”¯ä¸€å€¼={unique_count}, ç¼ºå¤±å€¼={na_count}")

        # ç¡®ä¿æ‰€æœ‰landuseåˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
        for col in landuse_columns:
            if not pd.api.types.is_numeric_dtype(df[col]):
                self.logger.info(f"è½¬æ¢ {col} ä¸ºæ•°å€¼ç±»å‹")
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # åˆ›å»ºlanduseå‘é‡ç‰¹å¾ - ä½¿ç”¨å­—ç¬¦ä¸²è¿æ¥çš„æ–¹å¼è¡¨ç¤ºå‘é‡
        # è¿™æ ·é¿å…åœ¨DataFrameä¸­å­˜å‚¨numpyæ•°ç»„
        landuse_vectors = []

        for idx, row in df.iterrows():
            vector = []
            for col in landuse_columns:
                value = row[col]
                # å¤„ç†ç¼ºå¤±å€¼ï¼Œç”¨0å¡«å……
                if pd.isna(value):
                    vector.append(0.0)
                else:
                    try:
                        vector.append(float(value))
                    except (ValueError, TypeError):
                        vector.append(0.0)
            # å°†å‘é‡è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¡¨ç¤ºï¼Œé¿å…numpyæ•°ç»„
            vector_str = ','.join(map(str, vector))
            landuse_vectors.append(vector_str)

        # æ·»åŠ landuseå‘é‡ç‰¹å¾
        df['landuse_vector'] = landuse_vectors

        self.logger.info(f"âœ… landuseç‰¹å¾å¤„ç†å®Œæˆ")
        self.logger.info(f"  åˆ›å»ºäº†1ä¸ªlanduseå‘é‡ç‰¹å¾ï¼ˆå­—ç¬¦ä¸²è¡¨ç¤ºï¼‰")

        return df

    def _prepare_features_for_training(self, X_processed):
        """å‡†å¤‡ç‰¹å¾ç”¨äºè®­ç»ƒï¼Œå¤„ç†landuseå‘é‡ç‰¹å¾

        Args:
            X_processed (pd.DataFrame): å¤„ç†åçš„ç‰¹å¾æ•°æ®

        Returns:
            np.array: é€‚åˆè®­ç»ƒçš„ç‰¹å¾çŸ©é˜µ
        """
        self.logger.info("å‡†å¤‡ç‰¹å¾ç”¨äºè®­ç»ƒ...")

        # æ£€æŸ¥æ˜¯å¦æœ‰landuse_vectorç‰¹å¾
        if 'landuse_vector' in X_processed.columns:
            self.logger.info("å¤„ç†landuse_vectorç‰¹å¾...")

            # åˆ†ç¦»landuseå‘é‡ç‰¹å¾å’Œå…¶ä»–ç‰¹å¾
            non_vector_cols = [col for col in X_processed.columns if col != 'landuse_vector']
            vector_col = 'landuse_vector'

            # å¤„ç†éå‘é‡ç‰¹å¾
            if non_vector_cols:
                non_vector_data = X_processed[non_vector_cols].values
            else:
                non_vector_data = np.empty((len(X_processed), 0))

            # å¤„ç†landuseå‘é‡ç‰¹å¾ - å°†å­—ç¬¦ä¸²å‘é‡è½¬æ¢ä¸ºæ•°å€¼æ•°ç»„
            vector_data_list = []
            for vector_str in X_processed[vector_col]:
                # å°†å­—ç¬¦ä¸²è½¬æ¢å›æ•°å€¼æ•°ç»„
                vector_values = [float(x) for x in vector_str.split(',')]
                vector_data_list.append(vector_values)

            vector_data = np.array(vector_data_list)

            # åˆå¹¶éå‘é‡ç‰¹å¾å’Œå‘é‡ç‰¹å¾
            final_features = np.hstack([non_vector_data, vector_data]) if non_vector_cols else vector_data

            self.logger.info(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {final_features.shape}")
            self.logger.info(f"  - éå‘é‡ç‰¹å¾: {non_vector_data.shape[1] if non_vector_cols else 0}")
            self.logger.info(f"  - landuseå‘é‡ç‰¹å¾: {vector_data.shape[1]}ä¸ªå…ƒç´ ")

        else:
            # æ²¡æœ‰å‘é‡ç‰¹å¾ï¼Œç›´æ¥è¿”å›æ•°å€¼
            final_features = X_processed.values
            self.logger.info(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {final_features.shape}")
            self.logger.info("æ²¡æœ‰æ‰¾åˆ°landuse_vectorç‰¹å¾")

        return final_features

    def _handle_missing_values(self, X):
        """å¤„ç†ç‰¹å¾ç¼ºå¤±å€¼ï¼Œæ”¯æŒlanduseå‘é‡ç‰¹å¾"""
        self.logger.info("å¤„ç†ç‰¹å¾ç¼ºå¤±å€¼...")

        initial_missing = X.isna().sum().sum()
        if initial_missing == 0:
            self.logger.info("æ²¡æœ‰å‘ç°ç¼ºå¤±å€¼")
            return X

        X_processed = X.copy()
        initial_length = len(X_processed)

        # å¤„ç†æ•°å€¼ç‰¹å¾ï¼ˆä¸åŒ…æ‹¬landuse_vectorï¼‰
        numeric_cols = X_processed.select_dtypes(include=[np.number]).columns
        # æ’é™¤landuse_vectoråˆ—
        numeric_cols = [col for col in numeric_cols if col != 'landuse_vector']

        if len(numeric_cols) > 0:
            for col in numeric_cols:
                if X_processed[col].isna().sum() > 0:
                    # ä½¿ç”¨ä¸­ä½æ•°å¡«å……
                    median_val = X_processed[col].median()
                    if pd.isna(median_val):  # å¦‚æœä¸­ä½æ•°ä¹Ÿæ˜¯NaNï¼Œç”¨0å¡«å……
                        median_val = 0
                    X_processed[col] = X_processed[col].fillna(median_val)
                    self.logger.debug(f"å¡«å……æ•°å€¼ç‰¹å¾ '{col}' çš„ç¼ºå¤±å€¼")

        # å¤„ç†landuse_vectorç‰¹å¾çš„ç¼ºå¤±å€¼
        if 'landuse_vector' in X_processed.columns:
            na_mask = X_processed['landuse_vector'].isna()
            if na_mask.any():
                self.logger.info(f"å¤„ç†landuse_vectorç‰¹å¾çš„ç¼ºå¤±å€¼")
                # ç”¨é›¶å‘é‡å­—ç¬¦ä¸²å¡«å……ç¼ºå¤±å€¼
                zero_vector_str = ','.join(['0.0'] * 10)  # å‡è®¾æœ‰10ä¸ªlanduse_hashç‰¹å¾
                X_processed.loc[na_mask, 'landuse_vector'] = zero_vector_str

        # åˆ†ç±»ç‰¹å¾å¤„ç†
        categorical_cols = X_processed.select_dtypes(include=['object']).columns
        # æ’é™¤landuse_vectoråˆ—ï¼ˆå®ƒæ˜¯å¯¹è±¡ç±»å‹ä½†éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
        categorical_cols = [col for col in categorical_cols if col != 'landuse_vector']

        for col in categorical_cols:
            if X_processed[col].isna().sum() > 0:
                if len(X_processed[col].mode()) > 0:
                    fill_value = X_processed[col].mode()[0]
                else:
                    fill_value = 'missing'
                X_processed[col] = X_processed[col].fillna(fill_value)
                self.logger.debug(f"å¡«å……åˆ†ç±»ç‰¹å¾ '{col}' çš„ç¼ºå¤±å€¼")

            # è½¬æ¢ä¸ºæ•°å€¼ç¼–ç 
            X_processed[col] = X_processed[col].astype('category').cat.codes

        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ç¼ºå¤±å€¼
        remaining_missing = X_processed.isna().sum().sum()
        if remaining_missing > 0:
            self.logger.warning(f"ä»æœ‰ {remaining_missing} ä¸ªç¼ºå¤±å€¼ï¼Œå°†åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ")
            X_processed = X_processed.dropna()
            removed_rows = initial_length - len(X_processed)
            if removed_rows > 0:
                self.logger.info(f"åˆ é™¤äº† {removed_rows} è¡ŒåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®")

        return X_processed

    def validate_data_consistency(self, X, y, station_groups, year_groups):
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
        self.logger.info("éªŒè¯æ•°æ®ä¸€è‡´æ€§...")

        lengths = {
            'X': len(X),
            'y': len(y),
            'station_groups': len(station_groups),
            'year_groups': len(year_groups)
        }

        # æ£€æŸ¥æ‰€æœ‰é•¿åº¦æ˜¯å¦ä¸€è‡´
        unique_lengths = set(lengths.values())
        if len(unique_lengths) != 1:
            self.logger.error(f"æ•°æ®é•¿åº¦ä¸ä¸€è‡´: {lengths}")
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰NaNå€¼
        if np.isnan(X).any():
            self.logger.warning("ç‰¹å¾æ•°æ®ä¸­åŒ…å«NaNå€¼")

        if np.isnan(y).any():
            self.logger.warning("ç›®æ ‡å˜é‡ä¸­åŒ…å«NaNå€¼")

        self.logger.info(f"âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡: æ‰€æœ‰æ•°æ®é•¿åº¦ = {list(unique_lengths)[0]}")
        return True

    def evaluate_predictions(self, y_true, y_pred):
        """è¯„ä¼°é¢„æµ‹ç»“æœ

        Args:
            y_true (array-like): çœŸå®å€¼
            y_pred (array-like): é¢„æµ‹å€¼

        Returns:
            dict: åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        # ç§»é™¤NaNå€¼
        mask = ~(np.isnan(y_true) | np.isnan(y_pred))
        y_true_clean = y_true[mask]
        y_pred_clean = y_pred[mask]

        if len(y_true_clean) == 0:
            return {
                'MAE': np.nan,
                'RMSE': np.nan,
                'R': np.nan,
                'R_pvalue': np.nan,
                'æ ·æœ¬æ•°': 0,
                'æ€»æ ·æœ¬æ•°': len(y_true),
                'æœ‰æ•ˆæ ·æœ¬æ¯”ä¾‹': 0.0
            }

        # è®¡ç®—MAEå’ŒRMSE
        mae = mean_absolute_error(y_true_clean, y_pred_clean)
        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))

        # å®‰å…¨è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
        def safe_pearsonr(x, y):
            """å®‰å…¨è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°ï¼Œå¤„ç†å¸¸æ•°æ•°ç»„æƒ…å†µ"""
            # æ£€æŸ¥è¾“å…¥æ•°ç»„æ˜¯å¦ä¸ºå¸¸æ•°
            if len(x) <= 1:
                return np.nan, np.nan

            if np.all(x == x[0]) or np.all(y == y[0]):
                # å¦‚æœä»»ä¸€æ•°ç»„æ˜¯å¸¸æ•°ï¼Œç›¸å…³ç³»æ•°æœªå®šä¹‰
                return np.nan, np.nan

            # æ£€æŸ¥æ–¹å·®æ˜¯å¦ä¸º0
            if np.std(x) == 0 or np.std(y) == 0:
                return np.nan, np.nan

            try:
                return pearsonr(x, y)
            except:
                return np.nan, np.nan

        # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
        r, p_value = safe_pearsonr(y_true_clean, y_pred_clean)

        total_samples = len(y_true)
        valid_samples = len(y_true_clean)
        valid_ratio = valid_samples / total_samples if total_samples > 0 else 0

        return {
            'MAE': mae,
            'RMSE': rmse,
            'R': r,
            'R_pvalue': p_value,
            'æ ·æœ¬æ•°': valid_samples,
            'æ€»æ ·æœ¬æ•°': total_samples,
            'æœ‰æ•ˆæ ·æœ¬æ¯”ä¾‹': valid_ratio
        }

    def cross_validate(self, X, y, groups, cv_type='station'):
        """æ‰§è¡Œäº¤å‰éªŒè¯

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            y (np.array): ç›®æ ‡å˜é‡
            groups (np.array): åˆ†ç»„ä¿¡æ¯
            cv_type (str): äº¤å‰éªŒè¯ç±»å‹ ('station' æˆ– 'yearly')

        Returns:
            dict: äº¤å‰éªŒè¯ç»“æœï¼ŒåŒ…å«èšåˆå€¼ã€å¹³å‡å€¼å’Œä¸­ä½æ•°
        """
        logo = LeaveOneGroupOut()

        all_predictions = []
        all_true_values = []
        fold_results = {}

        # ç”¨äºå­˜å‚¨å„æŠ˜å çš„æŒ‡æ ‡
        fold_maes = []
        fold_rmses = []
        fold_rs = []
        fold_samples = []

        unique_groups = np.unique(groups)
        total_folds = len(unique_groups)

        self.logger.info(f"å¼€å§‹{cv_type}äº¤å‰éªŒè¯ï¼Œå…±{total_folds}ä¸ªæŠ˜å ...")

        for fold, (train_idx, test_idx) in enumerate(logo.split(X, y, groups)):
            group_id = groups[test_idx[0]]
            test_size = len(test_idx)
            train_size = len(train_idx)

            self.logger.debug(f"{cv_type} Fold {fold + 1}: è®­ç»ƒé›†{train_size}æ ·æœ¬, æµ‹è¯•é›†{test_size}æ ·æœ¬")

            # åˆ†å‰²æ•°æ®
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]

            # è®­ç»ƒæ¨¡å‹
            model = xgb.XGBRegressor(**self.params)
            model.fit(X_train, y_train)

            # é¢„æµ‹
            y_pred = model.predict(X_test)

            # å­˜å‚¨ç»“æœ
            all_predictions.extend(y_pred)
            all_true_values.extend(y_test)

            # è®¡ç®—å½“å‰æŠ˜å æ€§èƒ½
            fold_metrics = self.evaluate_predictions(y_test, y_pred)
            fold_results[group_id] = fold_metrics

            # å®‰å…¨æ˜¾ç¤ºç›¸å…³ç³»æ•°
            r_display = fold_metrics['R']
            if np.isnan(r_display):
                r_display_str = "NaN"
            else:
                r_display_str = f"{r_display:.3f}"

            self.logger.info(
                f"  {cv_type} Fold {fold + 1}/{total_folds}: {group_id} "
                f"({test_size}æ ·æœ¬) - "
                f"MAE={fold_metrics['MAE']:.3f}, R={r_display_str}"
            )

            # å­˜å‚¨å„æŠ˜å æŒ‡æ ‡ç”¨äºç»Ÿè®¡
            fold_maes.append(fold_metrics['MAE'])
            fold_rmses.append(fold_metrics['RMSE'])
            fold_rs.append(fold_metrics['R'])
            fold_samples.append(fold_metrics['æ ·æœ¬æ•°'])

            self.logger.info(
                f"  {cv_type} Fold {fold + 1}/{total_folds}: {group_id} "
                f"({test_size}æ ·æœ¬) - "
                f"MAE={fold_metrics['MAE']:.3f}, R={fold_metrics['R']:.3f}"
            )

        # è®¡ç®—èšåˆæ€§èƒ½ï¼ˆæ‰€æœ‰æµ‹è¯•æ ·æœ¬ä¸€èµ·è®¡ç®—ï¼‰
        overall_metrics = self.evaluate_predictions(
            np.array(all_true_values),
            np.array(all_predictions)
        )

        # è®¡ç®—å„æŠ˜å æŒ‡æ ‡çš„å¹³å‡å€¼å’Œä¸­ä½æ•°
        def safe_statistic(values, func):
            """å®‰å…¨è®¡ç®—ç»Ÿè®¡é‡ï¼Œå¤„ç†NaNå€¼"""
            valid_values = [v for v in values if not np.isnan(v)]
            if len(valid_values) == 0:
                return np.nan
            return func(valid_values)

        mean_metrics = {
            'MAE': safe_statistic(fold_maes, np.mean),
            'RMSE': safe_statistic(fold_rmses, np.mean),
            'R': safe_statistic(fold_rs, np.mean),
            'æ ·æœ¬æ•°': np.sum(fold_samples)  # æ€»æ ·æœ¬æ•°
        }

        median_metrics = {
            'MAE': safe_statistic(fold_maes, np.median),
            'RMSE': safe_statistic(fold_rmses, np.median),
            'R': safe_statistic(fold_rs, np.median),
            'æ ·æœ¬æ•°': np.sum(fold_samples)
        }

        # è®¡ç®—å„æŠ˜å æŒ‡æ ‡çš„æ ‡å‡†å·®
        std_metrics = {
            'MAE': safe_statistic(fold_maes, np.std),
            'RMSE': safe_statistic(fold_rmses, np.std),
            'R': safe_statistic(fold_rs, np.std)
        }

        # å®‰å…¨æ˜¾ç¤ºæ€»ä½“ç›¸å…³ç³»æ•°
        overall_r_display = overall_metrics['R']
        mean_r_display = mean_metrics['R']
        median_r_display = median_metrics['R']

        if np.isnan(overall_r_display):
            overall_r_str = "NaN"
        else:
            overall_r_str = f"{overall_r_display:.3f}"

        if np.isnan(mean_r_display):
            mean_r_str = "NaN"
        else:
            mean_r_str = f"{mean_r_display:.3f}"

        if np.isnan(median_r_display):
            median_r_str = "NaN"
        else:
            median_r_str = f"{median_r_display:.3f}"

        self.logger.info(f"âœ… {cv_type}äº¤å‰éªŒè¯å®Œæˆ")
        self.logger.info(f"  èšåˆæ€§èƒ½: MAE={overall_metrics['MAE']:.3f}mm, R={overall_r_str}")
        self.logger.info(f"  å¹³å‡æ€§èƒ½: MAE={mean_metrics['MAE']:.3f}mm, R={mean_r_str}")
        self.logger.info(f"  ä¸­ä½æ•°æ€§èƒ½: MAE={median_metrics['MAE']:.3f}mm, R={median_r_str}")

        return {
            'overall': overall_metrics,  # èšåˆè®¡ç®—ï¼šæ‰€æœ‰æµ‹è¯•æ ·æœ¬ä¸€èµ·è®¡ç®—
            'mean': mean_metrics,  # å¹³å‡å€¼ï¼šå„æŠ˜å æŒ‡æ ‡çš„å¹³å‡
            'median': median_metrics,  # ä¸­ä½æ•°ï¼šå„æŠ˜å æŒ‡æ ‡çš„ä¸­ä½æ•°
            'std': std_metrics,  # æ ‡å‡†å·®ï¼šå„æŠ˜å æŒ‡æ ‡çš„å˜å¼‚ç¨‹åº¦
            'by_fold': fold_results,  # å„æŠ˜å è¯¦ç»†ç»“æœ
            'predictions': np.array(all_predictions),
            'true_values': np.array(all_true_values),
            'folds': total_folds,
            'fold_metrics': {  # å„æŠ˜å æŒ‡æ ‡åˆ—è¡¨
                'MAE': fold_maes,
                'RMSE': fold_rmses,
                'R': fold_rs,
                'samples': fold_samples
            }
        }

    def safe_statistic(values, func):
        """å®‰å…¨è®¡ç®—ç»Ÿè®¡é‡ï¼Œå¤„ç†NaNå€¼å’Œç©ºæ•°ç»„"""
        valid_values = [v for v in values if not np.isnan(v)]
        if len(valid_values) == 0:
            return np.nan
        return func(valid_values)

    def train_final_model(self, X, y):
        """è®­ç»ƒæœ€ç»ˆæ¨¡å‹

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            y (np.array): ç›®æ ‡å˜é‡

        Returns:
            xgb.XGBRegressor: è®­ç»ƒå¥½çš„XGBoostæ¨¡å‹
        """
        self.logger.info("è®­ç»ƒæœ€ç»ˆXGBoostæ¨¡å‹...")

        self.model = xgb.XGBRegressor(**self.params)
        self.model.fit(X, y)

        self.logger.info("âœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
        return self.model


    def run_complete_analysis(self, df, output_dir=None):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹

        Args:
            df (pd.DataFrame): è¾“å…¥æ•°æ®
            output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„

        Returns:
            dict: åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„å­—å…¸
        """
        self.logger.info("=" * 70)
        self.logger.info("ğŸš€ å¼€å§‹SWE XGBoostå®Œæ•´åˆ†ææµç¨‹")
        self.logger.info("=" * 70)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = f"./swe_model_results_{timestamp}"

        os.makedirs(output_dir, exist_ok=True)
        self.logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")

        try:
            # 1. æ•°æ®é¢„å¤„ç†
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 1: æ•°æ®é¢„å¤„ç†")
            self.logger.info("=" * 50)

            X, y, station_groups, year_groups = self.preprocess_data(df)

            results = {
                'preprocessing': {
                    'samples': len(X),
                    'features': len(self.feature_columns),
                    'stations': len(np.unique(station_groups)),
                    'years': len(np.unique(year_groups))
                }
            }

            # 2. ç«™ç‚¹äº¤å‰éªŒè¯
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 2: ç«™ç‚¹äº¤å‰éªŒè¯")
            self.logger.info("=" * 50)

            results['station_cv'] = self.cross_validate(X, y, station_groups, 'station')

            # 3. å¹´åº¦äº¤å‰éªŒè¯
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 3: å¹´åº¦äº¤å‰éªŒè¯")
            self.logger.info("=" * 50)

            results['yearly_cv'] = self.cross_validate(X, y, year_groups, 'yearly')

            # 4. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 4: è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
            self.logger.info("=" * 50)

            results['final_model'] = self.train_final_model(X, y)

            # 5. ç‰¹å¾é‡è¦æ€§åˆ†æ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 5: ç‰¹å¾é‡è¦æ€§åˆ†æ")
            self.logger.info("=" * 50)

            results['feature_importance'] = self.get_feature_importance()

            # 6. ä¿å­˜ç»“æœ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 6: ä¿å­˜ç»“æœ")
            self.logger.info("=" * 50)

            self._save_results(results, output_dir)

            # 7. ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report(results)
            print(report)
            self.logger.info("ğŸ¯ å®Œæ•´åˆ†æå®Œæˆï¼")
            return results

        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææµç¨‹å¤±è´¥: {str(e)}")
            raise

    def _create_scatter_plots(self, results, output_dir):
        """åˆ›å»ºä¸¤ç§äº¤å‰éªŒè¯æ–¹æ³•çš„é¢„æµ‹å€¼ä¸å®é™…å€¼æ•£ç‚¹å›¾

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        try:
            self.logger.info("ğŸ“Š ç”Ÿæˆæ•£ç‚¹å›¾...")

            # è®¾ç½®å›¾å½¢æ ·å¼
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'Arial']
            plt.rcParams['axes.unicode_minus'] = False
            sns.set_style("whitegrid")

            # åˆ›å»ºå­å›¾ - ç°åœ¨å·¦å›¾æ˜¯å¹´åº¦CVï¼Œå³å›¾æ˜¯ç«™ç‚¹CV
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # å·¦å›¾ï¼šå¹´åº¦äº¤å‰éªŒè¯æ•£ç‚¹å›¾ï¼ˆä½¿ç”¨ä¸­ä½æ•°æŒ‡æ ‡ï¼‰
            if 'yearly_cv' in results:
                # ä½¿ç”¨ä¸­ä½æ•°æŒ‡æ ‡
                median_metrics = results['yearly_cv']['median']
                self._plot_single_scatter(
                    ax1,
                    results['yearly_cv']['true_values'],
                    results['yearly_cv']['predictions'],
                    median_metrics,  # ä½¿ç”¨ä¸­ä½æ•°æŒ‡æ ‡
                    'å¹´åº¦äº¤å‰éªŒè¯',
                    'é¢„æµ‹ SWE (mm)',
                    use_median=True  # æ·»åŠ æ ‡è®°è¡¨æ˜ä½¿ç”¨ä¸­ä½æ•°
                )

            # å³å›¾ï¼šç«™ç‚¹äº¤å‰éªŒè¯æ•£ç‚¹å›¾ï¼ˆä½¿ç”¨èšåˆæŒ‡æ ‡ï¼‰
            if 'station_cv' in results:
                # ä½¿ç”¨èšåˆæŒ‡æ ‡
                overall_metrics = results['station_cv']['overall']
                self._plot_single_scatter(
                    ax2,
                    results['station_cv']['true_values'],
                    results['station_cv']['predictions'],
                    overall_metrics,  # ä½¿ç”¨èšåˆæŒ‡æ ‡
                    'ç«™ç‚¹äº¤å‰éªŒè¯',
                    'é¢„æµ‹ SWE (mm)',
                    use_median=False  # æ·»åŠ æ ‡è®°è¡¨æ˜ä½¿ç”¨èšåˆå€¼
                )

            # è°ƒæ•´å¸ƒå±€
            plt.tight_layout()

            # ä¿å­˜å›¾ç‰‡
            scatter_path = f'{output_dir}/scatter_plots.png'
            plt.savefig(scatter_path, dpi=300, bbox_inches='tight')
            plt.close()

            self.logger.info(f"âœ… æ•£ç‚¹å›¾ä¿å­˜: {scatter_path}")

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆæ•£ç‚¹å›¾å¤±è´¥: {str(e)}")

    def _plot_single_scatter(self, ax, y_true, y_pred, metrics, title, ylabel, use_median=False):
        """ç»˜åˆ¶å•ä¸ªæ•£ç‚¹å›¾

        Args:
            ax: matplotlibè½´å¯¹è±¡
            y_true: çœŸå®å€¼æ•°ç»„
            y_pred: é¢„æµ‹å€¼æ•°ç»„
            metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
            title: å›¾æ ‡é¢˜
            ylabel: çºµåæ ‡æ ‡ç­¾
            use_median: æ˜¯å¦ä½¿ç”¨ä¸­ä½æ•°æŒ‡æ ‡
        """
        # ç§»é™¤NaNå€¼
        mask = ~(np.isnan(y_true) | np.isnan(y_pred))
        y_true_clean = y_true[mask]
        y_pred_clean = y_pred[mask]

        if len(y_true_clean) == 0:
            ax.text(0.5, 0.5, 'æ— æœ‰æ•ˆæ•°æ®', ha='center', va='center', transform=ax.transAxes)
            ax.set_title(title)
            return

        # è®¾ç½®åæ ‡è½´èŒƒå›´åˆ°175mm
        max_range = 175
        min_val = 0
        max_val = max_range

        # 1:1 å‚è€ƒçº¿ - é»‘è‰²å®çº¿
        ax.plot([min_val, max_val], [min_val, max_val], 'k-', alpha=0.8, linewidth=2)

        # ä½¿ç”¨æ¸…æ™°çš„æ•£ç‚¹å›¾ï¼Œæ ¹æ®å¯†åº¦ç€è‰²
        try:
            # ä½¿ç”¨2Dæ ¸å¯†åº¦ä¼°è®¡æ¥ç€è‰²
            from scipy.stats import gaussian_kde

            # è®¡ç®—ç‚¹çš„å¯†åº¦
            xy = np.vstack([y_true_clean, y_pred_clean])
            z = gaussian_kde(xy)(xy)

            # æŒ‰å¯†åº¦æ’åºï¼Œç¡®ä¿é«˜å¯†åº¦ç‚¹åœ¨ä¸Šå±‚
            idx = z.argsort()
            y_true_sorted, y_pred_sorted, z_sorted = y_true_clean[idx], y_pred_clean[idx], z[idx]

            # ç»˜åˆ¶æ•£ç‚¹å›¾ï¼Œä½¿ç”¨æ¸…æ™°çš„ç‚¹
            scatter = ax.scatter(y_true_sorted, y_pred_sorted,
                                 c=z_sorted,
                                 cmap='viridis',
                                 s=15,
                                 alpha=0.7,
                                 edgecolors='none',
                                 marker='o')

            # æ·»åŠ é¢œè‰²æ¡
            cbar = plt.colorbar(scatter, ax=ax, label='ç‚¹å¯†åº¦')

        except Exception as e:
            self.logger.warning(f"å¯†åº¦ç€è‰²å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæ•£ç‚¹å›¾: {e}")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šæ™®é€šæ•£ç‚¹å›¾
            scatter = ax.scatter(y_true_clean, y_pred_clean,
                                 alpha=0.6, s=15, c='blue', edgecolors='none')

        # æ·»åŠ å›å½’è¶‹åŠ¿çº¿ï¼ˆè™šçº¿ï¼‰
        if len(y_true_clean) > 1:
            try:
                # è®¡ç®—çº¿æ€§å›å½’
                slope, intercept, r_value, p_value, std_err = stats.linregress(y_true_clean, y_pred_clean)

                # ç”Ÿæˆå›å½’çº¿æ•°æ®ç‚¹
                x_reg = np.linspace(min_val, max_val, 100)
                y_reg = slope * x_reg + intercept

                # ç»˜åˆ¶å›å½’çº¿ - çº¢è‰²è™šçº¿
                ax.plot(x_reg, y_reg, 'r--', alpha=0.8, linewidth=2)

            except Exception as e:
                self.logger.warning(f"è®¡ç®—å›å½’çº¿å¤±è´¥: {e}")

        # è®¾ç½®åæ ‡è½´
        ax.set_xlabel('å®é™… SWE (mm)')
        ax.set_ylabel(ylabel)

        # åœ¨æ ‡é¢˜ä¸­æ³¨æ˜ä½¿ç”¨çš„æŒ‡æ ‡ç±»å‹
        if use_median:
            title_with_metric = f'{title} (ä¸­ä½æ•°æŒ‡æ ‡)'
        else:
            title_with_metric = f'{title} (èšåˆæŒ‡æ ‡)'

        ax.set_title(title_with_metric, fontsize=14, fontweight='bold')

        # è®¾ç½®åæ ‡è½´èŒƒå›´
        ax.set_xlim([min_val, max_val])
        ax.set_ylim([min_val, max_val])
        ax.set_aspect('equal')

        # æ·»åŠ ç½‘æ ¼
        ax.grid(True, alpha=0.3)

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡† - æ ¹æ®æŒ‡æ ‡ç±»å‹æ˜¾ç¤ºä¸åŒçš„å€¼
        if use_median:
            # ä½¿ç”¨ä¸­ä½æ•°æŒ‡æ ‡
            mae = metrics['MAE']
            rmse = metrics['RMSE']
            r = metrics['R']
            n = len(y_true_clean)  # ä½¿ç”¨å®é™…æ ·æœ¬æ•°
        else:
            # ä½¿ç”¨èšåˆæŒ‡æ ‡
            mae = metrics['MAE']
            rmse = metrics['RMSE']
            r = metrics['R']
            n = len(y_true_clean)  # ä½¿ç”¨å®é™…æ ·æœ¬æ•°

        # å®‰å…¨å¤„ç†ç›¸å…³ç³»æ•°æ˜¾ç¤º
        if np.isnan(r):
            r_str = "NaN"
        else:
            r_str = f"{r:.3f}"

        stats_text = f'MAE = {mae:.2f} mm\nRMSE = {rmse:.2f} mm\nR = {r_str}\nN = {n}'

        # å³ä¸Šè§’ï¼Œæ— è¾¹æ¡†ï¼Œæ”¾å¤§å­—ä½“
        ax.text(0.95, 0.95, stats_text, transform=ax.transAxes,
                verticalalignment='top', horizontalalignment='right',
                fontsize=12,
                fontfamily='monospace',
                weight='bold',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    def _create_combined_scatter_plot(self, results, output_dir):
        """åˆ›å»ºåˆå¹¶çš„æ•£ç‚¹å›¾ï¼ˆä¸¤ç§æ–¹æ³•åœ¨åŒä¸€å›¾ä¸­ï¼‰

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        try:
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'Arial']
            plt.rcParams['axes.unicode_minus'] = False

            fig, ax = plt.subplots(figsize=(10, 8))

            # è®¾ç½®åæ ‡è½´èŒƒå›´åˆ°175mm
            max_range = 175
            min_val = 0
            max_val = max_range

            # 1:1 å‚è€ƒçº¿ - é»‘è‰²å®çº¿
            ax.plot([min_val, max_val], [min_val, max_val], 'k-', alpha=0.8, linewidth=2)

            colors = ['#ff7f0e', '#1f77b4']  # æ©™è‰²(å¹´åº¦CV)å’Œè“è‰²(ç«™ç‚¹CV)
            labels = ['å¹´åº¦CV (ä¸­ä½æ•°)', 'ç«™ç‚¹CV (èšåˆ)']
            methods = ['yearly_cv', 'station_cv']
            metric_types = ['median', 'overall']  # å¯¹åº”çš„æŒ‡æ ‡ç±»å‹

            # æ”¶é›†æ‰€æœ‰ç‚¹ç”¨äºå¯†åº¦è®¡ç®—
            all_true = []
            all_pred = []
            all_colors = []

            for i, (method, metric_type) in enumerate(zip(methods, metric_types)):
                if method in results:
                    y_true = results[method]['true_values']
                    y_pred = results[method]['predictions']

                    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
                    y_true_clean = y_true[mask]
                    y_pred_clean = y_pred[mask]

                    if len(y_true_clean) > 0:
                        # ä¸ºæ¯ç§æ–¹æ³•æ·»åŠ ç‚¹
                        scatter = ax.scatter(y_true_clean, y_pred_clean,
                                             c=colors[i],
                                             s=15,
                                             alpha=0.6,
                                             edgecolors='none',
                                             marker='o',
                                             label=labels[i])

                        all_true.extend(y_true_clean)
                        all_pred.extend(y_pred_clean)
                        all_colors.extend([colors[i]] * len(y_true_clean))

            if len(all_true) == 0:
                ax.text(0.5, 0.5, 'æ— æœ‰æ•ˆæ•°æ®', ha='center', va='center', transform=ax.transAxes)
                plt.savefig(f'{output_dir}/combined_scatter_plot.png', dpi=300, bbox_inches='tight')
                plt.close()
                return

            # åˆ†åˆ«è®¡ç®—ä¸¤ç§æ–¹æ³•çš„å›å½’çº¿
            for i, method in enumerate(methods):
                if method in results:
                    y_true = results[method]['true_values']
                    y_pred = results[method]['predictions']

                    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
                    y_true_clean = y_true[mask]
                    y_pred_clean = y_pred[mask]

                    if len(y_true_clean) > 1:
                        try:
                            slope, intercept, r_value, p_value, std_err = stats.linregress(y_true_clean, y_pred_clean)

                            x_reg = np.linspace(min_val, max_val, 100)
                            y_reg = slope * x_reg + intercept

                            # ä½¿ç”¨ä¸åŒçº¿å‹
                            linestyle = '--' if i == 0 else '-.'
                            ax.plot(x_reg, y_reg, color=colors[i], linestyle=linestyle,
                                    alpha=0.8, linewidth=2)
                        except Exception as e:
                            self.logger.warning(f"è®¡ç®—{method}å›å½’çº¿å¤±è´¥: {e}")

            # è®¾ç½®åæ ‡è½´
            ax.set_xlim([min_val, max_val])
            ax.set_ylim([min_val, max_val])
            ax.set_aspect('equal')
            ax.set_xlabel('å®é™… SWE (mm)')
            ax.set_ylabel('é¢„æµ‹ SWE (mm)')
            ax.set_title('SWEé¢„æµ‹å€¼ä¸å®é™…å€¼æ•£ç‚¹å›¾æ¯”è¾ƒ\n(å¹´åº¦CVä½¿ç”¨ä¸­ä½æ•°æŒ‡æ ‡ï¼Œç«™ç‚¹CVä½¿ç”¨èšåˆæŒ‡æ ‡)',
                         fontsize=14, fontweight='bold')
            ax.grid(True, alpha=0.3)

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯ - ä½¿ç”¨å¯¹åº”çš„æŒ‡æ ‡ç±»å‹
            stats_text = ""
            if 'yearly_cv' in results:
                yearly_metrics = results['yearly_cv']['median']  # ä½¿ç”¨ä¸­ä½æ•°
                yearly_r = yearly_metrics['R']
                yearly_r_str = f"{yearly_r:.3f}" if not np.isnan(yearly_r) else "NaN"
                stats_text += f"å¹´åº¦CV(ä¸­ä½æ•°):\nMAE={yearly_metrics['MAE']:.2f}\nRMSE={yearly_metrics['RMSE']:.2f}\nR={yearly_r_str}\nN={len(results['yearly_cv']['true_values'])}\n\n"

            if 'station_cv' in results:
                station_metrics = results['station_cv']['overall']  # ä½¿ç”¨èšåˆå€¼
                station_r = station_metrics['R']
                station_r_str = f"{station_r:.3f}" if not np.isnan(station_r) else "NaN"
                stats_text += f"ç«™ç‚¹CV(èšåˆ):\nMAE={station_metrics['MAE']:.2f}\nRMSE={station_metrics['RMSE']:.2f}\nR={station_r_str}\nN={len(results['station_cv']['true_values'])}"

            ax.text(0.95, 0.95, stats_text, transform=ax.transAxes,
                    verticalalignment='top', horizontalalignment='right',
                    fontsize=11,
                    fontfamily='monospace',
                    weight='bold',
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))

            # æ·»åŠ å›¾ä¾‹
            ax.legend(loc='lower left', framealpha=0.9)

            plt.tight_layout()
            combined_path = f'{output_dir}/combined_scatter_plot.png'
            plt.savefig(combined_path, dpi=300, bbox_inches='tight')
            plt.close()

            self.logger.info(f"âœ… åˆå¹¶æ•£ç‚¹å›¾ä¿å­˜: {combined_path}")

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆåˆå¹¶æ•£ç‚¹å›¾å¤±è´¥: {str(e)}")

    def _create_feature_importance_plot(self, results, output_dir):
        """åˆ›å»ºç‰¹å¾é‡è¦æ€§æ’åºå›¾ï¼ˆé‡è¦ç‰¹å¾åœ¨ä¸Šæ–¹ï¼‰

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        try:
            self.logger.info("ğŸ“Š ç”Ÿæˆç‰¹å¾é‡è¦æ€§æ’åºå›¾...")

            # è®¾ç½®å›¾å½¢æ ·å¼
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'Arial']
            plt.rcParams['axes.unicode_minus'] = False
            sns.set_style("whitegrid")

            if 'feature_importance' not in results:
                self.logger.warning("æ²¡æœ‰ç‰¹å¾é‡è¦æ€§æ•°æ®")
                return

            feature_importance_df = results['feature_importance']

            # é€‰æ‹©å‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼ˆå¦‚æœç‰¹å¾å¾ˆå¤šçš„è¯ï¼‰
            top_n = min(20, len(feature_importance_df))
            top_features = feature_importance_df.head(top_n)

            # åè½¬é¡ºåºï¼Œè®©é‡è¦ç‰¹å¾åœ¨ä¸Šé¢
            top_features = top_features.iloc[::-1]

            # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
            fig, ax = plt.subplots(figsize=(12, 10))

            # åˆ›å»ºé¢œè‰²æ˜ å°„ï¼ˆä»çº¢è‰²åˆ°è“è‰²ï¼Œé‡è¦ç‰¹å¾ç”¨æš–è‰²ï¼‰
            colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(top_features)))

            # ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾ï¼ˆé‡è¦ç‰¹å¾åœ¨ä¸Šé¢ï¼‰
            y_pos = np.arange(len(top_features))
            bars = ax.barh(y_pos,
                           top_features['importance'],
                           color=colors,
                           alpha=0.8,
                           edgecolor='black',
                           linewidth=0.5,
                           height=0.7)  # è°ƒæ•´æ¡å½¢é«˜åº¦

            # è®¾ç½®yè½´æ ‡ç­¾ï¼ˆé‡è¦ç‰¹å¾åœ¨ä¸Šé¢ï¼‰
            ax.set_yticks(y_pos)
            ax.set_yticklabels(top_features['feature'], fontsize=10)

            # è®¾ç½®xè½´
            ax.set_xlabel('ç‰¹å¾é‡è¦æ€§', fontsize=12, fontweight='bold')
            ax.set_title(f'XGBoostæ¨¡å‹ç‰¹å¾é‡è¦æ€§æ’åº (Top {top_n})',
                         fontsize=14, fontweight='bold', pad=20)

            # åœ¨æ¡å½¢æœ«ç«¯æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (bar, importance) in enumerate(zip(bars, top_features['importance'])):
                width = bar.get_width()
                ax.text(width + 0.001, bar.get_y() + bar.get_height() / 2,
                        f'{importance:.4f}',
                        ha='left', va='center', fontsize=9, fontweight='bold')

            # æ·»åŠ ç½‘æ ¼çº¿
            ax.grid(True, alpha=0.3, axis='x')

            # è®¾ç½®xè½´èŒƒå›´ï¼Œç•™å‡ºä¸€äº›ç©ºé—´ç»™æ ‡ç­¾
            x_max = top_features['importance'].max() * 1.15
            ax.set_xlim(0, x_max)

            # è°ƒæ•´å¸ƒå±€
            plt.tight_layout()

            # ä¿å­˜å›¾ç‰‡
            importance_path = f'{output_dir}/feature_importance_plot.png'
            plt.savefig(importance_path, dpi=300, bbox_inches='tight')
            plt.close()

            self.logger.info(f"âœ… ç‰¹å¾é‡è¦æ€§æ’åºå›¾ä¿å­˜: {importance_path}")

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆç‰¹å¾é‡è¦æ€§æ’åºå›¾å¤±è´¥: {str(e)}")

    def _create_feature_importance_comprehensive(self, results, output_dir):
        """åˆ›å»ºæ›´è¯¦ç»†çš„ç‰¹å¾é‡è¦æ€§åˆ†æå›¾ï¼ˆé‡è¦ç‰¹å¾åœ¨ä¸Šæ–¹ï¼‰

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        try:
            if 'feature_importance' not in results:
                return

            feature_importance_df = results['feature_importance']

            # åˆ›å»ºåŒ…å«å¤šä¸ªå­å›¾çš„ç»¼åˆå›¾è¡¨
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle('XGBoostæ¨¡å‹ç‰¹å¾é‡è¦æ€§ç»¼åˆåˆ†æ', fontsize=16, fontweight='bold')

            # 1. æ°´å¹³æ¡å½¢å›¾ï¼ˆä¸»è¦æ’åºå›¾ï¼‰- é‡è¦ç‰¹å¾åœ¨ä¸Šé¢
            top_n = min(15, len(feature_importance_df))
            top_features = feature_importance_df.head(top_n)
            top_features = top_features.iloc[::-1]  # åè½¬é¡ºåº

            colors1 = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(top_features)))
            y_pos = np.arange(len(top_features))
            bars1 = ax1.barh(y_pos, top_features['importance'],
                             color=colors1, alpha=0.8, edgecolor='grey', height=0.7)

            ax1.set_yticks(y_pos)
            ax1.set_yticklabels(top_features['feature'], fontsize=9)
            ax1.set_xlabel('ç‰¹å¾é‡è¦æ€§')
            ax1.set_title(f'Top {top_n} ç‰¹å¾é‡è¦æ€§æ’åºï¼ˆé‡è¦ç‰¹å¾åœ¨ä¸Šæ–¹ï¼‰')
            ax1.grid(True, alpha=0.3, axis='x')

            # åœ¨æ¡å½¢ä¸Šæ·»åŠ æ•°å€¼
            for i, bar in enumerate(bars1):
                width = bar.get_width()
                ax1.text(width, bar.get_y() + bar.get_height() / 2,
                         f'{width:.3f}', ha='left', va='center', fontsize=8, fontweight='bold')

            # è®¾ç½®xè½´èŒƒå›´
            x_max1 = top_features['importance'].max() * 1.15
            ax1.set_xlim(0, x_max1)

            # 2. é¥¼å›¾ï¼ˆæ˜¾ç¤ºå‰10ä¸ªç‰¹å¾çš„ç›¸å¯¹é‡è¦æ€§ï¼‰- æŒ‰é‡è¦æ€§æ’åº
            top_10 = feature_importance_df.head(10)
            # è®¡ç®—å…¶ä»–ç‰¹å¾çš„æ€»å’Œ
            others_sum = feature_importance_df['importance'].iloc[10:].sum()

            if others_sum > 0:
                pie_data = list(top_10['importance']) + [others_sum]
                pie_labels = list(top_10['feature']) + ['å…¶ä»–ç‰¹å¾']
            else:
                pie_data = list(top_10['importance'])
                pie_labels = list(top_10['feature'])

            colors2 = plt.cm.Set3(np.linspace(0, 1, len(pie_data)))
            wedges, texts, autotexts = ax2.pie(pie_data, labels=pie_labels, autopct='%1.1f%%',
                                               colors=colors2, startangle=90)
            ax2.set_title('å‰10ä¸ªç‰¹å¾é‡è¦æ€§åˆ†å¸ƒ')

            # ç¾åŒ–é¥¼å›¾æ–‡æœ¬
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')

            # 3. ç´¯ç§¯é‡è¦æ€§å›¾
            cumulative_importance = feature_importance_df['importance'].cumsum()
            features_count = range(1, len(cumulative_importance) + 1)

            ax3.plot(features_count, cumulative_importance, 'o-', linewidth=2, markersize=4, color='#2E86AB')
            ax3.fill_between(features_count, 0, cumulative_importance, alpha=0.3, color='#A5C8D9')
            ax3.set_xlabel('ç‰¹å¾æ•°é‡')
            ax3.set_ylabel('ç´¯ç§¯é‡è¦æ€§')
            ax3.set_title('ç‰¹å¾ç´¯ç§¯é‡è¦æ€§')
            ax3.grid(True, alpha=0.3)

            # æ ‡è®°80%å’Œ90%é‡è¦æ€§çš„ç‚¹
            idx_80 = (cumulative_importance >= 0.8).idxmax() if (cumulative_importance >= 0.8).any() else len(
                cumulative_importance) - 1
            idx_90 = (cumulative_importance >= 0.9).idxmax() if (cumulative_importance >= 0.9).any() else len(
                cumulative_importance) - 1

            ax3.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='80%é‡è¦æ€§')
            ax3.axhline(y=0.9, color='orange', linestyle='--', alpha=0.7, label='90%é‡è¦æ€§')
            ax3.axvline(x=idx_80 + 1, color='red', linestyle='--', alpha=0.5)
            ax3.axvline(x=idx_90 + 1, color='orange', linestyle='--', alpha=0.5)
            ax3.legend()

            # 4. ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡
            importance_stats = {
                'æ€»ç‰¹å¾æ•°': len(feature_importance_df),
                'å¹³å‡é‡è¦æ€§': f"{feature_importance_df['importance'].mean():.4f}",
                'æœ€å¤§é‡è¦æ€§': f"{feature_importance_df['importance'].max():.4f}",
                'æœ€å°é‡è¦æ€§': f"{feature_importance_df['importance'].min():.4f}",
                'é‡è¦æ€§æ ‡å‡†å·®': f"{feature_importance_df['importance'].std():.4f}",
                f'å‰5ä¸ªç‰¹å¾è´¡çŒ®åº¦': f"{feature_importance_df['importance'].head(5).sum() * 100:.1f}%",
                f'å‰10ä¸ªç‰¹å¾è´¡çŒ®åº¦': f"{feature_importance_df['importance'].head(10).sum() * 100:.1f}%",
                f'æœ€é‡è¦ç‰¹å¾': feature_importance_df['feature'].iloc[0]
            }

            # åˆ›å»ºç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
            ax4.axis('off')
            table_data = [[k, v] for k, v in importance_stats.items()]
            table = ax4.table(cellText=table_data,
                              colLabels=['ç»Ÿè®¡é¡¹', 'æ•°å€¼'],
                              cellLoc='left',
                              loc='center',
                              bbox=[0.1, 0.1, 0.8, 0.8])
            table.auto_set_font_size(False)
            table.set_fontsize(9)
            table.scale(1, 1.5)

            # è®¾ç½®è¡¨æ ¼æ ·å¼
            for i in range(len(table_data) + 1):
                table[(i, 0)].set_facecolor('#F0F0F0')
                table[(i, 0)].set_text_props(weight='bold')

            ax4.set_title('ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡ä¿¡æ¯')

            plt.tight_layout()

            # ä¿å­˜ç»¼åˆå›¾è¡¨
            comprehensive_path = f'{output_dir}/feature_importance_comprehensive.png'
            plt.savefig(comprehensive_path, dpi=300, bbox_inches='tight')
            plt.close()

            self.logger.info(f"âœ… ç‰¹å¾é‡è¦æ€§ç»¼åˆåˆ†æå›¾ä¿å­˜: {comprehensive_path}")

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆç‰¹å¾é‡è¦æ€§ç»¼åˆåˆ†æå›¾å¤±è´¥: {str(e)}")

    def _save_results(self, results, output_dir):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        try:
            self.logger.info("ğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")

            # 1. ä¿å­˜æœ€ç»ˆæ¨¡å‹
            if 'final_model' in results:
                model_path = f'{output_dir}/final_model.pkl'
                joblib.dump(results['final_model'], model_path)
                self.logger.info(f"âœ… æ¨¡å‹ä¿å­˜: {model_path}")

            # 2. ä¿å­˜ç«™ç‚¹äº¤å‰éªŒè¯é¢„æµ‹ç»“æœ
            if 'station_cv' in results:
                station_pred_df = pd.DataFrame({
                    'true_swe': results['station_cv']['true_values'],
                    'predicted_swe': results['station_cv']['predictions']
                })
                station_pred_path = f'{output_dir}/station_cv_predictions.csv'
                station_pred_df.to_csv(station_pred_path, index=False)
                self.logger.info(f"âœ… ç«™ç‚¹CVé¢„æµ‹ç»“æœä¿å­˜: {station_pred_path}")

                # ä¿å­˜ç«™ç‚¹CVå„æŠ˜å è¯¦ç»†ç»“æœ
                station_fold_results = []
                for station_id, metrics in results['station_cv']['by_fold'].items():
                    station_fold_results.append({
                        'station_id': station_id,
                        'mae': metrics['MAE'],
                        'rmse': metrics['RMSE'],
                        'r': metrics['R'],
                        'samples': metrics['æ ·æœ¬æ•°']
                    })
                station_fold_df = pd.DataFrame(station_fold_results)
                station_fold_path = f'{output_dir}/station_cv_fold_results.csv'
                station_fold_df.to_csv(station_fold_path, index=False)
                self.logger.info(f"âœ… ç«™ç‚¹CVå„æŠ˜å ç»“æœä¿å­˜: {station_fold_path}")

            # 3. ä¿å­˜å¹´åº¦äº¤å‰éªŒè¯é¢„æµ‹ç»“æœ
            if 'yearly_cv' in results:
                yearly_pred_df = pd.DataFrame({
                    'true_swe': results['yearly_cv']['true_values'],
                    'predicted_swe': results['yearly_cv']['predictions']
                })
                yearly_pred_path = f'{output_dir}/yearly_cv_predictions.csv'
                yearly_pred_df.to_csv(yearly_pred_path, index=False)
                self.logger.info(f"âœ… å¹´åº¦CVé¢„æµ‹ç»“æœä¿å­˜: {yearly_pred_path}")

                # ä¿å­˜å¹´åº¦CVå„æŠ˜å è¯¦ç»†ç»“æœ
                yearly_fold_results = []
                for year, metrics in results['yearly_cv']['by_fold'].items():
                    yearly_fold_results.append({
                        'year': year,
                        'mae': metrics['MAE'],
                        'rmse': metrics['RMSE'],
                        'r': metrics['R'],
                        'samples': metrics['æ ·æœ¬æ•°']
                    })
                yearly_fold_df = pd.DataFrame(yearly_fold_results)
                yearly_fold_path = f'{output_dir}/yearly_cv_fold_results.csv'
                yearly_fold_df.to_csv(yearly_fold_path, index=False)
                self.logger.info(f"âœ… å¹´åº¦CVå„æŠ˜å ç»“æœä¿å­˜: {yearly_fold_path}")

            # 4. ä¿å­˜ç‰¹å¾é‡è¦æ€§
            if 'feature_importance' in results:
                feature_path = f'{output_dir}/feature_importance.csv'
                results['feature_importance'].to_csv(feature_path, index=False)
                self.logger.info(f"âœ… ç‰¹å¾é‡è¦æ€§ä¿å­˜: {feature_path}")

            # 5. ä¿å­˜è¯¦ç»†çš„è¯„ä¼°ç»“æœï¼ˆJSONæ ¼å¼ï¼‰
            eval_results = {
                'training_info': {
                    'timestamp': datetime.now().isoformat(),
                    'feature_columns': self.feature_columns,
                    'total_samples': results.get('preprocessing', {}).get('samples', 0),
                    'total_stations': results.get('preprocessing', {}).get('stations', 0),
                    'total_years': results.get('preprocessing', {}).get('years', 0)
                },
                'model_parameters': self.params,
                'station_cross_validation': {
                    'overall': results['station_cv']['overall'],
                    'mean': results['station_cv']['mean'],
                    'median': results['station_cv']['median'],
                    'std': results['station_cv']['std'],
                    'folds': results['station_cv']['folds'],
                    'fold_metrics': results['station_cv']['fold_metrics'],
                    'by_fold': {str(k): v for k, v in results['station_cv']['by_fold'].items()}
                },
                'yearly_cross_validation': {
                    'overall': results['yearly_cv']['overall'],
                    'mean': results['yearly_cv']['mean'],
                    'median': results['yearly_cv']['median'],
                    'std': results['yearly_cv']['std'],
                    'folds': results['yearly_cv']['folds'],
                    'fold_metrics': results['yearly_cv']['fold_metrics'],
                    'by_fold': {str(k): v for k, v in results['yearly_cv']['by_fold'].items()}
                },
                'performance_comparison': {
                    'station_better_mae': results['station_cv']['overall']['MAE'] < results['yearly_cv']['overall'][
                        'MAE'],
                    'station_better_rmse': results['station_cv']['overall']['RMSE'] < results['yearly_cv']['overall'][
                        'RMSE'],
                    'station_better_r': results['station_cv']['overall']['R'] > results['yearly_cv']['overall']['R'],
                    'recommended_method': 'station' if results['station_cv']['overall']['R'] >
                                                       results['yearly_cv']['overall']['R'] else 'yearly'
                }
            }

            eval_path = f'{output_dir}/evaluation_results.json'
            with open(eval_path, 'w', encoding='utf-8') as f:
                json.dump(eval_results, f, indent=2, ensure_ascii=False, default=float)
            self.logger.info(f"âœ… è¯¦ç»†è¯„ä¼°ç»“æœä¿å­˜: {eval_path}")

            # 6. ä¿å­˜æ±‡æ€»æŠ¥å‘Šï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
            summary_report = self._generate_summary_report(results)
            summary_path = f'{output_dir}/model_summary_report.txt'
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write(summary_report)
            self.logger.info(f"âœ… æ±‡æ€»æŠ¥å‘Šä¿å­˜: {summary_path}")

            # 7. ä¿å­˜è®­ç»ƒé…ç½®
            config_info = {
                'training_config': {
                    'model_type': 'XGBoost',
                    'target_variable': 'swe',
                    'cross_validation_methods': ['station_loocv', 'yearly_loocv'],
                    'evaluation_metrics': ['MAE', 'RMSE', 'R'],
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                },
                'feature_info': {
                    'total_features': len(self.feature_columns),
                    'feature_list': self.feature_columns
                }
            }

            config_path = f'{output_dir}/training_config.json'
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config_info, f, indent=2, ensure_ascii=False)
            self.logger.info(f"âœ… è®­ç»ƒé…ç½®ä¿å­˜: {config_path}")

            # 8. åˆ›å»ºç»“æœæ‘˜è¦æ–‡ä»¶
            self._create_results_summary(results, output_dir)

            # 9. ç”Ÿæˆæ•£ç‚¹å›¾
            self.logger.info("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            self._create_scatter_plots(results, output_dir)
            self._create_combined_scatter_plot(results, output_dir)

            self.logger.info(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

            # 10. ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾
            self._create_feature_importance_plot(results, output_dir)
            self._create_feature_importance_comprehensive(results, output_dir)

            self.logger.info(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
            raise

    def get_feature_importance(self):
        """è·å–ç‰¹å¾é‡è¦æ€§ï¼Œå°†landuseå‘é‡ç‰¹å¾ä½œä¸ºä¸€ä¸ªæ•´ä½“"""
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ train_final_model æ–¹æ³•")

        importance_scores = self.model.feature_importances_

        # è®¡ç®—landuseå‘é‡ç‰¹å¾çš„æ€»é‡è¦æ€§
        # å‡è®¾landuseå‘é‡æ˜¯æœ€å10ä¸ªç‰¹å¾ï¼ˆæ ¹æ®landuse_hashåˆ—çš„æ•°é‡ï¼‰
        landuse_vector_length = 10  # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        total_features = len(importance_scores)

        # landuseå‘é‡ç‰¹å¾çš„é‡è¦æ€§æ˜¯æœ€ålanduse_vector_lengthä¸ªç‰¹å¾çš„å’Œ
        landuse_importance = np.sum(importance_scores[-landuse_vector_length:])

        # å…¶ä»–ç‰¹å¾çš„é‡è¦æ€§
        other_importance = importance_scores[:-landuse_vector_length]

        # æ„å»ºç‰¹å¾åç§°
        other_feature_names = [col for col in self.feature_columns if col != 'landuse_vector']
        feature_names = other_feature_names + ['landuse_vector']

        importance_values = list(other_importance) + [landuse_importance]

        feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance_values
        }).sort_values('importance', ascending=False)

        self.logger.info(f"ç‰¹å¾é‡è¦æ€§è®¡ç®—å®Œæˆ")
        self.logger.info(f"  landuseå‘é‡æ€»é‡è¦æ€§: {landuse_importance:.6f}")
        self.logger.info(f"  æœ€é«˜é‡è¦æ€§: {feature_importance_df['importance'].iloc[0]:.4f}")

        return feature_importance_df

    def _generate_summary_report(self, results):
        """ç”Ÿæˆç®€åŒ–çš„æ±‡æ€»æŠ¥å‘Šç”¨äºä¿å­˜

        Args:
            results (dict): åˆ†æç»“æœ

        Returns:
            str: æ±‡æ€»æŠ¥å‘Šæ–‡æœ¬
        """
        report_lines = []
        report_lines.append("SWEæ¨¡å‹è®­ç»ƒæ±‡æ€»æŠ¥å‘Š")
        report_lines.append("=" * 50)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")

        # åŸºæœ¬ç»Ÿè®¡
        if 'preprocessing' in results:
            preproc = results['preprocessing']
            report_lines.append("åŸºæœ¬ç»Ÿè®¡:")
            report_lines.append(f"  æ€»æ ·æœ¬æ•°: {preproc['samples']}")
            report_lines.append(f"  ç‰¹å¾æ•°é‡: {preproc['features']}")
            report_lines.append(f"  ç«™ç‚¹æ•°é‡: {preproc['stations']}")
            report_lines.append(f"  å¹´ä»½æ•°é‡: {preproc['years']}")
            report_lines.append("")

        # ç«™ç‚¹äº¤å‰éªŒè¯ç»“æœ
        if 'station_cv' in results:
            station = results['station_cv']
            report_lines.append("ç«™ç‚¹äº¤å‰éªŒè¯:")
            report_lines.append(f"  æŠ˜å æ•°: {station['folds']}")
            report_lines.append(f"  èšåˆMAE: {station['overall']['MAE']:.3f} mm")
            report_lines.append(f"  å¹³å‡MAE: {station['mean']['MAE']:.3f} mm")
            report_lines.append(f"  ä¸­ä½æ•°MAE: {station['median']['MAE']:.3f} mm")
            report_lines.append(f"  èšåˆR: {station['overall']['R']:.3f}")
            report_lines.append(f"  å¹³å‡R: {station['mean']['R']:.3f}")
            report_lines.append(f"  ä¸­ä½æ•°R: {station['median']['R']:.3f}")
            report_lines.append("")

        # å¹´åº¦äº¤å‰éªŒè¯ç»“æœ
        if 'yearly_cv' in results:
            yearly = results['yearly_cv']
            report_lines.append("å¹´åº¦äº¤å‰éªŒè¯:")
            report_lines.append(f"  æŠ˜å æ•°: {yearly['folds']}")
            report_lines.append(f"  èšåˆMAE: {yearly['overall']['MAE']:.3f} mm")
            report_lines.append(f"  å¹³å‡MAE: {yearly['mean']['MAE']:.3f} mm")
            report_lines.append(f"  ä¸­ä½æ•°MAE: {yearly['median']['MAE']:.3f} mm")
            report_lines.append(f"  èšåˆR: {yearly['overall']['R']:.3f}")
            report_lines.append(f"  å¹³å‡R: {yearly['mean']['R']:.3f}")
            report_lines.append(f"  ä¸­ä½æ•°R: {yearly['median']['R']:.3f}")
            report_lines.append("")

        # ç‰¹å¾é‡è¦æ€§
        if 'feature_importance' in results:
            top_features = results['feature_importance'].head(10)
            report_lines.append("å‰10ä¸ªé‡è¦ç‰¹å¾:")
            for i, (_, row) in enumerate(top_features.iterrows(), 1):
                report_lines.append(f"  {i:2d}. {row['feature']:<20} {row['importance']:.4f}")

        return "\n".join(report_lines)

    def _create_results_summary(self, results, output_dir):
        """åˆ›å»ºç»“æœæ‘˜è¦æ–‡ä»¶ï¼ˆCSVæ ¼å¼ï¼Œä¾¿äºåˆ†æï¼‰

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        try:
            # åˆ›å»ºæ€§èƒ½æ¯”è¾ƒæ‘˜è¦
            summary_data = []

            # ç«™ç‚¹CVæ‘˜è¦
            if 'station_cv' in results:
                station = results['station_cv']
                summary_data.append({
                    'method': 'station_cv',
                    'folds': station['folds'],
                    'overall_mae': station['overall']['MAE'],
                    'overall_rmse': station['overall']['RMSE'],
                    'overall_r': station['overall']['R'],
                    'mean_mae': station['mean']['MAE'],
                    'mean_rmse': station['mean']['RMSE'],
                    'mean_r': station['mean']['R'],
                    'median_mae': station['median']['MAE'],
                    'median_rmse': station['median']['RMSE'],
                    'median_r': station['median']['R'],
                    'std_mae': station['std']['MAE'],
                    'std_rmse': station['std']['RMSE'],
                    'std_r': station['std']['R'],
                    'samples': station['overall']['æ ·æœ¬æ•°']
                })

            # å¹´åº¦CVæ‘˜è¦
            if 'yearly_cv' in results:
                yearly = results['yearly_cv']
                summary_data.append({
                    'method': 'yearly_cv',
                    'folds': yearly['folds'],
                    'overall_mae': yearly['overall']['MAE'],
                    'overall_rmse': yearly['overall']['RMSE'],
                    'overall_r': yearly['overall']['R'],
                    'mean_mae': yearly['mean']['MAE'],
                    'mean_rmse': yearly['mean']['RMSE'],
                    'mean_r': yearly['mean']['R'],
                    'median_mae': yearly['median']['MAE'],
                    'median_rmse': yearly['median']['RMSE'],
                    'median_r': yearly['median']['R'],
                    'std_mae': yearly['std']['MAE'],
                    'std_rmse': yearly['std']['RMSE'],
                    'std_r': yearly['std']['R'],
                    'samples': yearly['overall']['æ ·æœ¬æ•°']
                })

            # ä¿å­˜æ‘˜è¦
            summary_df = pd.DataFrame(summary_data)
            summary_path = f'{output_dir}/performance_summary.csv'
            summary_df.to_csv(summary_path, index=False)
            self.logger.info(f"âœ… æ€§èƒ½æ‘˜è¦ä¿å­˜: {summary_path}")

        except Exception as e:
            self.logger.warning(f"åˆ›å»ºç»“æœæ‘˜è¦å¤±è´¥: {str(e)}")

    def _generate_report(self, results):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š

        Args:
            results (dict): åˆ†æç»“æœ

        Returns:
            str: æ ¼å¼åŒ–çš„æŠ¥å‘Šæ–‡æœ¬
        """
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("ğŸ“Š SWE XGBoostæ¨¡å‹åˆ†ææŠ¥å‘Š - è¯¦ç»†ç»Ÿè®¡")
        report_lines.append("=" * 80)

        # åŸºæœ¬ä¿¡æ¯
        report_lines.append(f"\nğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
        report_lines.append(f"  è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"  ç‰¹å¾æ•°é‡: {len(self.feature_columns)}")

        if 'preprocessing' in results:
            preproc = results['preprocessing']
            report_lines.append(f"  æ ·æœ¬æ•°é‡: {preproc['samples']}")
            report_lines.append(f"  ç«™ç‚¹æ•°é‡: {preproc['stations']}")
            report_lines.append(f"  å¹´ä»½æ•°é‡: {preproc['years']}")

        # ç«™ç‚¹äº¤å‰éªŒè¯ç»“æœ
        if 'station_cv' in results:
            station_overall = results['station_cv']['overall']
            station_mean = results['station_cv']['mean']
            station_median = results['station_cv']['median']
            station_std = results['station_cv']['std']

            # å®‰å…¨å¤„ç†ç›¸å…³ç³»æ•°æ˜¾ç¤º
            station_overall_r = station_overall['R']
            station_mean_r = station_mean['R']
            station_median_r = station_median['R']

            station_overall_r_str = f"{station_overall_r:.3f}" if not np.isnan(station_overall_r) else "NaN"
            station_mean_r_str = f"{station_mean_r:.3f}" if not np.isnan(station_mean_r) else "NaN"
            station_median_r_str = f"{station_median_r:.3f}" if not np.isnan(station_median_r) else "NaN"

            report_lines.append(f"\nğŸ“ ç«™ç‚¹äº¤å‰éªŒè¯ (ç©ºé—´è¯„ä¼°):")
            report_lines.append(f"  â”Œ{'â”€' * 20}â”¬{'â”€' * 10}â”¬{'â”€' * 10}â”¬{'â”€' * 10}â”")
            report_lines.append(f"  â”‚ {'æŒ‡æ ‡':18} â”‚ {'èšåˆ':8} â”‚ {'å¹³å‡':8} â”‚ {'ä¸­ä½æ•°':8} â”‚")
            report_lines.append(f"  â”œ{'â”€' * 20}â”¼{'â”€' * 10}â”¼{'â”€' * 10}â”¼{'â”€' * 10}â”¤")
            report_lines.append(
                f"  â”‚ {'MAE (mm)':18} â”‚ {station_overall['MAE']:8.3f} â”‚ {station_mean['MAE']:8.3f} â”‚ {station_median['MAE']:8.3f} â”‚")
            report_lines.append(
                f"  â”‚ {'RMSE (mm)':18} â”‚ {station_overall['RMSE']:8.3f} â”‚ {station_mean['RMSE']:8.3f} â”‚ {station_median['RMSE']:8.3f} â”‚")
            report_lines.append(
                f"  â”‚ {'R':18} â”‚ {station_overall_r_str:>8} â”‚ {station_mean_r_str:>8} â”‚ {station_median_r_str:>8} â”‚")
            report_lines.append(f"  â””{'â”€' * 20}â”´{'â”€' * 10}â”´{'â”€' * 10}â”´{'â”€' * 10}â”˜")

            report_lines.append(f"  æŠ˜å æ•°: {results['station_cv']['folds']}")
            report_lines.append(f"  æ ·æœ¬æ•°: {station_overall['æ ·æœ¬æ•°']}")

            # æ·»åŠ æ ‡å‡†å·®ä¿¡æ¯
            report_lines.append(
                f"  å„æŠ˜å æ ‡å‡†å·®: MAEÂ±{station_std['MAE']:.3f}, RMSEÂ±{station_std['RMSE']:.3f}, RÂ±{station_std['R']:.3f}")

        # å¹´åº¦äº¤å‰éªŒè¯ç»“æœ
        if 'yearly_cv' in results:
            yearly_overall = results['yearly_cv']['overall']
            yearly_mean = results['yearly_cv']['mean']
            yearly_median = results['yearly_cv']['median']
            yearly_std = results['yearly_cv']['std']

            # å®‰å…¨å¤„ç†ç›¸å…³ç³»æ•°æ˜¾ç¤º
            yearly_overall_r = yearly_overall['R']
            yearly_mean_r = yearly_mean['R']
            yearly_median_r = yearly_median['R']

            yearly_overall_r_str = f"{yearly_overall_r:.3f}" if not np.isnan(yearly_overall_r) else "NaN"
            yearly_mean_r_str = f"{yearly_mean_r:.3f}" if not np.isnan(yearly_mean_r) else "NaN"
            yearly_median_r_str = f"{yearly_median_r:.3f}" if not np.isnan(yearly_median_r) else "NaN"

            report_lines.append(f"\nğŸ“… å¹´åº¦äº¤å‰éªŒè¯ (æ—¶é—´è¯„ä¼°):")
            report_lines.append(f"  â”Œ{'â”€' * 20}â”¬{'â”€' * 10}â”¬{'â”€' * 10}â”¬{'â”€' * 10}â”")
            report_lines.append(f"  â”‚ {'æŒ‡æ ‡':18} â”‚ {'èšåˆ':8} â”‚ {'å¹³å‡':8} â”‚ {'ä¸­ä½æ•°':8} â”‚")
            report_lines.append(f"  â”œ{'â”€' * 20}â”¼{'â”€' * 10}â”¼{'â”€' * 10}â”¼{'â”€' * 10}â”¤")
            report_lines.append(
                f"  â”‚ {'MAE (mm)':18} â”‚ {yearly_overall['MAE']:8.3f} â”‚ {yearly_mean['MAE']:8.3f} â”‚ {yearly_median['MAE']:8.3f} â”‚")
            report_lines.append(
                f"  â”‚ {'RMSE (mm)':18} â”‚ {yearly_overall['RMSE']:8.3f} â”‚ {yearly_mean['RMSE']:8.3f} â”‚ {yearly_median['RMSE']:8.3f} â”‚")
            report_lines.append(
                f"  â”‚ {'R':18} â”‚ {yearly_overall_r_str:>8} â”‚ {yearly_mean_r_str:>8} â”‚ {yearly_median_r_str:>8} â”‚")
            report_lines.append(f"  â””{'â”€' * 20}â”´{'â”€' * 10}â”´{'â”€' * 10}â”´{'â”€' * 10}â”˜")

            report_lines.append(f"  æŠ˜å æ•°: {results['yearly_cv']['folds']}")
            report_lines.append(f"  æ ·æœ¬æ•°: {yearly_overall['æ ·æœ¬æ•°']}")
            report_lines.append(
                f"  å„æŠ˜å æ ‡å‡†å·®: MAEÂ±{yearly_std['MAE']:.3f}, RMSEÂ±{yearly_std['RMSE']:.3f}, RÂ±{yearly_std['R']:.3f}")

        # æ¨¡å‹å‚æ•°
        report_lines.append(f"\nâš™ï¸ æ¨¡å‹å‚æ•°:")
        for key, value in self.params.items():
            if key in ['n_estimators', 'max_depth', 'min_child_weight']:
                report_lines.append(f"  {key}: {value}")
            elif key in ['learning_rate', 'subsample', 'colsample_bytree', 'reg_alpha', 'gamma']:
                report_lines.append(f"  {key}: {value}")

        # ç‰¹å¾é‡è¦æ€§
        if 'feature_importance' in results:
            top_features = results['feature_importance'].head(5)
            report_lines.append(f"\nğŸ” å‰5ä¸ªé‡è¦ç‰¹å¾:")
            for i, (_, row) in enumerate(top_features.iterrows(), 1):
                report_lines.append(f"  {i}. {row['feature']}: {row['importance']:.4f}")

        # æ€§èƒ½æ¯”è¾ƒå’Œå»ºè®®
        if 'station_cv' in results and 'yearly_cv' in results:
            # ä½¿ç”¨èšåˆå€¼è¿›è¡Œæ¯”è¾ƒ
            station_r = results['station_cv']['overall']['R']
            yearly_r = results['yearly_cv']['overall']['R']

            # åªæœ‰å½“ä¸¤ä¸ªRå€¼éƒ½ä¸æ˜¯NaNæ—¶æ‰è¿›è¡Œæ¯”è¾ƒ
            if not np.isnan(station_r) and not np.isnan(yearly_r):
                report_lines.append(f"\nğŸ’¡ æ€§èƒ½åˆ†æå’Œå»ºè®®:")
                report_lines.append(f"  â€¢ èšåˆæ€§èƒ½æ¯”è¾ƒ:")
                if station_r > yearly_r:
                    report_lines.append(f"    ç«™ç‚¹CVä¼˜äºå¹´åº¦CV (R: {station_r:.3f} > {yearly_r:.3f})")
                else:
                    report_lines.append(f"    å¹´åº¦CVä¼˜äºç«™ç‚¹CV (R: {yearly_r:.3f} > {station_r:.3f})")
            else:
                report_lines.append(f"\nğŸ’¡ æ€§èƒ½åˆ†æ:")
                if np.isnan(station_r) and np.isnan(yearly_r):
                    report_lines.append(f"  â€¢ ä¸¤ç§æ–¹æ³•çš„ç›¸å…³ç³»æ•°å‡ä¸ºNaNï¼Œæ— æ³•æ¯”è¾ƒRå€¼")
                elif np.isnan(station_r):
                    report_lines.append(f"  â€¢ ç«™ç‚¹CVç›¸å…³ç³»æ•°ä¸ºNaNï¼Œå¹´åº¦CV R={yearly_r:.3f}")
                else:
                    report_lines.append(f"  â€¢ å¹´åº¦CVç›¸å…³ç³»æ•°ä¸ºNaNï¼Œç«™ç‚¹CV R={station_r:.3f}")

            # æ¯”è¾ƒç¨³å®šæ€§ï¼ˆæ ‡å‡†å·®è¶Šå°è¶Šç¨³å®šï¼‰
            station_mae_std = results['station_cv']['std']['MAE']
            yearly_mae_std = results['yearly_cv']['std']['MAE']

            if station_mae_std < yearly_mae_std:
                report_lines.append(f"  â€¢ ç¨³å®šæ€§æ¯”è¾ƒ: ç«™ç‚¹CVæ›´ç¨³å®š (MAEæ ‡å‡†å·®: {station_mae_std:.3f} < {yearly_mae_std:.3f})")
            else:
                report_lines.append(f"  â€¢ ç¨³å®šæ€§æ¯”è¾ƒ: å¹´åº¦CVæ›´ç¨³å®š (MAEæ ‡å‡†å·®: {yearly_mae_std:.3f} < {station_mae_std:.3f})")

            # æœ€ç»ˆå»ºè®®
            if station_r > yearly_r and station_mae_std < yearly_mae_std:
                report_lines.append(f"  âœ… å¼ºçƒˆæ¨èä½¿ç”¨ç«™ç‚¹äº¤å‰éªŒè¯è¿›è¡Œç©ºé—´è¯„ä¼°")
            elif yearly_r > station_r and yearly_mae_std < station_mae_std:
                report_lines.append(f"  âœ… å¼ºçƒˆæ¨èä½¿ç”¨å¹´åº¦äº¤å‰éªŒè¯è¿›è¡Œæ—¶é—´è¯„ä¼°")
            else:
                report_lines.append(f"  âš ï¸  ä¸¤ç§æ–¹æ³•å„æœ‰ä¼˜åŠ¿ï¼Œè¯·æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯é€‰æ‹©")

        report_lines.append("\n" + "=" * 80)

        return "\n".join(report_lines)

    def _analyze_landuse_features(self, df):
        """åˆ†ælanduseç‰¹å¾çš„ç›¸å…³æ€§å’Œé‡è¦æ€§"""
        landuse_vec_cols = [col for col in df.columns if col.startswith('landuse_vec_')]
        landuse_stat_cols = [col for col in df.columns if col.startswith('landuse_') and col not in landuse_vec_cols]

        if landuse_vec_cols:
            self.logger.info(f"landuseå‘é‡ç‰¹å¾åˆ†æ:")
            self.logger.info(f"  å‘é‡å…ƒç´ ç‰¹å¾: {len(landuse_vec_cols)} ä¸ª")
            self.logger.info(f"  ç»Ÿè®¡ç‰¹å¾: {len(landuse_stat_cols)} ä¸ª")

            # è®¡ç®—landuseç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§
            if self.target_column in df.columns:
                correlations = {}
                for col in landuse_vec_cols + landuse_stat_cols:
                    corr = df[col].corr(df[self.target_column])
                    correlations[col] = corr

                # æŒ‰ç›¸å…³æ€§ç»å¯¹å€¼æ’åº
                sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)
                self.logger.info("landuseç‰¹å¾ä¸SWEç›¸å…³æ€§ (å‰5ä¸ª):")
                for col, corr in sorted_correlations[:5]:
                    self.logger.info(f"  {col}: {corr:.3f}")
    # ä¾¿æ·ä½¿ç”¨å‡½æ•°


def train_swe_model(data_df, output_dir=None, params=None):
    """ä¾¿æ·å‡½æ•°ï¼šè®­ç»ƒSWEæ¨¡å‹

        Args:
            data_df (pd.DataFrame): åŒ…å«ç‰¹å¾å’ŒSWEçš„æ•°æ®
            output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„
            params (dict, optional): XGBoostå‚æ•°

        Returns:
            dict: åŒ…å«æ‰€æœ‰è®­ç»ƒç»“æœçš„å­—å…¸

        Example:
            >>> results = train_swe_model(df, output_dir='./results')
            >>> print(f"ç«™ç‚¹CV R: {results['station_cv']['overall']['R']:.3f}")
        """
    trainer = SWEXGBoostTrainer(params=params)
    return trainer.run_complete_analysis(data_df, output_dir)
