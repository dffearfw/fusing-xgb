import logging
import pandas as pd
import numpy as np
import xgboost as xgb
from matplotlib import pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
from scipy.stats import pearsonr
from sklearn.model_selection import LeaveOneGroupOut
import os
import joblib
import json
from datetime import datetime
import seaborn as sns
from scipy import stats

logger = logging.getLogger("SWEXGBoostTrainer")


class SWEXGBoostTrainer:
    """SWE XGBoostè®­ç»ƒå™¨ - é›†æˆæ•°æ®é¢„å¤„ç†ã€äº¤å‰éªŒè¯å’Œæ¨¡å‹è®­ç»ƒ"""

    # é»˜è®¤XGBoostå‚æ•°
    DEFAULT_PARAMS = {
        'n_estimators': 60,
        'learning_rate': 0.17,
        'max_depth': 5,
        'min_child_weight': 5,
        'gamma': 0,
        'subsample': 0.8,
        'colsample_bytree': 0.5,
        'reg_alpha': 0.05,
        'random_state': 42,
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse'
    }

    def __init__(self, params=None):
        """åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            params (dict, optional): XGBoostå‚æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å‚æ•°
        """
        self.logger = logger
        self.model = None
        self.feature_columns = None
        self.target_column = 'swe'

        # æ›´æ–°å‚æ•°
        self.params = self.DEFAULT_PARAMS.copy()
        if params:
            self.params.update(params)

        self.logger.info(f"åˆå§‹åŒ–SWE XGBoostè®­ç»ƒå™¨")
        self.logger.info(f"æ¨¡å‹å‚æ•°: {self.params}")

    def preprocess_data(self, df):
        """æ•°æ®é¢„å¤„ç†"""
        self.logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")

        # éªŒè¯æ•°æ®
        self.validate_data(df)

        # åˆ›å»ºæ•°æ®å‰¯æœ¬
        df_clean = df.copy()

        # è®°å½•åˆå§‹æ ·æœ¬æ•°
        initial_count = len(df_clean)
        self.logger.info(f"åˆå§‹æ ·æœ¬æ•°: {initial_count} è¡Œ")

        # å¯¹äºCSWEç‰¹å¾ï¼Œå°†æ— æ•ˆå€¼è®¾ä¸ºNaNè®©XGBoostå¤„ç†
        if 'cswe' in df_clean.columns:
            cswe_invalid_mask = df_clean['cswe'] > 200
            cswe_invalid_count = cswe_invalid_mask.sum()
            if cswe_invalid_count > 0:
                self.logger.info(f"å‘ç° {cswe_invalid_count} ä¸ªCSWEå¤§äº200mmçš„å€¼ï¼Œå°†ä½œä¸ºç¼ºå¤±å€¼ç”±XGBoostå¤„ç†")
                # å°†è¿™äº›æ— æ•ˆå€¼è®¾ä¸ºNaNï¼Œè®©XGBoostå¤„ç†
                df_clean.loc[cswe_invalid_mask, 'cswe'] = np.nan

        # å¤„ç†landuseç‹¬çƒ­ç¼–ç ç‰¹å¾
        df_clean = self._process_landuse_features(df_clean)

        # ç»Ÿè®¡ç«™ç‚¹æ ·æœ¬æ•°é‡
        self._analyze_station_samples(df_clean)

        # ç¡®å®šç‰¹å¾åˆ—ï¼ˆæ’é™¤station_id, date, sweã€hydrological_doyå’ŒåŸå§‹çš„landuse_hashåˆ—ï¼‰
        exclude_columns = ['station_id', 'date', self.target_column, 'hydrological_doy']

        # æ’é™¤åŸå§‹çš„landuse_hashåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        original_landuse_hash_columns = [col for col in df_clean.columns if col.startswith('landuse_hash_')]
        exclude_columns.extend(original_landuse_hash_columns)

        self.feature_columns = [col for col in df_clean.columns if col not in exclude_columns]

        if not self.feature_columns:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç‰¹å¾åˆ—")

        self.logger.info(f"ä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")

        # æ£€æŸ¥ç¼ºå¤±å€¼æƒ…å†µ
        total_missing = df_clean[self.feature_columns].isna().sum().sum()
        if total_missing > 0:
            self.logger.info(f"ç‰¹å¾æ•°æ®ä¸­å­˜åœ¨ {total_missing} ä¸ªç¼ºå¤±å€¼ï¼Œå°†ç”±XGBoostè‡ªåŠ¨å¤„ç†")

        # æ£€æŸ¥ç›®æ ‡å˜é‡ç¼ºå¤±å€¼
        target_missing = df_clean[self.target_column].isna().sum()
        if target_missing > 0:
            self.logger.info(f"ç›®æ ‡å˜é‡ä¸­å­˜åœ¨ {target_missing} ä¸ªç¼ºå¤±å€¼ï¼Œå°†ä¿ç•™è¿™äº›æ ·æœ¬è®©XGBoostå¤„ç†")

        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        X = df_clean[self.feature_columns].copy()
        y = df_clean[self.target_column].copy()

        # ç®€åŒ–ç¼ºå¤±å€¼å¤„ç†ï¼ˆä¸»è¦ä¾èµ–XGBoostï¼‰
        X_processed = self._handle_missing_values(X)

        # å‡†å¤‡åˆ†ç»„ä¿¡æ¯
        df_clean['year'] = pd.to_datetime(df_clean['date']).dt.year
        station_groups = df_clean['station_id'].values
        year_groups = df_clean['year'].values

        # ç»Ÿè®¡ä¿¡æ¯
        station_count = len(np.unique(station_groups))
        year_count = len(np.unique(year_groups))
        swe_mean = y.mean()
        swe_std = y.std()

        self.logger.info("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        self.logger.info(f"  ğŸ“Š æ ·æœ¬æ•°: {len(X_processed)}")
        self.logger.info(f"  ğŸ”§ ç‰¹å¾æ•°: {len(self.feature_columns)}")
        self.logger.info(f"  ğŸ“ ç«™ç‚¹æ•°: {station_count}")
        self.logger.info(f"  ğŸ“… å¹´ä»½æ•°: {year_count}")
        self.logger.info(f"  â„ï¸  SWEç»Ÿè®¡: å‡å€¼={swe_mean:.2f}mm, æ ‡å‡†å·®={swe_std:.2f}mm")

        # ç›´æ¥è¿”å›æ•°å€¼æ•°ç»„ï¼Œç¼ºå¤±å€¼ç”¨NaNè¡¨ç¤ºï¼ŒXGBoostä¼šå¤„ç†
        return X_processed.values, y.values, station_groups, year_groups

    def _analyze_station_samples(self, df):
        """åˆ†æç«™ç‚¹æ ·æœ¬æ•°é‡ï¼Œç‰¹åˆ«å…³æ³¨æ ·æœ¬æ•°å°‘çš„ç«™ç‚¹

        Args:
            df (pd.DataFrame): æ•°æ®
        """
        self.logger.info("åˆ†æç«™ç‚¹æ ·æœ¬æ•°é‡...")

        # ç»Ÿè®¡æ¯ä¸ªç«™ç‚¹çš„æ ·æœ¬æ•°é‡
        station_counts = df['station_id'].value_counts().sort_values()

        # ç»Ÿè®¡ä¸åŒæ ·æœ¬æ•°é‡çš„ç«™ç‚¹åˆ†å¸ƒ
        count_ranges = {
            '1 sample': (station_counts == 1).sum(),
            '2 samples': (station_counts == 2).sum(),
            '3 samples': (station_counts == 3).sum(),
            '4-10 samples': ((station_counts >= 4) & (station_counts <= 10)).sum(),
            '11-50 samples': ((station_counts >= 11) & (station_counts <= 50)).sum(),
            '51-100 samples': ((station_counts >= 51) & (station_counts <= 100)).sum(),
            '>100 samples': (station_counts > 100).sum()
        }

        self.logger.info("ç«™ç‚¹æ ·æœ¬æ•°é‡åˆ†å¸ƒ:")
        for range_name, count in count_ranges.items():
            self.logger.info(f"  {range_name}: {count} ä¸ªç«™ç‚¹")

        # ç‰¹åˆ«å…³æ³¨æ ·æœ¬æ•°å°äºç­‰äº3çš„ç«™ç‚¹
        small_stations = station_counts[station_counts <= 3]

        if len(small_stations) > 0:
            self.logger.info(f"\nğŸ“‹ æ ·æœ¬æ•°å°äºç­‰äº3ä¸ªçš„ç«™ç‚¹ (å…±{len(small_stations)}ä¸ª):")

            # æŒ‰æ ·æœ¬æ•°é‡åˆ†ç»„æ˜¾ç¤º
            for sample_count in [1, 2, 3]:
                stations_with_count = small_stations[small_stations == sample_count]
                if len(stations_with_count) > 0:
                    self.logger.info(
                        f"  {sample_count}ä¸ªæ ·æœ¬çš„ç«™ç‚¹ ({len(stations_with_count)}ä¸ª): {list(stations_with_count.index)}")

            # è®¡ç®—è¿™äº›ç«™ç‚¹çš„æ€»æ ·æœ¬æ•°
            total_small_samples = small_stations.sum()
            self.logger.info(f"  è¿™äº›ç«™ç‚¹çš„æ€»æ ·æœ¬æ•°: {total_small_samples}")

            # è®¡ç®—å æ¯”
            total_stations = len(station_counts)
            total_samples = len(df)
            small_stations_ratio = len(small_stations) / total_stations * 100
            small_samples_ratio = total_small_samples / total_samples * 100

            self.logger.info(f"  å°æ ·æœ¬ç«™ç‚¹å æ¯”: {small_stations_ratio:.1f}% ({len(small_stations)}/{total_stations})")
            self.logger.info(f"  å°æ ·æœ¬æ•°æ®å æ¯”: {small_samples_ratio:.1f}% ({total_small_samples}/{total_samples})")

        else:
            self.logger.info("æ²¡æœ‰å‘ç°æ ·æœ¬æ•°å°äºç­‰äº3ä¸ªçš„ç«™ç‚¹")

        # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
        self.logger.info(f"\nğŸ“Š ç«™ç‚¹æ ·æœ¬æ€»ä½“ç»Ÿè®¡:")
        self.logger.info(f"  æ€»ç«™ç‚¹æ•°: {len(station_counts)}")
        self.logger.info(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
        self.logger.info(f"  å¹³å‡æ¯ä¸ªç«™ç‚¹æ ·æœ¬æ•°: {len(df) / len(station_counts):.1f}")
        self.logger.info(f"  æœ€å°æ ·æœ¬æ•°: {station_counts.min()}")
        self.logger.info(f"  æœ€å¤§æ ·æœ¬æ•°: {station_counts.max()}")
        self.logger.info(f"  ä¸­ä½æ•°æ ·æœ¬æ•°: {station_counts.median()}")

    def _process_landuse_features(self, df):
        """å¤„ç†landuseç‹¬çƒ­ç¼–ç ç‰¹å¾

        Args:
            df (pd.DataFrame): åŸå§‹æ•°æ®

        Returns:
            pd.DataFrame: å¤„ç†åçš„æ•°æ®
        """
        self.logger.info("å¤„ç†landuseç‹¬çƒ­ç¼–ç ç‰¹å¾...")

        # æ‰¾å‡ºæ‰€æœ‰çš„landuseç‹¬çƒ­ç¼–ç åˆ—
        landuse_columns = [col for col in df.columns if
                           col.startswith('landuse_') and not col.startswith('landuse_hash_')]

        if not landuse_columns:
            self.logger.warning("æœªæ‰¾åˆ°landuseç‹¬çƒ­ç¼–ç ç‰¹å¾åˆ—")
            return df

        self.logger.info(f"æ‰¾åˆ° {len(landuse_columns)} ä¸ªlanduseç‹¬çƒ­ç¼–ç ç‰¹å¾")

        # æ£€æŸ¥landuseåˆ—çš„æ•°æ®ç±»å‹å’Œç»Ÿè®¡ä¿¡æ¯
        for col in landuse_columns:
            unique_count = df[col].nunique()
            na_count = df[col].isna().sum()
            dtype = df[col].dtype
            self.logger.debug(f"  {col}: ç±»å‹={dtype}, å”¯ä¸€å€¼={unique_count}, ç¼ºå¤±å€¼={na_count}")

        # ç¡®ä¿æ‰€æœ‰landuseåˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹ï¼ˆç‹¬çƒ­ç¼–ç åº”è¯¥æ˜¯0/1ï¼‰
        for col in landuse_columns:
            if not pd.api.types.is_numeric_dtype(df[col]):
                self.logger.info(f"è½¬æ¢ {col} ä¸ºæ•°å€¼ç±»å‹")
                df[col] = pd.to_numeric(df[col], errors='coerce')

        self.logger.info(f"âœ… landuseç‹¬çƒ­ç¼–ç ç‰¹å¾å¤„ç†å®Œæˆ")
        self.logger.info(f"  ä¿ç•™äº† {len(landuse_columns)} ä¸ªlanduseç‹¬çƒ­ç¼–ç ç‰¹å¾")

        return df

    def _handle_missing_values(self, X):
        """ç®€åŒ–ç¼ºå¤±å€¼å¤„ç†ï¼Œä¸»è¦ä¾èµ–XGBoostå†…ç½®æœºåˆ¶

        Args:
            X (pd.DataFrame): ç‰¹å¾æ•°æ®

        Returns:
            pd.DataFrame: å¤„ç†åçš„ç‰¹å¾æ•°æ®
        """
        self.logger.info("ç®€åŒ–ç¼ºå¤±å€¼å¤„ç†ï¼Œä¸»è¦ä¾èµ–XGBoostå†…ç½®æœºåˆ¶...")

        initial_missing = X.isna().sum().sum()
        if initial_missing == 0:
            self.logger.info("æ²¡æœ‰å‘ç°ç¼ºå¤±å€¼")
            return X

        X_processed = X.copy()

        # åªå¤„ç†åˆ†ç±»ç‰¹å¾ï¼Œæ•°å€¼ç‰¹å¾çš„ç¼ºå¤±å€¼ç•™ç»™XGBoostå¤„ç†
        categorical_cols = X_processed.select_dtypes(include=['object']).columns

        for col in categorical_cols:
            if X_processed[col].isna().sum() > 0:
                # ç”¨'missing'å¡«å……ï¼Œè®©XGBoostå­¦ä¹ å¦‚ä½•å¤„ç†
                X_processed[col] = X_processed[col].fillna('missing')

            # è½¬æ¢ä¸ºæ•°å€¼ç¼–ç 
            X_processed[col] = X_processed[col].astype('category').cat.codes

        self.logger.info(f"å‰©ä½™ç¼ºå¤±å€¼æ•°é‡: {X_processed.isna().sum().sum()} (å°†ç”±XGBoostå¤„ç†)")

        return X_processed

    def _create_swe_products_comparison(self, df, output_dir):
        """åˆ›å»ºå…¶ä»–SWEäº§å“ä¸å®æµ‹å€¼çš„å¯¹æ¯”æ•£ç‚¹å›¾

        Args:
            df (pd.DataFrame): åŸå§‹æ•°æ®
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        try:
            self.logger.info("ğŸ“Š ç”ŸæˆSWEäº§å“å¯¹æ¯”æ•£ç‚¹å›¾...")

            # å®šä¹‰è¦å¯¹æ¯”çš„SWEäº§å“åˆ—
            swe_products = ['cswe', 'era5_swe', 'glsnow', 'gldas']

            # æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦å­˜åœ¨è¿™äº›åˆ—
            available_products = [col for col in swe_products if col in df.columns]
            missing_products = [col for col in swe_products if col not in df.columns]

            if missing_products:
                self.logger.warning(f"ä»¥ä¸‹SWEäº§å“åˆ—ä¸å­˜åœ¨: {missing_products}")

            if not available_products:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„SWEäº§å“åˆ—è¿›è¡Œå¯¹æ¯”")
                return

            self.logger.info(f"å°†å¯¹æ¯”ä»¥ä¸‹SWEäº§å“: {available_products}")

            # è®¾ç½®å›¾å½¢æ ·å¼
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'Arial']
            plt.rcParams['axes.unicode_minus'] = False
            sns.set_style("whitegrid")

            # åˆ›å»ºå­å›¾
            n_products = len(available_products)
            n_cols = min(2, n_products)
            n_rows = (n_products + n_cols - 1) // n_cols

            fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 5 * n_rows))
            if n_products == 1:
                axes = [axes]
            elif n_rows > 1 and n_cols > 1:
                axes = axes.flatten()

            # ä¸ºæ¯ä¸ªSWEäº§å“åˆ›å»ºæ•£ç‚¹å›¾
            for i, product in enumerate(available_products):
                if i < len(axes):
                    ax = axes[i]
                    self._plot_single_swe_product_comparison(ax, df, product)

            # éšè—å¤šä½™çš„å­å›¾
            for i in range(len(available_products), len(axes)):
                axes[i].set_visible(False)

            # è°ƒæ•´å¸ƒå±€
            plt.tight_layout()

            # ä¿å­˜å›¾ç‰‡
            comparison_path = f'{output_dir}/swe_products_comparison.png'
            plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
            plt.close()

            self.logger.info(f"âœ… SWEäº§å“å¯¹æ¯”å›¾ä¿å­˜: {comparison_path}")

        except Exception as e:
            self.logger.warning(f"ç”ŸæˆSWEäº§å“å¯¹æ¯”å›¾å¤±è´¥: {str(e)}")

    def _plot_single_swe_product_comparison(self, ax, df, product_col):
        """ç»˜åˆ¶å•ä¸ªSWEäº§å“ä¸å®æµ‹å€¼çš„å¯¹æ¯”æ•£ç‚¹å›¾"""
        # ç§»é™¤NaNå€¼å’Œæ— æ•ˆå€¼
        if product_col == 'cswe':
            mask = (~np.isnan(df[self.target_column]) &
                    ~np.isnan(df[product_col]) &
                    (df[product_col] <= 200))
        else:
            mask = (~np.isnan(df[self.target_column]) &
                    ~np.isnan(df[product_col]))

        y_true = df[self.target_column][mask]
        y_product = df[product_col][mask]

        if len(y_true) == 0:
            ax.text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=ax.transAxes, fontsize=14)
            ax.set_title(f'{product_col.upper()} vs Observed SWE')
            return

        # è®¾ç½®åæ ‡è½´èŒƒå›´
        max_range = 175
        min_val = 0
        max_val = max_range

        # 1:1 å‚è€ƒçº¿
        ax.plot([min_val, max_val], [min_val, max_val], 'k-', alpha=0.8, linewidth=2)

        # æ•£ç‚¹å›¾
        ax.scatter(y_true, y_product, alpha=0.6, s=20, c='blue', edgecolors='none')

        # è®¾ç½®åæ ‡è½´
        ax.set_xlabel('Observed SWE (mm)', fontsize=14)
        ax.set_ylabel(f'{product_col.upper()} SWE (mm)', fontsize=14)
        ax.set_title(f'{product_col.upper()} vs Observed SWE', fontsize=14, fontweight='bold')
        ax.set_xlim([min_val, max_val])
        ax.set_ylim([min_val, max_val])
        ax.set_aspect('equal')
        ax.grid(True, alpha=0.3)

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        mae = mean_absolute_error(y_true, y_product)
        rmse = np.sqrt(mean_squared_error(y_true, y_product))

        # å®‰å…¨è®¡ç®—ç›¸å…³ç³»æ•°
        def safe_pearsonr(x, y):
            if len(x) <= 1 or np.all(x == x[0]) or np.all(y == y[0]):
                return np.nan, np.nan
            if np.std(x) == 0 or np.std(y) == 0:
                return np.nan, np.nan
            try:
                return pearsonr(x, y)
            except:
                return np.nan, np.nan

        r, p_value = safe_pearsonr(y_true, y_product)
        r_str = f"{r:.3f}" if not np.isnan(r) else "NaN"

        stats_text = f'MAE = {mae:.2f} mm\nRMSE = {rmse:.2f} mm\nR = {r_str}\nN = {len(y_true)}'

        ax.text(0.95, 0.95, stats_text, transform=ax.transAxes,
                verticalalignment='top', horizontalalignment='right',
                fontsize=13, fontfamily='monospace', weight='bold',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    def validate_data_consistency(self, X, y, station_groups, year_groups):
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
        self.logger.info("éªŒè¯æ•°æ®ä¸€è‡´æ€§...")

        lengths = {
            'X': len(X),
            'y': len(y),
            'station_groups': len(station_groups),
            'year_groups': len(year_groups)
        }

        # æ£€æŸ¥æ‰€æœ‰é•¿åº¦æ˜¯å¦ä¸€è‡´
        unique_lengths = set(lengths.values())
        if len(unique_lengths) != 1:
            self.logger.error(f"æ•°æ®é•¿åº¦ä¸ä¸€è‡´: {lengths}")
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰NaNå€¼
        if np.isnan(X).any():
            self.logger.warning("ç‰¹å¾æ•°æ®ä¸­åŒ…å«NaNå€¼")

        if np.isnan(y).any():
            self.logger.warning("ç›®æ ‡å˜é‡ä¸­åŒ…å«NaNå€¼")

        self.logger.info(f"âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡: æ‰€æœ‰æ•°æ®é•¿åº¦ = {list(unique_lengths)[0]}")
        return True

    def validate_data(self, df):
        """éªŒè¯è¾“å…¥æ•°æ®

        Args:
            df (pd.DataFrame): è¾“å…¥æ•°æ®

        Raises:
            ValueError: æ•°æ®éªŒè¯å¤±è´¥æ—¶æŠ›å‡º
        """
        self.logger.info("éªŒè¯è¾“å…¥æ•°æ®...")

        # æ£€æŸ¥DataFrameç±»å‹
        if not isinstance(df, pd.DataFrame):
            raise ValueError("è¾“å…¥æ•°æ®å¿…é¡»æ˜¯pandas DataFrame")

        # æ£€æŸ¥å¿…è¦åˆ—
        required_columns = ['station_id', 'date', self.target_column]
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")

        # æ£€æŸ¥ç›®æ ‡å˜é‡
        swe_na_count = df[self.target_column].isna().sum()
        swe_total_count = len(df)

        # åªæ˜¯è­¦å‘Šï¼Œä¸é˜»æ­¢è®­ç»ƒ
        if swe_na_count > 0:
            self.logger.warning(f"ç›®æ ‡å˜é‡ä¸­æœ‰ {swe_na_count} ä¸ªç¼ºå¤±å€¼ï¼ŒXGBoostå°†å­¦ä¹ å¦‚ä½•å¤„ç†")

        if swe_na_count == swe_total_count:
            raise ValueError(f"{self.target_column}åˆ—å…¨éƒ¨ä¸ºç©ºå€¼ï¼Œæ— æ³•è®­ç»ƒ")

        # æ£€æŸ¥CSWEåˆ—çš„æ— æ•ˆå€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'cswe' in df.columns:
            cswe_invalid_count = (df['cswe'] > 200).sum()
            if cswe_invalid_count > 0:
                self.logger.info(f"å‘ç° {cswe_invalid_count} ä¸ªCSWEå¤§äº200mmçš„å€¼ï¼Œå°†ä½œä¸ºç¼ºå¤±å€¼ç”±XGBoostå¤„ç†")

        # æ£€æŸ¥ç«™ç‚¹æ•°é‡
        station_count = df['station_id'].nunique()
        if station_count < 2:
            raise ValueError(f"ç«™ç‚¹æ•°é‡å¤ªå°‘ ({station_count})ï¼Œè‡³å°‘éœ€è¦2ä¸ªç«™ç‚¹")

        self.logger.info(f"âœ… æ•°æ®éªŒè¯é€šè¿‡: {len(df)} è¡Œ, {len(df.columns)} åˆ—, {station_count} ä¸ªç«™ç‚¹")

    def evaluate_predictions(self, y_true, y_pred):
        """è¯„ä¼°é¢„æµ‹ç»“æœ

        Args:
            y_true (array-like): çœŸå®å€¼
            y_pred (array-like): é¢„æµ‹å€¼

        Returns:
            dict: åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        # ç§»é™¤NaNå€¼
        mask = ~(np.isnan(y_true) | np.isnan(y_pred))
        y_true_clean = y_true[mask]
        y_pred_clean = y_pred[mask]

        if len(y_true_clean) == 0:
            return {
                'MAE': np.nan,
                'RMSE': np.nan,
                'R': np.nan,
                'R_pvalue': np.nan,
                'æ ·æœ¬æ•°': 0,
                'æ€»æ ·æœ¬æ•°': len(y_true),
                'æœ‰æ•ˆæ ·æœ¬æ¯”ä¾‹': 0.0
            }

        # è®¡ç®—MAEå’ŒRMSE
        mae = mean_absolute_error(y_true_clean, y_pred_clean)
        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))

        # å®‰å…¨è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
        def safe_pearsonr(x, y):
            """å®‰å…¨è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°ï¼Œå¤„ç†å¸¸æ•°æ•°ç»„æƒ…å†µ"""
            # æ£€æŸ¥è¾“å…¥æ•°ç»„æ˜¯å¦ä¸ºå¸¸æ•°
            if len(x) <= 1:
                return np.nan, np.nan

            if np.all(x == x[0]) or np.all(y == y[0]):
                # å¦‚æœä»»ä¸€æ•°ç»„æ˜¯å¸¸æ•°ï¼Œç›¸å…³ç³»æ•°æœªå®šä¹‰
                return np.nan, np.nan

            # æ£€æŸ¥æ–¹å·®æ˜¯å¦ä¸º0
            if np.std(x) == 0 or np.std(y) == 0:
                return np.nan, np.nan

            try:
                return pearsonr(x, y)
            except:
                return np.nan, np.nan

        # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
        r, p_value = safe_pearsonr(y_true_clean, y_pred_clean)

        total_samples = len(y_true)
        valid_samples = len(y_true_clean)
        valid_ratio = valid_samples / total_samples if total_samples > 0 else 0

        return {
            'MAE': mae,
            'RMSE': rmse,
            'R': r,
            'R_pvalue': p_value,
            'æ ·æœ¬æ•°': valid_samples,
            'æ€»æ ·æœ¬æ•°': total_samples,
            'æœ‰æ•ˆæ ·æœ¬æ¯”ä¾‹': valid_ratio
        }

    def cross_validate(self, X, y, groups, cv_type='station'):
        """æ‰§è¡Œäº¤å‰éªŒè¯

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            y (np.array): ç›®æ ‡å˜é‡
            groups (np.array): åˆ†ç»„ä¿¡æ¯
            cv_type (str): äº¤å‰éªŒè¯ç±»å‹ ('station' æˆ– 'yearly')

        Returns:
            dict: äº¤å‰éªŒè¯ç»“æœï¼ŒåŒ…å«èšåˆå€¼ã€å¹³å‡å€¼å’Œä¸­ä½æ•°
        """
        logo = LeaveOneGroupOut()

        all_predictions = []
        all_true_values = []
        fold_results = {}

        # ç”¨äºå­˜å‚¨å„æŠ˜å çš„æŒ‡æ ‡
        fold_maes = []
        fold_rmses = []
        fold_rs = []
        fold_samples = []

        unique_groups = np.unique(groups)
        total_folds = len(unique_groups)

        self.logger.info(f"å¼€å§‹{cv_type}äº¤å‰éªŒè¯ï¼Œå…±{total_folds}ä¸ªæŠ˜å ...")

        for fold, (train_idx, test_idx) in enumerate(logo.split(X, y, groups)):
            group_id = groups[test_idx[0]]
            test_size = len(test_idx)
            train_size = len(train_idx)

            self.logger.debug(f"{cv_type} Fold {fold + 1}: è®­ç»ƒé›†{train_size}æ ·æœ¬, æµ‹è¯•é›†{test_size}æ ·æœ¬")

            # åˆ†å‰²æ•°æ®
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]

            # è®­ç»ƒæ¨¡å‹
            model = xgb.XGBRegressor(**self.params)
            model.fit(X_train, y_train)

            # é¢„æµ‹
            y_pred = model.predict(X_test)

            # å­˜å‚¨ç»“æœ
            all_predictions.extend(y_pred)
            all_true_values.extend(y_test)

            # è®¡ç®—å½“å‰æŠ˜å æ€§èƒ½
            fold_metrics = self.evaluate_predictions(y_test, y_pred)
            fold_results[group_id] = fold_metrics

            # å®‰å…¨æ˜¾ç¤ºç›¸å…³ç³»æ•°
            r_display = fold_metrics['R']
            if np.isnan(r_display):
                r_display_str = "NaN"
            else:
                r_display_str = f"{r_display:.3f}"

            self.logger.info(
                f"  {cv_type} Fold {fold + 1}/{total_folds}: {group_id} "
                f"({test_size}æ ·æœ¬) - "
                f"MAE={fold_metrics['MAE']:.3f}, R={r_display_str}"
            )

            # å­˜å‚¨å„æŠ˜å æŒ‡æ ‡ç”¨äºç»Ÿè®¡
            fold_maes.append(fold_metrics['MAE'])
            fold_rmses.append(fold_metrics['RMSE'])
            fold_rs.append(fold_metrics['R'])
            fold_samples.append(fold_metrics['æ ·æœ¬æ•°'])

            self.logger.info(
                f"  {cv_type} Fold {fold + 1}/{total_folds}: {group_id} "
                f"({test_size}æ ·æœ¬) - "
                f"MAE={fold_metrics['MAE']:.3f}, R={fold_metrics['R']:.3f}"
            )

        # è®¡ç®—èšåˆæ€§èƒ½ï¼ˆæ‰€æœ‰æµ‹è¯•æ ·æœ¬ä¸€èµ·è®¡ç®—ï¼‰
        overall_metrics = self.evaluate_predictions(
            np.array(all_true_values),
            np.array(all_predictions)
        )

        # è®¡ç®—å„æŠ˜å æŒ‡æ ‡çš„å¹³å‡å€¼å’Œä¸­ä½æ•°
        def safe_statistic(values, func):
            """å®‰å…¨è®¡ç®—ç»Ÿè®¡é‡ï¼Œå¤„ç†NaNå€¼"""
            valid_values = [v for v in values if not np.isnan(v)]
            if len(valid_values) == 0:
                return np.nan
            return func(valid_values)

        mean_metrics = {
            'MAE': safe_statistic(fold_maes, np.mean),
            'RMSE': safe_statistic(fold_rmses, np.mean),
            'R': safe_statistic(fold_rs, np.mean),
            'æ ·æœ¬æ•°': np.sum(fold_samples)  # æ€»æ ·æœ¬æ•°
        }

        median_metrics = {
            'MAE': safe_statistic(fold_maes, np.median),
            'RMSE': safe_statistic(fold_rmses, np.median),
            'R': safe_statistic(fold_rs, np.median),
            'æ ·æœ¬æ•°': np.sum(fold_samples)
        }

        # è®¡ç®—å„æŠ˜å æŒ‡æ ‡çš„æ ‡å‡†å·®
        std_metrics = {
            'MAE': safe_statistic(fold_maes, np.std),
            'RMSE': safe_statistic(fold_rmses, np.std),
            'R': safe_statistic(fold_rs, np.std)
        }

        # å®‰å…¨æ˜¾ç¤ºæ€»ä½“ç›¸å…³ç³»æ•°
        overall_r_display = overall_metrics['R']
        mean_r_display = mean_metrics['R']
        median_r_display = median_metrics['R']

        if np.isnan(overall_r_display):
            overall_r_str = "NaN"
        else:
            overall_r_str = f"{overall_r_display:.3f}"

        if np.isnan(mean_r_display):
            mean_r_str = "NaN"
        else:
            mean_r_str = f"{mean_r_display:.3f}"

        if np.isnan(median_r_display):
            median_r_str = "NaN"
        else:
            median_r_str = f"{median_r_display:.3f}"

        self.logger.info(f"âœ… {cv_type}äº¤å‰éªŒè¯å®Œæˆ")
        self.logger.info(f"  èšåˆæ€§èƒ½: MAE={overall_metrics['MAE']:.3f}mm, R={overall_r_str}")
        self.logger.info(f"  å¹³å‡æ€§èƒ½: MAE={mean_metrics['MAE']:.3f}mm, R={mean_r_str}")
        self.logger.info(f"  ä¸­ä½æ•°æ€§èƒ½: MAE={median_metrics['MAE']:.3f}mm, R={median_r_str}")

        return {
            'overall': overall_metrics,  # èšåˆè®¡ç®—ï¼šæ‰€æœ‰æµ‹è¯•æ ·æœ¬ä¸€èµ·è®¡ç®—
            'mean': mean_metrics,  # å¹³å‡å€¼ï¼šå„æŠ˜å æŒ‡æ ‡çš„å¹³å‡
            'median': median_metrics,  # ä¸­ä½æ•°ï¼šå„æŠ˜å æŒ‡æ ‡çš„ä¸­ä½æ•°
            'std': std_metrics,  # æ ‡å‡†å·®ï¼šå„æŠ˜å æŒ‡æ ‡çš„å˜å¼‚ç¨‹åº¦
            'by_fold': fold_results,  # å„æŠ˜å è¯¦ç»†ç»“æœ
            'predictions': np.array(all_predictions),
            'true_values': np.array(all_true_values),
            'folds': total_folds,
            'fold_metrics': {  # å„æŠ˜å æŒ‡æ ‡åˆ—è¡¨
                'MAE': fold_maes,
                'RMSE': fold_rmses,
                'R': fold_rs,
                'samples': fold_samples
            }
        }

    def safe_statistic(values, func):
        """å®‰å…¨è®¡ç®—ç»Ÿè®¡é‡ï¼Œå¤„ç†NaNå€¼å’Œç©ºæ•°ç»„"""
        valid_values = [v for v in values if not np.isnan(v)]
        if len(valid_values) == 0:
            return np.nan
        return func(valid_values)

    def train_final_model(self, X, y):
        """è®­ç»ƒæœ€ç»ˆæ¨¡å‹

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            y (np.array): ç›®æ ‡å˜é‡

        Returns:
            xgb.XGBRegressor: è®­ç»ƒå¥½çš„XGBoostæ¨¡å‹
        """
        self.logger.info("è®­ç»ƒæœ€ç»ˆXGBoostæ¨¡å‹...")

        self.model = xgb.XGBRegressor(**self.params)
        self.model.fit(X, y)

        self.logger.info("âœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
        return self.model

    def run_complete_analysis(self, df, output_dir=None):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        self.logger.info("=" * 70)
        self.logger.info("ğŸš€ å¼€å§‹SWE XGBoostå®Œæ•´åˆ†ææµç¨‹")
        self.logger.info("=" * 70)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = f"./swe_model_results_{timestamp}"

        os.makedirs(output_dir, exist_ok=True)
        self.logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")

        try:
            # 1. æ•°æ®é¢„å¤„ç†
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 1: æ•°æ®é¢„å¤„ç†")
            self.logger.info("=" * 50)

            X, y, station_groups, year_groups = self.preprocess_data(df)

            results = {
                'preprocessing': {
                    'samples': len(X),
                    'features': len(self.feature_columns),
                    'stations': len(np.unique(station_groups)),
                    'years': len(np.unique(year_groups))
                }
            }

            # 2. ç«™ç‚¹äº¤å‰éªŒè¯
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 2: ç«™ç‚¹äº¤å‰éªŒè¯")
            self.logger.info("=" * 50)

            results['station_cv'] = self.cross_validate(X, y, station_groups, 'station')

            # 3. å¹´åº¦äº¤å‰éªŒè¯
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 3: å¹´åº¦äº¤å‰éªŒè¯")
            self.logger.info("=" * 50)

            results['yearly_cv'] = self.cross_validate(X, y, year_groups, 'yearly')

            # 4. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 4: è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
            self.logger.info("=" * 50)

            results['final_model'] = self.train_final_model(X, y)

            # 5. ç‰¹å¾é‡è¦æ€§åˆ†æ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 5: ç‰¹å¾é‡è¦æ€§åˆ†æ")
            self.logger.info("=" * 50)

            results['feature_importance'] = self.get_feature_importance()

            # 6. ä¿å­˜ç»“æœ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 6: ä¿å­˜ç»“æœ")
            self.logger.info("=" * 50)

            self._save_results(results, output_dir)

            # 7. ç”ŸæˆSWEäº§å“å¯¹æ¯”å›¾ï¼ˆä½¿ç”¨åŸå§‹æ•°æ®ï¼‰
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 7: ç”ŸæˆSWEäº§å“å¯¹æ¯”å›¾")
            self.logger.info("=" * 50)

            self._create_swe_products_comparison(df, output_dir)

            # 8. ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report(results)
            print(report)
            self.logger.info("ğŸ¯ å®Œæ•´åˆ†æå®Œæˆï¼")
            return results

        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææµç¨‹å¤±è´¥: {str(e)}")
            raise

    def _create_scatter_plots(self, results, output_dir):
        """åˆ›å»ºä¸¤ç§äº¤å‰éªŒè¯æ–¹æ³•çš„é¢„æµ‹å€¼ä¸å®é™…å€¼æ•£ç‚¹å›¾"""
        try:
            self.logger.info("ğŸ“Š ç”Ÿæˆæ•£ç‚¹å›¾...")

            # è®¾ç½®å›¾å½¢æ ·å¼ - ä½¿ç”¨æ”¯æŒä¸­æ–‡çš„å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Arial']
            plt.rcParams['axes.unicode_minus'] = False
            sns.set_style("whitegrid")

            # åˆ›å»ºå­å›¾
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # å·¦å›¾ï¼šå¹´åº¦äº¤å‰éªŒè¯æ•£ç‚¹å›¾
            if 'yearly_cv' in results:
                overall_metrics = results['yearly_cv']['overall']
                self._plot_single_scatter(
                    ax1,
                    results['yearly_cv']['true_values'],
                    results['yearly_cv']['predictions'],
                    overall_metrics,
                    'Yearly Cross-Validation',  # ä½¿ç”¨è‹±æ–‡æ ‡é¢˜
                    'Predicted SWE (mm)'
                )

            # å³å›¾ï¼šç«™ç‚¹äº¤å‰éªŒè¯æ•£ç‚¹å›¾
            if 'station_cv' in results:
                overall_metrics = results['station_cv']['overall']
                self._plot_single_scatter(
                    ax2,
                    results['station_cv']['true_values'],
                    results['station_cv']['predictions'],
                    overall_metrics,
                    'Station Cross-Validation',  # ä½¿ç”¨è‹±æ–‡æ ‡é¢˜
                    'Predicted SWE (mm)'
                )

            plt.tight_layout()
            scatter_path = f'{output_dir}/scatter_plots.png'
            plt.savefig(scatter_path, dpi=300, bbox_inches='tight')
            plt.close()

            self.logger.info(f"âœ… æ•£ç‚¹å›¾ä¿å­˜: {scatter_path}")

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆæ•£ç‚¹å›¾å¤±è´¥: {str(e)}")

    def _plot_single_scatter(self, ax, y_true, y_pred, metrics, title, ylabel):
        """ç»˜åˆ¶å•ä¸ªæ•£ç‚¹å›¾"""
        # ç§»é™¤NaNå€¼
        mask = ~(np.isnan(y_true) | np.isnan(y_pred))
        y_true_clean = y_true[mask]
        y_pred_clean = y_pred[mask]

        if len(y_true_clean) == 0:
            ax.text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=ax.transAxes, fontsize=14)
            ax.set_title(title, fontsize=16)
            return

        # è®¾ç½®åæ ‡è½´èŒƒå›´åˆ°175mm
        max_range = 175
        min_val = 0
        max_val = max_range

        # 1:1 å‚è€ƒçº¿
        ax.plot([min_val, max_val], [min_val, max_val], 'k-', alpha=0.8, linewidth=2)

        # ä½¿ç”¨æ•£ç‚¹å›¾
        try:
            from scipy.stats import gaussian_kde
            xy = np.vstack([y_true_clean, y_pred_clean])
            z = gaussian_kde(xy)(xy)
            idx = z.argsort()
            y_true_sorted, y_pred_sorted, z_sorted = y_true_clean[idx], y_pred_clean[idx], z[idx]

            scatter = ax.scatter(y_true_sorted, y_pred_sorted,
                                 c=z_sorted, cmap='viridis', s=15, alpha=0.7,
                                 edgecolors='none', marker='o')
            plt.colorbar(scatter, ax=ax, label='Point Density')
        except:
            scatter = ax.scatter(y_true_clean, y_pred_clean,
                                 alpha=0.6, s=15, c='blue', edgecolors='none')

        # æ·»åŠ å›å½’è¶‹åŠ¿çº¿
        if len(y_true_clean) > 1:
            try:
                slope, intercept, r_value, p_value, std_err = stats.linregress(y_true_clean, y_pred_clean)
                x_reg = np.linspace(min_val, max_val, 100)
                y_reg = slope * x_reg + intercept
                ax.plot(x_reg, y_reg, 'r--', alpha=0.8, linewidth=2)
            except:
                pass

        # è®¾ç½®åæ ‡è½´
        ax.set_xlabel('Observed SWE (mm)', fontsize=14)
        ax.set_ylabel(ylabel, fontsize=14)
        ax.set_title(title, fontsize=16, fontweight='bold')
        ax.set_xlim([min_val, max_val])
        ax.set_ylim([min_val, max_val])
        ax.set_aspect('equal')
        ax.grid(True, alpha=0.3)

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mae = metrics['MAE']
        rmse = metrics['RMSE']
        r = metrics['R']
        n = len(y_true_clean)

        r_str = f"{r:.3f}" if not np.isnan(r) else "NaN"
        stats_text = f'MAE = {mae:.2f} mm\nRMSE = {rmse:.2f} mm\nR = {r_str}\nN = {n}'

        ax.text(0.95, 0.95, stats_text, transform=ax.transAxes,
                verticalalignment='top', horizontalalignment='right',
                fontsize=14, fontfamily='monospace', weight='bold',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    def _create_combined_scatter_plot(self, results, output_dir):
        """åˆ›å»ºåˆå¹¶çš„æ•£ç‚¹å›¾"""
        try:
            # è®¾ç½®å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS', 'Arial']
            plt.rcParams['axes.unicode_minus'] = False

            fig, ax = plt.subplots(figsize=(10, 8))

            max_range = 175
            min_val = 0
            max_val = max_range

            # 1:1 å‚è€ƒçº¿
            ax.plot([min_val, max_val], [min_val, max_val], 'k-', alpha=0.8, linewidth=2)

            colors = ['#ff7f0e', '#1f77b4']
            labels = ['Yearly CV', 'Station CV']  # ä½¿ç”¨è‹±æ–‡æ ‡ç­¾
            methods = ['yearly_cv', 'station_cv']

            # æ”¶é›†æ‰€æœ‰ç‚¹
            for i, method in enumerate(methods):
                if method in results:
                    y_true = results[method]['true_values']
                    y_pred = results[method]['predictions']
                    mask = ~(np.isnan(y_true) | np.isnan(y_pred))
                    y_true_clean = y_true[mask]
                    y_pred_clean = y_pred[mask]

                    if len(y_true_clean) > 0:
                        ax.scatter(y_true_clean, y_pred_clean,
                                   c=colors[i], s=15, alpha=0.6,
                                   edgecolors='none', marker='o',
                                   label=labels[i])

            if len(ax.collections) == 0:
                ax.text(0.5, 0.5, 'No valid data', ha='center', va='center',
                        transform=ax.transAxes, fontsize=14)
            else:
                # è®¾ç½®åæ ‡è½´
                ax.set_xlim([min_val, max_val])
                ax.set_ylim([min_val, max_val])
                ax.set_aspect('equal')
                ax.set_xlabel('Observed SWE (mm)', fontsize=14)
                ax.set_ylabel('Predicted SWE (mm)', fontsize=14)
                ax.set_title('SWE Prediction vs Observation Comparison', fontsize=16, fontweight='bold')
                ax.grid(True, alpha=0.3)

                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                stats_text = ""
                if 'yearly_cv' in results:
                    yearly_metrics = results['yearly_cv']['overall']
                    yearly_r = yearly_metrics['R']
                    yearly_r_str = f"{yearly_r:.3f}" if not np.isnan(yearly_r) else "NaN"
                    stats_text += f"Yearly CV:\nMAE={yearly_metrics['MAE']:.2f}\nRMSE={yearly_metrics['RMSE']:.2f}\nR={yearly_r_str}\nN={len(results['yearly_cv']['true_values'])}\n\n"

                if 'station_cv' in results:
                    station_metrics = results['station_cv']['overall']
                    station_r = station_metrics['R']
                    station_r_str = f"{station_r:.3f}" if not np.isnan(station_r) else "NaN"
                    stats_text += f"Station CV:\nMAE={station_metrics['MAE']:.2f}\nRMSE={station_metrics['RMSE']:.2f}\nR={station_r_str}\nN={len(results['station_cv']['true_values'])}"

                ax.text(0.95, 0.95, stats_text, transform=ax.transAxes,
                        verticalalignment='top', horizontalalignment='right',
                        fontsize=13, fontfamily='monospace', weight='bold',
                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))

                ax.legend(loc='lower left', framealpha=0.9, fontsize=12)

            plt.tight_layout()
            combined_path = f'{output_dir}/combined_scatter_plot.png'
            plt.savefig(combined_path, dpi=300, bbox_inches='tight')
            plt.close()

            self.logger.info(f"âœ… åˆå¹¶æ•£ç‚¹å›¾ä¿å­˜: {combined_path}")

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆåˆå¹¶æ•£ç‚¹å›¾å¤±è´¥: {str(e)}")

    def _create_feature_importance_plot(self, results, output_dir):
        """åˆ›å»ºç‰¹å¾é‡è¦æ€§æ’åºå›¾ï¼ˆé‡è¦ç‰¹å¾åœ¨ä¸Šæ–¹ï¼‰

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        try:
            self.logger.info("ğŸ“Š ç”Ÿæˆç‰¹å¾é‡è¦æ€§æ’åºå›¾...")

            # è®¾ç½®å›¾å½¢æ ·å¼
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
            plt.rcParams['axes.unicode_minus'] = False
            sns.set_style("whitegrid")

            if 'feature_importance' not in results:
                self.logger.warning("æ²¡æœ‰ç‰¹å¾é‡è¦æ€§æ•°æ®")
                return

            feature_importance_df = results['feature_importance']

            # é€‰æ‹©å‰20ä¸ªæœ€é‡è¦çš„ç‰¹å¾
            top_n = min(20, len(feature_importance_df))
            top_features = feature_importance_df.head(top_n)

            # åè½¬é¡ºåºï¼Œè®©é‡è¦ç‰¹å¾åœ¨ä¸Šé¢
            top_features = top_features.iloc[::-1]

            # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
            fig, ax = plt.subplots(figsize=(12, 10))

            # åˆ›å»ºé¢œè‰²æ˜ å°„
            colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(top_features)))

            # ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾ï¼ˆé‡è¦ç‰¹å¾åœ¨ä¸Šé¢ï¼‰
            y_pos = np.arange(len(top_features))
            bars = ax.barh(y_pos,
                           top_features['importance'],
                           color=colors,
                           alpha=0.8,
                           edgecolor='black',
                           linewidth=0.5,
                           height=0.7)

            # è®¾ç½®yè½´æ ‡ç­¾
            ax.set_yticks(y_pos)
            ax.set_yticklabels(top_features['feature'], fontsize=10)

            # è®¾ç½®xè½´
            ax.set_xlabel('Feature Importance', fontsize=12, fontweight='bold')
            ax.set_title(f'XGBoost Model Feature Importance Ranking (Top {top_n})',
                         fontsize=14, fontweight='bold', pad=20)

            # åœ¨æ¡å½¢æœ«ç«¯æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (bar, importance) in enumerate(zip(bars, top_features['importance'])):
                width = bar.get_width()
                ax.text(width + 0.001, bar.get_y() + bar.get_height() / 2,
                        f'{importance:.4f}',
                        ha='left', va='center', fontsize=9, fontweight='bold')

            # æ·»åŠ ç½‘æ ¼çº¿
            ax.grid(True, alpha=0.3, axis='x')

            # è®¾ç½®xè½´èŒƒå›´
            x_max = top_features['importance'].max() * 1.15
            ax.set_xlim(0, x_max)

            # è°ƒæ•´å¸ƒå±€
            plt.tight_layout()

            # ä¿å­˜å›¾ç‰‡
            importance_path = f'{output_dir}/feature_importance_plot.png'
            plt.savefig(importance_path, dpi=300, bbox_inches='tight')
            plt.close()

            self.logger.info(f"âœ… ç‰¹å¾é‡è¦æ€§æ’åºå›¾ä¿å­˜: {importance_path}")

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆç‰¹å¾é‡è¦æ€§æ’åºå›¾å¤±è´¥: {str(e)}")

    def _create_feature_importance_comprehensive(self, results, output_dir):
        """åˆ›å»ºæ›´è¯¦ç»†çš„ç‰¹å¾é‡è¦æ€§åˆ†æå›¾

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        try:
            if 'feature_importance' not in results:
                return

            feature_importance_df = results['feature_importance']

            # è®¾ç½®å­—ä½“
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
            plt.rcParams['axes.unicode_minus'] = False

            # åˆ›å»ºåŒ…å«å¤šä¸ªå­å›¾çš„ç»¼åˆå›¾è¡¨
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle('XGBoost Model Feature Importance Analysis', fontsize=16, fontweight='bold')

            # 1. æ°´å¹³æ¡å½¢å›¾ï¼ˆä¸»è¦æ’åºå›¾ï¼‰
            top_n = min(15, len(feature_importance_df))
            top_features = feature_importance_df.head(top_n)
            top_features = top_features.iloc[::-1]  # åè½¬é¡ºåº

            colors1 = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(top_features)))
            y_pos = np.arange(len(top_features))
            bars1 = ax1.barh(y_pos, top_features['importance'],
                             color=colors1, alpha=0.8, edgecolor='grey', height=0.7)

            ax1.set_yticks(y_pos)
            ax1.set_yticklabels(top_features['feature'], fontsize=9)
            ax1.set_xlabel('Feature Importance')
            ax1.set_title(f'Top {top_n} Feature Importance Ranking')
            ax1.grid(True, alpha=0.3, axis='x')

            # åœ¨æ¡å½¢ä¸Šæ·»åŠ æ•°å€¼
            for i, bar in enumerate(bars1):
                width = bar.get_width()
                ax1.text(width, bar.get_y() + bar.get_height() / 2,
                         f'{width:.3f}', ha='left', va='center', fontsize=8, fontweight='bold')

            # è®¾ç½®xè½´èŒƒå›´
            x_max1 = top_features['importance'].max() * 1.15
            ax1.set_xlim(0, x_max1)

            # 2. é¥¼å›¾ï¼ˆæ˜¾ç¤ºå‰10ä¸ªç‰¹å¾çš„ç›¸å¯¹é‡è¦æ€§ï¼‰
            top_10 = feature_importance_df.head(10)
            others_sum = feature_importance_df['importance'].iloc[10:].sum()

            if others_sum > 0:
                pie_data = list(top_10['importance']) + [others_sum]
                pie_labels = list(top_10['feature']) + ['Other Features']
            else:
                pie_data = list(top_10['importance'])
                pie_labels = list(top_10['feature'])

            colors2 = plt.cm.Set3(np.linspace(0, 1, len(pie_data)))
            wedges, texts, autotexts = ax2.pie(pie_data, labels=pie_labels, autopct='%1.1f%%',
                                               colors=colors2, startangle=90)
            ax2.set_title('Top 10 Feature Importance Distribution')

            # ç¾åŒ–é¥¼å›¾æ–‡æœ¬
            for autotext in autotexts:
                autotext.set_color('white')
                autotext.set_fontweight('bold')

            # 3. ç´¯ç§¯é‡è¦æ€§å›¾
            cumulative_importance = feature_importance_df['importance'].cumsum()
            features_count = range(1, len(cumulative_importance) + 1)

            ax3.plot(features_count, cumulative_importance, 'o-', linewidth=2, markersize=4, color='#2E86AB')
            ax3.fill_between(features_count, 0, cumulative_importance, alpha=0.3, color='#A5C8D9')
            ax3.set_xlabel('Number of Features')
            ax3.set_ylabel('Cumulative Importance')
            ax3.set_title('Feature Cumulative Importance')
            ax3.grid(True, alpha=0.3)

            # æ ‡è®°80%å’Œ90%é‡è¦æ€§çš„ç‚¹
            idx_80 = (cumulative_importance >= 0.8).idxmax() if (cumulative_importance >= 0.8).any() else len(
                cumulative_importance) - 1
            idx_90 = (cumulative_importance >= 0.9).idxmax() if (cumulative_importance >= 0.9).any() else len(
                cumulative_importance) - 1

            ax3.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='80% Importance')
            ax3.axhline(y=0.9, color='orange', linestyle='--', alpha=0.7, label='90% Importance')
            ax3.axvline(x=idx_80 + 1, color='red', linestyle='--', alpha=0.5)
            ax3.axvline(x=idx_90 + 1, color='orange', linestyle='--', alpha=0.5)
            ax3.legend()

            # 4. ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡
            importance_stats = {
                'Total Features': len(feature_importance_df),
                'Mean Importance': f"{feature_importance_df['importance'].mean():.4f}",
                'Max Importance': f"{feature_importance_df['importance'].max():.4f}",
                'Min Importance': f"{feature_importance_df['importance'].min():.4f}",
                'Std Importance': f"{feature_importance_df['importance'].std():.4f}",
                'Top 5 Features': f"{feature_importance_df['importance'].head(5).sum() * 100:.1f}%",
                'Top 10 Features': f"{feature_importance_df['importance'].head(10).sum() * 100:.1f}%",
                'Most Important': feature_importance_df['feature'].iloc[0]
            }

            # åˆ›å»ºç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
            ax4.axis('off')
            table_data = [[k, v] for k, v in importance_stats.items()]
            table = ax4.table(cellText=table_data,
                              colLabels=['Statistic', 'Value'],
                              cellLoc='left',
                              loc='center',
                              bbox=[0.1, 0.1, 0.8, 0.8])
            table.auto_set_font_size(False)
            table.set_fontsize(9)
            table.scale(1, 1.5)

            # è®¾ç½®è¡¨æ ¼æ ·å¼
            for i in range(len(table_data) + 1):
                table[(i, 0)].set_facecolor('#F0F0F0')
                table[(i, 0)].set_text_props(weight='bold')

            ax4.set_title('Feature Importance Statistics')

            plt.tight_layout()

            # ä¿å­˜ç»¼åˆå›¾è¡¨
            comprehensive_path = f'{output_dir}/feature_importance_comprehensive.png'
            plt.savefig(comprehensive_path, dpi=300, bbox_inches='tight')
            plt.close()

            self.logger.info(f"âœ… ç‰¹å¾é‡è¦æ€§ç»¼åˆåˆ†æå›¾ä¿å­˜: {comprehensive_path}")

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆç‰¹å¾é‡è¦æ€§ç»¼åˆåˆ†æå›¾å¤±è´¥: {str(e)}")

    def _save_results(self, results, output_dir):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        try:
            self.logger.info("ğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")

            # 1. ä¿å­˜æœ€ç»ˆæ¨¡å‹
            if 'final_model' in results:
                model_path = f'{output_dir}/final_model.pkl'
                joblib.dump(results['final_model'], model_path)
                self.logger.info(f"âœ… æ¨¡å‹ä¿å­˜: {model_path}")

            # 2. ä¿å­˜ç«™ç‚¹äº¤å‰éªŒè¯é¢„æµ‹ç»“æœ
            if 'station_cv' in results:
                station_pred_df = pd.DataFrame({
                    'true_swe': results['station_cv']['true_values'],
                    'predicted_swe': results['station_cv']['predictions']
                })
                station_pred_path = f'{output_dir}/station_cv_predictions.csv'
                station_pred_df.to_csv(station_pred_path, index=False)
                self.logger.info(f"âœ… ç«™ç‚¹CVé¢„æµ‹ç»“æœä¿å­˜: {station_pred_path}")

                # ä¿å­˜ç«™ç‚¹CVå„æŠ˜å è¯¦ç»†ç»“æœ
                station_fold_results = []
                for station_id, metrics in results['station_cv']['by_fold'].items():
                    station_fold_results.append({
                        'station_id': station_id,
                        'mae': metrics['MAE'],
                        'rmse': metrics['RMSE'],
                        'r': metrics['R'],
                        'samples': metrics['æ ·æœ¬æ•°']
                    })
                station_fold_df = pd.DataFrame(station_fold_results)
                station_fold_path = f'{output_dir}/station_cv_fold_results.csv'
                station_fold_df.to_csv(station_fold_path, index=False)
                self.logger.info(f"âœ… ç«™ç‚¹CVå„æŠ˜å ç»“æœä¿å­˜: {station_fold_path}")

            # 3. ä¿å­˜å¹´åº¦äº¤å‰éªŒè¯é¢„æµ‹ç»“æœ
            if 'yearly_cv' in results:
                yearly_pred_df = pd.DataFrame({
                    'true_swe': results['yearly_cv']['true_values'],
                    'predicted_swe': results['yearly_cv']['predictions']
                })
                yearly_pred_path = f'{output_dir}/yearly_cv_predictions.csv'
                yearly_pred_df.to_csv(yearly_pred_path, index=False)
                self.logger.info(f"âœ… å¹´åº¦CVé¢„æµ‹ç»“æœä¿å­˜: {yearly_pred_path}")

                # ä¿å­˜å¹´åº¦CVå„æŠ˜å è¯¦ç»†ç»“æœ
                yearly_fold_results = []
                for year, metrics in results['yearly_cv']['by_fold'].items():
                    yearly_fold_results.append({
                        'year': year,
                        'mae': metrics['MAE'],
                        'rmse': metrics['RMSE'],
                        'r': metrics['R'],
                        'samples': metrics['æ ·æœ¬æ•°']
                    })
                yearly_fold_df = pd.DataFrame(yearly_fold_results)
                yearly_fold_path = f'{output_dir}/yearly_cv_fold_results.csv'
                yearly_fold_df.to_csv(yearly_fold_path, index=False)
                self.logger.info(f"âœ… å¹´åº¦CVå„æŠ˜å ç»“æœä¿å­˜: {yearly_fold_path}")

            # 4. ä¿å­˜ç‰¹å¾é‡è¦æ€§
            if 'feature_importance' in results:
                feature_path = f'{output_dir}/feature_importance.csv'
                results['feature_importance'].to_csv(feature_path, index=False)
                self.logger.info(f"âœ… ç‰¹å¾é‡è¦æ€§ä¿å­˜: {feature_path}")

            # 5. ä¿å­˜è¯¦ç»†çš„è¯„ä¼°ç»“æœï¼ˆJSONæ ¼å¼ï¼‰
            eval_results = {
                'training_info': {
                    'timestamp': datetime.now().isoformat(),
                    'feature_columns': self.feature_columns,
                    'total_samples': results.get('preprocessing', {}).get('samples', 0),
                    'total_stations': results.get('preprocessing', {}).get('stations', 0),
                    'total_years': results.get('preprocessing', {}).get('years', 0)
                },
                'model_parameters': self.params,
                'station_cross_validation': {
                    'overall': results['station_cv']['overall'],
                    'mean': results['station_cv']['mean'],
                    'median': results['station_cv']['median'],
                    'std': results['station_cv']['std'],
                    'folds': results['station_cv']['folds'],
                    'fold_metrics': results['station_cv']['fold_metrics'],
                    'by_fold': {str(k): v for k, v in results['station_cv']['by_fold'].items()}
                },
                'yearly_cross_validation': {
                    'overall': results['yearly_cv']['overall'],
                    'mean': results['yearly_cv']['mean'],
                    'median': results['yearly_cv']['median'],
                    'std': results['yearly_cv']['std'],
                    'folds': results['yearly_cv']['folds'],
                    'fold_metrics': results['yearly_cv']['fold_metrics'],
                    'by_fold': {str(k): v for k, v in results['yearly_cv']['by_fold'].items()}
                },
                'performance_comparison': {
                    'station_better_mae': results['station_cv']['overall']['MAE'] < results['yearly_cv']['overall'][
                        'MAE'],
                    'station_better_rmse': results['station_cv']['overall']['RMSE'] < results['yearly_cv']['overall'][
                        'RMSE'],
                    'station_better_r': results['station_cv']['overall']['R'] > results['yearly_cv']['overall']['R'],
                    'recommended_method': 'station' if results['station_cv']['overall']['R'] >
                                                       results['yearly_cv']['overall']['R'] else 'yearly'
                }
            }

            eval_path = f'{output_dir}/evaluation_results.json'
            with open(eval_path, 'w', encoding='utf-8') as f:
                json.dump(eval_results, f, indent=2, ensure_ascii=False, default=float)
            self.logger.info(f"âœ… è¯¦ç»†è¯„ä¼°ç»“æœä¿å­˜: {eval_path}")

            # 6. ä¿å­˜æ±‡æ€»æŠ¥å‘Šï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
            summary_report = self._generate_summary_report(results)
            summary_path = f'{output_dir}/model_summary_report.txt'
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write(summary_report)
            self.logger.info(f"âœ… æ±‡æ€»æŠ¥å‘Šä¿å­˜: {summary_path}")

            # 7. ä¿å­˜è®­ç»ƒé…ç½®
            config_info = {
                'training_config': {
                    'model_type': 'XGBoost',
                    'target_variable': 'swe',
                    'cross_validation_methods': ['station_loocv', 'yearly_loocv'],
                    'evaluation_metrics': ['MAE', 'RMSE', 'R'],
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                },
                'feature_info': {
                    'total_features': len(self.feature_columns),
                    'feature_list': self.feature_columns
                }
            }

            config_path = f'{output_dir}/training_config.json'
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config_info, f, indent=2, ensure_ascii=False)
            self.logger.info(f"âœ… è®­ç»ƒé…ç½®ä¿å­˜: {config_path}")

            # 8. åˆ›å»ºç»“æœæ‘˜è¦æ–‡ä»¶
            self._create_results_summary(results, output_dir)

            # 9. ç”Ÿæˆæ•£ç‚¹å›¾
            self.logger.info("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            self._create_scatter_plots(results, output_dir)
            self._create_combined_scatter_plot(results, output_dir)

            self.logger.info(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

            # 10. ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾
            self._create_feature_importance_plot(results, output_dir)
            self._create_feature_importance_comprehensive(results, output_dir)

            # 11. ç”ŸæˆSWEäº§å“å¯¹æ¯”å›¾ï¼ˆéœ€è¦åŸå§‹æ•°æ®ï¼‰
            # æˆ‘ä»¬éœ€è¦é‡æ–°åŠ è½½æ•°æ®æ¥è·å–åŸå§‹SWEäº§å“åˆ—
            try:
                # è¿™é‡Œå‡è®¾åŸå§‹æ•°æ®è·¯å¾„å­˜å‚¨åœ¨æŸä¸ªåœ°æ–¹ï¼Œæˆ–è€…æˆ‘ä»¬éœ€è¦åœ¨run_complete_analysisä¸­ä¼ é€’df
                # ç”±äºdfåœ¨run_complete_analysisä¸­å¯ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é‚£é‡Œè°ƒç”¨
                pass
            except Exception as e:
                self.logger.warning(f"ç”ŸæˆSWEäº§å“å¯¹æ¯”å›¾å¤±è´¥: {e}")

            self.logger.info(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
            raise

    def get_feature_importance(self):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ train_final_model æ–¹æ³•")

        importance_scores = self.model.feature_importances_

        self.logger.info(f"æ¨¡å‹ç‰¹å¾é‡è¦æ€§æ•°ç»„é•¿åº¦: {len(importance_scores)}")
        self.logger.info(f"ç‰¹å¾åˆ—æ•°é‡: {len(self.feature_columns)}")

        # æ£€æŸ¥é•¿åº¦æ˜¯å¦åŒ¹é…
        if len(importance_scores) != len(self.feature_columns):
            self.logger.warning(f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…: æ¨¡å‹è¾“å‡º{len(importance_scores)}ä¸ª, ç‰¹å¾åˆ—{len(self.feature_columns)}ä¸ª")
            # ä½¿ç”¨æˆªæ–­åˆ°æœ€å°é•¿åº¦
            min_length = min(len(importance_scores), len(self.feature_columns))
            importance_scores = importance_scores[:min_length]
            feature_names = self.feature_columns[:min_length]
            self.logger.info(f"æˆªæ–­å: ç‰¹å¾{min_length}ä¸ª")
        else:
            feature_names = self.feature_columns

        feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance_scores
        }).sort_values('importance', ascending=False)

        self.logger.info(f"ç‰¹å¾é‡è¦æ€§è®¡ç®—å®Œæˆ")
        self.logger.info(
            f"æœ€é«˜é‡è¦æ€§ç‰¹å¾: {feature_importance_df['feature'].iloc[0]} = {feature_importance_df['importance'].iloc[0]:.4f}")

        return feature_importance_df

    def _generate_summary_report(self, results):
        """ç”Ÿæˆç®€åŒ–çš„æ±‡æ€»æŠ¥å‘Šç”¨äºä¿å­˜

        Args:
            results (dict): åˆ†æç»“æœ

        Returns:
            str: æ±‡æ€»æŠ¥å‘Šæ–‡æœ¬
        """
        report_lines = []
        report_lines.append("SWEæ¨¡å‹è®­ç»ƒæ±‡æ€»æŠ¥å‘Š")
        report_lines.append("=" * 50)
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")

        # åŸºæœ¬ç»Ÿè®¡
        if 'preprocessing' in results:
            preproc = results['preprocessing']
            report_lines.append("åŸºæœ¬ç»Ÿè®¡:")
            report_lines.append(f"  æ€»æ ·æœ¬æ•°: {preproc['samples']}")
            report_lines.append(f"  ç‰¹å¾æ•°é‡: {preproc['features']}")
            report_lines.append(f"  ç«™ç‚¹æ•°é‡: {preproc['stations']}")
            report_lines.append(f"  å¹´ä»½æ•°é‡: {preproc['years']}")
            report_lines.append("")

        # ç«™ç‚¹äº¤å‰éªŒè¯ç»“æœ
        if 'station_cv' in results:
            station = results['station_cv']
            report_lines.append("ç«™ç‚¹äº¤å‰éªŒè¯:")
            report_lines.append(f"  æŠ˜å æ•°: {station['folds']}")
            report_lines.append(f"  èšåˆMAE: {station['overall']['MAE']:.3f} mm")
            report_lines.append(f"  å¹³å‡MAE: {station['mean']['MAE']:.3f} mm")
            report_lines.append(f"  ä¸­ä½æ•°MAE: {station['median']['MAE']:.3f} mm")
            report_lines.append(f"  èšåˆR: {station['overall']['R']:.3f}")
            report_lines.append(f"  å¹³å‡R: {station['mean']['R']:.3f}")
            report_lines.append(f"  ä¸­ä½æ•°R: {station['median']['R']:.3f}")
            report_lines.append("")

        # å¹´åº¦äº¤å‰éªŒè¯ç»“æœ
        if 'yearly_cv' in results:
            yearly = results['yearly_cv']
            report_lines.append("å¹´åº¦äº¤å‰éªŒè¯:")
            report_lines.append(f"  æŠ˜å æ•°: {yearly['folds']}")
            report_lines.append(f"  èšåˆMAE: {yearly['overall']['MAE']:.3f} mm")
            report_lines.append(f"  å¹³å‡MAE: {yearly['mean']['MAE']:.3f} mm")
            report_lines.append(f"  ä¸­ä½æ•°MAE: {yearly['median']['MAE']:.3f} mm")
            report_lines.append(f"  èšåˆR: {yearly['overall']['R']:.3f}")
            report_lines.append(f"  å¹³å‡R: {yearly['mean']['R']:.3f}")
            report_lines.append(f"  ä¸­ä½æ•°R: {yearly['median']['R']:.3f}")
            report_lines.append("")

        # ç‰¹å¾é‡è¦æ€§
        if 'feature_importance' in results:
            top_features = results['feature_importance'].head(10)
            report_lines.append("å‰10ä¸ªé‡è¦ç‰¹å¾:")
            for i, (_, row) in enumerate(top_features.iterrows(), 1):
                report_lines.append(f"  {i:2d}. {row['feature']:<20} {row['importance']:.4f}")

        return "\n".join(report_lines)

    def _create_results_summary(self, results, output_dir):
        """åˆ›å»ºç»“æœæ‘˜è¦æ–‡ä»¶ï¼ˆCSVæ ¼å¼ï¼Œä¾¿äºåˆ†æï¼‰

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        """
        try:
            # åˆ›å»ºæ€§èƒ½æ¯”è¾ƒæ‘˜è¦
            summary_data = []

            # ç«™ç‚¹CVæ‘˜è¦
            if 'station_cv' in results:
                station = results['station_cv']
                summary_data.append({
                    'method': 'station_cv',
                    'folds': station['folds'],
                    'overall_mae': station['overall']['MAE'],
                    'overall_rmse': station['overall']['RMSE'],
                    'overall_r': station['overall']['R'],
                    'mean_mae': station['mean']['MAE'],
                    'mean_rmse': station['mean']['RMSE'],
                    'mean_r': station['mean']['R'],
                    'median_mae': station['median']['MAE'],
                    'median_rmse': station['median']['RMSE'],
                    'median_r': station['median']['R'],
                    'std_mae': station['std']['MAE'],
                    'std_rmse': station['std']['RMSE'],
                    'std_r': station['std']['R'],
                    'samples': station['overall']['æ ·æœ¬æ•°']
                })

            # å¹´åº¦CVæ‘˜è¦
            if 'yearly_cv' in results:
                yearly = results['yearly_cv']
                summary_data.append({
                    'method': 'yearly_cv',
                    'folds': yearly['folds'],
                    'overall_mae': yearly['overall']['MAE'],
                    'overall_rmse': yearly['overall']['RMSE'],
                    'overall_r': yearly['overall']['R'],
                    'mean_mae': yearly['mean']['MAE'],
                    'mean_rmse': yearly['mean']['RMSE'],
                    'mean_r': yearly['mean']['R'],
                    'median_mae': yearly['median']['MAE'],
                    'median_rmse': yearly['median']['RMSE'],
                    'median_r': yearly['median']['R'],
                    'std_mae': yearly['std']['MAE'],
                    'std_rmse': yearly['std']['RMSE'],
                    'std_r': yearly['std']['R'],
                    'samples': yearly['overall']['æ ·æœ¬æ•°']
                })

            # ä¿å­˜æ‘˜è¦
            summary_df = pd.DataFrame(summary_data)
            summary_path = f'{output_dir}/performance_summary.csv'
            summary_df.to_csv(summary_path, index=False)
            self.logger.info(f"âœ… æ€§èƒ½æ‘˜è¦ä¿å­˜: {summary_path}")

        except Exception as e:
            self.logger.warning(f"åˆ›å»ºç»“æœæ‘˜è¦å¤±è´¥: {str(e)}")

    def _generate_report(self, results):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š

        Args:
            results (dict): åˆ†æç»“æœ

        Returns:
            str: æ ¼å¼åŒ–çš„æŠ¥å‘Šæ–‡æœ¬
        """
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("ğŸ“Š SWE XGBoostæ¨¡å‹åˆ†ææŠ¥å‘Š - è¯¦ç»†ç»Ÿè®¡")
        report_lines.append("=" * 80)

        # åŸºæœ¬ä¿¡æ¯
        report_lines.append(f"\nğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
        report_lines.append(f"  è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"  ç‰¹å¾æ•°é‡: {len(self.feature_columns)}")

        if 'preprocessing' in results:
            preproc = results['preprocessing']
            report_lines.append(f"  æ ·æœ¬æ•°é‡: {preproc['samples']}")
            report_lines.append(f"  ç«™ç‚¹æ•°é‡: {preproc['stations']}")
            report_lines.append(f"  å¹´ä»½æ•°é‡: {preproc['years']}")

        # ç«™ç‚¹äº¤å‰éªŒè¯ç»“æœ
        if 'station_cv' in results:
            station_overall = results['station_cv']['overall']
            station_mean = results['station_cv']['mean']
            station_median = results['station_cv']['median']
            station_std = results['station_cv']['std']

            # å®‰å…¨å¤„ç†ç›¸å…³ç³»æ•°æ˜¾ç¤º
            station_overall_r = station_overall['R']
            station_mean_r = station_mean['R']
            station_median_r = station_median['R']

            station_overall_r_str = f"{station_overall_r:.3f}" if not np.isnan(station_overall_r) else "NaN"
            station_mean_r_str = f"{station_mean_r:.3f}" if not np.isnan(station_mean_r) else "NaN"
            station_median_r_str = f"{station_median_r:.3f}" if not np.isnan(station_median_r) else "NaN"

            report_lines.append(f"\nğŸ“ ç«™ç‚¹äº¤å‰éªŒè¯ (ç©ºé—´è¯„ä¼°):")
            report_lines.append(f"  â”Œ{'â”€' * 20}â”¬{'â”€' * 10}â”¬{'â”€' * 10}â”¬{'â”€' * 10}â”")
            report_lines.append(f"  â”‚ {'æŒ‡æ ‡':18} â”‚ {'èšåˆ':8} â”‚ {'å¹³å‡':8} â”‚ {'ä¸­ä½æ•°':8} â”‚")
            report_lines.append(f"  â”œ{'â”€' * 20}â”¼{'â”€' * 10}â”¼{'â”€' * 10}â”¼{'â”€' * 10}â”¤")
            report_lines.append(
                f"  â”‚ {'MAE (mm)':18} â”‚ {station_overall['MAE']:8.3f} â”‚ {station_mean['MAE']:8.3f} â”‚ {station_median['MAE']:8.3f} â”‚")
            report_lines.append(
                f"  â”‚ {'RMSE (mm)':18} â”‚ {station_overall['RMSE']:8.3f} â”‚ {station_mean['RMSE']:8.3f} â”‚ {station_median['RMSE']:8.3f} â”‚")
            report_lines.append(
                f"  â”‚ {'R':18} â”‚ {station_overall_r_str:>8} â”‚ {station_mean_r_str:>8} â”‚ {station_median_r_str:>8} â”‚")
            report_lines.append(f"  â””{'â”€' * 20}â”´{'â”€' * 10}â”´{'â”€' * 10}â”´{'â”€' * 10}â”˜")

            report_lines.append(f"  æŠ˜å æ•°: {results['station_cv']['folds']}")
            report_lines.append(f"  æ ·æœ¬æ•°: {station_overall['æ ·æœ¬æ•°']}")

            # æ·»åŠ æ ‡å‡†å·®ä¿¡æ¯
            report_lines.append(
                f"  å„æŠ˜å æ ‡å‡†å·®: MAEÂ±{station_std['MAE']:.3f}, RMSEÂ±{station_std['RMSE']:.3f}, RÂ±{station_std['R']:.3f}")

        # å¹´åº¦äº¤å‰éªŒè¯ç»“æœ
        if 'yearly_cv' in results:
            yearly_overall = results['yearly_cv']['overall']
            yearly_mean = results['yearly_cv']['mean']
            yearly_median = results['yearly_cv']['median']
            yearly_std = results['yearly_cv']['std']

            # å®‰å…¨å¤„ç†ç›¸å…³ç³»æ•°æ˜¾ç¤º
            yearly_overall_r = yearly_overall['R']
            yearly_mean_r = yearly_mean['R']
            yearly_median_r = yearly_median['R']

            yearly_overall_r_str = f"{yearly_overall_r:.3f}" if not np.isnan(yearly_overall_r) else "NaN"
            yearly_mean_r_str = f"{yearly_mean_r:.3f}" if not np.isnan(yearly_mean_r) else "NaN"
            yearly_median_r_str = f"{yearly_median_r:.3f}" if not np.isnan(yearly_median_r) else "NaN"

            report_lines.append(f"\nğŸ“… å¹´åº¦äº¤å‰éªŒè¯ (æ—¶é—´è¯„ä¼°):")
            report_lines.append(f"  â”Œ{'â”€' * 20}â”¬{'â”€' * 10}â”¬{'â”€' * 10}â”¬{'â”€' * 10}â”")
            report_lines.append(f"  â”‚ {'æŒ‡æ ‡':18} â”‚ {'èšåˆ':8} â”‚ {'å¹³å‡':8} â”‚ {'ä¸­ä½æ•°':8} â”‚")
            report_lines.append(f"  â”œ{'â”€' * 20}â”¼{'â”€' * 10}â”¼{'â”€' * 10}â”¼{'â”€' * 10}â”¤")
            report_lines.append(
                f"  â”‚ {'MAE (mm)':18} â”‚ {yearly_overall['MAE']:8.3f} â”‚ {yearly_mean['MAE']:8.3f} â”‚ {yearly_median['MAE']:8.3f} â”‚")
            report_lines.append(
                f"  â”‚ {'RMSE (mm)':18} â”‚ {yearly_overall['RMSE']:8.3f} â”‚ {yearly_mean['RMSE']:8.3f} â”‚ {yearly_median['RMSE']:8.3f} â”‚")
            report_lines.append(
                f"  â”‚ {'R':18} â”‚ {yearly_overall_r_str:>8} â”‚ {yearly_mean_r_str:>8} â”‚ {yearly_median_r_str:>8} â”‚")
            report_lines.append(f"  â””{'â”€' * 20}â”´{'â”€' * 10}â”´{'â”€' * 10}â”´{'â”€' * 10}â”˜")

            report_lines.append(f"  æŠ˜å æ•°: {results['yearly_cv']['folds']}")
            report_lines.append(f"  æ ·æœ¬æ•°: {yearly_overall['æ ·æœ¬æ•°']}")
            report_lines.append(
                f"  å„æŠ˜å æ ‡å‡†å·®: MAEÂ±{yearly_std['MAE']:.3f}, RMSEÂ±{yearly_std['RMSE']:.3f}, RÂ±{yearly_std['R']:.3f}")

        # æ¨¡å‹å‚æ•°
        report_lines.append(f"\nâš™ï¸ æ¨¡å‹å‚æ•°:")
        for key, value in self.params.items():
            if key in ['n_estimators', 'max_depth', 'min_child_weight']:
                report_lines.append(f"  {key}: {value}")
            elif key in ['learning_rate', 'subsample', 'colsample_bytree', 'reg_alpha', 'gamma']:
                report_lines.append(f"  {key}: {value}")

        # ç‰¹å¾é‡è¦æ€§
        if 'feature_importance' in results:
            top_features = results['feature_importance'].head(5)
            report_lines.append(f"\nğŸ” å‰5ä¸ªé‡è¦ç‰¹å¾:")
            for i, (_, row) in enumerate(top_features.iterrows(), 1):
                report_lines.append(f"  {i}. {row['feature']}: {row['importance']:.4f}")

        # æ€§èƒ½æ¯”è¾ƒå’Œå»ºè®®
        if 'station_cv' in results and 'yearly_cv' in results:
            # ä½¿ç”¨èšåˆå€¼è¿›è¡Œæ¯”è¾ƒ
            station_r = results['station_cv']['overall']['R']
            yearly_r = results['yearly_cv']['overall']['R']

            # åªæœ‰å½“ä¸¤ä¸ªRå€¼éƒ½ä¸æ˜¯NaNæ—¶æ‰è¿›è¡Œæ¯”è¾ƒ
            if not np.isnan(station_r) and not np.isnan(yearly_r):
                report_lines.append(f"\nğŸ’¡ æ€§èƒ½åˆ†æå’Œå»ºè®®:")
                report_lines.append(f"  â€¢ èšåˆæ€§èƒ½æ¯”è¾ƒ:")
                if station_r > yearly_r:
                    report_lines.append(f"    ç«™ç‚¹CVä¼˜äºå¹´åº¦CV (R: {station_r:.3f} > {yearly_r:.3f})")
                else:
                    report_lines.append(f"    å¹´åº¦CVä¼˜äºç«™ç‚¹CV (R: {yearly_r:.3f} > {station_r:.3f})")
            else:
                report_lines.append(f"\nğŸ’¡ æ€§èƒ½åˆ†æ:")
                if np.isnan(station_r) and np.isnan(yearly_r):
                    report_lines.append(f"  â€¢ ä¸¤ç§æ–¹æ³•çš„ç›¸å…³ç³»æ•°å‡ä¸ºNaNï¼Œæ— æ³•æ¯”è¾ƒRå€¼")
                elif np.isnan(station_r):
                    report_lines.append(f"  â€¢ ç«™ç‚¹CVç›¸å…³ç³»æ•°ä¸ºNaNï¼Œå¹´åº¦CV R={yearly_r:.3f}")
                else:
                    report_lines.append(f"  â€¢ å¹´åº¦CVç›¸å…³ç³»æ•°ä¸ºNaNï¼Œç«™ç‚¹CV R={station_r:.3f}")

            # æ¯”è¾ƒç¨³å®šæ€§ï¼ˆæ ‡å‡†å·®è¶Šå°è¶Šç¨³å®šï¼‰
            station_mae_std = results['station_cv']['std']['MAE']
            yearly_mae_std = results['yearly_cv']['std']['MAE']

            if station_mae_std < yearly_mae_std:
                report_lines.append(f"  â€¢ ç¨³å®šæ€§æ¯”è¾ƒ: ç«™ç‚¹CVæ›´ç¨³å®š (MAEæ ‡å‡†å·®: {station_mae_std:.3f} < {yearly_mae_std:.3f})")
            else:
                report_lines.append(f"  â€¢ ç¨³å®šæ€§æ¯”è¾ƒ: å¹´åº¦CVæ›´ç¨³å®š (MAEæ ‡å‡†å·®: {yearly_mae_std:.3f} < {station_mae_std:.3f})")

            # æœ€ç»ˆå»ºè®®
            if station_r > yearly_r and station_mae_std < yearly_mae_std:
                report_lines.append(f"  âœ… å¼ºçƒˆæ¨èä½¿ç”¨ç«™ç‚¹äº¤å‰éªŒè¯è¿›è¡Œç©ºé—´è¯„ä¼°")
            elif yearly_r > station_r and yearly_mae_std < station_mae_std:
                report_lines.append(f"  âœ… å¼ºçƒˆæ¨èä½¿ç”¨å¹´åº¦äº¤å‰éªŒè¯è¿›è¡Œæ—¶é—´è¯„ä¼°")
            else:
                report_lines.append(f"  âš ï¸  ä¸¤ç§æ–¹æ³•å„æœ‰ä¼˜åŠ¿ï¼Œè¯·æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯é€‰æ‹©")

            # æ·»åŠ SWEäº§å“å¯¹æ¯”è¯´æ˜
            report_lines.append(f"\nğŸ“ˆ SWEäº§å“å¯¹æ¯”:")
            report_lines.append(f"  å·²ç”ŸæˆCSWEã€ERA5_SWEã€Glsnowã€GLDASç­‰äº§å“ä¸å®æµ‹å€¼çš„å¯¹æ¯”æ•£ç‚¹å›¾")
            report_lines.append(f"  è¯¦è§è¾“å‡ºç›®å½•ä¸­çš„ 'swe_products_comparison.png'")

        report_lines.append("\n" + "=" * 80)

        return "\n".join(report_lines)

    def _analyze_landuse_features(self, df):
        """åˆ†ælanduseç‰¹å¾çš„ç›¸å…³æ€§å’Œé‡è¦æ€§"""
        landuse_vec_cols = [col for col in df.columns if col.startswith('landuse_vec_')]
        landuse_stat_cols = [col for col in df.columns if col.startswith('landuse_') and col not in landuse_vec_cols]

        if landuse_vec_cols:
            self.logger.info(f"landuseå‘é‡ç‰¹å¾åˆ†æ:")
            self.logger.info(f"  å‘é‡å…ƒç´ ç‰¹å¾: {len(landuse_vec_cols)} ä¸ª")
            self.logger.info(f"  ç»Ÿè®¡ç‰¹å¾: {len(landuse_stat_cols)} ä¸ª")

            # è®¡ç®—landuseç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§
            if self.target_column in df.columns:
                correlations = {}
                for col in landuse_vec_cols + landuse_stat_cols:
                    corr = df[col].corr(df[self.target_column])
                    correlations[col] = corr

                # æŒ‰ç›¸å…³æ€§ç»å¯¹å€¼æ’åº
                sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)
                self.logger.info("landuseç‰¹å¾ä¸SWEç›¸å…³æ€§ (å‰5ä¸ª):")
                for col, corr in sorted_correlations[:5]:
                    self.logger.info(f"  {col}: {corr:.3f}")
    # ä¾¿æ·ä½¿ç”¨å‡½æ•°


def train_swe_model(data_df, output_dir=None, params=None):
    """ä¾¿æ·å‡½æ•°ï¼šè®­ç»ƒSWEæ¨¡å‹

        Args:
            data_df (pd.DataFrame): åŒ…å«ç‰¹å¾å’ŒSWEçš„æ•°æ®
            output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„
            params (dict, optional): XGBoostå‚æ•°

        Returns:
            dict: åŒ…å«æ‰€æœ‰è®­ç»ƒç»“æœçš„å­—å…¸

        Example:
            >>> results = train_swe_model(df, output_dir='./results')
            >>> print(f"ç«™ç‚¹CV R: {results['station_cv']['overall']['R']:.3f}")
        """
    trainer = SWEXGBoostTrainer(params=params)
    return trainer.run_complete_analysis(data_df, output_dir)
