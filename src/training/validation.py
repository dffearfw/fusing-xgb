import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import logging
import os
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('gnnw_verification.log', encoding='utf-8')
    ]
)
logger = logging.getLogger("GNNW_Verification")


# æ¨¡æ‹ŸGNNW_XGBoostTrainerç±»çš„å…³é”®æ–¹æ³•
class MockGNNW_XGBoostTrainer:
    """æ¨¡æ‹Ÿçš„GNNW-XGBoostè®­ç»ƒå™¨ç”¨äºéªŒè¯"""

    def __init__(self):
        self.logger = logger
        self.use_gnnwr = True

        # GNNWRç‰¹å¾åˆ—
        self.gnnwr_x_columns = ['aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation',
                                'std_slope',
                                'std_eastness', 'std_tpi', 'std_curvature1', 'std_curvature2', 'std_high', 'std_aspect',
                                'glsnow', 'cswe', 'snow_depth_snow_depth', 'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'doy',
                                'gldas',
                                'year', 'month', 'scp_start', 'scp_end', 'd1', 'd2', 'X', 'Y', 'Z', 'da', 'db', 'dc',
                                'dd']

        # æ¨¡æ‹Ÿç‰¹å¾åˆ—
        self.feature_columns = []

    def preprocess_data(self, df):
        """æ¨¡æ‹Ÿæ•°æ®é¢„å¤„ç†"""
        logger.info("æ¨¡æ‹Ÿæ•°æ®é¢„å¤„ç†...")

        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        if 'station_id' not in df.columns:
            df['station_id'] = np.arange(len(df))
        if 'swe' not in df.columns:
            df['swe'] = np.random.normal(50, 20, len(df))
        if 'date' not in df.columns:
            df['date'] = pd.date_range(start='2000-01-01', periods=len(df), freq='D')

        # ç¡®å®šç‰¹å¾åˆ—
        exclude_columns = ['station_id', 'date', 'swe']
        self.feature_columns = [col for col in df.columns if col not in exclude_columns]

        logger.info(f"ç‰¹å¾åˆ—æ•°: {len(self.feature_columns)}")
        logger.info(f"å‰10ä¸ªç‰¹å¾: {self.feature_columns[:10]}")

        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
        X = df[self.feature_columns].values
        y = df['swe'].values

        # åˆ†ç»„ä¿¡æ¯
        df['year'] = pd.to_datetime(df['date']).dt.year
        station_groups = df['station_id'].values
        year_groups = df['year'].values

        # GNNWRæ•°æ®
        gnnwr_data = df.copy()

        # ç¡®ä¿GNNWRéœ€è¦çš„åˆ—éƒ½å­˜åœ¨
        for col in self.gnnwr_x_columns:
            if col not in gnnwr_data.columns:
                gnnwr_data[col] = np.random.normal(0, 1, len(gnnwr_data))

        return X, y, station_groups, year_groups, gnnwr_data

    def _apply_gnnwr_weights(self, X, weights, feature_columns, gnnwr_x_columns):
        """åº”ç”¨GNNWRæƒé‡åˆ°ç‰¹å¾çŸ©é˜µ"""
        if weights is None:
            logger.warning("æƒé‡çŸ©é˜µä¸ºNoneï¼Œè¿”å›åŸå§‹ç‰¹å¾")
            return X

        logger.info(f"åº”ç”¨æƒé‡: Xå½¢çŠ¶={X.shape}, æƒé‡å½¢çŠ¶={weights.shape}")
        logger.info(f"ç‰¹å¾åˆ—æ•°={len(feature_columns)}, GNNWRç‰¹å¾åˆ—æ•°={len(gnnwr_x_columns)}")

        # åˆ›å»ºç‰¹å¾æ˜ å°„
        feature_to_gnnwr = {}
        for i, feat in enumerate(feature_columns):
            if feat in gnnwr_x_columns:
                gnnwr_idx = gnnwr_x_columns.index(feat)
                feature_to_gnnwr[i] = gnnwr_idx

        logger.info(f"åŒ¹é…çš„ç‰¹å¾æ•°: {len(feature_to_gnnwr)}/{len(feature_columns)}")

        if len(feature_to_gnnwr) == 0:
            logger.warning("æ²¡æœ‰åŒ¹é…çš„ç‰¹å¾ï¼Œæ— æ³•åº”ç”¨æƒé‡")
            return X

        # åº”ç”¨æƒé‡
        X_weighted = X.copy()
        for feat_idx, gnnwr_idx in feature_to_gnnwr.items():
            if gnnwr_idx < weights.shape[1]:
                X_weighted[:, feat_idx] = X[:, feat_idx] * weights[:, gnnwr_idx]

        return X_weighted


def load_sample_data(n_samples=1000, n_features=50):
    """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
    logger.info(f"ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®: {n_samples}æ ·æœ¬, {n_features}ç‰¹å¾")

    # åˆ›å»ºç‰¹å¾åç§°
    feature_names = []
    gnnwr_feature_names = [
        'aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation', 'std_slope',
        'std_eastness', 'std_tpi', 'std_curvature1', 'std_curvature2', 'std_high', 'std_aspect',
        'glsnow', 'cswe', 'snow_depth_snow_depth', 'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'doy', 'gldas',
        'year', 'month', 'scp_start', 'scp_end', 'd1', 'd2', 'X', 'Y', 'Z', 'da', 'db', 'dc', 'dd'
    ]

    # æ·»åŠ é¢å¤–çš„éGNNWRç‰¹å¾
    other_features = ['landuse_' + str(i) for i in range(1, n_features - len(gnnwr_feature_names) + 1)]

    feature_names = gnnwr_feature_names + other_features

    # ç”Ÿæˆæ•°æ®
    np.random.seed(42)
    data = {}

    # ç”Ÿæˆç‰¹å¾å€¼
    for i, feature in enumerate(feature_names):
        if feature in ['longitude', 'latitude']:
            data[feature] = np.random.uniform(-180, 180, n_samples) if feature == 'longitude' else np.random.uniform(
                -90, 90, n_samples)
        elif feature in ['elevation', 'X', 'Y', 'Z']:
            data[feature] = np.random.normal(0, 100, n_samples)
        else:
            data[feature] = np.random.normal(0, 1, n_samples)

    # æ·»åŠ å¿…è¦åˆ—
    data['station_id'] = np.random.choice(range(1, 101), n_samples)
    data['date'] = pd.date_range(start='2000-01-01', periods=n_samples, freq='D')
    data['swe'] = 50 + np.random.normal(0, 20, n_samples)  # ç›®æ ‡å˜é‡

    df = pd.DataFrame(data)
    logger.info(f"æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºå®Œæˆ: {len(df)}è¡Œ, {len(df.columns)}åˆ—")

    return df


def verify_weight_application(df, n_samples=100, random_seed=42):
    """ä¸¥æ ¼éªŒè¯æƒé‡æ˜¯å¦è¢«æ­£ç¡®åº”ç”¨åˆ°ç‰¹å¾ä¸Š - ä½¿ç”¨éšæœºæŠ½æ ·"""
    print("=" * 80)
    print("ğŸ§ª GNNW-XGBoostæƒé‡åº”ç”¨éªŒè¯æµ‹è¯• (éšæœºæŠ½æ ·)")
    print("=" * 80)

    # è®¾ç½®éšæœºç§å­
    np.random.seed(random_seed)

    # åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹
    trainer = MockGNNW_XGBoostTrainer()

    # é¢„å¤„ç†æ•°æ®
    print("\n1. æ•°æ®é¢„å¤„ç†...")
    X, y, station_groups, year_groups, gnnwr_data = trainer.preprocess_data(df)

    total_samples = len(X)
    print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"  ç‰¹å¾çŸ©é˜µå½¢çŠ¶: X={X.shape}, y={y.shape}")
    print(f"  GNNWRæ•°æ®å½¢çŠ¶: {gnnwr_data.shape}")
    print(f"  XGBoostç‰¹å¾åˆ—æ•°: {len(trainer.feature_columns)}")
    print(f"  GNNWRç‰¹å¾åˆ—æ•°: {len(trainer.gnnwr_x_columns)}")

    # éšæœºæŠ½æ ·
    if n_samples > total_samples:
        n_samples = total_samples
        print(f"  è­¦å‘Š: æ ·æœ¬æ•°è¶…è¿‡æ€»æ ·æœ¬æ•°ï¼Œä½¿ç”¨æ‰€æœ‰{total_samples}ä¸ªæ ·æœ¬")

    # éšæœºé€‰æ‹©ç´¢å¼•
    random_indices = np.random.choice(total_samples, n_samples, replace=False)
    random_indices = np.sort(random_indices)  # æ’åºä»¥ä¾¿é˜…è¯»

    print(f"\n2. éšæœºæŠ½æ ·: ä»{total_samples}ä¸ªæ ·æœ¬ä¸­éšæœºæŠ½å–{n_samples}ä¸ªæ ·æœ¬")
    print(f"   æŠ½æ ·ç´¢å¼•: {random_indices[:5]}...{random_indices[-5:]}")

    # è·å–æŠ½æ ·æ•°æ®
    X_test = X[random_indices]
    y_test = y[random_indices]
    gnnwr_data_test = gnnwr_data.iloc[random_indices].copy()

    # ç”Ÿæˆæ¨¡æ‹Ÿæƒé‡çŸ©é˜µï¼ˆæ¨¡æ‹ŸGNNWRè¾“å‡ºï¼‰
    print("\n3. ç”Ÿæˆæ¨¡æ‹Ÿæƒé‡çŸ©é˜µ...")

    # æ¨¡æ‹ŸGNNWRè¾“å‡ºæƒé‡çŸ©é˜µ (n_samples, n_gnnwr_features)
    # è¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿæƒé‡ä¸æ˜¯1.0ï¼Œä»¥æµ‹è¯•æƒé‡åº”ç”¨æ˜¯å¦æœ‰æ•ˆ
    n_gnnwr_features = len(trainer.gnnwr_x_columns)
    train_weights = np.random.normal(1.0, 0.3, (len(X_test), n_gnnwr_features))
    train_weights = np.clip(train_weights, 0.1, 2.0)  # é™åˆ¶æƒé‡èŒƒå›´

    print(f"   æ¨¡æ‹Ÿæƒé‡çŸ©é˜µå½¢çŠ¶: {train_weights.shape}")
    print(f"   æƒé‡ç»Ÿè®¡:")
    print(f"     å‡å€¼: {train_weights.mean():.6f}")
    print(f"     æ ‡å‡†å·®: {train_weights.std():.6f}")
    print(f"     èŒƒå›´: [{train_weights.min():.6f}, {train_weights.max():.6f}]")

    # è®¡ç®—æƒé‡ä¸1çš„å·®å¼‚
    weight_distance_from_one = np.abs(train_weights - 1).mean()
    print(f"   æƒé‡ä¸1çš„å¹³å‡è·ç¦»: {weight_distance_from_one:.6f}")

    # ç»Ÿè®¡æœ‰å¤šå°‘æƒé‡æ˜¾è‘—ä¸åŒäº1
    significant_weights = np.sum(np.abs(train_weights - 1) > 0.01) / train_weights.size
    print(f"   ä¸1å·®å¼‚å¤§äº0.01çš„æƒé‡æ¯”ä¾‹: {significant_weights:.2%}")

    if weight_distance_from_one < 0.01:
        print("   âš ï¸ è­¦å‘Š: æƒé‡éå¸¸æ¥è¿‘1ï¼ŒåŠ æƒå¯èƒ½æ²¡æœ‰æ•ˆæœ")
    else:
        print(f"   âœ… æƒé‡ä¸1æœ‰æ˜¾è‘—å·®å¼‚ï¼ŒåŠ æƒä¼šæœ‰æ•ˆæœ")

    # 5. åº”ç”¨æƒé‡
    print("\n5. åº”ç”¨æƒé‡åˆ°ç‰¹å¾...")

    # åŸå§‹ç‰¹å¾
    original_features = X_test.copy()

    # åº”ç”¨æƒé‡
    weighted_features = trainer._apply_gnnwr_weights(
        original_features, train_weights,
        trainer.feature_columns, trainer.gnnwr_x_columns
    )

    # æ£€æŸ¥æ˜¯å¦åº”ç”¨æˆåŠŸ
    if np.array_equal(original_features, weighted_features):
        print("   âš ï¸ è­¦å‘Š: åŠ æƒåç‰¹å¾ä¸åŸå§‹ç‰¹å¾å®Œå…¨ç›¸åŒï¼")
    else:
        print(f"   âœ… åŠ æƒåç‰¹å¾ä¸åŸå§‹ç‰¹å¾ä¸åŒ")

    # 6. è¯¦ç»†å¯¹æ¯”åŸå§‹ç‰¹å¾å’ŒåŠ æƒç‰¹å¾
    print("\n6. ç‰¹å¾å˜åŒ–åˆ†æ:")

    # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å˜åŒ–
    changes = weighted_features - original_features
    abs_changes = np.abs(changes)
    relative_changes = abs_changes / (np.abs(original_features) + 1e-10)  # é¿å…é™¤ä»¥0

    # æŒ‰ç‰¹å¾ç»Ÿè®¡å˜åŒ–
    feature_changes = []
    for i in range(original_features.shape[1]):
        feat_name = trainer.feature_columns[i] if i < len(trainer.feature_columns) else f"Feature_{i}"

        # æ£€æŸ¥è¿™ä¸ªç‰¹å¾æ˜¯å¦åœ¨GNNWRç‰¹å¾ä¸­
        is_gnnwr_feature = feat_name in trainer.gnnwr_x_columns

        # åªè®°å½•æœ‰å˜åŒ–çš„ç‰¹å¾
        feat_change_mean = changes[:, i].mean()
        feat_change_std = changes[:, i].std()
        feat_abs_change_mean = abs_changes[:, i].mean()
        feat_rel_change_mean = relative_changes[:, i].mean()

        # æ£€æŸ¥è¿™ä¸ªç‰¹å¾æ˜¯å¦æœ‰æ˜¾è‘—å˜åŒ–
        has_significant_change = feat_abs_change_mean > 0.001

        feature_changes.append({
            'feature': feat_name,
            'is_gnnwr': is_gnnwr_feature,
            'change_mean': feat_change_mean,
            'change_std': feat_change_std,
            'abs_change_mean': feat_abs_change_mean,
            'rel_change_mean': feat_rel_change_mean,
            'has_significant_change': has_significant_change
        })

    # åˆ›å»ºDataFrameæ˜¾ç¤ºç»“æœ
    changes_df = pd.DataFrame(feature_changes)

    # ç»Ÿè®¡GNNWRç‰¹å¾å’ŒéGNNWRç‰¹å¾
    gnnwr_features_df = changes_df[changes_df['is_gnnwr']]
    non_gnnwr_features_df = changes_df[~changes_df['is_gnnwr']]

    print(f"\n  GNNWRç‰¹å¾ç»Ÿè®¡:")
    print(f"    æ€»æ•°: {len(gnnwr_features_df)}")
    if len(gnnwr_features_df) > 0:
        gnnwr_mean_change = gnnwr_features_df['abs_change_mean'].mean()
        gnnwr_significant = gnnwr_features_df['has_significant_change'].sum()
        print(f"    å¹³å‡ç»å¯¹å˜åŒ–: {gnnwr_mean_change:.6f}")
        print(f"    æ˜¾è‘—å˜åŒ–ç‰¹å¾æ•°: {gnnwr_significant}/{len(gnnwr_features_df)}")

    print(f"\n  éGNNWRç‰¹å¾ç»Ÿè®¡:")
    print(f"    æ€»æ•°: {len(non_gnnwr_features_df)}")
    if len(non_gnnwr_features_df) > 0:
        non_gnnwr_mean_change = non_gnnwr_features_df['abs_change_mean'].mean()
        non_gnnwr_significant = non_gnnwr_features_df['has_significant_change'].sum()
        print(f"    å¹³å‡ç»å¯¹å˜åŒ–: {non_gnnwr_mean_change:.6f}")
        print(f"    æ˜¾è‘—å˜åŒ–ç‰¹å¾æ•°: {non_gnnwr_significant}/{len(non_gnnwr_features_df)}")

    # æ˜¾ç¤ºå˜åŒ–æœ€å¤§çš„ç‰¹å¾
    print("\n  å˜åŒ–æœ€å¤§çš„10ä¸ªç‰¹å¾:")
    print("  " + "-" * 100)

    sorted_changes = changes_df.sort_values('abs_change_mean', ascending=False)

    for idx, row in sorted_changes.head(10).iterrows():
        change_symbol = "âœ…" if row['has_significant_change'] else "âš ï¸"
        gnnwr_symbol = "G" if row['is_gnnwr'] else "N"
        print(f"    {change_symbol}[{gnnwr_symbol}] {row['feature']:<30} å¹³å‡å˜åŒ–: {row['change_mean']:+.6f}, "
              f"ç»å¯¹å˜åŒ–: {row['abs_change_mean']:.6f}, ç›¸å¯¹å˜åŒ–: {row['rel_change_mean']:.2%}")

    # 7. å¯è§†åŒ–å˜åŒ–
    print("\n7. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    visualize_feature_changes(original_features, weighted_features, changes_df)

    # 8. ç»Ÿè®¡æ€»ç»“
    print("\n8. éªŒè¯ç»“æœæ€»ç»“:")

    total_features = original_features.shape[1]
    gnnwr_features_count = sum([1 for f in trainer.feature_columns if f in trainer.gnnwr_x_columns])
    non_gnnwr_features_count = total_features - gnnwr_features_count

    print(f"   æ€»ç‰¹å¾æ•°: {total_features}")
    print(f"   GNNWRç‰¹å¾æ•°: {gnnwr_features_count} (åº”è¯¥è¢«åŠ æƒ)")
    print(f"   éGNNWRç‰¹å¾æ•°: {non_gnnwr_features_count} (ä¿æŒä¸å˜)")

    # æ€»ä½“å˜åŒ–ç»Ÿè®¡
    mean_abs_change = abs_changes.mean()
    significant_changes = (abs_changes > 0.001).sum() / abs_changes.size

    print(f"\n   æ‰€æœ‰ç‰¹å¾å¹³å‡ç»å¯¹å˜åŒ–: {mean_abs_change:.6f}")
    print(f"   æ˜¾è‘—å˜åŒ–(>0.001)çš„æ¯”ä¾‹: {significant_changes:.2%}")

    # éªŒè¯ç»“è®º
    verification_passed = False
    if len(gnnwr_features_df) > 0 and gnnwr_features_df['abs_change_mean'].mean() > 0.001:
        verification_passed = True
        print(f"\n   âœ… éªŒè¯é€šè¿‡: GNNWRç‰¹å¾è¢«æˆåŠŸåŠ æƒ")
        print(f"      å¹³å‡å˜åŒ–: {gnnwr_features_df['abs_change_mean'].mean():.6f}")
        print(f"      æƒé‡ä¸1çš„å¹³å‡è·ç¦»: {weight_distance_from_one:.6f}")
    else:
        print(f"\n   âŒ éªŒè¯å¤±è´¥: GNNWRç‰¹å¾æ²¡æœ‰è¢«æ­£ç¡®åŠ æƒ")
        print(f"      å¯èƒ½çš„åŸå› :")
        print(f"      1. ç‰¹å¾æ˜ å°„é”™è¯¯")
        print(f"      2. æƒé‡çŸ©é˜µåº”ç”¨é€»è¾‘é”™è¯¯")
        print(f"      3. GNNWRç‰¹å¾ä¸XGBoostç‰¹å¾ä¸åŒ¹é…")

    return {
        'original_features': original_features,
        'weighted_features': weighted_features,
        'weights': train_weights,
        'feature_changes': changes_df,
        'summary': {
            'verification_passed': verification_passed,
            'mean_abs_change': mean_abs_change,
            'significant_changes': significant_changes,
            'gnnwr_features_count': gnnwr_features_count,
            'total_features': total_features,
            'weight_std': train_weights.std(),
            'weight_distance_from_one': weight_distance_from_one
        }
    }


def visualize_feature_changes(original, weighted, changes_df):
    """å¯è§†åŒ–ç‰¹å¾å˜åŒ–"""

    # è®¾ç½®å›¾å½¢æ ·å¼
    plt.style.use('seaborn-v0_8-whitegrid')

    # 1. æƒé‡åˆ†å¸ƒå›¾
    print("  ç”Ÿæˆæƒé‡åˆ†å¸ƒå›¾...")

    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))

    # æƒé‡åˆ†å¸ƒç›´æ–¹å›¾
    if 'weights' in globals():
        weights = globals()['weights']
        ax1.hist(weights.flatten(), bins=50, alpha=0.7, edgecolor='black')
        ax1.axvline(x=1.0, color='red', linestyle='--', linewidth=2, label='æƒé‡=1.0')
        ax1.set_xlabel('æƒé‡å€¼')
        ax1.set_ylabel('é¢‘æ•°')
        ax1.set_title('GNNWRæƒé‡åˆ†å¸ƒ')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

    # ç‰¹å¾å˜åŒ–åˆ†å¸ƒ
    if len(changes_df) > 0:
        gnnwr_changes = changes_df[changes_df['is_gnnwr']]['abs_change_mean']
        non_gnnwr_changes = changes_df[~changes_df['is_gnnwr']]['abs_change_mean']

        ax2.hist([gnnwr_changes, non_gnnwr_changes],
                 bins=20, alpha=0.7, edgecolor='black',
                 label=['GNNWRç‰¹å¾', 'éGNNWRç‰¹å¾'])
        ax2.axvline(x=0.001, color='red', linestyle='--', linewidth=2, label='æ˜¾è‘—å˜åŒ–é˜ˆå€¼(0.001)')
        ax2.set_xlabel('ç‰¹å¾ç»å¯¹å˜åŒ–å‡å€¼')
        ax2.set_ylabel('ç‰¹å¾æ•°é‡')
        ax2.set_title('ç‰¹å¾å˜åŒ–åˆ†å¸ƒ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

    # 3. ç‰¹å¾å˜åŒ–å¯¹æ¯”å›¾ï¼ˆé€‰æ‹©3ä¸ªå˜åŒ–æœ€å¤§çš„GNNWRç‰¹å¾ï¼‰
    gnnwr_changes_sorted = changes_df[changes_df['is_gnnwr']].sort_values('abs_change_mean', ascending=False)

    if len(gnnwr_changes_sorted) >= 3:
        top_features = gnnwr_changes_sorted.head(3)

        for idx, (_, row) in enumerate(top_features.iterrows()):
            # æ‰¾åˆ°ç‰¹å¾ç´¢å¼•
            feat_name = row['feature']
            feat_idx = list(changes_df['feature']).index(feat_name)

            # ç»˜åˆ¶æ•£ç‚¹å›¾
            ax3.scatter(original[:, feat_idx], weighted[:, feat_idx],
                        alpha=0.6, s=20, label=feat_name)

        # æ·»åŠ å¯¹è§’çº¿
        min_val = min(original.min(), weighted.min())
        max_val = max(original.max(), weighted.max())
        ax3.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, linewidth=1)

        ax3.set_xlabel('åŸå§‹ç‰¹å¾å€¼')
        ax3.set_ylabel('åŠ æƒåç‰¹å¾å€¼')
        ax3.set_title('GNNWRç‰¹å¾åŠ æƒå‰åå¯¹æ¯”')
        ax3.legend(fontsize=9)
        ax3.grid(True, alpha=0.3)

    # 4. ç‰¹å¾å˜åŒ–çƒ­å›¾ï¼ˆå‰20ä¸ªç‰¹å¾ï¼‰
    if len(changes_df) > 0:
        # é€‰æ‹©å‰20ä¸ªç‰¹å¾
        n_features_show = min(20, len(changes_df))
        top_changes = changes_df.head(n_features_show)

        # å‡†å¤‡çƒ­å›¾æ•°æ®
        heatmap_data = []
        feature_labels = []

        for _, row in top_changes.iterrows():
            feat_idx = list(changes_df['feature']).index(row['feature'])
            heatmap_data.append(abs_changes[:, feat_idx].mean())
            feature_labels.append(row['feature'])

        # åˆ›å»ºçƒ­å›¾
        y_pos = np.arange(len(heatmap_data))
        colors = ['red' if changes_df.iloc[i]['is_gnnwr'] else 'blue' for i in range(len(heatmap_data))]

        ax4.barh(y_pos, heatmap_data, color=colors, alpha=0.7)
        ax4.set_yticks(y_pos)
        ax4.set_yticklabels(feature_labels, fontsize=8)
        ax4.set_xlabel('å¹³å‡ç»å¯¹å˜åŒ–')
        ax4.set_title('ç‰¹å¾å¹³å‡å˜åŒ–æ’å (çº¢=GNNWR, è“=éGNNWR)')
        ax4.grid(True, alpha=0.3, axis='x')

    plt.tight_layout()
    plt.savefig('weight_verification_summary.png', dpi=150, bbox_inches='tight')
    plt.show()

    # 5. è¯¦ç»†çš„ç‰¹å¾å˜åŒ–å¯¹æ¯”å›¾
    print("  ç”Ÿæˆè¯¦ç»†ç‰¹å¾å˜åŒ–å›¾...")

    if len(gnnwr_changes_sorted) >= 6:
        fig, axes = plt.subplots(2, 3, figsize=(15, 8))
        axes = axes.flatten()

        top_features = gnnwr_changes_sorted.head(6)

        for idx, (_, row) in enumerate(top_features.iterrows()):
            if idx >= len(axes):
                break

            ax = axes[idx]
            feat_name = row['feature']
            feat_idx = list(changes_df['feature']).index(feat_name)

            # ç»˜åˆ¶æ•£ç‚¹å›¾
            scatter = ax.scatter(original[:, feat_idx], weighted[:, feat_idx],
                                 alpha=0.7, s=15, c='blue', edgecolors='none')

            # æ·»åŠ å¯¹è§’çº¿
            min_val = min(original[:, feat_idx].min(), weighted[:, feat_idx].min())
            max_val = max(original[:, feat_idx].max(), weighted[:, feat_idx].max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, linewidth=1.5)

            ax.set_xlabel(f'åŸå§‹ {feat_name}')
            ax.set_ylabel(f'åŠ æƒ {feat_name}')
            ax.set_title(f'{feat_name}\nå˜åŒ–: {row["abs_change_mean"]:.4f}')
            ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('gnnwr_features_comparison.png', dpi=150, bbox_inches='tight')
        plt.show()


def run_complete_verification():
    """è¿è¡Œå®Œæ•´çš„éªŒè¯æµç¨‹"""
    print("=" * 80)
    print("ğŸ”¬ GNNW-XGBoostæƒé‡åº”ç”¨å®Œæ•´éªŒè¯")
    print("=" * 80)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    print(f"éªŒè¯æ—¶é—´: {timestamp}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = f"gnnw_verification_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)

    try:
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        print("\næ­¥éª¤ 1: ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
        df = load_sample_data(n_samples=1000, n_features=50)

        # è¿è¡ŒéªŒè¯
        print("\næ­¥éª¤ 2: è¿è¡Œæƒé‡åº”ç”¨éªŒè¯")
        results = verify_weight_application(df, n_samples=100, random_seed=42)

        if results:
            print("\n" + "=" * 80)
            print("âœ… éªŒè¯å®Œæˆï¼")

            summary = results['summary']
            if summary['verification_passed']:
                print("ğŸ¯ ç»“è®º: GNNWRæƒé‡æˆåŠŸåº”ç”¨åˆ°ç‰¹å¾ä¸Š")
                print(f"   GNNWRç‰¹å¾å¹³å‡å˜åŒ–: {summary['mean_abs_change']:.6f}")
                print(f"   æƒé‡çŸ©é˜µæ ‡å‡†å·®: {summary['weight_std']:.6f}")
                print(f"   æ˜¾è‘—å˜åŒ–æ¯”ä¾‹: {summary['significant_changes']:.2%}")
            else:
                print("âš ï¸  ç»“è®º: GNNWRæƒé‡åº”ç”¨å¯èƒ½å­˜åœ¨é—®é¢˜")
                print("   å»ºè®®æ£€æŸ¥:")
                print("   1. ç‰¹å¾æ˜ å°„æ˜¯å¦æ­£ç¡®")
                print("   2. æƒé‡çŸ©é˜µæ˜¯å¦æ­£ç¡®ç”Ÿæˆ")
                print("   3. åº”ç”¨æƒé‡çš„ä»£ç é€»è¾‘")

            # ä¿å­˜è¯¦ç»†ç»“æœ
            print(f"\nğŸ“ ä¿å­˜éªŒè¯ç»“æœåˆ°: {output_dir}/")

            # ä¿å­˜ç‰¹å¾å˜åŒ–æ•°æ®
            results['feature_changes'].to_csv(f'{output_dir}/feature_changes.csv', index=False)

            # ä¿å­˜æƒé‡çŸ©é˜µ
            np.save(f'{output_dir}/weights.npy', results['weights'])

            # ä¿å­˜åŸå§‹å’ŒåŠ æƒç‰¹å¾
            np.save(f'{output_dir}/original_features.npy', results['original_features'])
            np.save(f'{output_dir}/weighted_features.npy', results['weighted_features'])

            # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
            with open(f'{output_dir}/verification_summary.txt', 'w') as f:
                f.write("=" * 80 + "\n")
                f.write("GNNW-XGBoostæƒé‡åº”ç”¨éªŒè¯æŠ¥å‘Š\n")
                f.write("=" * 80 + "\n\n")
                f.write(f"éªŒè¯æ—¶é—´: {timestamp}\n\n")

                f.write("éªŒè¯ç»“æœ:\n")
                f.write(f"  éªŒè¯é€šè¿‡: {'æ˜¯' if summary['verification_passed'] else 'å¦'}\n")
                f.write(f"  æ€»ç‰¹å¾æ•°: {summary['total_features']}\n")
                f.write(f"  GNNWRç‰¹å¾æ•°: {summary['gnnwr_features_count']}\n")
                f.write(f"  æ‰€æœ‰ç‰¹å¾å¹³å‡ç»å¯¹å˜åŒ–: {summary['mean_abs_change']:.6f}\n")
                f.write(f"  æ˜¾è‘—å˜åŒ–æ¯”ä¾‹: {summary['significant_changes']:.2%}\n")
                f.write(f"  æƒé‡çŸ©é˜µæ ‡å‡†å·®: {summary['weight_std']:.6f}\n")
                f.write(f"  æƒé‡ä¸1çš„å¹³å‡è·ç¦»: {summary['weight_distance_from_one']:.6f}\n\n")

                # æ·»åŠ ç‰¹å¾å˜åŒ–è¯¦æƒ…
                f.write("ç‰¹å¾å˜åŒ–è¯¦æƒ…:\n")
                f.write("-" * 100 + "\n")
                changes_df = results['feature_changes']
                gnnwr_changes = changes_df[changes_df['is_gnnwr']].sort_values('abs_change_mean', ascending=False)

                if len(gnnwr_changes) > 0:
                    f.write("GNNWRç‰¹å¾å˜åŒ–æ’å:\n")
                    for idx, row in gnnwr_changes.head(10).iterrows():
                        f.write(f"  {row['feature']:<30}: å¹³å‡å˜åŒ–={row['abs_change_mean']:.6f}, "
                                f"ç›¸å¯¹å˜åŒ–={row['rel_change_mean']:.2%}\n")

        print(f"\nğŸ“Š å¯è§†åŒ–å›¾è¡¨:")
        print(f"  - weight_verification_summary.png")
        print(f"  - gnnwr_features_comparison.png")

        return results

    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None


def test_real_data_verification():
    """æµ‹è¯•çœŸå®æ•°æ®éªŒè¯"""
    print("=" * 80)
    print("ğŸ”¬ çœŸå®æ•°æ®GNNW-XGBoostéªŒè¯")
    print("=" * 80)

    try:
        # å°è¯•åŠ è½½çœŸå®æ•°æ®
        print("å°è¯•åŠ è½½çœŸå®æ•°æ®...")
        df = pd.read_excel('lu_onehot.xlsx.xlsx')
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(df)}è¡Œ, {len(df.columns)}åˆ—")

        # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
        print(f"  æ€»ç‰¹å¾æ•°: {len(df.columns)}")
        print(f"  å‰10ä¸ªåˆ—å: {list(df.columns[:10])}")

        # æ£€æŸ¥å¿…è¦åˆ—
        required_cols = ['station_id', 'swe']
        missing_cols = [col for col in required_cols if col not in df.columns]

        if missing_cols:
            print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            print("  æ­£åœ¨å°è¯•è‡ªåŠ¨å¤„ç†...")

            # å°è¯•é‡å‘½ååˆ—
            if 'station_id' not in df.columns:
                # å°è¯•æ‰¾åˆ°ç«™ç‚¹IDåˆ—
                possible_id_cols = ['stationid', 'station', 'site_id', 'site', 'id']
                for col in possible_id_cols:
                    if col in df.columns:
                        df = df.rename(columns={col: 'station_id'})
                        print(f"    é‡å‘½å '{col}' -> 'station_id'")
                        break

            if 'swe' not in df.columns:
                # å°è¯•æ‰¾åˆ°SWEåˆ—
                possible_swe_cols = ['snow_water_equivalent', 'snowwater', 'sw']
                for col in possible_swe_cols:
                    if col in df.columns:
                        df = df.rename(columns={col: 'swe'})
                        print(f"    é‡å‘½å '{col}' -> 'swe'")
                        break

        # æ£€æŸ¥GNNWRéœ€è¦çš„ç‰¹å¾
        print(f"\nğŸ” GNNWRç‰¹å¾æ£€æŸ¥:")
        gnnwr_required = ['longitude', 'latitude', 'elevation']
        missing_gnnwr = [col for col in gnnwr_required if col not in df.columns]

        if missing_gnnwr:
            print(f"âš ï¸  ç¼ºå°‘GNNWRç‰¹å¾: {missing_gnnwr}")
            print("  å°†åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•")

            for col in missing_gnnwr:
                if col == 'longitude':
                    df[col] = np.random.uniform(-180, 180, len(df))
                elif col == 'latitude':
                    df[col] = np.random.uniform(-90, 90, len(df))
                elif col == 'elevation':
                    df[col] = np.random.normal(0, 1000, len(df))

        # è¿è¡ŒéªŒè¯
        print("\nğŸš€ å¼€å§‹éªŒè¯...")
        results = verify_weight_application(df, n_samples=100, random_seed=42)

        return results

    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ 'aggregated_station_data.xlsx'")
        print("  å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•")
        return run_complete_verification()
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        print("  å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•")
        return run_complete_verification()


if __name__ == "__main__":
    print("é€‰æ‹©éªŒè¯æ¨¡å¼:")
    print("  1. ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•")
    print("  2. ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•")

    choice = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()

    if choice == "2":
        results = test_real_data_verification()
    else:
        results = run_complete_verification()

    if results:
        print("\n" + "=" * 80)
        print("ğŸ‰ éªŒè¯è„šæœ¬æ‰§è¡Œå®Œæˆï¼")
        print("=" * 80)

        # æä¾›åç»­å»ºè®®
        print("\nğŸ’¡ åç»­æ­¥éª¤å»ºè®®:")
        if results['summary']['verification_passed']:
            print("  1. æƒé‡åº”ç”¨éªŒè¯é€šè¿‡ï¼Œå¯ä»¥ç»§ç»­GNNW-XGBoostèåˆå®éªŒ")
            print("  2. æ£€æŸ¥äº¤å‰éªŒè¯ä¸­æ¯ä¸ªæŠ˜å çš„æƒé‡åº”ç”¨")
            print("  3. å¯¹æ¯”çº¯XGBoostå’ŒGNNW-XGBoostçš„æ€§èƒ½")
        else:
            print("  1. æ£€æŸ¥GNNWRè®­ç»ƒä»£ç ï¼Œç¡®ä¿æƒé‡çŸ©é˜µæ­£ç¡®ç”Ÿæˆ")
            print("  2. æ£€æŸ¥ç‰¹å¾æ˜ å°„é€»è¾‘ï¼Œç¡®ä¿GNNWRç‰¹å¾æ­£ç¡®å¯¹é½")
            print("  3. æ£€æŸ¥æƒé‡åº”ç”¨ä»£ç ï¼Œç¡®ä¿æ¯ä¸ªç‰¹å¾éƒ½è¢«æ­£ç¡®åŠ æƒ")
            print("  4. è€ƒè™‘å¢åŠ GNNWRè®­ç»ƒè½®æ•°æˆ–è°ƒæ•´ç½‘ç»œç»“æ„")

        print("\nğŸ“Š å…³é”®æŒ‡æ ‡:")
        print(f"  GNNWRç‰¹å¾æ•°: {results['summary']['gnnwr_features_count']}")
        print(f"  ç‰¹å¾å¹³å‡å˜åŒ–: {results['summary']['mean_abs_change']:.6f}")
        print(f"  æƒé‡çŸ©é˜µæ ‡å‡†å·®: {results['summary']['weight_std']:.6f}")