import logging
import sys
import os
import argparse
import pandas as pd
from cluster import train_swe_cluster_ensemble

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# ç°åœ¨å¯ä»¥ç›´æ¥å¯¼å…¥
from swe_trainer import SWEXGBoostTrainer, train_swe_model

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('swe_training.log', encoding='utf-8')
    ]
)

logger = logging.getLogger("SWETrainingMain")


def build_model_parameters(args):
    """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°æ„å»ºæ¨¡å‹å‚æ•°å­—å…¸

    Args:
        args: argparseå‚æ•°å¯¹è±¡

    Returns:
        dict: XGBoostå‚æ•°å­—å…¸
    """
    params = {
        'n_estimators': args.trees,
        'learning_rate': args.lr,
        'max_depth': args.depth,
        'min_child_weight': getattr(args, 'min_child_weight', 5),
        'gamma': getattr(args, 'gamma', 0),
        'subsample': args.subsample,
        'colsample_bytree': args.colsample,
        'reg_alpha': getattr(args, 'reg_alpha', 0.05),
        'random_state': 42,
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse'
    }

    # å¯é€‰ï¼šæ·»åŠ å…¶ä»–å‚æ•°ï¼Œå¦‚æœç”¨æˆ·åœ¨å‘½ä»¤è¡Œä¸­æŒ‡å®šäº†çš„è¯
    optional_params = ['reg_lambda', 'max_delta_step', 'scale_pos_weight']
    for param in optional_params:
        if hasattr(args, param) and getattr(args, param) is not None:
            params[param] = getattr(args, param)

    return params


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£ - ä¿®å¤ç‰ˆæœ¬ï¼šåˆ é™¤é‡å¤è®­ç»ƒ"""
    parser = argparse.ArgumentParser(
        description='SWE XGBoostæ¨¡å‹è®­ç»ƒ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python main.py -d data.csv
  python main.py -d data.csv -o ./results --trees 100 --lr 0.1
        """
    )

    parser.add_argument('--data', '-d', required=True,
                        help='è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ (æ”¯æŒCSV/Excel/Parquet)')
    parser.add_argument('--output', '-o', default=None,
                        help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤:è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³ç›®å½•)')
    parser.add_argument('--trees', '-n', type=int, default=60,
                        help='æ ‘çš„æ•°é‡ (é»˜è®¤: 60)')
    parser.add_argument('--lr', '--learning-rate', type=float, default=0.17,
                        help='å­¦ä¹ ç‡ (é»˜è®¤: 0.17)')
    parser.add_argument('--depth', type=int, default=5,
                        help='æ ‘çš„æœ€å¤§æ·±åº¦ (é»˜è®¤: 5)')
    parser.add_argument('--subsample', type=float, default=0.8,
                        help='å­é‡‡æ ·æ¯”ä¾‹ (é»˜è®¤: 0.8)')
    parser.add_argument('--colsample', type=float, default=0.5,
                        help='ç‰¹å¾é‡‡æ ·æ¯”ä¾‹ (é»˜è®¤: 0.5)')

    parser.add_argument('--cluster-mode', action='store_true',
                       help='ä½¿ç”¨èšç±»é›†æˆæ¨¡å¼')
    parser.add_argument('--n-clusters', type=int, default=4,
                       help='èšç±»æ•°é‡ (é»˜è®¤: 4)')
    parser.add_argument('--use-rf', action='store_true',
                        help='åœ¨èšç±»é›†æˆä¸­ä½¿ç”¨éšæœºæ£®æ—ä»£æ›¿XGBoost')
    parser.add_argument('--device', choices=['auto', 'cuda', 'cpu'], default='auto',
                        help='è®­ç»ƒè®¾å¤‡: auto(è‡ªåŠ¨é€‰æ‹©), cuda(GPU), cpu(CPU)')
    parser.add_argument('--num-workers', type=int, default=None,
                        help='æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: è‡ªåŠ¨è®¾ç½®)')
    parser.add_argument('--optimize', choices=['rf', 'xgb', 'gnnwr', 'all'],
                        help='ä½¿ç”¨Optunaä¼˜åŒ–æŒ‡å®šæ¨¡å‹çš„è¶…å‚æ•°')
    parser.add_argument('--n-trials', type=int, default=50,
                        help='Optunaä¼˜åŒ–è¯•éªŒæ¬¡æ•°')

    args = parser.parse_args()

    try:
        logger.info("ğŸš€ å¯åŠ¨SWEæ¨¡å‹è®­ç»ƒç¨‹åº")
        logger.info(f"è¾“å…¥æ–‡ä»¶: {args.data}")
        logger.info(f"è¾“å‡ºç›®å½•: {args.output or 'è‡ªåŠ¨ç”Ÿæˆ'}")

        # 1. åŠ è½½æ•°æ®
        logger.info("ğŸ“¥ åŠ è½½æ•°æ®...")
        df = load_data(args.data)

        if df.empty:
            logger.error("æ•°æ®åŠ è½½å¤±è´¥æˆ–æ•°æ®ä¸ºç©º")
            return 1

        logger.info(f"æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
        logger.info(f"æ•°æ®åˆ—: {list(df.columns)}")

        # æ„å»ºæ¨¡å‹å‚æ•°
        params = build_model_parameters(args)
        logger.info(f"æ¨¡å‹å‚æ•°: n_estimators={params['n_estimators']}, "
                    f"learning_rate={params['learning_rate']}, "
                    f"max_depth={params['max_depth']}")

        if args.cluster_mode:
            # ä½¿ç”¨èšç±»é›†æˆæ¨¡å¼
            logger.info("ğŸ¯ ä½¿ç”¨èšç±»é›†æˆæ¨¡å¼")
            logger.info(f"èšç±»æ•°é‡: {args.n_clusters}")
            logger.info(f"ä½¿ç”¨{'éšæœºæ£®æ—' if args.use_rf else 'XGBoost'}ä½œä¸ºåŸºç¡€æ¨¡å‹")

            results = train_swe_cluster_ensemble(
                data_df=df,
                output_dir=args.output,
                n_clusters=args.n_clusters,
                params=build_model_parameters(args),
                use_rf=args.use_rf,
                device=args.device
            )
        else:
            # ä½¿ç”¨åŸæœ‰æ¨¡å¼ï¼ˆä¿æŒXGBoostä¸å˜ï¼‰
            from swe_trainer import train_swe_model
            logger.info("ğŸ¯ ä½¿ç”¨æ ‡å‡†XGBoostæ¨¡å¼")
            results = train_swe_model(
                data_df=df,
                output_dir=args.output,
                params=params
            )

        # åœ¨ä¸»å‡½æ•°ä¸­æ·»åŠ ä¼˜åŒ–é€»è¾‘
        if args.optimize:
            from optuna_optimizer import optimize_swe_model
            logger.info(f"å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–: {args.optimize}, è¯•éªŒæ¬¡æ•°: {args.n_trials}")

            if args.optimize == 'all':
                for model_type in ['rf', 'gnnwr']:
                    best_params = optimize_swe_model(df, model_type, args.n_trials)
                    logger.info(f"{model_type} æœ€ä½³å‚æ•°: {best_params}")
            else:
                best_params = optimize_swe_model(df, args.optimize, args.n_trials)
                logger.info(f"æœ€ä½³å‚æ•°: {best_params}")

        logger.info("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

        # æ˜¾ç¤ºå…³é”®ç»“æœ
        print_summary(results)

        return 0

    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return 1


def load_data(file_path):
    """åŠ è½½æ•°æ®æ–‡ä»¶

    Args:
        file_path (str): æ–‡ä»¶è·¯å¾„

    Returns:
        pd.DataFrame: åŠ è½½çš„æ•°æ®

    Raises:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨æ—¶æŠ›å‡º
        ValueError: æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒæ—¶æŠ›å‡º
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    file_ext = os.path.splitext(file_path)[1].lower()

    try:
        if file_ext == '.csv':
            df = pd.read_csv(file_path)
            logger.info(f"CSVæ–‡ä»¶åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
            return df

        elif file_ext in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
            logger.info(f"Excelæ–‡ä»¶åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
            return df

        elif file_ext == '.parquet':
            df = pd.read_parquet(file_path)
            logger.info(f"Parquetæ–‡ä»¶åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
            return df

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ã€‚æ”¯æŒæ ¼å¼: CSV, Excel, Parquet")

    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        # å°è¯•å…¶ä»–ç¼–ç æ–¹å¼åŠ è½½CSV
        if file_ext == '.csv':
            try:
                logger.info("å°è¯•ä½¿ç”¨GBKç¼–ç åŠ è½½CSV...")
                df = pd.read_csv(file_path, encoding='gbk')
                logger.info(f"ä½¿ç”¨GBKç¼–ç åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
                return df
            except:
                try:
                    logger.info("å°è¯•ä½¿ç”¨latin1ç¼–ç åŠ è½½CSV...")
                    df = pd.read_csv(file_path, encoding='latin1')
                    logger.info(f"ä½¿ç”¨latin1ç¼–ç åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
                    return df
                except:
                    raise ValueError(f"CSVæ–‡ä»¶æ— æ³•ç”¨ä»»ä½•ç¼–ç åŠ è½½: {e}")
        else:
            raise


def print_summary(results):
    """æ‰“å°ç»“æœæ‘˜è¦ - ä¿®å¤ç‰ˆæœ¬ï¼šå¤„ç†ç¼ºå¤±çš„station_cv"""
    print("\n" + "=" * 70)
    print("ğŸ‰ SWEæ¨¡å‹è®­ç»ƒå®Œæˆæ‘˜è¦")
    print("=" * 70)

    # ç«™ç‚¹äº¤å‰éªŒè¯ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'station_cv' in results:
        station = results['station_cv']['overall']
        print(f"\nğŸ“ ç«™ç‚¹äº¤å‰éªŒè¯ (ç©ºé—´è¯„ä¼°):")
        print(f"   MAE:  {station['MAE']:8.3f} mm")
        print(f"   RMSE: {station['RMSE']:8.3f} mm")
        print(f"   R:    {station['R']:8.3f}")
        print(f"   æ ·æœ¬æ•°: {station.get('æ ·æœ¬æ•°', station.get('samples', 'N/A')):>6}")
        print(f"   æŠ˜å æ•°: {results['station_cv']['folds']:6d}")

    # å¹´åº¦äº¤å‰éªŒè¯ç»“æœ
    if 'yearly_cv' in results:
        yearly = results['yearly_cv']['overall']
        print(f"\nğŸ“… å¹´åº¦äº¤å‰éªŒè¯ (æ—¶é—´è¯„ä¼°):")
        print(f"   MAE:  {yearly['MAE']:8.3f} mm")
        print(f"   RMSE: {yearly['RMSE']:8.3f} mm")
        print(f"   R:    {yearly['R']:8.3f}")
        print(f"   æ ·æœ¬æ•°: {yearly.get('æ ·æœ¬æ•°', yearly.get('samples', 'N/A')):>6}")
        print(f"   æŠ˜å æ•°: {results['yearly_cv']['folds']:6d}")

    # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'feature_importance' in results:
        top_features = results['feature_importance'].head(3)
        print(f"\nğŸ” é‡è¦ç‰¹å¾ Top 3:")
        for i, (_, row) in enumerate(top_features.iterrows(), 1):
            print(f"   {i}. {row['feature']:20} {row['importance']:.4f}")

    # æ€§èƒ½æ¯”è¾ƒï¼ˆå¦‚æœä¸¤è€…éƒ½å­˜åœ¨ï¼‰
    if 'station_cv' in results and 'yearly_cv' in results:
        station_r = results['station_cv']['overall']['R']
        yearly_r = results['yearly_cv']['overall']['R']

        print(f"\nğŸ’¡ å»ºè®®:")
        if station_r > yearly_r:
            print(f"   ç«™ç‚¹CVæ€§èƒ½æ›´ä¼˜ï¼Œæ¨èç”¨äºç©ºé—´è¯„ä¼°")
        else:
            print(f"   å¹´åº¦CVæ€§èƒ½æ›´ä¼˜ï¼Œæ¨èç”¨äºæ—¶é—´è¯„ä¼°")
    else:
        print(f"\nğŸ’¡ å»ºè®®:")
        print(f"   ä½¿ç”¨å¹´åº¦äº¤å‰éªŒè¯ç»“æœè¿›è¡Œè¯„ä¼°")

    print("=" * 70)
    print("ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°è¾“å‡ºç›®å½•")


def interactive_mode():
    """äº¤äº’å¼æ¨¡å¼"""
    print("\nğŸ” SWEæ¨¡å‹è®­ç»ƒ - äº¤äº’æ¨¡å¼")
    print("-" * 50)

    try:
        # è·å–æ•°æ®æ–‡ä»¶è·¯å¾„
        data_file = input("è¯·è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„: ").strip()
        if not data_file:
            print("âŒ å¿…é¡»æä¾›æ•°æ®æ–‡ä»¶è·¯å¾„")
            return

        if not os.path.exists(data_file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return

        # åŠ è½½æ•°æ®
        print("ğŸ“¥ åŠ è½½æ•°æ®...")
        df = load_data(data_file)
        if df.empty:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
            return

        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")

        # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        print(f"  ç«™ç‚¹æ•°é‡: {df['station_id'].nunique()}")
        print(f"  æ—¥æœŸèŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
        print(f"  SWEç»Ÿè®¡: å‡å€¼={df['swe'].mean():.2f}mm, æ ‡å‡†å·®={df['swe'].std():.2f}mm")

        # é€‰æ‹©è¾“å‡ºç›®å½•
        output_dir = input("\nè¯·è¾“å…¥è¾“å‡ºç›®å½• (å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
        if not output_dir:
            output_dir = None
            print("ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•")

        # å¯é€‰ï¼šè‡ªå®šä¹‰å‚æ•°
        print(f"\nâš™ï¸ æ¨¡å‹å‚æ•° (ä½¿ç”¨é»˜è®¤å€¼è¯·ç›´æ¥å›è½¦):")

        trees = input(f"æ ‘çš„æ•°é‡ [é»˜è®¤: 60]: ").strip()
        lr = input(f"å­¦ä¹ ç‡ [é»˜è®¤: 0.17]: ").strip()
        depth = input(f"æœ€å¤§æ·±åº¦ [é»˜è®¤: 5]: ").strip()

        # æ„å»ºå‚æ•°å­—å…¸
        params = {}
        if trees:
            params['n_estimators'] = int(trees)
        if lr:
            params['learning_rate'] = float(lr)
        if depth:
            params['max_depth'] = int(depth)

        # ç¡®è®¤å¼€å§‹è®­ç»ƒ
        print(f"\nğŸ” è®­ç»ƒé…ç½®:")
        print(f"  æ•°æ®æ–‡ä»¶: {data_file}")
        print(f"  è¾“å‡ºç›®å½•: {output_dir or 'è‡ªåŠ¨ç”Ÿæˆ'}")
        print(f"  æ ‘çš„æ•°é‡: {params.get('n_estimators', 60)}")
        print(f"  å­¦ä¹ ç‡: {params.get('learning_rate', 0.17)}")
        print(f"  æœ€å¤§æ·±åº¦: {params.get('max_depth', 5)}")

        confirm = input("\nå¼€å§‹è®­ç»ƒæ¨¡å‹? (y/n): ").strip().lower()
        if confirm != 'y':
            print("âŒ å–æ¶ˆè®­ç»ƒ")
            return

        # è®­ç»ƒæ¨¡å‹
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        results = train_swe_model(df, output_dir, params)

        # æ˜¾ç¤ºç»“æœ
        print_summary(results)

        print(f"\nğŸ¯ è®­ç»ƒå®Œæˆï¼")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åº“æ˜¯å¦å®‰è£…"""
    required_packages = {
        'pandas': 'pd',
        'numpy': 'np',
        'xgboost': 'xgb',
        'scikit-learn': 'sklearn',
        'scipy': 'scipy'
    }

    missing_packages = []

    for package, short_name in required_packages.items():
        try:
            if package == 'scikit-learn':
                import sklearn
            else:
                __import__(package)
        except ImportError:
            missing_packages.append(package)

    if missing_packages:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–åº“:")
        for package in missing_packages:
            print(f"  - {package}")
        print(f"\nè¯·å®‰è£…: pip install {' '.join(missing_packages)}")
        return False

    return True


def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    print("""
SWE XGBoostæ¨¡å‹è®­ç»ƒå·¥å…·

ä½¿ç”¨æ–¹æ³•:

1. å‘½ä»¤è¡Œæ¨¡å¼:
   python main.py --data <æ•°æ®æ–‡ä»¶> [é€‰é¡¹]

2. äº¤äº’æ¨¡å¼:
   python main.py

æ”¯æŒçš„æ•°æ®æ ¼å¼:
   â€¢ CSVæ–‡ä»¶ (.csv)
   â€¢ Excelæ–‡ä»¶ (.xlsx, .xls)  
   â€¢ Parquetæ–‡ä»¶ (.parquet)

å¿…è¦æ•°æ®åˆ—:
   â€¢ station_id: ç«™ç‚¹ID
   â€¢ date: æ—¥æœŸ
   â€¢ swe: é›ªæ°´å½“é‡å€¼

è¾“å‡ºç»“æœ:
   â€¢ è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ (.pkl)
   â€¢ äº¤å‰éªŒè¯é¢„æµ‹ç»“æœ (.csv)
   â€¢ ç‰¹å¾é‡è¦æ€§æ’åº (.csv)
   â€¢ è¯¦ç»†è¯„ä¼°æŠ¥å‘Š (.json, .txt)

ç¤ºä¾‹:
   python main.py -d data.csv -o ./results --trees 100 --lr 0.1
    """)


if __name__ == "__main__":
    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    print("=" * 60)
    print("â„ï¸  SWE XGBoostæ¨¡å‹è®­ç»ƒå·¥å…·")
    print("=" * 60)

    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        sys.exit(1)

    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œè¿›å…¥äº¤äº’æ¨¡å¼
    if len(sys.argv) == 1:
        interactive_mode()
    else:
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¸®åŠ©è¯·æ±‚
        if any(arg in sys.argv for arg in ['-h', '--help', 'help']):
            show_help()
            sys.exit(0)

        # è¿è¡Œå‘½ä»¤è¡Œæ¨¡å¼
        exit_code = main()
        sys.exit(exit_code)