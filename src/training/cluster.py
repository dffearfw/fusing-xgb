import logging
import warnings

import torch.nn as nn
# import logger
import numpy as np
import pandas as pd
import torch
import xgboost as xgb
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, mean_squared_error
from scipy.stats import pearsonr
from sklearn.model_selection import LeaveOneGroupOut
import joblib
import os
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from torch import optim
from torch.utils.data import DataLoader
from tqdm import tqdm

# ç¦ç”¨TF32ç›¸å…³è­¦å‘Šï¼ˆCPUä¸Šä¸éœ€è¦ï¼‰
warnings.filterwarnings("ignore", message=".*TF32.*")

# CPUæ€§èƒ½ä¼˜åŒ–è®¾ç½®
torch.set_num_threads(24)  # i9-14900KFæœ‰24ä¸ªç‰©ç†æ ¸å¿ƒ
os.environ['OMP_NUM_THREADS'] = '24'
os.environ['MKL_NUM_THREADS'] = '24'
os.environ['OPENMP'] = '1'

# ç¦ç”¨CUDAç›¸å…³è®¾ç½®ï¼ˆé¿å…ä¸å¿…è¦çš„GPUæ£€æŸ¥ï¼‰
torch.backends.cudnn.enabled = False

# è®¾ç½®çŸ©é˜µä¹˜æ³•ç²¾åº¦ï¼ˆCPUä¸Šä½¿ç”¨é«˜ç²¾åº¦ï¼‰
torch.set_float32_matmul_precision('high')

# å…ˆåˆ›å»ºlogger
logger = logging.getLogger("SWEClusterEnsemble")

# ç„¶åå¯¼å…¥å¢å¼ºç‰ˆGNNWR
try:
    from GNNWR import EnhancedSpatialDataset, EnhancedGNNWRTrainer, SpatialWeightCalculator

    HAS_ENHANCED_GNNWR = True
    logger.info("æˆåŠŸå¯¼å…¥å¢å¼ºç‰ˆGNNWR")
except ImportError as e:
    logger.warning(f"æ— æ³•å¯¼å…¥å¢å¼ºç‰ˆGNNWR: {e}")
    try:
        # å°è¯•å¯¼å…¥åŸºç¡€ç‰ˆ
        from GNNWR import SpatialDataset, GNNWRTrainer

        HAS_ENHANCED_GNNWR = False
        logger.info("ä½¿ç”¨åŸºç¡€ç‰ˆGNNWR")
    except ImportError:
        logger.error("æ— æ³•å¯¼å…¥ä»»ä½•GNNWRç‰ˆæœ¬")
        HAS_ENHANCED_GNNWR = False


        # åˆ›å»ºè™šæ‹Ÿç±»ä»¥é¿å…åç»­é”™è¯¯
        class EnhancedSpatialDataset:
            def __init__(self, features, targets, coords=None):
                self.features = features
                self.targets = targets
                self.coords = coords

            def __len__(self):
                return len(self.features)

            def __getitem__(self, idx):
                if self.coords is not None:
                    return self.features[idx], self.targets[idx], self.coords[idx]
                else:
                    return self.features[idx], self.targets[idx]


        class EnhancedGNNWRTrainer:
            def __init__(self, *args, **kwargs):
                logger.warning("ä½¿ç”¨è™šæ‹ŸEnhancedGNNWRTrainer")

            def train(self, *args, **kwargs):
                logger.warning("è™šæ‹Ÿè®­ç»ƒæ–¹æ³•")

            def predict(self, features, coords=None):
                logger.warning("è™šæ‹Ÿé¢„æµ‹æ–¹æ³•")
                return np.random.normal(50, 20, len(features))


        class SpatialDataset:
            def __init__(self, features, targets):
                self.features = features
                self.targets = targets

            def __len__(self):
                return len(self.features)

            def __getitem__(self, idx):
                return self.features[idx], self.targets[idx]


        class GNNWRTrainer:
            def __init__(self, *args, **kwargs):
                logger.warning("ä½¿ç”¨è™šæ‹ŸGNNWRTrainer")

            def train(self, *args, **kwargs):
                logger.warning("è™šæ‹Ÿè®­ç»ƒæ–¹æ³•")

            def predict(self, features):
                logger.warning("è™šæ‹Ÿé¢„æµ‹æ–¹æ³•")
                return np.random.normal(50, 20, len(features))


class SWEClusterEnsemble:
    """SWEèšç±»é›†æˆå›å½’å™¨ - ä½¿ç”¨å¢å¼ºç‰ˆGNNWRè¿›è¡Œé›†æˆ"""



    DEFAULT_PARAMS = {
        'n_estimators': 60,
        'learning_rate': 0.17,
        'max_depth': 5,
        'min_child_weight': 5,
        'gamma': 0,
        'subsample': 0.8,
        'colsample_bytree': 0.5,
        'reg_alpha': 0.05,
        'random_state': 42,
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse'
    }

    def __init__(self, n_clusters=4, params=None, gnnwr_params=None, use_enhanced_gnnwr=True, use_rf=False, device='auto'):
        """åˆå§‹åŒ–èšç±»é›†æˆå›å½’å™¨

        Args:
            n_clusters (int): èšç±»æ•°é‡
            params (dict): XGBoostå‚æ•°
            gnnwr_params (dict): GNNWRå‚æ•°
            use_enhanced_gnnwr (bool): æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰ˆGNNWR
        """
        self.logger = logging.getLogger("SWEClusterEnsemble")

        self.n_clusters = n_clusters
        self.kmeans = None
        self.cluster_assignments = None
        self.cluster_models = {}
        self.gnnwr_trainer = None
        self.feature_columns = None
        self.target_column = 'swe'
        self.use_enhanced_gnnwr = use_enhanced_gnnwr and HAS_ENHANCED_GNNWR
        self.device = device
        self.use_rf = use_rf

        # å…³é”®ä¿®å¤ï¼šç¡®ä¿paramsä¸ä¸ºNone
        if params is None:
            params = {}  # ç¡®ä¿paramsè‡³å°‘æ˜¯ç©ºå­—å…¸

        if use_rf:
            # RFå‚æ•°
            self.rf_params = {
                'n_estimators': params.get('n_estimators', 100),
                'max_depth': params.get('max_depth', None),
                'min_samples_split': 2,
                'min_samples_leaf': 1,
                'random_state': 42,
                'n_jobs': -1
            }
            self.params = params if params else self.DEFAULT_PARAMS.copy()
        else:
            # åŸæœ‰çš„XGBå‚æ•°
            self.params = self.DEFAULT_PARAMS.copy()
            if params:
                self.params.update(params)


        # GNNWRå‚æ•°
        self.gnnwr_params = {
            'hidden_dims': [128, 64, 32, 16],
            'learning_rate': 0.001,
            'epochs': 200,
            'batch_size': 64,
            'patience': 20,
            'bandwidth':5.0,
            'use_spatial_weights': True,
            'device': device,  # ä¼ é€’è®¾å¤‡å‚æ•°
            'dropout_rate': 0.3,  # æ·»åŠ dropout
            'weight_decay': 1e-4,  # æƒé‡è¡°å‡
            'num_workers': min(6, os.cpu_count() // 2)
        }
        if gnnwr_params:
            self.gnnwr_params.update(gnnwr_params)

        self.logger.info(f"åˆå§‹åŒ–SWEèšç±»é›†æˆå›å½’å™¨ï¼Œèšç±»æ•°: {n_clusters}")
        self.logger.info(f"ä½¿ç”¨{'å¢å¼ºç‰ˆ' if self.use_enhanced_gnnwr else 'åŸºç¡€ç‰ˆ'}GNNWR")
        self.logger.info(f"GNNWRå‚æ•°: {self.gnnwr_params}")

    def preprocess_data(self, df):
        """æ•°æ®é¢„å¤„ç† - å®Œæ•´è°ƒè¯•ç‰ˆæœ¬"""
        self.logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")

        # ç¡®å®šç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—
        if self.feature_columns is None:
            # è‡ªåŠ¨é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆæ’é™¤ç›®æ ‡åˆ—å’Œå…¶ä»–éç‰¹å¾åˆ—ï¼‰
            exclude_cols = [self.target_column, 'station_id', 'year', 'date', 'station', 'group',
                            'longitude', 'latitude', 'lon', 'lat']  # æ’é™¤åæ ‡åˆ—
            self.feature_columns = [col for col in df.columns if
                                    col not in exclude_cols and df[col].dtype in [np.int64, np.float64]]

        self.logger.info(f"ä½¿ç”¨ç‰¹å¾: {self.feature_columns}")

        # æå–ç‰¹å¾å’Œç›®æ ‡
        X = df[self.feature_columns].values
        y = df[self.target_column].values

        # å¤„ç†ç¼ºå¤±å€¼
        if np.isnan(X).any():
            self.logger.info("å¤„ç†ç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼")
            self.feature_imputer = SimpleImputer(strategy='median')
            X = self.feature_imputer.fit_transform(X)
        else:
            self.feature_imputer = None

        # åˆ›å»ºåˆ†ç»„ä¿¡æ¯
        if 'station_id' in df.columns:
            station_groups = df['station_id'].values
        elif 'station' in df.columns:
            station_groups = df['station'].values
        else:
            # å¦‚æœæ²¡æœ‰ç«™ç‚¹ä¿¡æ¯ï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºåˆ†ç»„
            station_groups = np.arange(len(df))
            self.logger.warning("æœªæ‰¾åˆ°ç«™ç‚¹ä¿¡æ¯ï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºåˆ†ç»„")

        if 'year' in df.columns:
            year_groups = df['year'].values
        else:
            # å¦‚æœæ²¡æœ‰å¹´ä»½ä¿¡æ¯ï¼Œåˆ›å»ºè™šæ‹Ÿå¹´ä»½
            year_groups = np.ones(len(df), dtype=int)
            self.logger.warning("æœªæ‰¾åˆ°å¹´ä»½ä¿¡æ¯ï¼Œä½¿ç”¨ç»Ÿä¸€å¹´ä»½åˆ†ç»„")

        # === åæ ‡è°ƒè¯•éƒ¨åˆ† ===
        self.logger.info("=== åæ ‡è°ƒè¯•ä¿¡æ¯ ===")

        # æ£€æŸ¥åæ ‡åˆ—çš„å­˜åœ¨å’Œå†…å®¹
        coord_columns = ['longitude', 'latitude', 'lon', 'lat']
        available_coords = [col for col in coord_columns if col in df.columns]
        self.logger.info(f"æ‰¾åˆ°çš„åæ ‡åˆ—: {available_coords}")

        for col in available_coords:
            if col in df.columns:
                non_na_count = df[col].notna().sum()
                dtype = df[col].dtype
                min_val = df[col].min() if non_na_count > 0 else "N/A"
                max_val = df[col].max() if non_na_count > 0 else "N/A"
                self.logger.info(f"  {col}: éç©ºå€¼={non_na_count}, ç±»å‹={dtype}, èŒƒå›´=[{min_val}, {max_val}]")

        # æå–åæ ‡ä¿¡æ¯
        coords = None
        if all(col in df.columns for col in ['longitude', 'latitude']):
            coords = df[['longitude', 'latitude']].values
            self.logger.info(f"âœ… ä½¿ç”¨ç»çº¬åº¦åæ ‡: {len(coords)} ä¸ªç‚¹")
            self.logger.info(
                f"   åæ ‡èŒƒå›´: lon[{coords[:, 0].min():.2f}, {coords[:, 0].max():.2f}], lat[{coords[:, 1].min():.2f}, {coords[:, 1].max():.2f}]")

            # æ£€æŸ¥æ˜¯å¦æœ‰NaNåæ ‡
            nan_coords = np.isnan(coords).any(axis=1).sum()
            if nan_coords > 0:
                self.logger.warning(f"âš ï¸  å‘ç° {nan_coords} ä¸ªåæ ‡åŒ…å«NaNå€¼")
                # ä½¿ç”¨å‡å€¼å¡«å……NaNåæ ‡
                for i in range(coords.shape[1]):
                    col_mean = np.nanmean(coords[:, i])
                    nan_mask = np.isnan(coords[:, i])
                    coords[nan_mask, i] = col_mean
                    self.logger.info(f"   åˆ— {i} çš„NaNå€¼å·²ç”¨å‡å€¼ {col_mean:.4f} å¡«å……")

        elif all(col in df.columns for col in ['lon', 'lat']):
            coords = df[['lon', 'lat']].values
            self.logger.info(f"âœ… ä½¿ç”¨ç»çº¬åº¦åæ ‡: {len(coords)} ä¸ªç‚¹")
        else:
            self.logger.warning("âŒ æœªæ‰¾åˆ°åæ ‡ä¿¡æ¯ï¼Œå°†ä½¿ç”¨è™šæ‹Ÿåæ ‡")
            unique_stations = np.unique(station_groups)
            station_to_coord = {station: [i, i] for i, station in enumerate(unique_stations)}
            coords = np.array([station_to_coord[station] for station in station_groups])
            self.logger.info(f"   ç”Ÿæˆè™šæ‹Ÿåæ ‡: {len(coords)} ä¸ªç‚¹")

        self.logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(X)}ä¸ªæ ·æœ¬, {X.shape[1]}ä¸ªç‰¹å¾")
        self.logger.info(f"ç«™ç‚¹æ•°: {len(np.unique(station_groups))}, å¹´ä»½æ•°: {len(np.unique(year_groups))}")
        self.logger.info(f"åæ ‡æœ€ç»ˆçŠ¶æ€: {'å¯ç”¨' if coords is not None else 'ä¸å¯ç”¨'}")

        return X, y, station_groups, year_groups, coords

    def perform_clustering(self, X, groups):
        """æ‰§è¡Œèšç±»åˆ†æ

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            groups (np.array): åˆ†ç»„ä¿¡æ¯

        Returns:
            np.array: èšç±»æ ‡ç­¾
        """
        self.logger.info(f"æ‰§è¡ŒK-meansèšç±»ï¼Œèšç±»æ•°: {self.n_clusters}")

        # æŒ‰ç«™ç‚¹èšåˆç‰¹å¾
        unique_groups = np.unique(groups)
        group_features = []

        for group in unique_groups:
            group_mask = groups == group
            group_data = X[group_mask]
            # ä½¿ç”¨æ¯ä¸ªç«™ç‚¹çš„ç‰¹å¾å‡å€¼ä½œä¸ºèšç±»ç‰¹å¾
            group_mean = np.nanmean(group_data, axis=0)
            group_features.append(group_mean)

        group_features = np.array(group_features)

        # å¤„ç†å¯èƒ½çš„NaNå€¼
        if np.isnan(group_features).any():
            self.logger.info("å¤„ç†èšç±»ç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼")
            cluster_imputer = SimpleImputer(strategy='median')
            group_features = cluster_imputer.fit_transform(group_features)

        # æ‰§è¡ŒK-meansèšç±»
        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)
        group_clusters = self.kmeans.fit_predict(group_features)

        # å°†èšç±»æ ‡ç­¾æ˜ å°„å›åŸå§‹æ ·æœ¬
        cluster_assignments = np.zeros(len(X), dtype=int)
        for i, group in enumerate(unique_groups):
            group_mask = groups == group
            cluster_assignments[group_mask] = group_clusters[i]

        # ç»Ÿè®¡æ¯ä¸ªèšç±»çš„æ ·æœ¬æ•°
        cluster_counts = np.bincount(cluster_assignments)
        self.logger.info(f"èšç±»åˆ†å¸ƒ: {dict(enumerate(cluster_counts))}")

        return cluster_assignments

    def train_cluster_models(self, X, y, cluster_labels):
        """ä¸ºæ¯ä¸ªèšç±»è®­ç»ƒXGBoostæ¨¡å‹

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            y (np.array): ç›®æ ‡å˜é‡
            cluster_labels (np.array): èšç±»æ ‡ç­¾
        """
        self.logger.info("è®­ç»ƒå„èšç±»XGBoostæ¨¡å‹...")
        self.cluster_models = {}

        for cluster_id in range(self.n_clusters):
            cluster_mask = cluster_labels == cluster_id
            cluster_size = np.sum(cluster_mask)

            if cluster_size < 5:
                self.logger.warning(f"èšç±» {cluster_id} æ ·æœ¬æ•°è¿‡å°‘ ({cluster_size})ï¼Œè·³è¿‡è®­ç»ƒ")
                continue

            X_cluster = X[cluster_mask]
            y_cluster = y[cluster_mask]

            if self.use_rf:
                from sklearn.ensemble import RandomForestRegressor
                model = RandomForestRegressor(**self.rf_params)
            else:
                import xgboost as xgb
                model = xgb.XGBRegressor(**self.params)

            model.fit(X_cluster, y_cluster)

            self.cluster_models[cluster_id] = model

            # è¯„ä¼°èšç±»æ¨¡å‹æ€§èƒ½
            y_pred_cluster = model.predict(X_cluster)
            cluster_mae = mean_absolute_error(y_cluster, y_pred_cluster)
            cluster_rmse = np.sqrt(mean_squared_error(y_cluster, y_pred_cluster))

            self.logger.info(f"  èšç±» {cluster_id}: {cluster_size}æ ·æœ¬, MAE={cluster_mae:.3f}, RMSE={cluster_rmse:.3f}")

    def _get_cluster_predictions(self, X, cluster_labels):
        """è·å–å„èšç±»æ¨¡å‹çš„é¢„æµ‹ç»“æœ

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            cluster_labels (np.array): èšç±»æ ‡ç­¾

        Returns:
            np.array: å„èšç±»æ¨¡å‹çš„é¢„æµ‹ç»“æœçŸ©é˜µ
        """
        cluster_predictions = np.zeros((len(X), self.n_clusters))

        for cluster_id, model in self.cluster_models.items():
            cluster_mask = cluster_labels == cluster_id
            if np.any(cluster_mask):
                predictions = model.predict(X[cluster_mask])
                cluster_predictions[cluster_mask, cluster_id] = predictions

        return cluster_predictions

    def train_gnnwr_model(self, X, y, cluster_predictions, coords=None):
        """è®­ç»ƒGNNWRé›†æˆæ¨¡å‹ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        self.logger.info("=== train_gnnwr_methodè¯¦ç»†è°ƒè¯• ===")
        self.logger.info(f"è¾“å…¥å‚æ•°IDæ£€æŸ¥:")
        self.logger.info(f"  coords id: {id(coords)}")
        self.logger.info(f"  coords is None: {coords is None}")

        # ç«‹å³æ£€æŸ¥åæ ‡æ•°æ®
        if coords is None:
            self.logger.error("âŒ åæ ‡æ•°æ®åœ¨æ–¹æ³•å…¥å£å¤„å°±ä¸ºNone!")
            raise ValueError("åæ ‡æ•°æ®åœ¨æ–¹æ³•å…¥å£å¤„å°±ä¸ºNone")

        self.logger.info(f"  coordsç±»å‹: {type(coords)}")
        self.logger.info(f"  coordså½¢çŠ¶: {coords.shape if hasattr(coords, 'shape') else 'No shape'}")
        self.logger.info(f"  coordsé•¿åº¦: {len(coords) if hasattr(coords, '__len__') else 'No length'}")

        self.logger.info("è®­ç»ƒGNNWRé›†æˆæ¨¡å‹...")

        # ä½¿ç”¨ç‰¹å¾ï¼šåŸå§‹ç‰¹å¾ + å„èšç±»é¢„æµ‹
        gnnwr_features = np.hstack([X, cluster_predictions])

        # æ·»åŠ ç»´åº¦è°ƒè¯•
        self.logger.info(f"è¾“å…¥ç‰¹å¾ç»´åº¦è°ƒè¯•:")
        self.logger.info(f"  Xå½¢çŠ¶: {X.shape}")
        self.logger.info(f"  cluster_predictionså½¢çŠ¶: {cluster_predictions.shape}")
        self.logger.info(f"  åˆå¹¶ågnnwr_featureså½¢çŠ¶: {gnnwr_features.shape}")

        # å…³é”®ä¿®å¤ï¼šåˆ›å»ºåæ ‡æ•°æ®çš„å‰¯æœ¬ï¼Œé¿å…è¢«å…¶ä»–æ–¹æ³•ä¿®æ”¹
        if coords is not None:
            coords_copy = coords.copy()  # åˆ›å»ºå‰¯æœ¬
            self.logger.info(f"åˆ›å»ºåæ ‡å‰¯æœ¬ï¼ŒåŸid: {id(coords)}, å‰¯æœ¬id: {id(coords_copy)}")
        else:
            coords_copy = None
            self.logger.error("åæ ‡æ•°æ®ä¸ºNone")
            raise ValueError("åæ ‡æ•°æ®ä¸ºNone")

        # å¤„ç†ç¼ºå¤±å€¼
        if np.isnan(gnnwr_features).any():
            self.logger.info("å¤„ç†GNNWRç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼")
            self.gnnwr_imputer = SimpleImputer(strategy='median')
            gnnwr_features_imputed = self.gnnwr_imputer.fit_transform(gnnwr_features)
        else:
            gnnwr_features_imputed = gnnwr_features
            self.gnnwr_imputer = None

        # æ·»åŠ å¤„ç†åçš„ç»´åº¦è°ƒè¯•
        self.logger.info(f"å¤„ç†åç‰¹å¾ç»´åº¦: {gnnwr_features_imputed.shape}")

        # æ ¹æ®æ•°æ®å¤§å°è‡ªåŠ¨è°ƒæ•´å‚æ•°ï¼ˆç»Ÿä¸€è®¾ç½®ï¼‰
        n_samples = len(gnnwr_features_imputed)
        batch_size = min(128, max(32, n_samples // 100))  # è‡ªé€‚åº”æ‰¹æ¬¡å¤§å°
        num_workers = min(6, os.cpu_count() // 2)  # ä½¿ç”¨ä¸€åŠCPUæ ¸å¿ƒ

        self.logger.info(f"æ•°æ®åŠ è½½å™¨é…ç½®: batch_size={batch_size}, workers={num_workers}")

        if self.use_enhanced_gnnwr:
            # ä½¿ç”¨å¢å¼ºç‰ˆGNNWR
            self.logger.info("ä½¿ç”¨å¢å¼ºç‰ˆGNNWRè®­ç»ƒå™¨")

            # æ£€æŸ¥æ ·æœ¬æ•°é‡ï¼Œå¦‚æœå¤ªå¤šåˆ™ä½¿ç”¨ç®€åŒ–æ¨¡å¼
            # å…³é”®ä¿®å¤ï¼šä½¿ç”¨ coords_copy è€Œä¸æ˜¯ coords
            use_spatial = self.gnnwr_params['use_spatial_weights'] and coords_copy is not None

            if not use_spatial:
                self.logger.warning(f"æ ·æœ¬æ•°é‡è¾ƒå¤§ ({n_samples}) æˆ–åæ ‡ä¸å¯ç”¨ï¼Œç¦ç”¨ç©ºé—´æƒé‡è®¡ç®—")
                # å³ä½¿ç¦ç”¨ç©ºé—´æƒé‡ï¼Œä¹Ÿè¦ä¼ é€’åæ ‡æ•°æ®
                dataset = EnhancedSpatialDataset(
                    features=gnnwr_features_imputed,
                    targets=y,
                    coords=coords_copy  # ä»ç„¶ä¼ é€’åæ ‡ï¼Œåªæ˜¯è®­ç»ƒå™¨ä¸ä½¿ç”¨
                )
            else:
                # æ­£å¸¸æ¨¡å¼
                dataset = EnhancedSpatialDataset(
                    features=gnnwr_features_imputed,
                    targets=y,
                    coords=coords_copy
                )

            train_loader = DataLoader(
                dataset,
                batch_size=batch_size,  # ä¿®å¤ï¼šä½¿ç”¨è‡ªé€‚åº”æ‰¹æ¬¡å¤§å°
                shuffle=True,
                num_workers=num_workers,
                pin_memory=False,  # å¦‚æœä½¿ç”¨GPUåˆ™å¯ç”¨
                persistent_workers=num_workers > 0
            )

            # åˆå§‹åŒ–å¢å¼ºç‰ˆGNNWRè®­ç»ƒå™¨
            input_dim = gnnwr_features_imputed.shape[1]
            self.logger.info(f"åˆå§‹åŒ–GNNWRè®­ç»ƒå™¨ï¼Œè¾“å…¥ç»´åº¦: {input_dim}")

            self.gnnwr_trainer = EnhancedGNNWRTrainer(
                input_dim=input_dim,
                coords=coords_copy if use_spatial else None,  # å…³é”®ä¿®å¤ï¼šä½¿ç”¨å‰¯æœ¬
                hidden_dims=self.gnnwr_params['hidden_dims'],
                learning_rate=self.gnnwr_params['learning_rate'],
                bandwidth=self.gnnwr_params['bandwidth'],
                use_spatial_weights=use_spatial
            )

            # è®­ç»ƒæ¨¡å‹
            self.logger.info(f"å¼€å§‹å¢å¼ºç‰ˆGNNWRè®­ç»ƒï¼Œè¾“å…¥ç»´åº¦: {input_dim}")
            try:
                self.gnnwr_trainer.train(
                    train_loader,
                    epochs=self.gnnwr_params['epochs'],
                    patience=self.gnnwr_params['patience']
                )
            except MemoryError as e:
                self.logger.error(f"å†…å­˜ä¸è¶³: {e}ï¼Œå›é€€åˆ°åŸºç¡€ç‰ˆGNNWR")
                self.use_enhanced_gnnwr = False
                self.train_gnnwr_model(X, y, cluster_predictions, coords)
                return
        else:
            # ä½¿ç”¨åŸºç¡€ç‰ˆGNNWRï¼ˆæ— ç©ºé—´æƒé‡ï¼Œå†…å­˜å‹å¥½ï¼‰
            self.logger.info("ä½¿ç”¨åŸºç¡€ç‰ˆGNNWRè®­ç»ƒå™¨")

            # åˆ›å»ºæ•°æ®é›†
            dataset = SpatialDataset(gnnwr_features_imputed, y)

            # ä¿®å¤ï¼šåŸºç¡€ç‰ˆä¹Ÿä½¿ç”¨ä¼˜åŒ–é…ç½®
            train_loader = DataLoader(
                dataset,
                batch_size=batch_size,  # ä½¿ç”¨è‡ªé€‚åº”æ‰¹æ¬¡å¤§å°
                shuffle=True,
                num_workers=num_workers,  # æ·»åŠ å¤šçº¿ç¨‹æ”¯æŒ
                pin_memory=False,
                persistent_workers=num_workers > 0
            )

            # åˆå§‹åŒ–åŸºç¡€ç‰ˆGNNWRè®­ç»ƒå™¨
            input_dim = gnnwr_features_imputed.shape[1]
            self.gnnwr_trainer = GNNWRTrainer(
                input_dim=input_dim,
                hidden_dims=self.gnnwr_params['hidden_dims'],
                learning_rate=self.gnnwr_params['learning_rate']
            )

            # è®­ç»ƒæ¨¡å‹
            self.logger.info(f"å¼€å§‹åŸºç¡€ç‰ˆGNNWRè®­ç»ƒï¼Œè¾“å…¥ç»´åº¦: {input_dim}")
            self.gnnwr_trainer.train(
                train_loader,
                epochs=self.gnnwr_params['epochs'],
                patience=self.gnnwr_params['patience']
            )

        # è®¡ç®—è®­ç»ƒé›†æ€§èƒ½
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨ coords_copy è€Œä¸æ˜¯ coords
        y_pred = self.predict_with_gnnwr(gnnwr_features_imputed, None, coords_copy)
        mae = mean_absolute_error(y, y_pred)
        rmse = np.sqrt(mean_squared_error(y, y_pred))
        r_value, _ = pearsonr(y, y_pred)

        self.logger.info(f"GNNWRæ¨¡å‹è®­ç»ƒå®Œæˆ: MAE={mae:.3f}, RMSE={rmse:.3f}, R={r_value:.3f}")

    def predict_with_gnnwr(self, X, cluster_predictions=None, coords=None):
        """ä½¿ç”¨GNNWRè¿›è¡Œé¢„æµ‹ - ä¿®å¤ç‰ˆæœ¬"""
        if self.gnnwr_trainer is None:
            raise ValueError("GNNWRæ¨¡å‹å°šæœªè®­ç»ƒ")

        self.logger.info(f"é¢„æµ‹æ—¶ç‰¹å¾ç»´åº¦è°ƒè¯•:")
        self.logger.info(f"  Xå½¢çŠ¶: {X.shape}")

        # å…³é”®ä¿®å¤ï¼šå¦‚æœä¼ å…¥äº†cluster_predictionsï¼Œè¯´æ˜Xå·²ç»æ˜¯åŸå§‹ç‰¹å¾
        # éœ€è¦é‡æ–°åˆå¹¶ç‰¹å¾ï¼Œä½†è¦ç¡®ä¿ç»´åº¦ä¸€è‡´
        if cluster_predictions is not None:
            self.logger.info(f"  éœ€è¦åˆå¹¶cluster_predictions: {cluster_predictions.shape}")

            # æ£€æŸ¥Xçš„ç»´åº¦æ˜¯å¦å·²ç»åŒ…å«äº†èšç±»é¢„æµ‹
            expected_original_dim = X.shape[1] - self.n_clusters
            if X.shape[1] == expected_original_dim + self.n_clusters:
                # Xå·²ç»åŒ…å«äº†èšç±»é¢„æµ‹ï¼Œç›´æ¥ä½¿ç”¨
                gnnwr_features = X
                self.logger.info(f"  Xå·²ç»åŒ…å«èšç±»é¢„æµ‹ï¼Œç›´æ¥ä½¿ç”¨")
            else:
                # éœ€è¦åˆå¹¶
                gnnwr_features = np.hstack([X, cluster_predictions])
                self.logger.info(f"  åˆå¹¶åç‰¹å¾ç»´åº¦: {gnnwr_features.shape}")
        else:
            # å¦‚æœcluster_predictionsä¸ºNoneï¼Œè¯´æ˜Xå·²ç»æ˜¯åˆå¹¶åçš„ç‰¹å¾
            self.logger.info(f"  Xå·²ç»æ˜¯åˆå¹¶åçš„ç‰¹å¾")
            gnnwr_features = X

        # å¤„ç†ç¼ºå¤±å€¼
        if self.gnnwr_imputer is not None:
            gnnwr_features_imputed = self.gnnwr_imputer.transform(gnnwr_features)
        else:
            gnnwr_features_imputed = gnnwr_features

        # ç»´åº¦éªŒè¯
        expected_dim = self.gnnwr_trainer.model.feature_network[0].in_features
        actual_dim = gnnwr_features_imputed.shape[1]

        if actual_dim != expected_dim:
            self.logger.error(f"ç»´åº¦ä¸åŒ¹é…: è¾“å…¥ç‰¹å¾{actual_dim}ç»´, æ¨¡å‹æœŸæœ›{expected_dim}ç»´")
            raise ValueError(f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: è¾“å…¥{actual_dim} vs æ¨¡å‹{expected_dim}")

        # é¢„æµ‹
        if self.use_enhanced_gnnwr:
            return self.gnnwr_trainer.predict(gnnwr_features_imputed, coords)
        else:
            return self.gnnwr_trainer.predict(gnnwr_features_imputed)

    def validate_feature_dimensions(self, features, stage="training"):
        """éªŒè¯ç‰¹å¾ç»´åº¦ä¸€è‡´æ€§"""
        if self.gnnwr_trainer is None:
            return True

        # è·å–æ¨¡å‹æœŸæœ›çš„è¾“å…¥ç»´åº¦
        if hasattr(self.gnnwr_trainer.model, 'feature_network'):
            expected_dim = self.gnnwr_trainer.model.feature_network[0].in_features
            actual_dim = features.shape[1]

            if actual_dim != expected_dim:
                self.logger.error(f"{stage}é˜¶æ®µç»´åº¦ä¸åŒ¹é…: å®é™…{actual_dim}ç»´, æœŸæœ›{expected_dim}ç»´")
                return False

        return True

    def evaluate_predictions(self, y_true, y_pred):
        """è¯„ä¼°é¢„æµ‹æ€§èƒ½

        Args:
            y_true (np.array): çœŸå®å€¼
            y_pred (np.array): é¢„æµ‹å€¼

        Returns:
            dict: è¯„ä¼°æŒ‡æ ‡
        """
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r_value, p_value = pearsonr(y_true, y_pred)

        return {
            'MAE': mae,
            'RMSE': rmse,
            'R': r_value,
            'R_squared': r_value ** 2,
            'samples': len(y_true)
        }

    def cross_validate(self, X, y, groups, coords=None, cv_type='station'):
        """æ‰§è¡Œäº¤å‰éªŒè¯ - è¯¦ç»†åæ ‡è°ƒè¯•"""
        from sklearn.model_selection import LeaveOneGroupOut
        logo = LeaveOneGroupOut()

        all_predictions = []
        all_true_values = []
        fold_results = {}

        unique_groups = np.unique(groups)
        total_folds = len(unique_groups)

        self.logger.info(f"å¼€å§‹{cv_type}äº¤å‰éªŒè¯ï¼Œå…±{total_folds}ä¸ªæŠ˜å ...")
        self.logger.info(f"åˆå§‹åæ ‡çŠ¶æ€: {'å¯ç”¨' if coords is not None else 'ä¸å¯ç”¨'}")

        # åœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡Œä¸€æ¬¡èšç±»
        self.logger.info("åœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡Œèšç±»åˆ†é…...")
        self.cluster_assignments = self.perform_clustering(X, groups)

        for fold, (train_idx, test_idx) in enumerate(logo.split(X, y, groups)):
            group_id = groups[test_idx[0]]
            test_size = len(test_idx)
            train_size = len(train_idx)

            self.logger.info(f"=== Fold {fold + 1} è¯¦ç»†è°ƒè¯• ===")
            self.logger.info(f"è®­ç»ƒé›†å¤§å°: {train_size}, æµ‹è¯•é›†å¤§å°: {test_size}")

            # åˆ†å‰²æ•°æ®
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]
            groups_train, groups_test = groups[train_idx], groups[test_idx]

            # åˆ†å‰²åæ ‡ - è¯¦ç»†æ£€æŸ¥
            if coords is not None:
                coords_train = coords[train_idx]
                coords_test = coords[test_idx]

                self.logger.info(f"åæ ‡åˆ†å‰²ç»“æœ:")
                self.logger.info(f"  coords_trainç±»å‹: {type(coords_train)}")
                self.logger.info(f"  coords_trainå½¢çŠ¶: {coords_train.shape}")
                self.logger.info(f"  coords_testç±»å‹: {type(coords_test)}")
                self.logger.info(f"  coords_testå½¢çŠ¶: {coords_test.shape}")

                # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºæ•°ç»„
                if len(coords_train) == 0:
                    self.logger.error(f"âš ï¸  Fold {fold + 1}: coords_trainä¸ºç©ºæ•°ç»„!")
                if len(coords_test) == 0:
                    self.logger.error(f"âš ï¸  Fold {fold + 1}: coords_testä¸ºç©ºæ•°ç»„!")

            else:
                self.logger.error(f"âŒ Fold {fold + 1}: åˆå§‹coordsä¸ºNone!")
                coords_train = None
                coords_test = None

            # ä½¿ç”¨å›ºå®šçš„èšç±»åˆ†é…
            train_cluster_labels = self.cluster_assignments[train_idx]
            test_cluster_labels = self.cluster_assignments[test_idx]

            # è®­ç»ƒèšç±»é›†æˆæ¨¡å‹
            try:
                # ç¬¬ä¸€æ­¥ï¼šä¸ºæ¯ä¸ªèšç±»è®­ç»ƒæ¨¡å‹
                self.train_cluster_models(X_train, y_train, train_cluster_labels)

                # ç¬¬äºŒæ­¥ï¼šè·å–è®­ç»ƒé›†ä¸Šçš„èšç±»é¢„æµ‹
                cluster_predictions_train = self._get_cluster_predictions(X_train, train_cluster_labels)

                # ç¬¬ä¸‰æ­¥ï¼šè®­ç»ƒGNNWRé›†æˆæ¨¡å‹ - æ·»åŠ å‰ç½®æ£€æŸ¥
                if coords_train is None:
                    raise ValueError(f"Fold {fold + 1}: coords_trainä¸ºNoneï¼Œæ— æ³•è®­ç»ƒGNNWR")
                if len(coords_train) == 0:
                    raise ValueError(f"Fold {fold + 1}: coords_trainä¸ºç©ºæ•°ç»„ï¼Œæ— æ³•è®­ç»ƒGNNWR")

                self.train_gnnwr_model(X_train, y_train, cluster_predictions_train, coords_train)

                # ç¬¬å››æ­¥ï¼šé¢„æµ‹æµ‹è¯•é›† - å…³é”®ä¿®å¤
                cluster_predictions_test = self._get_cluster_predictions(X_test, test_cluster_labels)

                # å…³é”®ä¿®å¤ï¼šæµ‹è¯•é›†ç‰¹å¾ä¹Ÿéœ€è¦ä¸èšç±»é¢„æµ‹åˆå¹¶
                test_features_combined = np.hstack([X_test, cluster_predictions_test])
                self.logger.info(f"æµ‹è¯•é›†åˆå¹¶ç‰¹å¾å½¢çŠ¶: {test_features_combined.shape}")

                y_pred = self.predict_with_gnnwr(test_features_combined, None, coords_test)  # ç¬¬äºŒä¸ªå‚æ•°ä¼ None

                # å­˜å‚¨ç»“æœ
                all_predictions.extend(y_pred)
                all_true_values.extend(y_test)

                # è®¡ç®—å½“å‰æŠ˜å æ€§èƒ½
                fold_metrics = self.evaluate_predictions(y_test, y_pred)
                fold_results[group_id] = fold_metrics

                self.logger.info(
                    f"  {cv_type} Fold {fold + 1}/{total_folds}: {group_id} "
                    f"(èšç±»{test_cluster_labels[0]}, {test_size}æ ·æœ¬) - "
                    f"MAE={fold_metrics['MAE']:.3f}, R={fold_metrics['R']:.3f}"
                )

            except Exception as e:
                self.logger.error(f"æŠ˜å  {fold + 1} è®­ç»ƒå¤±è´¥: {e}")
                import traceback
                self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                continue

        # è®¡ç®—æ€»ä½“æ€§èƒ½
        overall_metrics = self.evaluate_predictions(
            np.array(all_true_values),
            np.array(all_predictions)
        )

        self.logger.info(f"âœ… {cv_type}äº¤å‰éªŒè¯å®Œæˆ")
        self.logger.info(f"  èšåˆæ€§èƒ½: MAE={overall_metrics['MAE']:.3f}mm, R={overall_metrics['R']:.3f}")

        return {
            'overall': overall_metrics,
            'by_fold': fold_results,
            'predictions': np.array(all_predictions),
            'true_values': np.array(all_true_values),
            'folds': total_folds,
            'cluster_assignments': self.cluster_assignments
        }

    def run_complete_analysis(self, df, output_dir=None):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹

        Args:
            df (pd.DataFrame): è¾“å…¥æ•°æ®
            output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„

        Returns:
            dict: åˆ†æç»“æœ
        """
        self.logger.info("=" * 70)
        self.logger.info("ğŸš€ å¼€å§‹SWEèšç±»é›†æˆå›å½’å®Œæ•´åˆ†ææµç¨‹")
        self.logger.info("=" * 70)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = f"./swe_cluster_ensemble_results_{timestamp}"

        os.makedirs(output_dir, exist_ok=True)
        self.logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")

        try:
            # 1. æ•°æ®é¢„å¤„ç†
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 1: æ•°æ®é¢„å¤„ç†")
            self.logger.info("=" * 50)

            X, y, station_groups, year_groups, coords = self.preprocess_data(df)

            results = {
                'preprocessing': {
                    'samples': len(X),
                    'features': len(self.feature_columns),
                    'stations': len(np.unique(station_groups)),
                    'years': len(np.unique(year_groups)),
                    'n_clusters': self.n_clusters,
                    'has_coords': coords is not None
                }
            }

            # 2. åœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡Œèšç±»
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 2: ç«™ç‚¹çº§èšç±»åˆ†æ")
            self.logger.info("=" * 50)

            self.cluster_assignments = self.perform_clustering(X, station_groups)
            results['cluster_assignments'] = self.cluster_assignments

            # # 3. ç«™ç‚¹äº¤å‰éªŒè¯ï¼ˆä½¿ç”¨å›ºå®šèšç±»ï¼‰
            # self.logger.info("\n" + "=" * 50)
            # self.logger.info("æ­¥éª¤ 3: ç«™ç‚¹äº¤å‰éªŒè¯")
            # self.logger.info("=" * 50)
            #
            # results['station_cv'] = self.cross_validate(X, y, station_groups, coords, 'station')

            # 4. å¹´åº¦äº¤å‰éªŒè¯ï¼ˆä½¿ç”¨å›ºå®šèšç±»ï¼‰
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 4: å¹´åº¦äº¤å‰éªŒè¯")
            self.logger.info("=" * 50)

            results['yearly_cv'] = self.cross_validate(X, y, year_groups, coords, 'yearly')

            # 5. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 5: è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
            self.logger.info("=" * 50)

            self.fit(X, y, station_groups, coords)

            results['final_model'] = {
                'kmeans': self.kmeans,
                'cluster_models': self.cluster_models,
                'gnnwr_trainer': self.gnnwr_trainer,
                'cluster_assignments': self.cluster_assignments,
                'feature_columns': self.feature_columns
            }

            # 6. ä¿å­˜ç»“æœ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 6: ä¿å­˜ç»“æœ")
            self.logger.info("=" * 50)

            self._save_results(results, output_dir)

            # 7. ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report(results)
            print(report)
            self.logger.info("ğŸ¯ èšç±»é›†æˆåˆ†æå®Œæˆï¼")
            return results

        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææµç¨‹å¤±è´¥: {str(e)}")
            raise

    def fit(self, X, y, station_groups, coords=None):
        """åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            y (np.array): ç›®æ ‡å˜é‡
            station_groups (np.array): ç«™ç‚¹åˆ†ç»„ä¿¡æ¯
            coords (np.array): åæ ‡æ•°æ®
        """
        self.logger.info("åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒèšç±»é›†æˆæ¨¡å‹...")

        # ç¬¬ä¸€æ­¥ï¼šåœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡Œèšç±»
        self.logger.info(f"åœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡ŒK-meansèšç±»ï¼Œèšç±»æ•°: {self.n_clusters}")
        self.cluster_assignments = self.perform_clustering(X, station_groups)

        # ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªèšç±»è®­ç»ƒæ¨¡å‹
        self.train_cluster_models(X, y, self.cluster_assignments)

        # ç¬¬ä¸‰æ­¥ï¼šè®­ç»ƒGNNWRé›†æˆæ¨¡å‹
        cluster_predictions = self._get_cluster_predictions(X, self.cluster_assignments)
        self.train_gnnwr_model(X, y, cluster_predictions, coords)

        self.logger.info("âœ… èšç±»é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")

    def predict(self, X, coords=None):
        """é¢„æµ‹æ–°æ ·æœ¬

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            coords (np.array): åæ ‡æ•°æ®

        Returns:
            np.array: é¢„æµ‹ç»“æœ
        """
        if self.kmeans is None or not self.cluster_models or self.gnnwr_trainer is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")

        # ç¬¬ä¸€æ­¥ï¼šèšç±»
        if np.isnan(X).any():
            imputer = SimpleImputer(strategy='median')
            X_imputed = imputer.fit_transform(X)
        else:
            X_imputed = X

        cluster_labels = self.kmeans.predict(X_imputed)

        # ç¬¬äºŒæ­¥ï¼šå„èšç±»æ¨¡å‹é¢„æµ‹
        cluster_predictions = np.zeros((len(X), self.n_clusters))

        for cluster_id, model in self.cluster_models.items():
            cluster_mask = cluster_labels == cluster_id
            if np.any(cluster_mask):
                cluster_predictions[cluster_mask, cluster_id] = model.predict(X[cluster_mask])

        # ç¬¬ä¸‰æ­¥ï¼šGNNWRé›†æˆé¢„æµ‹
        return self.predict_with_gnnwr(X, cluster_predictions, coords)

    def _save_results(self, results, output_dir):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•
        """
        self.logger.info("ä¿å­˜åˆ†æç»“æœ...")

        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(output_dir, 'swe_cluster_ensemble_model.pkl')
        joblib.dump({
            'kmeans': self.kmeans,
            'cluster_models': self.cluster_models,
            'gnnwr_trainer': self.gnnwr_trainer,
            'feature_columns': self.feature_columns,
            'params': self.params,
            'gnnwr_params': self.gnnwr_params,
            'n_clusters': self.n_clusters,
            'use_enhanced_gnnwr': self.use_enhanced_gnnwr
        }, model_path)

        # ä¿å­˜ç»“æœæ•°æ®
        results_path = os.path.join(output_dir, 'analysis_results.pkl')
        joblib.dump(results, results_path)

        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        report_path = os.path.join(output_dir, 'analysis_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(self._generate_report(results))

        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._create_visualizations(results, output_dir)

        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

    def _generate_report(self, results):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š - ä¿®å¤ç‰ˆæœ¬ï¼šå¤„ç†ç¼ºå¤±çš„station_cv"""
        report = []
        report.append("=" * 70)
        report.append("â„ï¸ SWEèšç±»é›†æˆå›å½’åˆ†ææŠ¥å‘Š")
        report.append("=" * 70)
        report.append("")

        # æ•°æ®æ¦‚å†µ
        preprocessing = results['preprocessing']
        report.append("ğŸ“Š æ•°æ®æ¦‚å†µ:")
        report.append(f"  æ ·æœ¬æ•°é‡: {preprocessing['samples']}")
        report.append(f"  ç‰¹å¾æ•°é‡: {preprocessing['features']}")
        report.append(f"  ç«™ç‚¹æ•°é‡: {preprocessing['stations']}")
        report.append(f"  å¹´ä»½æ•°é‡: {preprocessing['years']}")
        report.append(f"  èšç±»æ•°é‡: {preprocessing['n_clusters']}")
        report.append(f"  ä½¿ç”¨åæ ‡: {'æ˜¯' if preprocessing['has_coords'] else 'å¦'}")
        report.append(f"  GNNWRç‰ˆæœ¬: {'å¢å¼ºç‰ˆ' if self.use_enhanced_gnnwr else 'åŸºç¡€ç‰ˆ'}")
        report.append("")

        # ç«™ç‚¹äº¤å‰éªŒè¯ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'station_cv' in results:
            station_cv = results['station_cv']
            station_overall = station_cv['overall']
            report.append("ğŸ”ï¸ ç«™ç‚¹äº¤å‰éªŒè¯ç»“æœ:")
            report.append(f"  æŠ˜å æ•°é‡: {station_cv['folds']}")
            report.append(f"  MAE: {station_overall['MAE']:.3f} mm")
            report.append(f"  RMSE: {station_overall['RMSE']:.3f} mm")
            report.append(f"  R: {station_overall['R']:.3f}")
            report.append(f"  RÂ²: {station_overall['R_squared']:.3f}")
            report.append("")
        else:
            report.append("ğŸ”ï¸ ç«™ç‚¹äº¤å‰éªŒè¯: å·²è·³è¿‡")
            report.append("")

        # å¹´åº¦äº¤å‰éªŒè¯ç»“æœ
        yearly_cv = results['yearly_cv']
        yearly_overall = yearly_cv['overall']
        report.append("ğŸ“… å¹´åº¦äº¤å‰éªŒè¯ç»“æœ:")
        report.append(f"  æŠ˜å æ•°é‡: {yearly_cv['folds']}")
        report.append(f"  MAE: {yearly_overall['MAE']:.3f} mm")
        report.append(f"  RMSE: {yearly_overall['RMSE']:.3f} mm")
        report.append(f"  R: {yearly_overall['R']:.3f}")
        report.append(f"  RÂ²: {yearly_overall['R_squared']:.3f}")
        report.append("")

        # èšç±»åˆ†å¸ƒ
        cluster_counts = np.bincount(results['cluster_assignments'])
        report.append("ğŸ” èšç±»åˆ†å¸ƒ:")
        for cluster_id, count in enumerate(cluster_counts):
            report.append(
                f"  èšç±» {cluster_id}: {count} ä¸ªæ ·æœ¬ ({count / len(results['cluster_assignments']) * 100:.1f}%)")
        report.append("")

        report.append("ğŸ¯ æ¨¡å‹é…ç½®:")
        report.append(f"  åŸºç¡€æ¨¡å‹: {'éšæœºæ£®æ—' if self.use_rf else 'XGBoost'}")
        if hasattr(self, 'params') and self.params:
            report.append(f"  æ¨¡å‹å‚æ•°: {self.params}")
        report.append(f"  GNNWRå‚æ•°: {self.gnnwr_params}")

        return "\n".join(report)

    def _create_visualizations(self, results, output_dir):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨ - ä½¿ç”¨è‹±æ–‡æ ‡ç­¾"""
        self.logger.info("Generating visualizations...")

        try:
            # æ£€æŸ¥å¿…è¦çš„é”®æ˜¯å¦å­˜åœ¨
            if 'yearly_cv' not in results:
                self.logger.warning("Missing yearly CV results, skipping visualization")
                return

            plt.figure(figsize=(12, 10))

            # 1. å¹´åº¦äº¤å‰éªŒè¯æ•£ç‚¹å›¾
            plt.subplot(2, 2, 1)
            yearly_cv = results['yearly_cv']
            y_true_yearly = yearly_cv['true_values']
            y_pred_yearly = yearly_cv['predictions']

            plt.scatter(y_true_yearly, y_pred_yearly, alpha=0.6, s=20, color='orange')
            plt.plot([y_true_yearly.min(), y_true_yearly.max()],
                     [y_true_yearly.min(), y_true_yearly.max()], 'r--', alpha=0.8)
            plt.xlabel('True SWE (mm)')
            plt.ylabel('Predicted SWE (mm)')
            plt.title(
                f'Yearly Cross-Validation\nMAE={yearly_cv["overall"]["MAE"]:.2f}, R={yearly_cv["overall"]["R"]:.3f}')
            plt.grid(True, alpha=0.3)

            # 2. æ®‹å·®åˆ†å¸ƒå›¾
            plt.subplot(2, 2, 2)
            residuals = y_true_yearly - y_pred_yearly
            plt.hist(residuals, bins=30, alpha=0.7, color='skyblue')
            plt.xlabel('Residuals (mm)')
            plt.ylabel('Frequency')
            plt.title('Residual Distribution')
            plt.grid(True, alpha=0.3)

            # 3. èšç±»åˆ†å¸ƒå›¾
            plt.subplot(2, 2, 3)
            if 'cluster_assignments' in results:
                cluster_assignments = results['cluster_assignments']
                cluster_counts = np.bincount(cluster_assignments)
                colors = plt.cm.Set3(np.linspace(0, 1, len(cluster_counts)))

                bars = plt.bar(range(len(cluster_counts)), cluster_counts, color=colors)
                plt.xlabel('Cluster ID')
                plt.ylabel('Sample Count')
                plt.title('Cluster Distribution')
                plt.xticks(range(len(cluster_counts)))

                # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, count in zip(bars, cluster_counts):
                    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                             f'{count}', ha='center', va='bottom')
            else:
                plt.text(0.5, 0.5, 'No Cluster Data', ha='center', va='center', transform=plt.gca().transAxes)
                plt.title('Cluster Distribution')

            # 4. æ€§èƒ½å›¾
            plt.subplot(2, 2, 4)
            yearly = results['yearly_cv']['overall']
            metrics = ['MAE', 'RMSE', 'R']
            values = [yearly['MAE'], yearly['RMSE'], yearly['R']]
            colors = ['skyblue', 'lightgreen', 'lightcoral']

            bars = plt.bar(metrics, values, color=colors, alpha=0.7)
            plt.ylabel('Value')
            plt.title('Yearly CV Performance')

            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                         f'{value:.3f}', ha='center', va='bottom')

            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'performance_visualization.png'),
                        dpi=300, bbox_inches='tight')
            plt.close()

            self.logger.info("âœ… Visualization completed")

        except Exception as e:
            self.logger.warning(f"Visualization failed: {e}")


def get_feature_importance(self):
    """è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆåŸºäºå„èšç±»æ¨¡å‹çš„å¹³å‡é‡è¦æ€§ï¼‰"""
    if not self.cluster_models:
        raise ValueError("èšç±»æ¨¡å‹å°šæœªè®­ç»ƒ")

    # æ”¶é›†æ‰€æœ‰ç‰¹å¾çš„é‡è¦æ€§
    all_importances = []
    for cluster_id, model in self.cluster_models.items():
        importance_scores = model.feature_importances_
        all_importances.append(importance_scores)

    # è®¡ç®—å¹³å‡é‡è¦æ€§
    avg_importance = np.mean(all_importances, axis=0)

    # åˆ›å»ºDataFrame
    feature_importance_df = pd.DataFrame({
        'feature': self.feature_columns,
        'importance': avg_importance
    }).sort_values('importance', ascending=False)

    self.logger.info(f"ç‰¹å¾é‡è¦æ€§è®¡ç®—å®Œæˆï¼Œæœ€é«˜é‡è¦æ€§ç‰¹å¾: {feature_importance_df['feature'].iloc[0]}")
    return feature_importance_df


def analyze_cluster_characteristics(self, df):
    """åˆ†æå„èšç±»çš„ç‰¹å¾

    Args:
        df (pd.DataFrame): åŸå§‹æ•°æ®

    Returns:
        dict: èšç±»åˆ†æç»“æœ
    """
    if self.cluster_assignments is None:
        raise ValueError("èšç±»å°šæœªæ‰§è¡Œ")

    self.logger.info("åˆ†æå„èšç±»ç‰¹å¾...")

    cluster_stats = {}
    feature_cols = [col for col in self.feature_columns if col in df.columns]

    for cluster_id in range(self.n_clusters):
        cluster_mask = self.cluster_assignments == cluster_id
        cluster_data = df[cluster_mask]
        cluster_size = len(cluster_data)

        if cluster_size == 0:
            continue

        stats = {
            'size': cluster_size,
            'swe_mean': cluster_data[self.target_column].mean(),
            'swe_std': cluster_data[self.target_column].std(),
            'features': {}
        }

        # è®¡ç®—å„ç‰¹å¾çš„ç»Ÿè®¡é‡
        for feature in feature_cols:
            if feature in cluster_data.columns:
                stats['features'][feature] = {
                    'mean': cluster_data[feature].mean(),
                    'std': cluster_data[feature].std(),
                    'median': cluster_data[feature].median()
                }

        cluster_stats[cluster_id] = stats

        self.logger.info(f"  èšç±» {cluster_id}: {cluster_size}æ ·æœ¬, SWEå‡å€¼={stats['swe_mean']:.2f}mm")

    return cluster_stats


def create_cluster_analysis_report(self, df, output_dir):
    """åˆ›å»ºèšç±»åˆ†ææŠ¥å‘Š

    Args:
        df (pd.DataFrame): åŸå§‹æ•°æ®
        output_dir (str): è¾“å‡ºç›®å½•
    """
    try:
        self.logger.info("åˆ›å»ºèšç±»åˆ†ææŠ¥å‘Š...")

        # è·å–èšç±»ç»Ÿè®¡
        cluster_stats = self.analyze_cluster_characteristics(df)

        # åˆ›å»ºèšç±»ç‰¹å¾å¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. èšç±»å¤§å°åˆ†å¸ƒ
        cluster_sizes = [stats['size'] for stats in cluster_stats.values()]
        cluster_ids = list(cluster_stats.keys())

        axes[0, 0].bar(cluster_ids, cluster_sizes, color=plt.cm.Set3(np.linspace(0, 1, len(cluster_ids))))
        axes[0, 0].set_title('å„èšç±»æ ·æœ¬æ•°é‡')
        axes[0, 0].set_xlabel('èšç±»ID')
        axes[0, 0].set_ylabel('æ ·æœ¬æ•°é‡')
        for i, v in enumerate(cluster_sizes):
            axes[0, 0].text(i, v, str(v), ha='center', va='bottom')

        # 2. å„èšç±»SWEå‡å€¼
        swe_means = [stats['swe_mean'] for stats in cluster_stats.values()]
        axes[0, 1].bar(cluster_ids, swe_means, color=plt.cm.Set3(np.linspace(0, 1, len(cluster_ids))))
        axes[0, 1].set_title('å„èšç±»SWEå‡å€¼')
        axes[0, 1].set_xlabel('èšç±»ID')
        axes[0, 1].set_ylabel('SWEå‡å€¼ (mm)')
        for i, v in enumerate(swe_means):
            axes[0, 1].text(i, v, f'{v:.1f}', ha='center', va='bottom')

        # 3. é‡è¦ç‰¹å¾åœ¨å„èšç±»çš„åˆ†å¸ƒ
        feature_importance = self.get_feature_importance()
        top_features = feature_importance.head(3)['feature'].tolist()

        for i, feature in enumerate(top_features):
            if i >= 2:  # åªæ˜¾ç¤ºå‰ä¸¤ä¸ªç‰¹å¾
                break
            feature_means = []
            for cluster_id, stats in cluster_stats.items():
                if feature in stats['features']:
                    feature_means.append(stats['features'][feature]['mean'])
                else:
                    feature_means.append(0)

            axes[1, i].bar(cluster_ids, feature_means,
                           color=plt.cm.Set3(np.linspace(0, 1, len(cluster_ids))))
            axes[1, i].set_title(f'{feature}åœ¨å„èšç±»çš„å‡å€¼')
            axes[1, i].set_xlabel('èšç±»ID')
            axes[1, i].set_ylabel(f'{feature}å‡å€¼')

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'cluster_analysis.png'),
                    dpi=300, bbox_inches='tight')
        plt.close()

        # ä¿å­˜èšç±»ç»Ÿè®¡åˆ°CSV
        cluster_report = []
        for cluster_id, stats in cluster_stats.items():
            row = {
                'cluster_id': cluster_id,
                'size': stats['size'],
                'swe_mean': stats['swe_mean'],
                'swe_std': stats['swe_std']
            }

            # æ·»åŠ é‡è¦ç‰¹å¾ä¿¡æ¯
            for feature in top_features:
                if feature in stats['features']:
                    row[f'{feature}_mean'] = stats['features'][feature]['mean']
                    row[f'{feature}_std'] = stats['features'][feature]['std']
                else:
                    row[f'{feature}_mean'] = np.nan
                    row[f'{feature}_std'] = np.nan

            cluster_report.append(row)

        cluster_df = pd.DataFrame(cluster_report)
        cluster_df.to_csv(os.path.join(output_dir, 'cluster_statistics.csv'), index=False)

        self.logger.info(f"âœ… èšç±»åˆ†ææŠ¥å‘Šä¿å­˜å®Œæˆ")

    except Exception as e:
        self.logger.warning(f"åˆ›å»ºèšç±»åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")


def compare_with_baseline(self, df, output_dir):
    """ä¸åŸºçº¿æ¨¡å‹æ¯”è¾ƒ

    Args:
        df (pd.DataFrame): åŸå§‹æ•°æ®
        output_dir (str): è¾“å‡ºç›®å½•
    """
    try:
        self.logger.info("ä¸åŸºçº¿æ¨¡å‹æ¯”è¾ƒ...")

        # é¢„å¤„ç†æ•°æ®
        X, y, station_groups, year_groups, coords = self.preprocess_data(df)

        # è®­ç»ƒæ™®é€šXGBoostæ¨¡å‹ä½œä¸ºåŸºçº¿
        from swe_trainer import SWEXGBoostTrainer
        baseline_trainer = SWEXGBoostTrainer(params=self.params)

        # ç«™ç‚¹äº¤å‰éªŒè¯
        baseline_station_results = baseline_trainer.cross_validate(X, y, station_groups, 'station')
        baseline_yearly_results = baseline_trainer.cross_validate(X, y, year_groups, 'yearly')

        # æ¯”è¾ƒç»“æœ
        comparison = {
            'station_cv': {
                'baseline_mae': baseline_station_results['overall']['MAE'],
                'ensemble_mae': self.cross_validate(X, y, station_groups, coords, 'station')['overall']['MAE'],
                'baseline_r': baseline_station_results['overall']['R'],
                'ensemble_r': self.cross_validate(X, y, station_groups, coords, 'station')['overall']['R'],
                'improvement_mae': (baseline_station_results['overall']['MAE'] -
                                    self.cross_validate(X, y, station_groups, coords, 'station')['overall']['MAE']),
                'improvement_r': (self.cross_validate(X, y, station_groups, coords, 'station')['overall']['R'] -
                                  baseline_station_results['overall']['R'])
            },
            'yearly_cv': {
                'baseline_mae': baseline_yearly_results['overall']['MAE'],
                'ensemble_mae': self.cross_validate(X, y, year_groups, coords, 'yearly')['overall']['MAE'],
                'baseline_r': baseline_yearly_results['overall']['R'],
                'ensemble_r': self.cross_validate(X, y, year_groups, coords, 'yearly')['overall']['R'],
                'improvement_mae': (baseline_yearly_results['overall']['MAE'] -
                                    self.cross_validate(X, y, year_groups, coords, 'yearly')['overall']['MAE']),
                'improvement_r': (self.cross_validate(X, y, year_groups, coords, 'yearly')['overall']['R'] -
                                  baseline_yearly_results['overall']['R'])
            }
        }

        # åˆ›å»ºæ¯”è¾ƒå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

        # MAEæ¯”è¾ƒ
        methods = ['åŸºçº¿', 'èšç±»é›†æˆ']
        station_mae = [comparison['station_cv']['baseline_mae'], comparison['station_cv']['ensemble_mae']]
        yearly_mae = [comparison['yearly_cv']['baseline_mae'], comparison['yearly_cv']['ensemble_mae']]

        x = np.arange(len(methods))
        width = 0.35

        ax1.bar(x - width / 2, station_mae, width, label='ç«™ç‚¹CV', alpha=0.7)
        ax1.bar(x + width / 2, yearly_mae, width, label='å¹´åº¦CV', alpha=0.7)
        ax1.set_xlabel('æ¨¡å‹ç±»å‹')
        ax1.set_ylabel('MAE (mm)')
        ax1.set_title('MAEæ¯”è¾ƒ')
        ax1.set_xticks(x)
        ax1.set_xticklabels(methods)
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # Rå€¼æ¯”è¾ƒ
        station_r = [comparison['station_cv']['baseline_r'], comparison['station_cv']['ensemble_r']]
        yearly_r = [comparison['yearly_cv']['baseline_r'], comparison['yearly_cv']['ensemble_r']]

        ax2.bar(x - width / 2, station_r, width, label='ç«™ç‚¹CV', alpha=0.7)
        ax2.bar(x + width / 2, yearly_r, width, label='å¹´åº¦CV', alpha=0.7)
        ax2.set_xlabel('æ¨¡å‹ç±»å‹')
        ax2.set_ylabel('R')
        ax2.set_title('ç›¸å…³ç³»æ•°æ¯”è¾ƒ')
        ax2.set_xticks(x)
        ax2.set_xticklabels(methods)
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'baseline_comparison.png'),
                    dpi=300, bbox_inches='tight')
        plt.close()

        # ä¿å­˜æ¯”è¾ƒç»“æœ
        comparison_df = pd.DataFrame([
            {
                'method': 'station_cv',
                'baseline_mae': comparison['station_cv']['baseline_mae'],
                'ensemble_mae': comparison['station_cv']['ensemble_mae'],
                'improvement_mae': comparison['station_cv']['improvement_mae'],
                'baseline_r': comparison['station_cv']['baseline_r'],
                'ensemble_r': comparison['station_cv']['ensemble_r'],
                'improvement_r': comparison['station_cv']['improvement_r']
            },
            {
                'method': 'yearly_cv',
                'baseline_mae': comparison['yearly_cv']['baseline_mae'],
                'ensemble_mae': comparison['yearly_cv']['ensemble_mae'],
                'improvement_mae': comparison['yearly_cv']['improvement_mae'],
                'baseline_r': comparison['yearly_cv']['baseline_r'],
                'ensemble_r': comparison['yearly_cv']['ensemble_r'],
                'improvement_r': comparison['yearly_cv']['improvement_r']
            }
        ])

        comparison_df.to_csv(os.path.join(output_dir, 'baseline_comparison.csv'), index=False)

        self.logger.info("âœ… åŸºçº¿æ¯”è¾ƒå®Œæˆ")
        return comparison

    except Exception as e:
        self.logger.warning(f"åŸºçº¿æ¯”è¾ƒå¤±è´¥: {e}")
        return None


# ä¾¿æ·ä½¿ç”¨å‡½æ•°
def train_swe_cluster_ensemble(data_df, output_dir=None, n_clusters=4, params=None, use_rf=False,
                               use_enhanced_gnnwr=True, gnnwr_params=None, device='auto'):
    """ä¾¿æ·å‡½æ•°ï¼šè®­ç»ƒSWEèšç±»é›†æˆæ¨¡å‹

    Args:
        data_df (pd.DataFrame): åŒ…å«ç‰¹å¾å’ŒSWEçš„æ•°æ®
        output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„
        n_clusters (int, optional): èšç±»æ•°é‡
        params (dict, optional): XGBoostå‚æ•°
        use_enhanced_gnnwr (bool): æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰ˆGNNWR
        gnnwr_params (dict): GNNWRå‚æ•°

    Returns:
        dict: åŒ…å«æ‰€æœ‰è®­ç»ƒç»“æœçš„å­—å…¸
    """
    trainer = SWEClusterEnsemble(
        n_clusters=n_clusters,
        params=params,
        gnnwr_params=gnnwr_params,
        use_enhanced_gnnwr=use_enhanced_gnnwr,
        use_rf = use_rf, # ä¼ é€’è¿™ä¸ªå‚æ•°
        device = device  # æ·»åŠ deviceå‚æ•°
    )
    return trainer.run_complete_analysis(data_df, output_dir)


def load_swe_cluster_ensemble(model_path):
    """åŠ è½½å·²è®­ç»ƒçš„SWEèšç±»é›†æˆæ¨¡å‹

    Args:
        model_path (str): æ¨¡å‹æ–‡ä»¶è·¯å¾„

    Returns:
        SWEClusterEnsemble: åŠ è½½çš„æ¨¡å‹å®ä¾‹
    """
    model_data = joblib.load(model_path)

    trainer = SWEClusterEnsemble(
        n_clusters=model_data['n_clusters'],
        params=model_data['params'],
        gnnwr_params=model_data['gnnwr_params'],
        use_enhanced_gnnwr=model_data.get('use_enhanced_gnnwr', True)
    )

    trainer.kmeans = model_data['kmeans']
    trainer.cluster_models = model_data['cluster_models']
    trainer.gnnwr_trainer = model_data['gnnwr_trainer']
    trainer.feature_columns = model_data['feature_columns']
    trainer.cluster_assignments = model_data.get('cluster_assignments')

    # æ¢å¤RFå‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'rf_params' in model_data:
        trainer.rf_params = model_data['rf_params']

    return trainer


# æµ‹è¯•å‡½æ•°
def test_cluster_ensemble():
    """æµ‹è¯•èšç±»é›†æˆæ¨¡å‹"""
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    n_features = 10

    # ç”Ÿæˆç©ºé—´åæ ‡
    coords = np.random.uniform(0, 100, (n_samples, 2))

    # ç”Ÿæˆç‰¹å¾
    features = np.random.randn(n_samples, n_features)

    # åˆ›å»ºå…·æœ‰ç©ºé—´ç›¸å…³æ€§çš„ç›®æ ‡å˜é‡
    spatial_effect = np.exp(-0.01 * coords[:, 0]) + np.sin(0.1 * coords[:, 1])
    targets = (features[:, 0] + 2 * features[:, 1] + 0.5 * spatial_effect +
               np.random.normal(0, 0.1, n_samples))

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®æ¡†
    df = pd.DataFrame(features, columns=[f'feature_{i}' for i in range(n_features)])
    df['swe'] = targets
    df['station_id'] = [f'station_{i % 20}' for i in range(n_samples)]
    df['year'] = np.random.randint(2018, 2023, n_samples)
    df['longitude'] = coords[:, 0]
    df['latitude'] = coords[:, 1]

    # è®­ç»ƒæ¨¡å‹
    results = train_swe_cluster_ensemble(
        data_df=df,
        n_clusters=3,
        use_enhanced_gnnwr=True
    )

    return results


class PureGNNWRModel(nn.Module):
    """çº¯å‡€ç‰ˆGNNWRæ¨¡å‹ - ç›´æ¥ç‰¹å¾è¾“å…¥ï¼Œä¸“æ³¨æ·±åº¦å­¦ä¹ ä¼˜åŒ–"""

    def __init__(self, input_dim, hidden_dims=[128, 64, 32, 16], output_dim=1,
                 dropout_rate=0.3, use_batch_norm=True):
        super(PureGNNWRModel, self).__init__()

        # æ·±åº¦ç‰¹å¾æå–ç½‘ç»œ
        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))

            if use_batch_norm:
                layers.append(nn.BatchNorm1d(hidden_dim))

            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))
            prev_dim = hidden_dim

        self.feature_network = nn.Sequential(*layers)

        # è¾“å‡ºå±‚
        self.output_layer = nn.Sequential(
            nn.Linear(prev_dim, prev_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate / 2),
            nn.Linear(prev_dim // 2, output_dim)
        )

    def forward(self, x, spatial_weights=None, coords=None):
        # ç‰¹å¾æå–
        features = self.feature_network(x)

        # ç©ºé—´å¹³æ»‘ï¼ˆå¦‚æœæä¾›äº†ç©ºé—´æƒé‡ï¼‰
        if spatial_weights is not None:
            # ç©ºé—´å¹³æ»‘ï¼šæ¯ä¸ªä½ç½®çš„ç‰¹å¾æ˜¯å…¶é‚»è¿‘ä½ç½®çš„åŠ æƒå¹³å‡
            row_sums = torch.sum(spatial_weights, dim=1, keepdim=True)
            normalized_weights = spatial_weights / torch.where(row_sums > 0, row_sums, torch.tensor(1.0))
            smoothed_features = torch.matmul(normalized_weights, features)
            output = self.output_layer(smoothed_features)
        else:
            output = self.output_layer(features)

        return output.squeeze()


class PureGNNWRTrainer:
    """çº¯å‡€ç‰ˆGNNWRè®­ç»ƒå™¨ - å…¨å¥—æ·±åº¦å­¦ä¹ ä¼˜åŒ–"""

    def __init__(self, input_dim, coords, hidden_dims=[128, 64, 32, 16],
                 learning_rate=0.001, bandwidth=10.0, dropout_rate=0.3,
                 weight_decay=1e-4, device='auto', output_std_penalty=0.01):

        # è®¾å¤‡è®¾ç½®
        if device == 'auto':
            self.device = torch.device('cpu')
            torch.set_num_threads(16)
        else:
            self.device = torch.device(device)

        self.output_std_penalty = output_std_penalty
        self.logger = logging.getLogger("PureGNNWR")
        self.logger.info(f"çº¯å‡€ç‰ˆGNNWR - ä½¿ç”¨è®¾å¤‡: {self.device}")

        # æ¨¡å‹åˆå§‹åŒ–
        self.model = PureGNNWRModel(
            input_dim=input_dim,
            hidden_dims=hidden_dims,
            dropout_rate=dropout_rate
        ).to(self.device)

        # ä¼˜åŒ–å™¨ - ä½¿ç”¨AdamW
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay,
            betas=(0.9, 0.999)
        )

        self.criterion = nn.HuberLoss()

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=10
        )

        self.criterion = nn.HuberLoss()  # ä½¿ç”¨HuberLossæ›´ç¨³å®š

        # ç©ºé—´æƒé‡è®¡ç®—
        self.coords = coords.copy() if coords is not None else None
        self.bandwidth = bandwidth

    def _compute_spatial_weights(self, batch_coords):
        """è®¡ç®—ç©ºé—´æƒé‡çŸ©é˜µ"""
        n_batch = batch_coords.shape[0]
        if n_batch <= 1:
            return torch.ones((n_batch, n_batch), device=self.device)

        # è®¡ç®—æ¬§æ°è·ç¦»
        diff = batch_coords.unsqueeze(1) - batch_coords.unsqueeze(0)
        distances = torch.sqrt(torch.sum(diff ** 2, dim=2) + 1e-8)

        # é«˜æ–¯æ ¸å‡½æ•°
        weights = torch.exp(-0.5 * (distances / self.bandwidth) ** 2)

        return weights

    def train(self, train_loader, val_loader=None, epochs=200, early_stopping_patience=20):
        """å®Œæ•´æ·±åº¦å­¦ä¹ è®­ç»ƒæµç¨‹ - ä¿®å¤ç‰ˆæœ¬"""

        self.model.train()
        best_val_loss = float('inf')
        patience_counter = 0
        train_losses = []
        val_losses = []

        pbar = tqdm(range(epochs), desc="è®­ç»ƒè¿›åº¦")

        for epoch in pbar:
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            epoch_train_loss = 0.0
            batch_count = 0

            for batch in train_loader:
                if len(batch) == 3:
                    batch_features, batch_targets, batch_coords = batch
                else:
                    batch_features, batch_targets = batch
                    batch_coords = None

                batch_features = batch_features.to(self.device)
                batch_targets = batch_targets.to(self.device)

                # é‡è¦ï¼šæ¯æ¬¡è¿­ä»£å‰æ¸…é›¶æ¢¯åº¦
                self.optimizer.zero_grad()

                # è®¡ç®—ç©ºé—´æƒé‡ï¼ˆå¦‚æœæœ‰åæ ‡ï¼‰
                spatial_weights = None
                if batch_coords is not None:
                    batch_coords = batch_coords.to(self.device)
                    spatial_weights = self._compute_spatial_weights(batch_coords)

                # å‰å‘ä¼ æ’­
                outputs = self.model(batch_features, spatial_weights, batch_coords)

                # è®¡ç®—ä¸»æŸå¤±
                main_loss = self.criterion(outputs, batch_targets)

                # æ·»åŠ è¾“å‡ºå¤šæ ·æ€§æƒ©ç½šï¼ˆé˜²æ­¢è¾“å‡ºæ’å®šï¼‰
                output_std = torch.std(outputs)
                diversity_loss = -self.output_std_penalty * output_std  # é¼“åŠ±è¾“å‡ºæœ‰æ–¹å·®

                # æ€»æŸå¤±
                total_loss = main_loss + diversity_loss

                # é‡è¦ï¼šåªè°ƒç”¨ä¸€æ¬¡ backward() å’Œ step()
                total_loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                # æ›´æ–°å‚æ•°
                self.optimizer.step()

                # åªè®°å½•ä¸»æŸå¤±ç”¨äºæ˜¾ç¤º
                epoch_train_loss += main_loss.item()
                batch_count += 1

            # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
            epoch_train_loss /= len(train_loader)
            train_losses.append(epoch_train_loss)

            # éªŒè¯é˜¶æ®µ
            if val_loader is not None:
                val_loss = self.validate(val_loader)
                val_losses.append(val_loss)

                # å­¦ä¹ ç‡è°ƒåº¦
                self.scheduler.step(val_loss)

                # æ›´æ–°è¿›åº¦æ¡
                current_lr = self.optimizer.param_groups[0]['lr']
                pbar.set_postfix({
                    'train_loss': f'{epoch_train_loss:.4f}',
                    'val_loss': f'{val_loss:.4f}',
                    'lr': f'{current_lr:.2e}',
                    'patience': f'{patience_counter}/{early_stopping_patience}'
                })

                # æ—©åœé€»è¾‘
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    torch.save(self.model.state_dict(), 'best_pure_gnnwr_model.pth')
                else:
                    patience_counter += 1

                if patience_counter >= early_stopping_patience:
                    pbar.set_description("è®­ç»ƒå®Œæˆ (æ—©åœ)")
                    self.logger.info(f"æ—©åœåœ¨epoch {epoch}, æœ€ä½³éªŒè¯loss: {best_val_loss:.6f}")
                    # åŠ è½½æœ€ä½³æ¨¡å‹
                    self.model.load_state_dict(torch.load('best_pure_gnnwr_model.pth'))
                    break
            else:
                # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œä½¿ç”¨è®­ç»ƒloss
                self.scheduler.step(epoch_train_loss)

                # æ›´æ–°è¿›åº¦æ¡ï¼ˆæ— éªŒè¯é›†ç‰ˆæœ¬ï¼‰
                current_lr = self.optimizer.param_groups[0]['lr']
                pbar.set_postfix({
                    'train_loss': f'{epoch_train_loss:.4f}',
                    'lr': f'{current_lr:.2e}',
                    'patience': f'{patience_counter}/{early_stopping_patience}'
                })

            # æ—¥å¿—è¾“å‡º
            if epoch % 10 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                if val_loader is not None:
                    self.logger.info(f"Epoch {epoch:3d} | Train Loss: {epoch_train_loss:.6f} | "
                                     f"Val Loss: {val_loss:.6f} | LR: {current_lr:.2e}")
                else:
                    self.logger.info(f"Epoch {epoch:3d} | Train Loss: {epoch_train_loss:.6f} | "
                                     f"LR: {current_lr:.2e}")

        pbar.close()

        return train_losses, val_losses

    def validate(self, val_loader):
        """éªŒè¯é›†è¯„ä¼°"""
        self.model.eval()
        val_loss = 0.0

        with torch.no_grad():
            for batch in val_loader:
                if len(batch) == 3:
                    batch_features, batch_targets, batch_coords = batch
                else:
                    batch_features, batch_targets = batch
                    batch_coords = None

                batch_features = batch_features.to(self.device)
                batch_targets = batch_targets.to(self.device)

                spatial_weights = None
                if batch_coords is not None:
                    batch_coords = batch_coords.to(self.device)
                    spatial_weights = self._compute_spatial_weights(batch_coords)

                outputs = self.model(batch_features, spatial_weights, batch_coords)
                loss = self.criterion(outputs, batch_targets)
                val_loss += loss.item()

        return val_loss / len(val_loader)

    def predict(self, features, coords=None):
        """é¢„æµ‹"""
        self.model.eval()

        with torch.no_grad():
            features_tensor = torch.FloatTensor(features).to(self.device)

            # åˆ†æ‰¹é¢„æµ‹é¿å…å†…å­˜æº¢å‡º
            batch_size = 1024
            predictions = []

            for i in range(0, len(features), batch_size):
                batch_features = features_tensor[i:i + batch_size]

                spatial_weights = None
                if coords is not None:
                    batch_coords = torch.FloatTensor(coords[i:i + batch_size]).to(self.device)
                    spatial_weights = self._compute_spatial_weights(batch_coords)

                batch_pred = self.model(batch_features, spatial_weights, batch_coords)
                predictions.append(batch_pred.cpu().numpy())

            return np.concatenate(predictions)


def train_pure_gnnwr_analysis(df, output_dir=None, test_size=0.2, random_state=42):
    """
    è¿è¡Œçº¯å‡€ç‰ˆGNNWRåˆ†æ - åŒ…å«å®Œæ•´äº¤å‰éªŒè¯
    """
    from sklearn.model_selection import train_test_split, LeaveOneGroupOut
    from sklearn.metrics import mean_absolute_error, mean_squared_error
    from scipy.stats import pearsonr
    import numpy as np

    logger = logging.getLogger("PureGNNWRAnalysis")
    logger.info("=" * 60)
    logger.info("ğŸš€ å¼€å§‹çº¯å‡€ç‰ˆGNNWRå®Œæ•´åˆ†ææµç¨‹")
    logger.info("=" * 60)

    try:
        # ä½¿ç”¨SWEClusterEnsembleçš„æ•°æ®é¢„å¤„ç†
        ensemble = SWEClusterEnsemble(n_clusters=1)  # ä¸´æ—¶å®ä¾‹ç”¨äºæ•°æ®é¢„å¤„ç†
        X, y, station_groups, year_groups, coords = ensemble.preprocess_data(df)

        logger.info(f"æ•°æ®åŠ è½½: {len(X)}æ ·æœ¬, {X.shape[1]}ç‰¹å¾")
        logger.info(f"ç«™ç‚¹æ•°: {len(np.unique(station_groups))}, å¹´ä»½æ•°: {len(np.unique(year_groups))}")

        # 1. ç«™ç‚¹äº¤å‰éªŒè¯
        logger.info("\n" + "=" * 50)
        logger.info("æ­¥éª¤ 1: ç«™ç‚¹äº¤å‰éªŒè¯")
        logger.info("=" * 50)

        station_cv_results = pure_gnnwr_cross_validate(
            X, y, station_groups, coords, 'station', logger
        )

        # 2. å¹´åº¦äº¤å‰éªŒè¯
        logger.info("\n" + "=" * 50)
        logger.info("æ­¥éª¤ 2: å¹´åº¦äº¤å‰éªŒè¯")
        logger.info("=" * 50)

        yearly_cv_results = pure_gnnwr_cross_validate(
            X, y, year_groups, coords, 'yearly', logger
        )

        # 3. æ ‡å‡†è®­ç»ƒæµ‹è¯•é›†åˆ†å‰²
        logger.info("\n" + "=" * 50)
        logger.info("æ­¥éª¤ 3: æ ‡å‡†è®­ç»ƒæµ‹è¯•é›†éªŒè¯")
        logger.info("=" * 50)

        X_train, X_test, y_train, y_test, coords_train, coords_test, station_train, station_test = train_test_split(
            X, y, coords, station_groups, test_size=test_size, random_state=random_state
        )

        logger.info(f"æ•°æ®åˆ’åˆ†: è®­ç»ƒé›† {len(X_train)}, æµ‹è¯•é›† {len(X_test)}")

        # åˆ›å»ºæ•°æ®é›†
        train_dataset = EnhancedSpatialDataset(X_train, y_train, coords_train)
        test_dataset = EnhancedSpatialDataset(X_test, y_test, coords_test)

        # æ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)
        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)

        # è®­ç»ƒçº¯å‡€ç‰ˆGNNWR
        trainer = PureGNNWRTrainer(
            input_dim=X.shape[1],
            coords=coords_train,
            hidden_dims=[128, 64, 32, 16],
            learning_rate=0.001,
            dropout_rate=0.3,
            weight_decay=1e-4,
            device='cpu'  # ä½¿ç”¨CPU
        )

        logger.info("å¼€å§‹çº¯å‡€ç‰ˆGNNWRè®­ç»ƒ...")

        # è®­ç»ƒ
        train_losses, val_losses = trainer.train(train_loader, test_loader, epochs=200)

        # æœ€ç»ˆè¯„ä¼°
        y_pred = trainer.predict(X_test, coords_test)

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        test_metrics = evaluate_predictions(y_test, y_pred)

        # æ•´åˆæ‰€æœ‰ç»“æœ
        results = {
            'station_cv': station_cv_results,
            'yearly_cv': yearly_cv_results,
            'standard_test': test_metrics,
            'trainer': trainer,
            'data_info': {
                'total_samples': len(X),
                'n_features': X.shape[1],
                'n_stations': len(np.unique(station_groups)),
                'n_years': len(np.unique(year_groups)),
                'train_size': len(X_train),
                'test_size': len(X_test)
            }
        }

        # === æ–°å¢ï¼šä¿å­˜ç»“æœå’Œç”Ÿæˆå›¾è¡¨ ===
        if output_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = f"./pure_gnnwr_results_{timestamp}"

        os.makedirs(output_dir, exist_ok=True)
        logger.info(f"ä¿å­˜ç»“æœåˆ°: {output_dir}")

        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(output_dir, 'pure_gnnwr_model.pth')
        torch.save({
            'model_state_dict': trainer.model.state_dict(),
            'config': {
                'input_dim': X.shape[1],
                'hidden_dims': [128, 64, 32, 16],
                'learning_rate': 0.001
            }
        }, model_path)

        # ä¿å­˜ç»“æœæ•°æ®
        results_path = os.path.join(output_dir, 'pure_gnnwr_results.pkl')
        joblib.dump(results, results_path)

        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        create_pure_gnnwr_visualizations(results, output_dir)

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report_path = os.path.join(output_dir, 'pure_gnnwr_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(generate_detailed_report(results))

        # è¾“å‡ºç»¼åˆæŠ¥å‘Š
        print_comprehensive_report(results)

        logger.info("ğŸ¯ çº¯å‡€ç‰ˆGNNWRå®Œæ•´åˆ†æå®Œæˆ!")
        return results, trainer

    except Exception as e:
        logger.error(f"çº¯å‡€ç‰ˆGNNWRåˆ†æå¤±è´¥: {e}")
        raise


def create_pure_gnnwr_visualizations(results, output_dir):
    """ç”Ÿæˆçº¯å‡€ç‰ˆGNNWRçš„å¯è§†åŒ–å›¾è¡¨ - åŒ…å«æµ‹è¯•é›†å¤§å°ä¿¡æ¯"""
    import matplotlib.pyplot as plt
    import seaborn as sns

    plt.figure(figsize=(18, 12))

    # 1. ç«™ç‚¹äº¤å‰éªŒè¯æ•£ç‚¹å›¾
    plt.subplot(2, 4, 1)
    station_cv = results['station_cv']
    y_true_station = station_cv['true_values']
    y_pred_station = station_cv['predictions']

    plt.scatter(y_true_station, y_pred_station, alpha=0.6, s=20, color='blue')
    plt.plot([y_true_station.min(), y_true_station.max()],
             [y_true_station.min(), y_true_station.max()], 'r--', alpha=0.8)
    plt.xlabel('True SWE (mm)')
    plt.ylabel('Predicted SWE (mm)')
    plt.title(f'Station CV\nMAE={station_cv["overall"]["MAE"]:.2f}, R={station_cv["overall"]["R"]:.3f}')
    plt.grid(True, alpha=0.3)

    # 2. å¹´åº¦äº¤å‰éªŒè¯æ•£ç‚¹å›¾
    plt.subplot(2, 4, 2)
    yearly_cv = results['yearly_cv']
    y_true_yearly = yearly_cv['true_values']
    y_pred_yearly = yearly_cv['predictions']

    plt.scatter(y_true_yearly, y_pred_yearly, alpha=0.6, s=20, color='green')
    plt.plot([y_true_yearly.min(), y_true_yearly.max()],
             [y_true_yearly.min(), y_true_yearly.max()], 'r--', alpha=0.8)
    plt.xlabel('True SWE (mm)')
    plt.ylabel('Predicted SWE (mm)')
    plt.title(f'Yearly CV\nMAE={yearly_cv["overall"]["MAE"]:.2f}, R={yearly_cv["overall"]["R"]:.3f}')
    plt.grid(True, alpha=0.3)

    # 3. æµ‹è¯•é›†å¤§å°åˆ†å¸ƒ - ç«™ç‚¹
    plt.subplot(2, 4, 3)
    station_test_sizes = [info['test_size'] for info in station_cv['by_fold'].values()]
    plt.hist(station_test_sizes, bins=20, alpha=0.7, color='skyblue')
    plt.xlabel('æµ‹è¯•é›†å¤§å° (æ ·æœ¬æ•°)')
    plt.ylabel('æŠ˜å æ•°é‡')
    plt.title(f'ç«™ç‚¹CVæµ‹è¯•é›†å¤§å°åˆ†å¸ƒ\nå¹³å‡={np.mean(station_test_sizes):.1f}')

    # 4. æµ‹è¯•é›†å¤§å°åˆ†å¸ƒ - å¹´åº¦
    plt.subplot(2, 4, 4)
    yearly_test_sizes = [info['test_size'] for info in yearly_cv['by_fold'].values()]
    plt.hist(yearly_test_sizes, bins=20, alpha=0.7, color='lightgreen')
    plt.xlabel('æµ‹è¯•é›†å¤§å° (æ ·æœ¬æ•°)')
    plt.ylabel('æŠ˜å æ•°é‡')
    plt.title(f'å¹´åº¦CVæµ‹è¯•é›†å¤§å°åˆ†å¸ƒ\nå¹³å‡={np.mean(yearly_test_sizes):.1f}')

    # 5. æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
    plt.subplot(2, 4, 5)
    methods = ['Station CV', 'Yearly CV', 'Standard Test']
    mae_values = [
        station_cv['overall']['MAE'],
        yearly_cv['overall']['MAE'],
        results['standard_test']['MAE']
    ]

    bars = plt.bar(methods, mae_values, color=['skyblue', 'lightgreen', 'lightcoral'])
    plt.ylabel('MAE (mm)')
    plt.title('Performance Comparison (MAE)')
    for bar, value in zip(bars, mae_values):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                 f'{value:.2f}', ha='center', va='bottom')

    # 6. Rå€¼å¯¹æ¯”æŸ±çŠ¶å›¾
    plt.subplot(2, 4, 6)
    r_values = [
        station_cv['overall']['R'],
        yearly_cv['overall']['R'],
        results['standard_test']['R']
    ]

    bars = plt.bar(methods, r_values, color=['skyblue', 'lightgreen', 'lightcoral'])
    plt.ylabel('R')
    plt.title('Performance Comparison (R)')
    for bar, value in zip(bars, r_values):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                 f'{value:.3f}', ha='center', va='bottom')

    # 7. æŠ˜å ç»Ÿè®¡
    plt.subplot(2, 4, 7)
    fold_stats = {
        'æ€»ç«™ç‚¹æŠ˜å ': station_cv['total_folds'],
        'æˆåŠŸç«™ç‚¹æŠ˜å ': station_cv['folds'],
        'æ€»å¹´åº¦æŠ˜å ': yearly_cv['total_folds'],
        'æˆåŠŸå¹´åº¦æŠ˜å ': yearly_cv['folds']
    }

    plt.bar(fold_stats.keys(), fold_stats.values(), color='lightgray')
    plt.xticks(rotation=45, ha='right')
    plt.ylabel('æ•°é‡')
    plt.title('äº¤å‰éªŒè¯æŠ˜å ç»Ÿè®¡')

    # 8. æ®‹å·®åˆ†å¸ƒ
    plt.subplot(2, 4, 8)
    residuals = y_true_station - y_pred_station
    plt.hist(residuals, bins=30, alpha=0.7, color='orange')
    plt.xlabel('æ®‹å·® (mm)')
    plt.ylabel('é¢‘ç‡')
    plt.title('ç«™ç‚¹CVæ®‹å·®åˆ†å¸ƒ')

    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'pure_gnnwr_comprehensive_analysis.png'),
                dpi=300, bbox_inches='tight')
    plt.close()

    print(f"âœ… ç»¼åˆå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜")


def generate_detailed_report(results):
    """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
    report = []
    report.append("=" * 80)
    report.append("ğŸ¯ çº¯å‡€ç‰ˆGNNWRè¯¦ç»†åˆ†ææŠ¥å‘Š")
    report.append("=" * 80)
    report.append("")

    # æ•°æ®æ¦‚å†µ
    data_info = results['data_info']
    report.append("ğŸ“Š æ•°æ®æ¦‚å†µ:")
    report.append(f"  æ€»æ ·æœ¬æ•°: {data_info['total_samples']}")
    report.append(f"  ç‰¹å¾æ•°é‡: {data_info['n_features']}")
    report.append(f"  ç«™ç‚¹æ•°é‡: {data_info['n_stations']}")
    report.append(f"  å¹´ä»½æ•°é‡: {data_info['n_years']}")
    report.append(f"  è®­ç»ƒé›†å¤§å°: {data_info['train_size']}")
    report.append(f"  æµ‹è¯•é›†å¤§å°: {data_info['test_size']}")
    report.append("")

    # ç«™ç‚¹äº¤å‰éªŒè¯è¯¦ç»†ç»“æœ
    station_cv = results['station_cv']
    station_overall = station_cv['overall']
    report.append("ğŸ”ï¸ ç«™ç‚¹äº¤å‰éªŒè¯è¯¦ç»†ç»“æœ:")
    report.append(f"  æ€»æŠ˜å æ•°: {station_cv['total_folds']}")
    report.append(f"  æˆåŠŸæŠ˜å : {station_cv['folds']}")
    report.append(f"  è·³è¿‡æŠ˜å : {station_cv.get('skipped_folds', 0)}")
    report.append(f"  MAE: {station_overall['MAE']:.3f} mm")
    report.append(f"  RMSE: {station_overall['RMSE']:.3f} mm")
    report.append(f"  R: {station_overall['R']:.3f}")
    report.append(f"  RÂ²: {station_overall['R_squared']:.3f}")
    report.append(f"  æ ·æœ¬æ•°: {station_overall['samples']}")
    report.append("")

    # å¹´åº¦äº¤å‰éªŒè¯è¯¦ç»†ç»“æœ
    yearly_cv = results['yearly_cv']
    yearly_overall = yearly_cv['overall']
    report.append("ğŸ“… å¹´åº¦äº¤å‰éªŒè¯è¯¦ç»†ç»“æœ:")
    report.append(f"  æ€»æŠ˜å æ•°: {yearly_cv['total_folds']}")
    report.append(f"  æˆåŠŸæŠ˜å : {yearly_cv['folds']}")
    report.append(f"  è·³è¿‡æŠ˜å : {yearly_cv.get('skipped_folds', 0)}")
    report.append(f"  MAE: {yearly_overall['MAE']:.3f} mm")
    report.append(f"  RMSE: {yearly_overall['RMSE']:.3f} mm")
    report.append(f"  R: {yearly_overall['R']:.3f}")
    report.append(f"  RÂ²: {yearly_overall['R_squared']:.3f}")
    report.append(f"  æ ·æœ¬æ•°: {yearly_overall['samples']}")
    report.append("")

    # æ ‡å‡†æµ‹è¯•é›†ç»“æœ
    standard_test = results['standard_test']
    report.append("ğŸ§ª æ ‡å‡†æµ‹è¯•é›†ç»“æœ:")
    report.append(f"  MAE: {standard_test['MAE']:.3f} mm")
    report.append(f"  RMSE: {standard_test['RMSE']:.3f} mm")
    report.append(f"  R: {standard_test['R']:.3f}")
    report.append(f"  RÂ²: {standard_test['R_squared']:.3f}")
    report.append(f"  æ ·æœ¬æ•°: {standard_test['samples']}")
    report.append("")

    # æ€§èƒ½æ€»ç»“
    report.append("ğŸ“ˆ æ€§èƒ½æ€»ç»“:")
    best_mae = min(station_overall['MAE'], yearly_overall['MAE'], standard_test['MAE'])
    best_r = max(station_overall['R'], yearly_overall['R'], standard_test['R'])
    report.append(f"  æœ€ä½³MAE: {best_mae:.3f} mm")
    report.append(f"  æœ€ä½³Rå€¼: {best_r:.3f}")
    report.append("")

    report.append("=" * 80)
    report.append("æŠ¥å‘Šç”Ÿæˆæ—¶é—´: " + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    report.append("=" * 80)

    return "\n".join(report)


def pure_gnnwr_cross_validate_fixed(X, y, groups, coords, cv_type, logger):
    """ä¿®å¤çš„äº¤å‰éªŒè¯ - æ­£ç¡®ç†è§£LOGOé€»è¾‘"""
    from sklearn.model_selection import LeaveOneGroupOut

    logo = LeaveOneGroupOut()
    all_predictions = []
    all_true_values = []
    fold_results = {}
    skipped_folds = 0

    unique_groups = np.unique(groups)
    total_folds = len(unique_groups)

    logger.info(f"å¼€å§‹{cv_type}äº¤å‰éªŒè¯ï¼Œå…±{total_folds}ä¸ªæŠ˜å ...")

    for fold, (train_idx, test_idx) in enumerate(logo.split(X, y, groups)):
        group_id = groups[test_idx[0]]
        test_size = len(test_idx)
        train_size = len(train_idx)

        # è®­ç»ƒé›†åº”è¯¥æ˜¯å¾ˆå¤§çš„ï¼ˆæ‰€æœ‰å…¶ä»–ç«™ç‚¹ï¼‰ï¼Œæµ‹è¯•é›†å¯èƒ½å¾ˆå°
        logger.info(f"Fold {fold + 1}/{total_folds}: {cv_type} {group_id}, è®­ç»ƒé›†={train_size}, æµ‹è¯•é›†={test_size}")

        # åˆ†å‰²æ•°æ®
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        # åˆ†å‰²åæ ‡
        coords_train = coords[train_idx] if coords is not None else None
        coords_test = coords[test_idx] if coords is not None else None

        # éªŒè¯æ•°æ®åˆ†å‰²æ­£ç¡®æ€§
        unique_train_groups = len(np.unique(groups[train_idx]))
        unique_test_groups = len(np.unique(groups[test_idx]))

        logger.debug(f"  è®­ç»ƒé›†åŒ…å« {unique_train_groups} ä¸ª{'' if cv_type == 'station' else 'å¹´ä»½'}")
        logger.debug(f"  æµ‹è¯•é›†åŒ…å« {unique_test_groups} ä¸ª{'' if cv_type == 'station' else 'å¹´ä»½'}")

        try:
            # ä½¿ç”¨å®Œæ•´æ¨¡å‹ï¼ˆè®­ç»ƒé›†å¾ˆå¤§ï¼Œå¯ä»¥ç”¨å¤æ‚æ¨¡å‹ï¼‰
            trainer = PureGNNWRTrainer(
                input_dim=X.shape[1],
                coords=coords_train,
                hidden_dims=[128, 64, 32, 16],  # ä½¿ç”¨å®Œæ•´æ¨¡å‹
                learning_rate=0.001,
                dropout_rate=0.3,
                device='cpu',
                output_std_penalty=0.05  # é˜²æ­¢è¾“å‡ºæ’å®š
            )

            # åˆ›å»ºæ•°æ®é›† - ä½¿ç”¨è¾ƒå¤§çš„batch_sizeï¼ˆè®­ç»ƒé›†å¤§ï¼‰
            train_dataset = EnhancedSpatialDataset(X_train, y_train, coords_train)
            train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)

            # è®­ç»ƒ - å¯ä»¥ä½¿ç”¨æ›´å¤šepochï¼ˆè®­ç»ƒé›†å¤§ï¼Œä¸å®¹æ˜“è¿‡æ‹Ÿåˆï¼‰
            trainer.train(train_loader, epochs=100, early_stopping_patience=15)

            # é¢„æµ‹
            y_pred = trainer.predict(X_test, coords_test)

            # æ£€æŸ¥é¢„æµ‹ç»“æœè´¨é‡
            if len(test_idx) > 1 and np.std(y_pred) < 1e-6:  # åªæœ‰æµ‹è¯•é›†>1æ—¶æ‰æ£€æŸ¥
                logger.warning(f"æŠ˜å  {fold + 1}: é¢„æµ‹ç»“æœæ’å®šï¼Œå¯èƒ½æ¨¡å‹æœ‰é—®é¢˜")
                # ä½†ä»ç„¶è®°å½•ç»“æœ

            # å­˜å‚¨ç»“æœ
            all_predictions.extend(y_pred)
            all_true_values.extend(y_test)

            # è®¡ç®—å½“å‰æŠ˜å æ€§èƒ½
            fold_metrics = evaluate_predictions(y_test, y_pred)
            fold_results[group_id] = {
                **fold_metrics,
                'train_size': train_size,
                'test_size': test_size
            }

            logger.info(
                f"  {cv_type} Fold {fold + 1}: {group_id} - "
                f"Train={train_size}, Test={test_size}, "
                f"MAE={fold_metrics['MAE']:.3f}, R={fold_metrics['R']:.3f}"
            )

        except Exception as e:
            logger.error(f"æŠ˜å  {fold + 1} è®­ç»ƒå¤±è´¥: {e}")
            skipped_folds += 1
            continue

    # è®¡ç®—æ€»ä½“æ€§èƒ½
    if len(all_true_values) == 0:
        logger.error(f"{cv_type}äº¤å‰éªŒè¯æ²¡æœ‰æœ‰æ•ˆç»“æœ")
        return {
            'overall': {'MAE': 0, 'RMSE': 0, 'R': 0, 'R_squared': 0, 'samples': 0},
            'by_fold': {},
            'predictions': np.array([]),
            'true_values': np.array([]),
            'folds': 0,
            'total_folds': total_folds,
            'skipped_folds': skipped_folds
        }

    overall_metrics = evaluate_predictions(
        np.array(all_true_values),
        np.array(all_predictions)
    )

    # åˆ†ææŠ˜å ç»“æœ
    successful_folds = len(fold_results)
    avg_test_size = np.mean([info['test_size'] for info in fold_results.values()])

    logger.info(f"âœ… {cv_type}äº¤å‰éªŒè¯å®Œæˆ")
    logger.info(f"  æ€»æŠ˜å æ•°: {total_folds}, æˆåŠŸ: {successful_folds}, å¤±è´¥: {skipped_folds}")
    logger.info(f"  å¹³å‡æµ‹è¯•é›†å¤§å°: {avg_test_size:.1f} æ ·æœ¬/æŠ˜å ")
    logger.info(f"  èšåˆæ€§èƒ½: MAE={overall_metrics['MAE']:.3f}, R={overall_metrics['R']:.3f}")

    return {
        'overall': overall_metrics,
        'by_fold': fold_results,
        'predictions': np.array(all_predictions),
        'true_values': np.array(all_true_values),
        'folds': successful_folds,
        'total_folds': total_folds,
        'skipped_folds': skipped_folds,
        'avg_test_size': avg_test_size
    }


def _is_constant_data(data, axis=None):
    """æ£€æŸ¥æ•°æ®æ˜¯å¦æ’å®šï¼ˆæ‰€æœ‰å€¼ç›¸åŒï¼‰"""
    if data is None or len(data) == 0:
        return True

    if axis is not None:
        # å¯¹äºå¤šç»´æ•°æ®ï¼Œæ£€æŸ¥æ¯ä¸ªç‰¹å¾æ˜¯å¦æ’å®š
        return np.all(np.std(data, axis=axis) == 0)
    else:
        # å¯¹äºä¸€ç»´æ•°æ®
        return np.std(data) == 0


def evaluate_predictions(y_true, y_pred):
    """è¯„ä¼°é¢„æµ‹æ€§èƒ½ - ä¿®å¤å¸¸æ•°è¾“å…¥é—®é¢˜"""
    from sklearn.metrics import mean_absolute_error, mean_squared_error
    from scipy.stats import pearsonr
    import numpy as np
    import warnings

    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))

    # æ£€æŸ¥æ•°æ®æ˜¯å¦æ’å®š
    y_true_std = np.std(y_true)
    y_pred_std = np.std(y_pred)

    # å¦‚æœä»»ä¸€æ•°ç»„æ˜¯å¸¸æ•°ï¼Œç›¸å…³ç³»æ•°è®¾ä¸º0
    if y_true_std == 0 or y_pred_std == 0:
        r_value = 0.0
        warnings.warn(f"æ£€æµ‹åˆ°å¸¸æ•°è¾“å…¥: y_true_std={y_true_std:.6f}, y_pred_std={y_pred_std:.6f}ï¼Œç›¸å…³ç³»æ•°è®¾ä¸º0")
    else:
        try:
            r_value, _ = pearsonr(y_true, y_pred)
        except:
            r_value = 0.0

    return {
        'MAE': mae,
        'RMSE': rmse,
        'R': r_value,
        'R_squared': r_value ** 2,
        'samples': len(y_true),
        'y_true_std': y_true_std,
        'y_pred_std': y_pred_std
    }


def print_comprehensive_report(results):
    """æ‰“å°ç»¼åˆæŠ¥å‘Š"""
    print("\n" + "=" * 70)
    print("ğŸ¯ çº¯å‡€ç‰ˆGNNWRå®Œæ•´åˆ†ææŠ¥å‘Š")
    print("=" * 70)

    # æ•°æ®æ¦‚å†µ
    data_info = results['data_info']
    print(f"\nğŸ“Š æ•°æ®æ¦‚å†µ:")
    print(f"  æ€»æ ·æœ¬æ•°: {data_info['total_samples']}")
    print(f"  ç‰¹å¾æ•°é‡: {data_info['n_features']}")
    print(f"  ç«™ç‚¹æ•°é‡: {data_info['n_stations']}")
    print(f"  å¹´ä»½æ•°é‡: {data_info['n_years']}")

    # ç«™ç‚¹äº¤å‰éªŒè¯ç»“æœ
    station_cv = results['station_cv']['overall']
    print(f"\nğŸ”ï¸ ç«™ç‚¹äº¤å‰éªŒè¯:")
    print(f"  æŠ˜å æ•°é‡: {results['station_cv']['folds']}")
    print(f"  MAE: {station_cv['MAE']:.3f} mm")
    print(f"  RMSE: {station_cv['RMSE']:.3f} mm")
    print(f"  R: {station_cv['R']:.3f}")
    print(f"  RÂ²: {station_cv['R_squared']:.3f}")

    # å¹´åº¦äº¤å‰éªŒè¯ç»“æœ
    yearly_cv = results['yearly_cv']['overall']
    print(f"\nğŸ“… å¹´åº¦äº¤å‰éªŒè¯:")
    print(f"  æŠ˜å æ•°é‡: {results['yearly_cv']['folds']}")
    print(f"  MAE: {yearly_cv['MAE']:.3f} mm")
    print(f"  RMSE: {yearly_cv['RMSE']:.3f} mm")
    print(f"  R: {yearly_cv['R']:.3f}")
    print(f"  RÂ²: {yearly_cv['R_squared']:.3f}")

    # æ ‡å‡†æµ‹è¯•é›†ç»“æœ
    standard_test = results['standard_test']
    print(f"\nğŸ§ª æ ‡å‡†æµ‹è¯•é›†:")
    print(f"  æ ·æœ¬æ•°é‡: {standard_test['samples']}")
    print(f"  MAE: {standard_test['MAE']:.3f} mm")
    print(f"  RMSE: {standard_test['RMSE']:.3f} mm")
    print(f"  R: {standard_test['R']:.3f}")
    print(f"  RÂ²: {standard_test['R_squared']:.3f}")

    print("=" * 70)


# åœ¨SWEClusterEnsembleç±»ä¸­æ·»åŠ ä¸€ä¸ªä¾¿æ·æ–¹æ³•
def SWEClusterEnsemble_run_pure_comparison(self, df):
    """
    åœ¨SWEClusterEnsembleç±»ä¸­æ·»åŠ çš„æ–¹æ³•
    ç”¨äºå¿«é€Ÿè¿è¡Œçº¯å‡€ç‰ˆå¯¹æ¯”å®éªŒ
    """
    return train_pure_gnnwr_analysis(df)





if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("æµ‹è¯•èšç±»é›†æˆæ¨¡å‹...")

    # ç›´æ¥è¿è¡Œçº¯å‡€ç‰ˆå¯¹æ¯”ï¼ˆéœ€è¦å…ˆæœ‰æ•°æ®æ–‡ä»¶ï¼‰
    try:
        import pandas as pd
        df = pd.read_excel("lu_onehot.xlsx")  # æ›¿æ¢ä¸ºæ‚¨çš„æ•°æ®æ–‡ä»¶
        results, trainer = train_pure_gnnwr_analysis(df)
    except Exception as e:
        print(f"ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        print("è¯·ç¡®ä¿æœ‰æ•°æ®æ–‡ä»¶å¹¶ä¿®æ”¹æ–‡ä»¶è·¯å¾„")
    print("æµ‹è¯•å®Œæˆï¼")