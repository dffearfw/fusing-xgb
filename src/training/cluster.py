import logging

import logger
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, mean_squared_error
from scipy.stats import pearsonr
from sklearn.model_selection import LeaveOneGroupOut
import joblib
import os
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from torch.utils.data import DataLoader

# å…ˆåˆ›å»ºlogger
logger = logging.getLogger("SWEClusterEnsemble")

# ç„¶åå¯¼å…¥å¢å¼ºç‰ˆGNNWR
try:
    from GNNWR import EnhancedSpatialDataset, EnhancedGNNWRTrainer, SpatialWeightCalculator

    HAS_ENHANCED_GNNWR = True
    logger.info("æˆåŠŸå¯¼å…¥å¢å¼ºç‰ˆGNNWR")
except ImportError as e:
    logger.warning(f"æ— æ³•å¯¼å…¥å¢å¼ºç‰ˆGNNWR: {e}")
    try:
        # å°è¯•å¯¼å…¥åŸºç¡€ç‰ˆ
        from GNNWR import SpatialDataset, GNNWRTrainer

        HAS_ENHANCED_GNNWR = False
        logger.info("ä½¿ç”¨åŸºç¡€ç‰ˆGNNWR")
    except ImportError:
        logger.error("æ— æ³•å¯¼å…¥ä»»ä½•GNNWRç‰ˆæœ¬")
        HAS_ENHANCED_GNNWR = False


        # åˆ›å»ºè™šæ‹Ÿç±»ä»¥é¿å…åç»­é”™è¯¯
        class EnhancedSpatialDataset:
            def __init__(self, features, targets, coords=None):
                self.features = features
                self.targets = targets
                self.coords = coords

            def __len__(self):
                return len(self.features)

            def __getitem__(self, idx):
                if self.coords is not None:
                    return self.features[idx], self.targets[idx], self.coords[idx]
                else:
                    return self.features[idx], self.targets[idx]


        class EnhancedGNNWRTrainer:
            def __init__(self, *args, **kwargs):
                logger.warning("ä½¿ç”¨è™šæ‹ŸEnhancedGNNWRTrainer")

            def train(self, *args, **kwargs):
                logger.warning("è™šæ‹Ÿè®­ç»ƒæ–¹æ³•")

            def predict(self, features, coords=None):
                logger.warning("è™šæ‹Ÿé¢„æµ‹æ–¹æ³•")
                return np.random.normal(50, 20, len(features))


        class SpatialDataset:
            def __init__(self, features, targets):
                self.features = features
                self.targets = targets

            def __len__(self):
                return len(self.features)

            def __getitem__(self, idx):
                return self.features[idx], self.targets[idx]


        class GNNWRTrainer:
            def __init__(self, *args, **kwargs):
                logger.warning("ä½¿ç”¨è™šæ‹ŸGNNWRTrainer")

            def train(self, *args, **kwargs):
                logger.warning("è™šæ‹Ÿè®­ç»ƒæ–¹æ³•")

            def predict(self, features):
                logger.warning("è™šæ‹Ÿé¢„æµ‹æ–¹æ³•")
                return np.random.normal(50, 20, len(features))


class SWEClusterEnsemble:
    """SWEèšç±»é›†æˆå›å½’å™¨ - ä½¿ç”¨å¢å¼ºç‰ˆGNNWRè¿›è¡Œé›†æˆ"""

    DEFAULT_PARAMS = {
        'n_estimators': 60,
        'learning_rate': 0.17,
        'max_depth': 5,
        'min_child_weight': 5,
        'gamma': 0,
        'subsample': 0.8,
        'colsample_bytree': 0.5,
        'reg_alpha': 0.05,
        'random_state': 42,
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse'
    }

    def __init__(self, n_clusters=4, params=None, gnnwr_params=None, use_enhanced_gnnwr=True):
        """åˆå§‹åŒ–èšç±»é›†æˆå›å½’å™¨

        Args:
            n_clusters (int): èšç±»æ•°é‡
            params (dict): XGBoostå‚æ•°
            gnnwr_params (dict): GNNWRå‚æ•°
            use_enhanced_gnnwr (bool): æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰ˆGNNWR
        """
        self.logger = logging.getLogger("SWEClusterEnsemble")

        self.n_clusters = n_clusters
        self.kmeans = None
        self.cluster_assignments = None
        self.cluster_models = {}
        self.gnnwr_trainer = None
        self.feature_columns = None
        self.target_column = 'swe'
        self.use_enhanced_gnnwr = use_enhanced_gnnwr and HAS_ENHANCED_GNNWR

        # XGBoostå‚æ•°
        self.params = self.DEFAULT_PARAMS.copy()
        if params:
            self.params.update(params)

        # GNNWRå‚æ•°
        self.gnnwr_params = {
            'hidden_dims': [64, 32, 16],
            'learning_rate': 0.001,
            'epochs': 100,
            'batch_size': 32,
            'patience': 10,
            'bandwidth': None,
            'use_spatial_weights': True
        }
        if gnnwr_params:
            self.gnnwr_params.update(gnnwr_params)

        self.logger.info(f"åˆå§‹åŒ–SWEèšç±»é›†æˆå›å½’å™¨ï¼Œèšç±»æ•°: {n_clusters}")
        self.logger.info(f"ä½¿ç”¨{'å¢å¼ºç‰ˆ' if self.use_enhanced_gnnwr else 'åŸºç¡€ç‰ˆ'}GNNWR")
        self.logger.info(f"GNNWRå‚æ•°: {self.gnnwr_params}")

    def preprocess_data(self, df):
        """æ•°æ®é¢„å¤„ç†

        Args:
            df (pd.DataFrame): è¾“å…¥æ•°æ®

        Returns:
            tuple: (X, y, station_groups, year_groups, coords)
        """
        self.logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")

        # ç¡®å®šç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—
        if self.feature_columns is None:
            # è‡ªåŠ¨é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆæ’é™¤ç›®æ ‡åˆ—å’Œå…¶ä»–éç‰¹å¾åˆ—ï¼‰
            exclude_cols = [self.target_column, 'station_id', 'year', 'date', 'station', 'group',
                            'longitude', 'latitude', 'lon', 'lat']  # æ’é™¤åæ ‡åˆ—
            self.feature_columns = [col for col in df.columns if
                                    col not in exclude_cols and df[col].dtype in [np.int64, np.float64]]

        self.logger.info(f"ä½¿ç”¨ç‰¹å¾: {self.feature_columns}")

        # æå–ç‰¹å¾å’Œç›®æ ‡
        X = df[self.feature_columns].values
        y = df[self.target_column].values

        # å¤„ç†ç¼ºå¤±å€¼
        if np.isnan(X).any():
            self.logger.info("å¤„ç†ç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼")
            self.feature_imputer = SimpleImputer(strategy='median')
            X = self.feature_imputer.fit_transform(X)
        else:
            self.feature_imputer = None

        # åˆ›å»ºåˆ†ç»„ä¿¡æ¯
        if 'station_id' in df.columns:
            station_groups = df['station_id'].values
        elif 'station' in df.columns:
            station_groups = df['station'].values
        else:
            # å¦‚æœæ²¡æœ‰ç«™ç‚¹ä¿¡æ¯ï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºåˆ†ç»„
            station_groups = np.arange(len(df))
            self.logger.warning("æœªæ‰¾åˆ°ç«™ç‚¹ä¿¡æ¯ï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºåˆ†ç»„")

        if 'year' in df.columns:
            year_groups = df['year'].values
        else:
            # å¦‚æœæ²¡æœ‰å¹´ä»½ä¿¡æ¯ï¼Œåˆ›å»ºè™šæ‹Ÿå¹´ä»½
            year_groups = np.ones(len(df), dtype=int)
            self.logger.warning("æœªæ‰¾åˆ°å¹´ä»½ä¿¡æ¯ï¼Œä½¿ç”¨ç»Ÿä¸€å¹´ä»½åˆ†ç»„")

        # æå–åæ ‡ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        coords = None
        if all(col in df.columns for col in ['longitude', 'latitude']):
            coords = df[['longitude', 'latitude']].values
            self.logger.info(f"ä½¿ç”¨ç»çº¬åº¦åæ ‡: {len(coords)} ä¸ªç‚¹")
        elif all(col in df.columns for col in ['lon', 'lat']):
            coords = df[['lon', 'lat']].values
            self.logger.info(f"ä½¿ç”¨ç»çº¬åº¦åæ ‡: {len(coords)} ä¸ªç‚¹")
        else:
            self.logger.warning("æœªæ‰¾åˆ°åæ ‡ä¿¡æ¯ï¼Œå°†ä½¿ç”¨è™šæ‹Ÿåæ ‡")
            # åˆ›å»ºåŸºäºç«™ç‚¹IDçš„è™šæ‹Ÿåæ ‡
            unique_stations = np.unique(station_groups)
            station_to_coord = {station: [i, i] for i, station in enumerate(unique_stations)}
            coords = np.array([station_to_coord[station] for station in station_groups])

        self.logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(X)}ä¸ªæ ·æœ¬, {X.shape[1]}ä¸ªç‰¹å¾")
        self.logger.info(f"ç«™ç‚¹æ•°: {len(np.unique(station_groups))}, å¹´ä»½æ•°: {len(np.unique(year_groups))}")

        return X, y, station_groups, year_groups, coords

    def perform_clustering(self, X, groups):
        """æ‰§è¡Œèšç±»åˆ†æ

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            groups (np.array): åˆ†ç»„ä¿¡æ¯

        Returns:
            np.array: èšç±»æ ‡ç­¾
        """
        self.logger.info(f"æ‰§è¡ŒK-meansèšç±»ï¼Œèšç±»æ•°: {self.n_clusters}")

        # æŒ‰ç«™ç‚¹èšåˆç‰¹å¾
        unique_groups = np.unique(groups)
        group_features = []

        for group in unique_groups:
            group_mask = groups == group
            group_data = X[group_mask]
            # ä½¿ç”¨æ¯ä¸ªç«™ç‚¹çš„ç‰¹å¾å‡å€¼ä½œä¸ºèšç±»ç‰¹å¾
            group_mean = np.nanmean(group_data, axis=0)
            group_features.append(group_mean)

        group_features = np.array(group_features)

        # å¤„ç†å¯èƒ½çš„NaNå€¼
        if np.isnan(group_features).any():
            self.logger.info("å¤„ç†èšç±»ç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼")
            cluster_imputer = SimpleImputer(strategy='median')
            group_features = cluster_imputer.fit_transform(group_features)

        # æ‰§è¡ŒK-meansèšç±»
        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)
        group_clusters = self.kmeans.fit_predict(group_features)

        # å°†èšç±»æ ‡ç­¾æ˜ å°„å›åŸå§‹æ ·æœ¬
        cluster_assignments = np.zeros(len(X), dtype=int)
        for i, group in enumerate(unique_groups):
            group_mask = groups == group
            cluster_assignments[group_mask] = group_clusters[i]

        # ç»Ÿè®¡æ¯ä¸ªèšç±»çš„æ ·æœ¬æ•°
        cluster_counts = np.bincount(cluster_assignments)
        self.logger.info(f"èšç±»åˆ†å¸ƒ: {dict(enumerate(cluster_counts))}")

        return cluster_assignments

    def train_cluster_models(self, X, y, cluster_labels):
        """ä¸ºæ¯ä¸ªèšç±»è®­ç»ƒXGBoostæ¨¡å‹

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            y (np.array): ç›®æ ‡å˜é‡
            cluster_labels (np.array): èšç±»æ ‡ç­¾
        """
        self.logger.info("è®­ç»ƒå„èšç±»XGBoostæ¨¡å‹...")
        self.cluster_models = {}

        for cluster_id in range(self.n_clusters):
            cluster_mask = cluster_labels == cluster_id
            cluster_size = np.sum(cluster_mask)

            if cluster_size < 5:
                self.logger.warning(f"èšç±» {cluster_id} æ ·æœ¬æ•°è¿‡å°‘ ({cluster_size})ï¼Œè·³è¿‡è®­ç»ƒ")
                continue

            X_cluster = X[cluster_mask]
            y_cluster = y[cluster_mask]

            # è®­ç»ƒXGBoostæ¨¡å‹
            model = xgb.XGBRegressor(**self.params)
            model.fit(X_cluster, y_cluster)

            self.cluster_models[cluster_id] = model

            # è¯„ä¼°èšç±»æ¨¡å‹æ€§èƒ½
            y_pred_cluster = model.predict(X_cluster)
            cluster_mae = mean_absolute_error(y_cluster, y_pred_cluster)
            cluster_rmse = np.sqrt(mean_squared_error(y_cluster, y_pred_cluster))

            self.logger.info(f"  èšç±» {cluster_id}: {cluster_size}æ ·æœ¬, MAE={cluster_mae:.3f}, RMSE={cluster_rmse:.3f}")

    def _get_cluster_predictions(self, X, cluster_labels):
        """è·å–å„èšç±»æ¨¡å‹çš„é¢„æµ‹ç»“æœ

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            cluster_labels (np.array): èšç±»æ ‡ç­¾

        Returns:
            np.array: å„èšç±»æ¨¡å‹çš„é¢„æµ‹ç»“æœçŸ©é˜µ
        """
        cluster_predictions = np.zeros((len(X), self.n_clusters))

        for cluster_id, model in self.cluster_models.items():
            cluster_mask = cluster_labels == cluster_id
            if np.any(cluster_mask):
                predictions = model.predict(X[cluster_mask])
                cluster_predictions[cluster_mask, cluster_id] = predictions

        return cluster_predictions

    def train_gnnwr_model(self, X, y, cluster_predictions, coords=None):
        """è®­ç»ƒGNNWRé›†æˆæ¨¡å‹

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            y (np.array): ç›®æ ‡å˜é‡
            cluster_predictions (np.array): å„èšç±»æ¨¡å‹çš„é¢„æµ‹ç»“æœ
            coords (np.array): åæ ‡æ•°æ®
        """
        self.logger.info("è®­ç»ƒGNNWRé›†æˆæ¨¡å‹...")

        # ä½¿ç”¨ç‰¹å¾ï¼šåŸå§‹ç‰¹å¾ + å„èšç±»é¢„æµ‹
        gnnwr_features = np.hstack([X, cluster_predictions])

        # å¤„ç†ç¼ºå¤±å€¼
        if np.isnan(gnnwr_features).any():
            self.logger.info("å¤„ç†GNNWRç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼")
            self.gnnwr_imputer = SimpleImputer(strategy='median')
            gnnwr_features_imputed = self.gnnwr_imputer.fit_transform(gnnwr_features)
        else:
            gnnwr_features_imputed = gnnwr_features
            self.gnnwr_imputer = None

        if self.use_enhanced_gnnwr:
            # ä½¿ç”¨å¢å¼ºç‰ˆGNNWR
            self.logger.info("ä½¿ç”¨å¢å¼ºç‰ˆGNNWRè®­ç»ƒå™¨")

            # åˆ›å»ºæ•°æ®é›†
            dataset = EnhancedSpatialDataset(
                features=gnnwr_features_imputed,
                targets=y,
                coords=coords
            )

            train_loader = DataLoader(
                dataset,
                batch_size=self.gnnwr_params['batch_size'],
                shuffle=True
            )

            # åˆå§‹åŒ–å¢å¼ºç‰ˆGNNWRè®­ç»ƒå™¨
            input_dim = gnnwr_features_imputed.shape[1]
            self.gnnwr_trainer = EnhancedGNNWRTrainer(
                input_dim=input_dim,
                coords=coords,
                hidden_dims=self.gnnwr_params['hidden_dims'],
                learning_rate=self.gnnwr_params['learning_rate'],
                bandwidth=self.gnnwr_params['bandwidth'],
                use_spatial_weights=self.gnnwr_params['use_spatial_weights']
            )

            # è®­ç»ƒæ¨¡å‹
            self.logger.info(f"å¼€å§‹å¢å¼ºç‰ˆGNNWRè®­ç»ƒï¼Œè¾“å…¥ç»´åº¦: {input_dim}")
            self.gnnwr_trainer.train(
                train_loader,
                epochs=self.gnnwr_params['epochs'],
                patience=self.gnnwr_params['patience']
            )
        else:
            # ä½¿ç”¨åŸºç¡€ç‰ˆGNNWR
            self.logger.info("ä½¿ç”¨åŸºç¡€ç‰ˆGNNWRè®­ç»ƒå™¨")

            # åˆ›å»ºæ•°æ®é›†
            dataset = SpatialDataset(gnnwr_features_imputed, y)
            train_loader = DataLoader(
                dataset,
                batch_size=self.gnnwr_params['batch_size'],
                shuffle=True
            )

            # åˆå§‹åŒ–åŸºç¡€ç‰ˆGNNWRè®­ç»ƒå™¨
            input_dim = gnnwr_features_imputed.shape[1]
            self.gnnwr_trainer = GNNWRTrainer(
                input_dim=input_dim,
                hidden_dims=self.gnnwr_params['hidden_dims'],
                learning_rate=self.gnnwr_params['learning_rate']
            )

            # è®­ç»ƒæ¨¡å‹
            self.logger.info(f"å¼€å§‹åŸºç¡€ç‰ˆGNNWRè®­ç»ƒï¼Œè¾“å…¥ç»´åº¦: {input_dim}")
            self.gnnwr_trainer.train(
                train_loader,
                epochs=self.gnnwr_params['epochs'],
                patience=self.gnnwr_params['patience']
            )

        # è®¡ç®—è®­ç»ƒé›†æ€§èƒ½
        y_pred = self.predict_with_gnnwr(gnnwr_features_imputed, cluster_predictions, coords)
        mae = mean_absolute_error(y, y_pred)
        rmse = np.sqrt(mean_squared_error(y, y_pred))
        r_value, _ = pearsonr(y, y_pred)

        self.logger.info(f"GNNWRæ¨¡å‹è®­ç»ƒå®Œæˆ: MAE={mae:.3f}, RMSE={rmse:.3f}, R={r_value:.3f}")

    def predict_with_gnnwr(self, X, cluster_predictions, coords=None):
        """ä½¿ç”¨GNNWRè¿›è¡Œé¢„æµ‹

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            cluster_predictions (np.array): å„èšç±»æ¨¡å‹çš„é¢„æµ‹ç»“æœ
            coords (np.array): åæ ‡æ•°æ®

        Returns:
            np.array: é¢„æµ‹ç»“æœ
        """
        if self.gnnwr_trainer is None:
            raise ValueError("GNNWRæ¨¡å‹å°šæœªè®­ç»ƒ")

        # ä½¿ç”¨ç‰¹å¾ï¼šåŸå§‹ç‰¹å¾ + å„èšç±»é¢„æµ‹
        gnnwr_features = np.hstack([X, cluster_predictions])

        # å¤„ç†ç¼ºå¤±å€¼
        if self.gnnwr_imputer is not None:
            gnnwr_features_imputed = self.gnnwr_imputer.transform(gnnwr_features)
        else:
            gnnwr_features_imputed = gnnwr_features

        # é¢„æµ‹
        if self.use_enhanced_gnnwr:
            return self.gnnwr_trainer.predict(gnnwr_features_imputed, coords)
        else:
            return self.gnnwr_trainer.predict(gnnwr_features_imputed)

    def evaluate_predictions(self, y_true, y_pred):
        """è¯„ä¼°é¢„æµ‹æ€§èƒ½

        Args:
            y_true (np.array): çœŸå®å€¼
            y_pred (np.array): é¢„æµ‹å€¼

        Returns:
            dict: è¯„ä¼°æŒ‡æ ‡
        """
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r_value, p_value = pearsonr(y_true, y_pred)

        return {
            'MAE': mae,
            'RMSE': rmse,
            'R': r_value,
            'R_squared': r_value ** 2,
            'samples': len(y_true)
        }

    def cross_validate(self, X, y, groups, coords=None, cv_type='station'):
        """æ‰§è¡Œäº¤å‰éªŒè¯ - ä½¿ç”¨GNNWRé›†æˆ"""
        logo = LeaveOneGroupOut()

        all_predictions = []
        all_true_values = []
        fold_results = {}

        unique_groups = np.unique(groups)
        total_folds = len(unique_groups)

        self.logger.info(f"å¼€å§‹{cv_type}äº¤å‰éªŒè¯ï¼Œå…±{total_folds}ä¸ªæŠ˜å ...")

        # åœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡Œä¸€æ¬¡èšç±»
        self.logger.info("åœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡Œèšç±»åˆ†é…...")
        self.cluster_assignments = self.perform_clustering(X, groups)

        for fold, (train_idx, test_idx) in enumerate(logo.split(X, y, groups)):
            group_id = groups[test_idx[0]]
            test_size = len(test_idx)
            train_size = len(train_idx)

            self.logger.debug(f"{cv_type} Fold {fold + 1}: è®­ç»ƒé›†{train_size}æ ·æœ¬, æµ‹è¯•é›†{test_size}æ ·æœ¬")

            # åˆ†å‰²æ•°æ®
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]
            groups_train, groups_test = groups[train_idx], groups[test_idx]
            # åˆ†å‰²åæ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            coords_train = coords[train_idx] if coords is not None else None
            coords_test = coords[test_idx] if coords is not None else None

            # ä½¿ç”¨å›ºå®šçš„èšç±»åˆ†é…
            train_cluster_labels = self.cluster_assignments[train_idx]
            test_cluster_labels = self.cluster_assignments[test_idx]

            # è®­ç»ƒèšç±»é›†æˆæ¨¡å‹
            try:
                # ç¬¬ä¸€æ­¥ï¼šä¸ºæ¯ä¸ªèšç±»è®­ç»ƒæ¨¡å‹
                self.train_cluster_models(X_train, y_train, train_cluster_labels)

                # ç¬¬äºŒæ­¥ï¼šè·å–è®­ç»ƒé›†ä¸Šçš„èšç±»é¢„æµ‹
                cluster_predictions_train = self._get_cluster_predictions(X_train, train_cluster_labels)

                # ç¬¬ä¸‰æ­¥ï¼šè®­ç»ƒGNNWRé›†æˆæ¨¡å‹
                self.train_gnnwr_model(X_train, y_train, cluster_predictions_train, coords_train)

                # ç¬¬å››æ­¥ï¼šé¢„æµ‹æµ‹è¯•é›†
                cluster_predictions_test = self._get_cluster_predictions(X_test, test_cluster_labels)
                y_pred = self.predict_with_gnnwr(X_test, cluster_predictions_test, coords_test)

                # å­˜å‚¨ç»“æœ
                all_predictions.extend(y_pred)
                all_true_values.extend(y_test)

                # è®¡ç®—å½“å‰æŠ˜å æ€§èƒ½
                fold_metrics = self.evaluate_predictions(y_test, y_pred)
                fold_results[group_id] = fold_metrics

                self.logger.info(
                    f"  {cv_type} Fold {fold + 1}/{total_folds}: {group_id} "
                    f"(èšç±»{test_cluster_labels[0]}, {test_size}æ ·æœ¬) - "
                    f"MAE={fold_metrics['MAE']:.3f}, R={fold_metrics['R']:.3f}"
                )

            except Exception as e:
                self.logger.error(f"æŠ˜å  {fold + 1} è®­ç»ƒå¤±è´¥: {e}")
                continue

        # è®¡ç®—æ€»ä½“æ€§èƒ½
        overall_metrics = self.evaluate_predictions(
            np.array(all_true_values),
            np.array(all_predictions)
        )

        self.logger.info(f"âœ… {cv_type}äº¤å‰éªŒè¯å®Œæˆ")
        self.logger.info(f"  èšåˆæ€§èƒ½: MAE={overall_metrics['MAE']:.3f}mm, R={overall_metrics['R']:.3f}")

        return {
            'overall': overall_metrics,
            'by_fold': fold_results,
            'predictions': np.array(all_predictions),
            'true_values': np.array(all_true_values),
            'folds': total_folds,
            'cluster_assignments': self.cluster_assignments
        }

    def run_complete_analysis(self, df, output_dir=None):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹

        Args:
            df (pd.DataFrame): è¾“å…¥æ•°æ®
            output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„

        Returns:
            dict: åˆ†æç»“æœ
        """
        self.logger.info("=" * 70)
        self.logger.info("ğŸš€ å¼€å§‹SWEèšç±»é›†æˆå›å½’å®Œæ•´åˆ†ææµç¨‹")
        self.logger.info("=" * 70)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = f"./swe_cluster_ensemble_results_{timestamp}"

        os.makedirs(output_dir, exist_ok=True)
        self.logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")

        try:
            # 1. æ•°æ®é¢„å¤„ç†
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 1: æ•°æ®é¢„å¤„ç†")
            self.logger.info("=" * 50)

            X, y, station_groups, year_groups, coords = self.preprocess_data(df)

            results = {
                'preprocessing': {
                    'samples': len(X),
                    'features': len(self.feature_columns),
                    'stations': len(np.unique(station_groups)),
                    'years': len(np.unique(year_groups)),
                    'n_clusters': self.n_clusters,
                    'has_coords': coords is not None
                }
            }

            # 2. åœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡Œèšç±»
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 2: ç«™ç‚¹çº§èšç±»åˆ†æ")
            self.logger.info("=" * 50)

            self.cluster_assignments = self.perform_clustering(X, station_groups)
            results['cluster_assignments'] = self.cluster_assignments

            # 3. ç«™ç‚¹äº¤å‰éªŒè¯ï¼ˆä½¿ç”¨å›ºå®šèšç±»ï¼‰
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 3: ç«™ç‚¹äº¤å‰éªŒè¯")
            self.logger.info("=" * 50)

            results['station_cv'] = self.cross_validate(X, y, station_groups, coords, 'station')

            # 4. å¹´åº¦äº¤å‰éªŒè¯ï¼ˆä½¿ç”¨å›ºå®šèšç±»ï¼‰
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 4: å¹´åº¦äº¤å‰éªŒè¯")
            self.logger.info("=" * 50)

            results['yearly_cv'] = self.cross_validate(X, y, year_groups, coords, 'yearly')

            # 5. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 5: è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
            self.logger.info("=" * 50)

            self.fit(X, y, station_groups, coords)

            results['final_model'] = {
                'kmeans': self.kmeans,
                'cluster_models': self.cluster_models,
                'gnnwr_trainer': self.gnnwr_trainer,
                'cluster_assignments': self.cluster_assignments,
                'feature_columns': self.feature_columns
            }

            # 6. ä¿å­˜ç»“æœ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 6: ä¿å­˜ç»“æœ")
            self.logger.info("=" * 50)

            self._save_results(results, output_dir)

            # 7. ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report(results)
            print(report)
            self.logger.info("ğŸ¯ èšç±»é›†æˆåˆ†æå®Œæˆï¼")
            return results

        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææµç¨‹å¤±è´¥: {str(e)}")
            raise

    def fit(self, X, y, station_groups, coords=None):
        """åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            y (np.array): ç›®æ ‡å˜é‡
            station_groups (np.array): ç«™ç‚¹åˆ†ç»„ä¿¡æ¯
            coords (np.array): åæ ‡æ•°æ®
        """
        self.logger.info("åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒèšç±»é›†æˆæ¨¡å‹...")

        # ç¬¬ä¸€æ­¥ï¼šåœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡Œèšç±»
        self.logger.info(f"åœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡ŒK-meansèšç±»ï¼Œèšç±»æ•°: {self.n_clusters}")
        self.cluster_assignments = self.perform_clustering(X, station_groups)

        # ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªèšç±»è®­ç»ƒæ¨¡å‹
        self.train_cluster_models(X, y, self.cluster_assignments)

        # ç¬¬ä¸‰æ­¥ï¼šè®­ç»ƒGNNWRé›†æˆæ¨¡å‹
        cluster_predictions = self._get_cluster_predictions(X, self.cluster_assignments)
        self.train_gnnwr_model(X, y, cluster_predictions, coords)

        self.logger.info("âœ… èšç±»é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")

    def predict(self, X, coords=None):
        """é¢„æµ‹æ–°æ ·æœ¬

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            coords (np.array): åæ ‡æ•°æ®

        Returns:
            np.array: é¢„æµ‹ç»“æœ
        """
        if self.kmeans is None or not self.cluster_models or self.gnnwr_trainer is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")

        # ç¬¬ä¸€æ­¥ï¼šèšç±»
        if np.isnan(X).any():
            imputer = SimpleImputer(strategy='median')
            X_imputed = imputer.fit_transform(X)
        else:
            X_imputed = X

        cluster_labels = self.kmeans.predict(X_imputed)

        # ç¬¬äºŒæ­¥ï¼šå„èšç±»æ¨¡å‹é¢„æµ‹
        cluster_predictions = np.zeros((len(X), self.n_clusters))

        for cluster_id, model in self.cluster_models.items():
            cluster_mask = cluster_labels == cluster_id
            if np.any(cluster_mask):
                cluster_predictions[cluster_mask, cluster_id] = model.predict(X[cluster_mask])

        # ç¬¬ä¸‰æ­¥ï¼šGNNWRé›†æˆé¢„æµ‹
        return self.predict_with_gnnwr(X, cluster_predictions, coords)

    def _save_results(self, results, output_dir):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•
        """
        self.logger.info("ä¿å­˜åˆ†æç»“æœ...")

        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(output_dir, 'swe_cluster_ensemble_model.pkl')
        joblib.dump({
            'kmeans': self.kmeans,
            'cluster_models': self.cluster_models,
            'gnnwr_trainer': self.gnnwr_trainer,
            'feature_columns': self.feature_columns,
            'params': self.params,
            'gnnwr_params': self.gnnwr_params,
            'n_clusters': self.n_clusters,
            'use_enhanced_gnnwr': self.use_enhanced_gnnwr
        }, model_path)

        # ä¿å­˜ç»“æœæ•°æ®
        results_path = os.path.join(output_dir, 'analysis_results.pkl')
        joblib.dump(results, results_path)

        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        report_path = os.path.join(output_dir, 'analysis_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(self._generate_report(results))

        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._create_visualizations(results, output_dir)

        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

    def _generate_report(self, results):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š

        Args:
            results (dict): åˆ†æç»“æœ

        Returns:
            str: æŠ¥å‘Šæ–‡æœ¬
        """
        report = []
        report.append("=" * 70)
        report.append("â„ï¸ SWEèšç±»é›†æˆå›å½’åˆ†ææŠ¥å‘Š")
        report.append("=" * 70)
        report.append("")

        # æ•°æ®æ¦‚å†µ
        preprocessing = results['preprocessing']
        report.append("ğŸ“Š æ•°æ®æ¦‚å†µ:")
        report.append(f"  æ ·æœ¬æ•°é‡: {preprocessing['samples']}")
        report.append(f"  ç‰¹å¾æ•°é‡: {preprocessing['features']}")
        report.append(f"  ç«™ç‚¹æ•°é‡: {preprocessing['stations']}")
        report.append(f"  å¹´ä»½æ•°é‡: {preprocessing['years']}")
        report.append(f"  èšç±»æ•°é‡: {preprocessing['n_clusters']}")
        report.append(f"  ä½¿ç”¨åæ ‡: {'æ˜¯' if preprocessing['has_coords'] else 'å¦'}")
        report.append(f"  GNNWRç‰ˆæœ¬: {'å¢å¼ºç‰ˆ' if self.use_enhanced_gnnwr else 'åŸºç¡€ç‰ˆ'}")
        report.append("")

        # ç«™ç‚¹äº¤å‰éªŒè¯ç»“æœ
        station_cv = results['station_cv']
        station_overall = station_cv['overall']
        report.append("ğŸ”ï¸ ç«™ç‚¹äº¤å‰éªŒè¯ç»“æœ:")
        report.append(f"  æŠ˜å æ•°é‡: {station_cv['folds']}")
        report.append(f"  MAE: {station_overall['MAE']:.3f} mm")
        report.append(f"  RMSE: {station_overall['RMSE']:.3f} mm")
        report.append(f"  R: {station_overall['R']:.3f}")
        report.append(f"  RÂ²: {station_overall['R_squared']:.3f}")
        report.append("")

        # å¹´åº¦äº¤å‰éªŒè¯ç»“æœ
        yearly_cv = results['yearly_cv']
        yearly_overall = yearly_cv['overall']
        report.append("ğŸ“… å¹´åº¦äº¤å‰éªŒè¯ç»“æœ:")
        report.append(f"  æŠ˜å æ•°é‡: {yearly_cv['folds']}")
        report.append(f"  MAE: {yearly_overall['MAE']:.3f} mm")
        report.append(f"  RMSE: {yearly_overall['RMSE']:.3f} mm")
        report.append(f"  R: {yearly_overall['R']:.3f}")
        report.append(f"  RÂ²: {yearly_overall['R_squared']:.3f}")
        report.append("")

        # èšç±»åˆ†å¸ƒ
        cluster_counts = np.bincount(results['cluster_assignments'])
        report.append("ğŸ” èšç±»åˆ†å¸ƒ:")
        for cluster_id, count in enumerate(cluster_counts):
            report.append(
                f"  èšç±» {cluster_id}: {count} ä¸ªæ ·æœ¬ ({count / len(results['cluster_assignments']) * 100:.1f}%)")
        report.append("")

        report.append("ğŸ¯ æ¨¡å‹é…ç½®:")
        report.append(f"  XGBoostå‚æ•°: {self.params}")
        report.append(f"  GNNWRå‚æ•°: {self.gnnwr_params}")

        return "\n".join(report)

    def _create_visualizations(self, results, output_dir):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•
        """
        self.logger.info("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

        try:
            # 1. é¢„æµ‹å€¼ä¸çœŸå®å€¼æ•£ç‚¹å›¾
            plt.figure(figsize=(12, 10))

            # ç«™ç‚¹äº¤å‰éªŒè¯æ•£ç‚¹å›¾
            plt.subplot(2, 2, 1)
            station_cv = results['station_cv']
            y_true_station = station_cv['true_values']
            y_pred_station = station_cv['predictions']

            plt.scatter(y_true_station, y_pred_station, alpha=0.6, s=20)
            plt.plot([y_true_station.min(), y_true_station.max()],
                     [y_true_station.min(), y_true_station.max()], 'r--', alpha=0.8)
            plt.xlabel('çœŸå®SWE (mm)')
            plt.ylabel('é¢„æµ‹SWE (mm)')
            plt.title(f'ç«™ç‚¹äº¤å‰éªŒè¯\nMAE={station_cv["overall"]["MAE"]:.2f}, R={station_cv["overall"]["R"]:.3f}')
            plt.grid(True, alpha=0.3)

            # å¹´åº¦äº¤å‰éªŒè¯æ•£ç‚¹å›¾
            plt.subplot(2, 2, 2)
            yearly_cv = results['yearly_cv']
            y_true_yearly = yearly_cv['true_values']
            y_pred_yearly = yearly_cv['predictions']

            plt.scatter(y_true_yearly, y_pred_yearly, alpha=0.6, s=20, color='orange')
            plt.plot([y_true_yearly.min(), y_true_yearly.max()],
                     [y_true_yearly.min(), y_true_yearly.max()], 'r--', alpha=0.8)
            plt.xlabel('çœŸå®SWE (mm)')
            plt.ylabel('é¢„æµ‹SWE (mm)')
            plt.title(f'å¹´åº¦äº¤å‰éªŒè¯\nMAE={yearly_cv["overall"]["MAE"]:.2f}, R={yearly_cv["overall"]["R"]:.3f}')
            plt.grid(True, alpha=0.3)

            # 3. èšç±»åˆ†å¸ƒå›¾
            plt.subplot(2, 2, 3)
            cluster_assignments = results['cluster_assignments']
            cluster_counts = np.bincount(cluster_assignments)
            colors = plt.cm.Set3(np.linspace(0, 1, len(cluster_counts)))

            bars = plt.bar(range(len(cluster_counts)), cluster_counts, color=colors)
            plt.xlabel('èšç±»ID')
            plt.ylabel('æ ·æœ¬æ•°é‡')
            plt.title('èšç±»åˆ†å¸ƒ')
            plt.xticks(range(len(cluster_counts)))

            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, count in zip(bars, cluster_counts):
                plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                         f'{count}', ha='center', va='bottom')

            # 4. æ€§èƒ½å¯¹æ¯”å›¾
            plt.subplot(2, 2, 4)
            methods = ['ç«™ç‚¹CV', 'å¹´åº¦CV']
            maes = [station_cv['overall']['MAE'], yearly_cv['overall']['MAE']]
            rs = [station_cv['overall']['R'], yearly_cv['overall']['R']]

            x = np.arange(len(methods))
            width = 0.35

            fig, ax1 = plt.subplots(figsize=(10, 6))
            ax2 = ax1.twinx()

            bars1 = ax1.bar(x - width / 2, maes, width, label='MAE', alpha=0.7, color='skyblue')
            bars2 = ax2.bar(x + width / 2, rs, width, label='R', alpha=0.7, color='lightcoral')

            ax1.set_xlabel('éªŒè¯æ–¹æ³•')
            ax1.set_ylabel('MAE (mm)', color='skyblue')
            ax2.set_ylabel('R', color='lightcoral')
            ax1.set_xticks(x)
            ax1.set_xticklabels(methods)
            ax1.legend(loc='upper left')
            ax2.legend(loc='upper right')
            plt.title('äº¤å‰éªŒè¯æ€§èƒ½å¯¹æ¯”')

            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'performance_visualization.png'),
                        dpi=300, bbox_inches='tight')
            plt.close()

        except Exception as e:
            self.logger.warning(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")


def get_feature_importance(self):
    """è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆåŸºäºå„èšç±»æ¨¡å‹çš„å¹³å‡é‡è¦æ€§ï¼‰"""
    if not self.cluster_models:
        raise ValueError("èšç±»æ¨¡å‹å°šæœªè®­ç»ƒ")

    # æ”¶é›†æ‰€æœ‰ç‰¹å¾çš„é‡è¦æ€§
    all_importances = []
    for cluster_id, model in self.cluster_models.items():
        importance_scores = model.feature_importances_
        all_importances.append(importance_scores)

    # è®¡ç®—å¹³å‡é‡è¦æ€§
    avg_importance = np.mean(all_importances, axis=0)

    # åˆ›å»ºDataFrame
    feature_importance_df = pd.DataFrame({
        'feature': self.feature_columns,
        'importance': avg_importance
    }).sort_values('importance', ascending=False)

    self.logger.info(f"ç‰¹å¾é‡è¦æ€§è®¡ç®—å®Œæˆï¼Œæœ€é«˜é‡è¦æ€§ç‰¹å¾: {feature_importance_df['feature'].iloc[0]}")
    return feature_importance_df


def analyze_cluster_characteristics(self, df):
    """åˆ†æå„èšç±»çš„ç‰¹å¾

    Args:
        df (pd.DataFrame): åŸå§‹æ•°æ®

    Returns:
        dict: èšç±»åˆ†æç»“æœ
    """
    if self.cluster_assignments is None:
        raise ValueError("èšç±»å°šæœªæ‰§è¡Œ")

    self.logger.info("åˆ†æå„èšç±»ç‰¹å¾...")

    cluster_stats = {}
    feature_cols = [col for col in self.feature_columns if col in df.columns]

    for cluster_id in range(self.n_clusters):
        cluster_mask = self.cluster_assignments == cluster_id
        cluster_data = df[cluster_mask]
        cluster_size = len(cluster_data)

        if cluster_size == 0:
            continue

        stats = {
            'size': cluster_size,
            'swe_mean': cluster_data[self.target_column].mean(),
            'swe_std': cluster_data[self.target_column].std(),
            'features': {}
        }

        # è®¡ç®—å„ç‰¹å¾çš„ç»Ÿè®¡é‡
        for feature in feature_cols:
            if feature in cluster_data.columns:
                stats['features'][feature] = {
                    'mean': cluster_data[feature].mean(),
                    'std': cluster_data[feature].std(),
                    'median': cluster_data[feature].median()
                }

        cluster_stats[cluster_id] = stats

        self.logger.info(f"  èšç±» {cluster_id}: {cluster_size}æ ·æœ¬, SWEå‡å€¼={stats['swe_mean']:.2f}mm")

    return cluster_stats


def create_cluster_analysis_report(self, df, output_dir):
    """åˆ›å»ºèšç±»åˆ†ææŠ¥å‘Š

    Args:
        df (pd.DataFrame): åŸå§‹æ•°æ®
        output_dir (str): è¾“å‡ºç›®å½•
    """
    try:
        self.logger.info("åˆ›å»ºèšç±»åˆ†ææŠ¥å‘Š...")

        # è·å–èšç±»ç»Ÿè®¡
        cluster_stats = self.analyze_cluster_characteristics(df)

        # åˆ›å»ºèšç±»ç‰¹å¾å¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. èšç±»å¤§å°åˆ†å¸ƒ
        cluster_sizes = [stats['size'] for stats in cluster_stats.values()]
        cluster_ids = list(cluster_stats.keys())

        axes[0, 0].bar(cluster_ids, cluster_sizes, color=plt.cm.Set3(np.linspace(0, 1, len(cluster_ids))))
        axes[0, 0].set_title('å„èšç±»æ ·æœ¬æ•°é‡')
        axes[0, 0].set_xlabel('èšç±»ID')
        axes[0, 0].set_ylabel('æ ·æœ¬æ•°é‡')
        for i, v in enumerate(cluster_sizes):
            axes[0, 0].text(i, v, str(v), ha='center', va='bottom')

        # 2. å„èšç±»SWEå‡å€¼
        swe_means = [stats['swe_mean'] for stats in cluster_stats.values()]
        axes[0, 1].bar(cluster_ids, swe_means, color=plt.cm.Set3(np.linspace(0, 1, len(cluster_ids))))
        axes[0, 1].set_title('å„èšç±»SWEå‡å€¼')
        axes[0, 1].set_xlabel('èšç±»ID')
        axes[0, 1].set_ylabel('SWEå‡å€¼ (mm)')
        for i, v in enumerate(swe_means):
            axes[0, 1].text(i, v, f'{v:.1f}', ha='center', va='bottom')

        # 3. é‡è¦ç‰¹å¾åœ¨å„èšç±»çš„åˆ†å¸ƒ
        feature_importance = self.get_feature_importance()
        top_features = feature_importance.head(3)['feature'].tolist()

        for i, feature in enumerate(top_features):
            if i >= 2:  # åªæ˜¾ç¤ºå‰ä¸¤ä¸ªç‰¹å¾
                break
            feature_means = []
            for cluster_id, stats in cluster_stats.items():
                if feature in stats['features']:
                    feature_means.append(stats['features'][feature]['mean'])
                else:
                    feature_means.append(0)

            axes[1, i].bar(cluster_ids, feature_means,
                           color=plt.cm.Set3(np.linspace(0, 1, len(cluster_ids))))
            axes[1, i].set_title(f'{feature}åœ¨å„èšç±»çš„å‡å€¼')
            axes[1, i].set_xlabel('èšç±»ID')
            axes[1, i].set_ylabel(f'{feature}å‡å€¼')

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'cluster_analysis.png'),
                    dpi=300, bbox_inches='tight')
        plt.close()

        # ä¿å­˜èšç±»ç»Ÿè®¡åˆ°CSV
        cluster_report = []
        for cluster_id, stats in cluster_stats.items():
            row = {
                'cluster_id': cluster_id,
                'size': stats['size'],
                'swe_mean': stats['swe_mean'],
                'swe_std': stats['swe_std']
            }

            # æ·»åŠ é‡è¦ç‰¹å¾ä¿¡æ¯
            for feature in top_features:
                if feature in stats['features']:
                    row[f'{feature}_mean'] = stats['features'][feature]['mean']
                    row[f'{feature}_std'] = stats['features'][feature]['std']
                else:
                    row[f'{feature}_mean'] = np.nan
                    row[f'{feature}_std'] = np.nan

            cluster_report.append(row)

        cluster_df = pd.DataFrame(cluster_report)
        cluster_df.to_csv(os.path.join(output_dir, 'cluster_statistics.csv'), index=False)

        self.logger.info(f"âœ… èšç±»åˆ†ææŠ¥å‘Šä¿å­˜å®Œæˆ")

    except Exception as e:
        self.logger.warning(f"åˆ›å»ºèšç±»åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")


def compare_with_baseline(self, df, output_dir):
    """ä¸åŸºçº¿æ¨¡å‹æ¯”è¾ƒ

    Args:
        df (pd.DataFrame): åŸå§‹æ•°æ®
        output_dir (str): è¾“å‡ºç›®å½•
    """
    try:
        self.logger.info("ä¸åŸºçº¿æ¨¡å‹æ¯”è¾ƒ...")

        # é¢„å¤„ç†æ•°æ®
        X, y, station_groups, year_groups, coords = self.preprocess_data(df)

        # è®­ç»ƒæ™®é€šXGBoostæ¨¡å‹ä½œä¸ºåŸºçº¿
        from swe_trainer import SWEXGBoostTrainer
        baseline_trainer = SWEXGBoostTrainer(params=self.params)

        # ç«™ç‚¹äº¤å‰éªŒè¯
        baseline_station_results = baseline_trainer.cross_validate(X, y, station_groups, 'station')
        baseline_yearly_results = baseline_trainer.cross_validate(X, y, year_groups, 'yearly')

        # æ¯”è¾ƒç»“æœ
        comparison = {
            'station_cv': {
                'baseline_mae': baseline_station_results['overall']['MAE'],
                'ensemble_mae': self.cross_validate(X, y, station_groups, coords, 'station')['overall']['MAE'],
                'baseline_r': baseline_station_results['overall']['R'],
                'ensemble_r': self.cross_validate(X, y, station_groups, coords, 'station')['overall']['R'],
                'improvement_mae': (baseline_station_results['overall']['MAE'] -
                                    self.cross_validate(X, y, station_groups, coords, 'station')['overall']['MAE']),
                'improvement_r': (self.cross_validate(X, y, station_groups, coords, 'station')['overall']['R'] -
                                  baseline_station_results['overall']['R'])
            },
            'yearly_cv': {
                'baseline_mae': baseline_yearly_results['overall']['MAE'],
                'ensemble_mae': self.cross_validate(X, y, year_groups, coords, 'yearly')['overall']['MAE'],
                'baseline_r': baseline_yearly_results['overall']['R'],
                'ensemble_r': self.cross_validate(X, y, year_groups, coords, 'yearly')['overall']['R'],
                'improvement_mae': (baseline_yearly_results['overall']['MAE'] -
                                    self.cross_validate(X, y, year_groups, coords, 'yearly')['overall']['MAE']),
                'improvement_r': (self.cross_validate(X, y, year_groups, coords, 'yearly')['overall']['R'] -
                                  baseline_yearly_results['overall']['R'])
            }
        }

        # åˆ›å»ºæ¯”è¾ƒå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

        # MAEæ¯”è¾ƒ
        methods = ['åŸºçº¿', 'èšç±»é›†æˆ']
        station_mae = [comparison['station_cv']['baseline_mae'], comparison['station_cv']['ensemble_mae']]
        yearly_mae = [comparison['yearly_cv']['baseline_mae'], comparison['yearly_cv']['ensemble_mae']]

        x = np.arange(len(methods))
        width = 0.35

        ax1.bar(x - width / 2, station_mae, width, label='ç«™ç‚¹CV', alpha=0.7)
        ax1.bar(x + width / 2, yearly_mae, width, label='å¹´åº¦CV', alpha=0.7)
        ax1.set_xlabel('æ¨¡å‹ç±»å‹')
        ax1.set_ylabel('MAE (mm)')
        ax1.set_title('MAEæ¯”è¾ƒ')
        ax1.set_xticks(x)
        ax1.set_xticklabels(methods)
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # Rå€¼æ¯”è¾ƒ
        station_r = [comparison['station_cv']['baseline_r'], comparison['station_cv']['ensemble_r']]
        yearly_r = [comparison['yearly_cv']['baseline_r'], comparison['yearly_cv']['ensemble_r']]

        ax2.bar(x - width / 2, station_r, width, label='ç«™ç‚¹CV', alpha=0.7)
        ax2.bar(x + width / 2, yearly_r, width, label='å¹´åº¦CV', alpha=0.7)
        ax2.set_xlabel('æ¨¡å‹ç±»å‹')
        ax2.set_ylabel('R')
        ax2.set_title('ç›¸å…³ç³»æ•°æ¯”è¾ƒ')
        ax2.set_xticks(x)
        ax2.set_xticklabels(methods)
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'baseline_comparison.png'),
                    dpi=300, bbox_inches='tight')
        plt.close()

        # ä¿å­˜æ¯”è¾ƒç»“æœ
        comparison_df = pd.DataFrame([
            {
                'method': 'station_cv',
                'baseline_mae': comparison['station_cv']['baseline_mae'],
                'ensemble_mae': comparison['station_cv']['ensemble_mae'],
                'improvement_mae': comparison['station_cv']['improvement_mae'],
                'baseline_r': comparison['station_cv']['baseline_r'],
                'ensemble_r': comparison['station_cv']['ensemble_r'],
                'improvement_r': comparison['station_cv']['improvement_r']
            },
            {
                'method': 'yearly_cv',
                'baseline_mae': comparison['yearly_cv']['baseline_mae'],
                'ensemble_mae': comparison['yearly_cv']['ensemble_mae'],
                'improvement_mae': comparison['yearly_cv']['improvement_mae'],
                'baseline_r': comparison['yearly_cv']['baseline_r'],
                'ensemble_r': comparison['yearly_cv']['ensemble_r'],
                'improvement_r': comparison['yearly_cv']['improvement_r']
            }
        ])

        comparison_df.to_csv(os.path.join(output_dir, 'baseline_comparison.csv'), index=False)

        self.logger.info("âœ… åŸºçº¿æ¯”è¾ƒå®Œæˆ")
        return comparison

    except Exception as e:
        self.logger.warning(f"åŸºçº¿æ¯”è¾ƒå¤±è´¥: {e}")
        return None


# ä¾¿æ·ä½¿ç”¨å‡½æ•°
def train_swe_cluster_ensemble(data_df, output_dir=None, n_clusters=4, params=None,
                               use_enhanced_gnnwr=True, gnnwr_params=None):
    """ä¾¿æ·å‡½æ•°ï¼šè®­ç»ƒSWEèšç±»é›†æˆæ¨¡å‹

    Args:
        data_df (pd.DataFrame): åŒ…å«ç‰¹å¾å’ŒSWEçš„æ•°æ®
        output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„
        n_clusters (int, optional): èšç±»æ•°é‡
        params (dict, optional): XGBoostå‚æ•°
        use_enhanced_gnnwr (bool): æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰ˆGNNWR
        gnnwr_params (dict): GNNWRå‚æ•°

    Returns:
        dict: åŒ…å«æ‰€æœ‰è®­ç»ƒç»“æœçš„å­—å…¸
    """
    trainer = SWEClusterEnsemble(
        n_clusters=n_clusters,
        params=params,
        gnnwr_params=gnnwr_params,
        use_enhanced_gnnwr=use_enhanced_gnnwr
    )
    return trainer.run_complete_analysis(data_df, output_dir)


def load_swe_cluster_ensemble(model_path):
    """åŠ è½½å·²è®­ç»ƒçš„SWEèšç±»é›†æˆæ¨¡å‹

    Args:
        model_path (str): æ¨¡å‹æ–‡ä»¶è·¯å¾„

    Returns:
        SWEClusterEnsemble: åŠ è½½çš„æ¨¡å‹å®ä¾‹
    """
    model_data = joblib.load(model_path)

    trainer = SWEClusterEnsemble(
        n_clusters=model_data['n_clusters'],
        params=model_data['params'],
        gnnwr_params=model_data['gnnwr_params'],
        use_enhanced_gnnwr=model_data.get('use_enhanced_gnnwr', True)
    )

    trainer.kmeans = model_data['kmeans']
    trainer.cluster_models = model_data['cluster_models']
    trainer.gnnwr_trainer = model_data['gnnwr_trainer']
    trainer.feature_columns = model_data['feature_columns']
    trainer.cluster_assignments = model_data.get('cluster_assignments')

    return trainer


# æµ‹è¯•å‡½æ•°
def test_cluster_ensemble():
    """æµ‹è¯•èšç±»é›†æˆæ¨¡å‹"""
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    n_features = 10

    # ç”Ÿæˆç©ºé—´åæ ‡
    coords = np.random.uniform(0, 100, (n_samples, 2))

    # ç”Ÿæˆç‰¹å¾
    features = np.random.randn(n_samples, n_features)

    # åˆ›å»ºå…·æœ‰ç©ºé—´ç›¸å…³æ€§çš„ç›®æ ‡å˜é‡
    spatial_effect = np.exp(-0.01 * coords[:, 0]) + np.sin(0.1 * coords[:, 1])
    targets = (features[:, 0] + 2 * features[:, 1] + 0.5 * spatial_effect +
               np.random.normal(0, 0.1, n_samples))

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®æ¡†
    df = pd.DataFrame(features, columns=[f'feature_{i}' for i in range(n_features)])
    df['swe'] = targets
    df['station_id'] = [f'station_{i % 20}' for i in range(n_samples)]
    df['year'] = np.random.randint(2018, 2023, n_samples)
    df['longitude'] = coords[:, 0]
    df['latitude'] = coords[:, 1]

    # è®­ç»ƒæ¨¡å‹
    results = train_swe_cluster_ensemble(
        data_df=df,
        n_clusters=3,
        use_enhanced_gnnwr=True
    )

    return results


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("æµ‹è¯•èšç±»é›†æˆæ¨¡å‹...")
    results = test_cluster_ensemble()
    print("æµ‹è¯•å®Œæˆï¼")