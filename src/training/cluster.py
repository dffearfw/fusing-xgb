import logging
import unittest
import warnings

import torch.nn as nn
# import logger
import numpy as np
import pandas as pd
import torch
import xgboost as xgb
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, mean_squared_error
from scipy.stats import pearsonr
from sklearn.model_selection import LeaveOneGroupOut
import joblib
import os
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from torch import optim
from torch.amp import GradScaler, autocast
from torch.utils.data import DataLoader
from tqdm import tqdm
from GNNWR import EnhancedGNNWRTrainer


# ç¦ç”¨TF32ç›¸å…³è­¦å‘Šï¼ˆCPUä¸Šä¸éœ€è¦ï¼‰
warnings.filterwarnings("ignore", message=".*TF32.*")

# CPUæ€§èƒ½ä¼˜åŒ–è®¾ç½®
torch.set_num_threads(24)  # i9-14900KFæœ‰24ä¸ªç‰©ç†æ ¸å¿ƒ
os.environ['OMP_NUM_THREADS'] = '24'
os.environ['MKL_NUM_THREADS'] = '24'
os.environ['OPENMP'] = '1'

# ç¦ç”¨CUDAç›¸å…³è®¾ç½®ï¼ˆé¿å…ä¸å¿…è¦çš„GPUæ£€æŸ¥ï¼‰
torch.backends.cudnn.enabled = True

# è®¾ç½®çŸ©é˜µä¹˜æ³•ç²¾åº¦ï¼ˆCPUä¸Šä½¿ç”¨é«˜ç²¾åº¦ï¼‰
torch.set_float32_matmul_precision('high')

# å…ˆåˆ›å»ºlogger
logger = logging.getLogger("SWEClusterEnsemble")

# ç„¶åå¯¼å…¥å¢å¼ºç‰ˆGNNWR
try:
    from GNNWR import EnhancedSpatialDataset, EnhancedGNNWRTrainer, SpatialWeightCalculator

    HAS_ENHANCED_GNNWR = True
    logger.info("æˆåŠŸå¯¼å…¥å¢å¼ºç‰ˆGNNWR")
except ImportError as e:
    logger.warning(f"æ— æ³•å¯¼å…¥å¢å¼ºç‰ˆGNNWR: {e}")
    try:
        # å°è¯•å¯¼å…¥åŸºç¡€ç‰ˆ
        from GNNWR import SpatialDataset, GNNWRTrainer

        HAS_ENHANCED_GNNWR = False
        logger.info("ä½¿ç”¨åŸºç¡€ç‰ˆGNNWR")
    except ImportError:
        logger.error("æ— æ³•å¯¼å…¥ä»»ä½•GNNWRç‰ˆæœ¬")
        HAS_ENHANCED_GNNWR = False


        # åˆ›å»ºè™šæ‹Ÿç±»ä»¥é¿å…åç»­é”™è¯¯
        class EnhancedSpatialDataset:
            def __init__(self, features, targets, coords=None):
                self.features = features
                self.targets = targets
                self.coords = coords

            def __len__(self):
                return len(self.features)

            def __getitem__(self, idx):
                if self.coords is not None:
                    return self.features[idx], self.targets[idx], self.coords[idx]
                else:
                    return self.features[idx], self.targets[idx]


        class EnhancedGNNWRTrainer:
            def __init__(self, *args, **kwargs):
                logger.warning("ä½¿ç”¨è™šæ‹ŸEnhancedGNNWRTrainer")

            def train(self, *args, **kwargs):
                logger.warning("è™šæ‹Ÿè®­ç»ƒæ–¹æ³•")

            def predict(self, features, coords=None):
                logger.warning("è™šæ‹Ÿé¢„æµ‹æ–¹æ³•")
                return np.random.normal(50, 20, len(features))


        class SpatialDataset:
            def __init__(self, features, targets):
                self.features = features
                self.targets = targets

            def __len__(self):
                return len(self.features)

            def __getitem__(self, idx):
                return self.features[idx], self.targets[idx]


        class GNNWRTrainer:
            def __init__(self, *args, **kwargs):
                logger.warning("ä½¿ç”¨è™šæ‹ŸGNNWRTrainer")

            def train(self, *args, **kwargs):
                logger.warning("è™šæ‹Ÿè®­ç»ƒæ–¹æ³•")

            def predict(self, features):
                logger.warning("è™šæ‹Ÿé¢„æµ‹æ–¹æ³•")
                return np.random.normal(50, 20, len(features))


class SWEClusterEnsemble:
    """SWEèšç±»é›†æˆå›å½’å™¨ - ä½¿ç”¨å¢å¼ºç‰ˆGNNWRè¿›è¡Œé›†æˆ"""



    DEFAULT_PARAMS = {
        'n_estimators': 60,
        'learning_rate': 0.17,
        'max_depth': 5,
        'min_child_weight': 5,
        'gamma': 0,
        'subsample': 0.8,
        'colsample_bytree': 0.5,
        'reg_alpha': 0.05,
        'random_state': 42,
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse'
    }

    def __init__(self, n_clusters=4, params=None, gnnwr_params=None,
                 use_enhanced_gnnwr=True, use_rf=False, device='auto',
                 mixed_precision=True, cpu_workers=24):
        """åˆå§‹åŒ–èšç±»é›†æˆå›å½’å™¨

        Args:
            n_clusters (int): èšç±»æ•°é‡
            params (dict): XGBoostå‚æ•°
            gnnwr_params (dict): GNNWRå‚æ•°
            use_enhanced_gnnwr (bool): æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰ˆGNNWR
            device (str): è®¾å¤‡ç±»å‹ 'auto', 'cuda', 'cpu'
            mixed_precision (bool): æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦
            cpu_workers (int): CPUå·¥ä½œçº¿ç¨‹æ•°
        """
        self.logger = logging.getLogger("SWEClusterEnsemble")

        # è®¾å¤‡é…ç½®
        if device == 'auto':
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
                torch.backends.cudnn.benchmark = True
                self.logger.info(f"è‡ªåŠ¨é€‰æ‹©GPU: {torch.cuda.get_device_name()}")
            else:
                self.device = torch.device('cpu')
                torch.set_num_threads(cpu_workers)
                self.logger.info(f"ä½¿ç”¨CPU: {cpu_workers}çº¿ç¨‹")
        else:
            self.device = torch.device(device)
            if device == 'cpu':
                torch.set_num_threads(cpu_workers)

        self.mixed_precision = mixed_precision and self.device.type == 'cuda'
        self.cpu_workers = cpu_workers

        self.n_clusters = n_clusters
        self.kmeans = None
        self.cluster_assignments = None
        self.cluster_models = {}
        self.gnnwr_trainer = None
        self.feature_columns = None
        self.target_column = 'swe'
        self.use_enhanced_gnnwr = use_enhanced_gnnwr and HAS_ENHANCED_GNNWR
        self.use_rf = use_rf

        # å…³é”®ä¿®å¤ï¼šç¡®ä¿paramsä¸ä¸ºNone
        if params is None:
            params = {}

        if use_rf:
            # RFå‚æ•° - ä¼˜åŒ–CPUä½¿ç”¨
            self.rf_params = {
                'n_estimators': params.get('n_estimators', 100),
                'max_depth': params.get('max_depth', None),
                'min_samples_split': 2,
                'min_samples_leaf': 1,
                'random_state': 42,
                'n_jobs': min(16, cpu_workers)  # ä¼˜åŒ–CPUä½¿ç”¨
            }
            self.params = params if params else self.DEFAULT_PARAMS.copy()
        else:
            # XGBå‚æ•° - å¦‚æœä½¿ç”¨GPUè®­ç»ƒXGBoost
            self.params = self.DEFAULT_PARAMS.copy()
            if params:
                self.params.update(params)

            # å¦‚æœä½¿ç”¨GPUä¸”å®‰è£…äº†æ”¯æŒGPUçš„XGBoost
            if self.device.type == 'cuda' and not use_rf:
                self.params['tree_method'] = 'gpu_hist'
                self.params['predictor'] = 'gpu_predictor'
                self.logger.info("XGBoostä½¿ç”¨GPUåŠ é€Ÿ")

        # GNNWRå‚æ•° - æ·»åŠ GPUå’Œæ··åˆç²¾åº¦æ”¯æŒ
        self.gnnwr_params = {
            'hidden_dims': [256, 128, 64, 32],  # æ›´å¤§çš„æ¨¡å‹å……åˆ†åˆ©ç”¨GPU
            'learning_rate': 0.001,
            'epochs': 200,
            'batch_size': 512,  # æ›´å¤§çš„æ‰¹æ¬¡å¤§å°
            'patience': 20,
            'bandwidth': 5.0,
            'use_spatial_weights': True,
            'device': self.device,  # ä¼ é€’è®¾å¤‡å‚æ•°
            'mixed_precision': self.mixed_precision,  # æ··åˆç²¾åº¦
            'cpu_workers': self.cpu_workers,  # CPUå·¥ä½œçº¿ç¨‹
            'dropout_rate': 0.3,
            'weight_decay': 1e-4,
            'num_workers': min(12, self.cpu_workers // 2)  # ä¼˜åŒ–æ•°æ®åŠ è½½
        }
        if gnnwr_params:
            self.gnnwr_params.update(gnnwr_params)

        self.logger.info(f"åˆå§‹åŒ–SWEèšç±»é›†æˆå›å½’å™¨ï¼Œèšç±»æ•°: {n_clusters}")
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        self.logger.info(f"æ··åˆç²¾åº¦: {self.mixed_precision}")

    def preprocess_data(self, df):
        """æ•°æ®é¢„å¤„ç† - å®Œæ•´è°ƒè¯•ç‰ˆæœ¬"""
        self.logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")

        # ç¡®å®šç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—
        if self.feature_columns is None:
            # è‡ªåŠ¨é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆæ’é™¤ç›®æ ‡åˆ—å’Œå…¶ä»–éç‰¹å¾åˆ—ï¼‰
            exclude_cols = [self.target_column, 'station_id', 'year', 'date', 'station', 'group',
                            'longitude', 'latitude', 'lon', 'lat']  # æ’é™¤åæ ‡åˆ—
            self.feature_columns = [col for col in df.columns if
                                    col not in exclude_cols and df[col].dtype in [np.int64, np.float64]]

        self.logger.info(f"ä½¿ç”¨ç‰¹å¾: {self.feature_columns}")

        # æå–ç‰¹å¾å’Œç›®æ ‡
        X = df[self.feature_columns].values
        y = df[self.target_column].values

        # å¤„ç†ç¼ºå¤±å€¼
        if np.isnan(X).any():
            self.logger.info("å¤„ç†ç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼")
            self.feature_imputer = SimpleImputer(strategy='median')
            X = self.feature_imputer.fit_transform(X)
        else:
            self.feature_imputer = None

        # åˆ›å»ºåˆ†ç»„ä¿¡æ¯
        if 'station_id' in df.columns:
            station_groups = df['station_id'].values
        elif 'station' in df.columns:
            station_groups = df['station'].values
        else:
            # å¦‚æœæ²¡æœ‰ç«™ç‚¹ä¿¡æ¯ï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºåˆ†ç»„
            station_groups = np.arange(len(df))
            self.logger.warning("æœªæ‰¾åˆ°ç«™ç‚¹ä¿¡æ¯ï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºåˆ†ç»„")

        if 'year' in df.columns:
            year_groups = df['year'].values
        else:
            # å¦‚æœæ²¡æœ‰å¹´ä»½ä¿¡æ¯ï¼Œåˆ›å»ºè™šæ‹Ÿå¹´ä»½
            year_groups = np.ones(len(df), dtype=int)
            self.logger.warning("æœªæ‰¾åˆ°å¹´ä»½ä¿¡æ¯ï¼Œä½¿ç”¨ç»Ÿä¸€å¹´ä»½åˆ†ç»„")

        # === åæ ‡è°ƒè¯•éƒ¨åˆ† ===
        self.logger.info("=== åæ ‡è°ƒè¯•ä¿¡æ¯ ===")

        # æ£€æŸ¥åæ ‡åˆ—çš„å­˜åœ¨å’Œå†…å®¹
        coord_columns = ['longitude', 'latitude', 'lon', 'lat']
        available_coords = [col for col in coord_columns if col in df.columns]
        self.logger.info(f"æ‰¾åˆ°çš„åæ ‡åˆ—: {available_coords}")

        for col in available_coords:
            if col in df.columns:
                non_na_count = df[col].notna().sum()
                dtype = df[col].dtype
                min_val = df[col].min() if non_na_count > 0 else "N/A"
                max_val = df[col].max() if non_na_count > 0 else "N/A"
                self.logger.info(f"  {col}: éç©ºå€¼={non_na_count}, ç±»å‹={dtype}, èŒƒå›´=[{min_val}, {max_val}]")

        # æå–åæ ‡ä¿¡æ¯
        coords = None
        if all(col in df.columns for col in ['longitude', 'latitude']):
            coords = df[['longitude', 'latitude']].values
            self.logger.info(f"âœ… ä½¿ç”¨ç»çº¬åº¦åæ ‡: {len(coords)} ä¸ªç‚¹")
            self.logger.info(
                f"   åæ ‡èŒƒå›´: lon[{coords[:, 0].min():.2f}, {coords[:, 0].max():.2f}], lat[{coords[:, 1].min():.2f}, {coords[:, 1].max():.2f}]")

            # æ£€æŸ¥æ˜¯å¦æœ‰NaNåæ ‡
            nan_coords = np.isnan(coords).any(axis=1).sum()
            if nan_coords > 0:
                self.logger.warning(f"âš ï¸  å‘ç° {nan_coords} ä¸ªåæ ‡åŒ…å«NaNå€¼")
                # ä½¿ç”¨å‡å€¼å¡«å……NaNåæ ‡
                for i in range(coords.shape[1]):
                    col_mean = np.nanmean(coords[:, i])
                    nan_mask = np.isnan(coords[:, i])
                    coords[nan_mask, i] = col_mean
                    self.logger.info(f"   åˆ— {i} çš„NaNå€¼å·²ç”¨å‡å€¼ {col_mean:.4f} å¡«å……")

        elif all(col in df.columns for col in ['lon', 'lat']):
            coords = df[['lon', 'lat']].values
            self.logger.info(f"âœ… ä½¿ç”¨ç»çº¬åº¦åæ ‡: {len(coords)} ä¸ªç‚¹")
        else:
            self.logger.warning("âŒ æœªæ‰¾åˆ°åæ ‡ä¿¡æ¯ï¼Œå°†ä½¿ç”¨è™šæ‹Ÿåæ ‡")
            unique_stations = np.unique(station_groups)
            station_to_coord = {station: [i, i] for i, station in enumerate(unique_stations)}
            coords = np.array([station_to_coord[station] for station in station_groups])
            self.logger.info(f"   ç”Ÿæˆè™šæ‹Ÿåæ ‡: {len(coords)} ä¸ªç‚¹")

        self.logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(X)}ä¸ªæ ·æœ¬, {X.shape[1]}ä¸ªç‰¹å¾")
        self.logger.info(f"ç«™ç‚¹æ•°: {len(np.unique(station_groups))}, å¹´ä»½æ•°: {len(np.unique(year_groups))}")
        self.logger.info(f"åæ ‡æœ€ç»ˆçŠ¶æ€: {'å¯ç”¨' if coords is not None else 'ä¸å¯ç”¨'}")

        return X, y, station_groups, year_groups, coords

    def perform_clustering(self, X, groups):
        """æ‰§è¡Œèšç±»åˆ†æ

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            groups (np.array): åˆ†ç»„ä¿¡æ¯

        Returns:
            np.array: èšç±»æ ‡ç­¾
        """
        self.logger.info(f"æ‰§è¡ŒK-meansèšç±»ï¼Œèšç±»æ•°: {self.n_clusters}")

        # æŒ‰ç«™ç‚¹èšåˆç‰¹å¾
        unique_groups = np.unique(groups)
        group_features = []

        for group in unique_groups:
            group_mask = groups == group
            group_data = X[group_mask]
            # ä½¿ç”¨æ¯ä¸ªç«™ç‚¹çš„ç‰¹å¾å‡å€¼ä½œä¸ºèšç±»ç‰¹å¾
            group_mean = np.nanmean(group_data, axis=0)
            group_features.append(group_mean)

        group_features = np.array(group_features)

        # å¤„ç†å¯èƒ½çš„NaNå€¼
        if np.isnan(group_features).any():
            self.logger.info("å¤„ç†èšç±»ç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼")
            cluster_imputer = SimpleImputer(strategy='median')
            group_features = cluster_imputer.fit_transform(group_features)

        # æ‰§è¡ŒK-meansèšç±»
        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)
        group_clusters = self.kmeans.fit_predict(group_features)

        # å°†èšç±»æ ‡ç­¾æ˜ å°„å›åŸå§‹æ ·æœ¬
        cluster_assignments = np.zeros(len(X), dtype=int)
        for i, group in enumerate(unique_groups):
            group_mask = groups == group
            cluster_assignments[group_mask] = group_clusters[i]

        # ç»Ÿè®¡æ¯ä¸ªèšç±»çš„æ ·æœ¬æ•°
        cluster_counts = np.bincount(cluster_assignments)
        self.logger.info(f"èšç±»åˆ†å¸ƒ: {dict(enumerate(cluster_counts))}")

        return cluster_assignments

    def train_cluster_models(self, X, y, cluster_labels):
        """ä¸ºæ¯ä¸ªèšç±»è®­ç»ƒæ¨¡å‹ - ä¼˜åŒ–ç‰ˆæœ¬"""
        self.logger.info("è®­ç»ƒå„èšç±»æ¨¡å‹...")
        self.cluster_models = {}

        # ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œè®­ç»ƒèšç±»æ¨¡å‹
        if self.use_rf and len(np.unique(cluster_labels)) > 1:
            # å¯¹äºéšæœºæ£®æ—ï¼Œä½¿ç”¨å¤šè¿›ç¨‹
            self._train_cluster_models_parallel(X, y, cluster_labels)
        else:
            # é¡ºåºè®­ç»ƒ
            for cluster_id in range(self.n_clusters):
                self._train_single_cluster_model(X, y, cluster_labels, cluster_id)

    def _train_single_cluster_model(self, X, y, cluster_labels, cluster_id):
        """è®­ç»ƒå•ä¸ªèšç±»æ¨¡å‹"""
        cluster_mask = cluster_labels == cluster_id
        cluster_size = np.sum(cluster_mask)

        if cluster_size < 5:
            self.logger.warning(f"èšç±» {cluster_id} æ ·æœ¬æ•°è¿‡å°‘ ({cluster_size})ï¼Œè·³è¿‡è®­ç»ƒ")
            return

        X_cluster = X[cluster_mask]
        y_cluster = y[cluster_mask]

        if self.use_rf:
            from sklearn.ensemble import RandomForestRegressor
            model = RandomForestRegressor(**self.rf_params)
        else:
            import xgboost as xgb
            model = xgb.XGBRegressor(**self.params)

        model.fit(X_cluster, y_cluster)
        self.cluster_models[cluster_id] = model

        y_pred_cluster = model.predict(X_cluster)
        cluster_mae = mean_absolute_error(y_cluster, y_pred_cluster)
        cluster_rmse = np.sqrt(mean_squared_error(y_cluster, y_pred_cluster))

        self.logger.info(f"  èšç±» {cluster_id}: {cluster_size}æ ·æœ¬, MAE={cluster_mae:.3f}, RMSE={cluster_rmse:.3f}")

    def _train_cluster_models_parallel(self, X, y, cluster_labels):
        """å¹¶è¡Œè®­ç»ƒèšç±»æ¨¡å‹ - å……åˆ†åˆ©ç”¨14900KF"""
        from concurrent.futures import ProcessPoolExecutor, as_completed

        self.logger.info("ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œè®­ç»ƒèšç±»æ¨¡å‹...")

        def train_single_cluster(args):
            """å•ä¸ªèšç±»çš„è®­ç»ƒå‡½æ•°"""
            cluster_id, X_cluster, y_cluster, use_rf, params, rf_params = args

            if use_rf:
                from sklearn.ensemble import RandomForestRegressor
                model = RandomForestRegressor(**rf_params)
            else:
                import xgboost as xgb
                model = xgb.XGBRegressor(**params)

            model.fit(X_cluster, y_cluster)
            return cluster_id, model

        # å‡†å¤‡è®­ç»ƒä»»åŠ¡
        tasks = []
        for cluster_id in range(self.n_clusters):
            cluster_mask = cluster_labels == cluster_id
            cluster_size = np.sum(cluster_mask)

            if cluster_size >= 5:  # åªè®­ç»ƒæœ‰è¶³å¤Ÿæ ·æœ¬çš„èšç±»
                X_cluster = X[cluster_mask]
                y_cluster = y[cluster_mask]

                tasks.append((
                    cluster_id, X_cluster, y_cluster,
                    self.use_rf, self.params, self.rf_params
                ))

        # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œè®­ç»ƒ
        with ProcessPoolExecutor(max_workers=min(self.cpu_workers, len(tasks))) as executor:
            futures = [executor.submit(train_single_cluster, task) for task in tasks]

            for future in as_completed(futures):
                try:
                    cluster_id, model = future.result()
                    self.cluster_models[cluster_id] = model
                    self.logger.info(f"  å®Œæˆèšç±» {cluster_id} è®­ç»ƒ")
                except Exception as e:
                    self.logger.error(f"èšç±»è®­ç»ƒå¤±è´¥: {e}")

    def _get_cluster_predictions(self, X, cluster_labels):
        """è·å–å„èšç±»æ¨¡å‹çš„é¢„æµ‹ç»“æœ

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            cluster_labels (np.array): èšç±»æ ‡ç­¾

        Returns:
            np.array: å„èšç±»æ¨¡å‹çš„é¢„æµ‹ç»“æœçŸ©é˜µ
        """
        cluster_predictions = np.zeros((len(X), self.n_clusters))

        for cluster_id, model in self.cluster_models.items():
            cluster_mask = cluster_labels == cluster_id
            if np.any(cluster_mask):
                predictions = model.predict(X[cluster_mask])
                cluster_predictions[cluster_mask, cluster_id] = predictions

        return cluster_predictions

    def train_gnnwr_model(self, X, y, cluster_predictions, coords=None):
        """è®­ç»ƒGNNWRé›†æˆæ¨¡å‹ - GPUä¼˜åŒ–ç‰ˆæœ¬"""
        self.logger.info("=== train_gnnwr_method GPUä¼˜åŒ–ç‰ˆæœ¬ ===")
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        self.logger.info(f"æ··åˆç²¾åº¦: {self.mixed_precision}")

        # ç«‹å³æ£€æŸ¥åæ ‡æ•°æ®
        if coords is None:
            self.logger.error("âŒ åæ ‡æ•°æ®åœ¨æ–¹æ³•å…¥å£å¤„å°±ä¸ºNone!")
            raise ValueError("åæ ‡æ•°æ®åœ¨æ–¹æ³•å…¥å£å¤„å°±ä¸ºNone")

        self.logger.info("è®­ç»ƒGNNWRé›†æˆæ¨¡å‹...")

        # ä½¿ç”¨ç‰¹å¾ï¼šåŸå§‹ç‰¹å¾ + å„èšç±»é¢„æµ‹
        gnnwr_features = np.hstack([X, cluster_predictions])

        # æ·»åŠ ç»´åº¦è°ƒè¯•
        self.logger.info(f"è¾“å…¥ç‰¹å¾ç»´åº¦è°ƒè¯•:")
        self.logger.info(f"  Xå½¢çŠ¶: {X.shape}")
        self.logger.info(f"  cluster_predictionså½¢çŠ¶: {cluster_predictions.shape}")
        self.logger.info(f"  åˆå¹¶ågnnwr_featureså½¢çŠ¶: {gnnwr_features.shape}")

        # å…³é”®ä¿®å¤ï¼šåˆ›å»ºåæ ‡æ•°æ®çš„å‰¯æœ¬ï¼Œé¿å…è¢«å…¶ä»–æ–¹æ³•ä¿®æ”¹
        if coords is not None:
            coords_copy = coords.copy()
            self.logger.info(f"åˆ›å»ºåæ ‡å‰¯æœ¬ï¼ŒåŸid: {id(coords)}, å‰¯æœ¬id: {id(coords_copy)}")
        else:
            coords_copy = None
            self.logger.error("åæ ‡æ•°æ®ä¸ºNone")
            raise ValueError("åæ ‡æ•°æ®ä¸ºNone")

        # å¤„ç†ç¼ºå¤±å€¼
        if np.isnan(gnnwr_features).any():
            self.logger.info("å¤„ç†GNNWRç‰¹å¾ä¸­çš„ç¼ºå¤±å€¼")
            self.gnnwr_imputer = SimpleImputer(strategy='median')
            gnnwr_features_imputed = self.gnnwr_imputer.fit_transform(gnnwr_features)
        else:
            gnnwr_features_imputed = gnnwr_features
            self.gnnwr_imputer = None

        # æ ¹æ®æ•°æ®å¤§å°è‡ªåŠ¨è°ƒæ•´å‚æ•°
        n_samples = len(gnnwr_features_imputed)
        batch_size = min(512, max(64, n_samples // 50))  # è‡ªé€‚åº”æ‰¹æ¬¡å¤§å°ï¼Œå……åˆ†åˆ©ç”¨GPU

        self.logger.info(f"æ•°æ®åŠ è½½å™¨é…ç½®: batch_size={batch_size}")

        if self.use_enhanced_gnnwr:
            # ä½¿ç”¨å¢å¼ºç‰ˆGNNWR
            self.logger.info("ä½¿ç”¨å¢å¼ºç‰ˆGNNWRè®­ç»ƒå™¨")

            # æ£€æŸ¥æ ·æœ¬æ•°é‡ï¼Œå¦‚æœå¤ªå¤šåˆ™ä½¿ç”¨ç®€åŒ–æ¨¡å¼
            use_spatial = self.gnnwr_params['use_spatial_weights'] and coords_copy is not None

            if not use_spatial:
                self.logger.warning(f"æ ·æœ¬æ•°é‡è¾ƒå¤§ ({n_samples}) æˆ–åæ ‡ä¸å¯ç”¨ï¼Œç¦ç”¨ç©ºé—´æƒé‡è®¡ç®—")
                dataset = EnhancedSpatialDataset(
                    features=gnnwr_features_imputed,
                    targets=y,
                    coords=coords_copy
                )
            else:
                dataset = EnhancedSpatialDataset(
                    features=gnnwr_features_imputed,
                    targets=y,
                    coords=coords_copy
                )

            # ä½¿ç”¨ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
            train_loader = self.create_optimized_dataloader(
                dataset,
                batch_size=batch_size,
                shuffle=True
            )

            # åˆå§‹åŒ–å¢å¼ºç‰ˆGNNWRè®­ç»ƒå™¨ - ä½¿ç”¨ä¼˜åŒ–å‚æ•°
            input_dim = gnnwr_features_imputed.shape[1]
            self.logger.info(f"åˆå§‹åŒ–GNNWRè®­ç»ƒå™¨ï¼Œè¾“å…¥ç»´åº¦: {input_dim}")

            self.gnnwr_trainer = EnhancedGNNWRTrainer(
                input_dim=input_dim,
                coords=coords_copy if use_spatial else None,
                hidden_dims=self.gnnwr_params['hidden_dims'],
                learning_rate=self.gnnwr_params['learning_rate'],
                bandwidth=self.gnnwr_params['bandwidth'],
                use_spatial_weights=use_spatial,
                device=self.device,  # ä¼ é€’è®¾å¤‡
                mixed_precision=self.mixed_precision,  # æ··åˆç²¾åº¦
                cpu_workers=self.cpu_workers  # CPUå·¥ä½œçº¿ç¨‹
            )

            # è®­ç»ƒæ¨¡å‹
            self.logger.info(f"å¼€å§‹å¢å¼ºç‰ˆGNNWRè®­ç»ƒï¼Œè¾“å…¥ç»´åº¦: {input_dim}")
            try:
                self.gnnwr_trainer.train(
                    train_loader,
                    epochs=self.gnnwr_params['epochs'],
                    patience=self.gnnwr_params['patience']
                )
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    self.logger.error("GPUå†…å­˜ä¸è¶³ï¼Œå°è¯•å‡å°æ‰¹æ¬¡å¤§å°")
                    # é‡æ–°å°è¯•è¾ƒå°çš„æ‰¹æ¬¡
                    self.gnnwr_params['batch_size'] = self.gnnwr_params['batch_size'] // 2
                    self.train_gnnwr_model(X, y, cluster_predictions, coords)
                    return
                else:
                    raise e
        else:
            # ä½¿ç”¨åŸºç¡€ç‰ˆGNNWRï¼ˆæ— ç©ºé—´æƒé‡ï¼Œå†…å­˜å‹å¥½ï¼‰
            self.logger.info("ä½¿ç”¨åŸºç¡€ç‰ˆGNNWRè®­ç»ƒå™¨")

            # åˆ›å»ºæ•°æ®é›†
            dataset = SpatialDataset(gnnwr_features_imputed, y)

            # ä½¿ç”¨ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
            train_loader = self.create_optimized_dataloader(
                dataset,
                batch_size=batch_size,
                shuffle=True
            )

            # åˆå§‹åŒ–åŸºç¡€ç‰ˆGNNWRè®­ç»ƒå™¨
            input_dim = gnnwr_features_imputed.shape[1]
            self.gnnwr_trainer = GNNWRTrainer(
                input_dim=input_dim,
                hidden_dims=self.gnnwr_params['hidden_dims'],
                learning_rate=self.gnnwr_params['learning_rate'],
                device=self.device  # ä¼ é€’è®¾å¤‡
            )

            # è®­ç»ƒæ¨¡å‹
            self.logger.info(f"å¼€å§‹åŸºç¡€ç‰ˆGNNWRè®­ç»ƒï¼Œè¾“å…¥ç»´åº¦: {input_dim}")
            self.gnnwr_trainer.train(
                train_loader,
                epochs=self.gnnwr_params['epochs'],
                patience=self.gnnwr_params['patience']
            )

        # è®¡ç®—è®­ç»ƒé›†æ€§èƒ½
        y_pred = self.predict_with_gnnwr(gnnwr_features_imputed, None, coords_copy)
        mae = mean_absolute_error(y, y_pred)
        rmse = np.sqrt(mean_squared_error(y, y_pred))
        r_value, _ = pearsonr(y, y_pred)

        self.logger.info(f"GNNWRæ¨¡å‹è®­ç»ƒå®Œæˆ: MAE={mae:.3f}, RMSE={rmse:.3f}, R={r_value:.3f}")

    def cross_validate(self, X, y, groups, coords=None, cv_type='station'):
        """æ‰§è¡Œäº¤å‰éªŒè¯ - GPUä¼˜åŒ–ç‰ˆæœ¬"""
        from sklearn.model_selection import LeaveOneGroupOut
        logo = LeaveOneGroupOut()

        all_predictions = []
        all_true_values = []
        fold_results = {}

        unique_groups = np.unique(groups)
        total_folds = len(unique_groups)

        self.logger.info(f"å¼€å§‹{cv_type}äº¤å‰éªŒè¯ï¼Œå…±{total_folds}ä¸ªæŠ˜å ...")
        self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡Œä¸€æ¬¡èšç±»
        self.logger.info("åœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡Œèšç±»åˆ†é…...")
        self.cluster_assignments = self.perform_clustering(X, groups)

        for fold, (train_idx, test_idx) in enumerate(logo.split(X, y, groups)):
            group_id = groups[test_idx[0]]
            test_size = len(test_idx)
            train_size = len(train_idx)

            self.logger.info(f"=== Fold {fold + 1} ===")
            self.logger.info(f"è®­ç»ƒé›†å¤§å°: {train_size}, æµ‹è¯•é›†å¤§å°: {test_size}")

            # åˆ†å‰²æ•°æ®
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]
            groups_train, groups_test = groups[train_idx], groups[test_idx]

            # åˆ†å‰²åæ ‡
            if coords is not None:
                coords_train = coords[train_idx]
                coords_test = coords[test_idx]
            else:
                coords_train = None
                coords_test = None

            # ä½¿ç”¨å›ºå®šçš„èšç±»åˆ†é…
            train_cluster_labels = self.cluster_assignments[train_idx]
            test_cluster_labels = self.cluster_assignments[test_idx]

            # è®­ç»ƒèšç±»é›†æˆæ¨¡å‹
            try:
                # ç¬¬ä¸€æ­¥ï¼šä¸ºæ¯ä¸ªèšç±»è®­ç»ƒæ¨¡å‹ - ä½¿ç”¨å¤šçº¿ç¨‹
                self.train_cluster_models(X_train, y_train, train_cluster_labels)

                # ç¬¬äºŒæ­¥ï¼šè·å–è®­ç»ƒé›†ä¸Šçš„èšç±»é¢„æµ‹
                cluster_predictions_train = self._get_cluster_predictions(X_train, train_cluster_labels)

                # ç¬¬ä¸‰æ­¥ï¼šè®­ç»ƒGNNWRé›†æˆæ¨¡å‹
                if coords_train is None:
                    raise ValueError(f"Fold {fold + 1}: coords_trainä¸ºNoneï¼Œæ— æ³•è®­ç»ƒGNNWR")

                self.train_gnnwr_model(X_train, y_train, cluster_predictions_train, coords_train)

                # ç¬¬å››æ­¥ï¼šé¢„æµ‹æµ‹è¯•é›†
                cluster_predictions_test = self._get_cluster_predictions(X_test, test_cluster_labels)

                # å…³é”®ä¿®å¤ï¼šæµ‹è¯•é›†ç‰¹å¾ä¹Ÿéœ€è¦ä¸èšç±»é¢„æµ‹åˆå¹¶
                test_features_combined = np.hstack([X_test, cluster_predictions_test])
                self.logger.info(f"æµ‹è¯•é›†åˆå¹¶ç‰¹å¾å½¢çŠ¶: {test_features_combined.shape}")

                y_pred = self.predict_with_gnnwr(test_features_combined, None, coords_test)

                # å­˜å‚¨ç»“æœ
                all_predictions.extend(y_pred)
                all_true_values.extend(y_test)

                # è®¡ç®—å½“å‰æŠ˜å æ€§èƒ½
                fold_metrics = self.evaluate_predictions(y_test, y_pred)
                fold_results[group_id] = fold_metrics

                self.logger.info(
                    f"  {cv_type} Fold {fold + 1}/{total_folds}: {group_id} "
                    f"(èšç±»{test_cluster_labels[0]}, {test_size}æ ·æœ¬) - "
                    f"MAE={fold_metrics['MAE']:.3f}, R={fold_metrics['R']:.3f}"
                )

            except Exception as e:
                self.logger.error(f"æŠ˜å  {fold + 1} è®­ç»ƒå¤±è´¥: {e}")
                import traceback
                self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                continue

        # è®¡ç®—æ€»ä½“æ€§èƒ½
        overall_metrics = self.evaluate_predictions(
            np.array(all_true_values),
            np.array(all_predictions)
        )

        self.logger.info(f"âœ… {cv_type}äº¤å‰éªŒè¯å®Œæˆ")
        self.logger.info(f"  èšåˆæ€§èƒ½: MAE={overall_metrics['MAE']:.3f}mm, R={overall_metrics['R']:.3f}")

        return {
            'overall': overall_metrics,
            'by_fold': fold_results,
            'predictions': np.array(all_predictions),
            'true_values': np.array(all_true_values),
            'folds': total_folds,
            'cluster_assignments': self.cluster_assignments
        }

    def predict_with_gnnwr(self, X, cluster_predictions=None, coords=None):
        """ä½¿ç”¨GNNWRè¿›è¡Œé¢„æµ‹ - GPUä¼˜åŒ–ç‰ˆæœ¬"""
        if self.gnnwr_trainer is None:
            raise ValueError("GNNWRæ¨¡å‹å°šæœªè®­ç»ƒ")

        self.logger.info(f"é¢„æµ‹æ—¶ç‰¹å¾ç»´åº¦è°ƒè¯•:")
        self.logger.info(f"  Xå½¢çŠ¶: {X.shape}")

        # å…³é”®ä¿®å¤ï¼šå¦‚æœä¼ å…¥äº†cluster_predictionsï¼Œè¯´æ˜Xå·²ç»æ˜¯åŸå§‹ç‰¹å¾
        if cluster_predictions is not None:
            self.logger.info(f"  éœ€è¦åˆå¹¶cluster_predictions: {cluster_predictions.shape}")

            # æ£€æŸ¥Xçš„ç»´åº¦æ˜¯å¦å·²ç»åŒ…å«äº†èšç±»é¢„æµ‹
            expected_original_dim = X.shape[1] - self.n_clusters
            if X.shape[1] == expected_original_dim + self.n_clusters:
                # Xå·²ç»åŒ…å«äº†èšç±»é¢„æµ‹ï¼Œç›´æ¥ä½¿ç”¨
                gnnwr_features = X
                self.logger.info(f"  Xå·²ç»åŒ…å«èšç±»é¢„æµ‹ï¼Œç›´æ¥ä½¿ç”¨")
            else:
                # éœ€è¦åˆå¹¶
                gnnwr_features = np.hstack([X, cluster_predictions])
                self.logger.info(f"  åˆå¹¶åç‰¹å¾ç»´åº¦: {gnnwr_features.shape}")
        else:
            # å¦‚æœcluster_predictionsä¸ºNoneï¼Œè¯´æ˜Xå·²ç»æ˜¯åˆå¹¶åçš„ç‰¹å¾
            self.logger.info(f"  Xå·²ç»æ˜¯åˆå¹¶åçš„ç‰¹å¾")
            gnnwr_features = X

        # å¤„ç†ç¼ºå¤±å€¼
        if self.gnnwr_imputer is not None:
            gnnwr_features_imputed = self.gnnwr_imputer.transform(gnnwr_features)
        else:
            gnnwr_features_imputed = gnnwr_features

        # ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°è¿›è¡Œé¢„æµ‹
        batch_size = 2048 if self.device.type == 'cuda' else 1024

        # åˆ†æ‰¹é¢„æµ‹ä»¥é¿å…å†…å­˜é—®é¢˜
        predictions = []
        for i in range(0, len(gnnwr_features_imputed), batch_size):
            end_idx = min(i + batch_size, len(gnnwr_features_imputed))
            batch_features = gnnwr_features_imputed[i:end_idx]
            batch_coords = coords[i:end_idx] if coords is not None else None

            batch_pred = self.gnnwr_trainer.predict(batch_features, batch_coords)
            predictions.append(batch_pred)

        return np.concatenate(predictions)

    def validate_feature_dimensions(self, features, stage="training"):
        """éªŒè¯ç‰¹å¾ç»´åº¦ä¸€è‡´æ€§"""
        if self.gnnwr_trainer is None:
            return True

        # è·å–æ¨¡å‹æœŸæœ›çš„è¾“å…¥ç»´åº¦
        if hasattr(self.gnnwr_trainer.model, 'feature_network'):
            expected_dim = self.gnnwr_trainer.model.feature_network[0].in_features
            actual_dim = features.shape[1]

            if actual_dim != expected_dim:
                self.logger.error(f"{stage}é˜¶æ®µç»´åº¦ä¸åŒ¹é…: å®é™…{actual_dim}ç»´, æœŸæœ›{expected_dim}ç»´")
                return False

        return True

    def evaluate_predictions(self, y_true, y_pred):
        """è¯„ä¼°é¢„æµ‹æ€§èƒ½

        Args:
            y_true (np.array): çœŸå®å€¼
            y_pred (np.array): é¢„æµ‹å€¼

        Returns:
            dict: è¯„ä¼°æŒ‡æ ‡
        """
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r_value, p_value = pearsonr(y_true, y_pred)

        return {
            'MAE': mae,
            'RMSE': rmse,
            'R': r_value,
            'R_squared': r_value ** 2,
            'samples': len(y_true)
        }

    def create_optimized_dataloader(self, dataset, batch_size=512, shuffle=True):
        """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
        num_workers = min(12, self.cpu_workers // 2)  # å……åˆ†åˆ©ç”¨14900KF

        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=self.device.type == 'cuda',
            persistent_workers=num_workers > 0,
            prefetch_factor=2 if num_workers > 0 else None
        )

    def run_complete_analysis(self, df, output_dir=None):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹ - GPUä¼˜åŒ–ç‰ˆæœ¬"""
        self.logger.info("=" * 70)
        self.logger.info("ğŸš€ å¼€å§‹SWEèšç±»é›†æˆå›å½’å®Œæ•´åˆ†ææµç¨‹ (GPUä¼˜åŒ–ç‰ˆ)")
        self.logger.info("=" * 70)

        # æ˜¾ç¤ºç¡¬ä»¶ä¿¡æ¯
        if self.device.type == 'cuda':
            gpu_info = f"GPU: {torch.cuda.get_device_name()}, å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB"
            self.logger.info(f"ç¡¬ä»¶é…ç½®: {gpu_info}, CPUçº¿ç¨‹: {self.cpu_workers}")
        else:
            self.logger.info(f"ç¡¬ä»¶é…ç½®: CPUæ¨¡å¼, {self.cpu_workers}çº¿ç¨‹")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = f"./swe_cluster_ensemble_results_{timestamp}"

        os.makedirs(output_dir, exist_ok=True)
        self.logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")

        try:
            # 1. æ•°æ®é¢„å¤„ç†
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 1: æ•°æ®é¢„å¤„ç†")
            self.logger.info("=" * 50)

            X, y, station_groups, year_groups, coords = self.preprocess_data(df)

            results = {
                'preprocessing': {
                    'samples': len(X),
                    'features': len(self.feature_columns),
                    'stations': len(np.unique(station_groups)),
                    'years': len(np.unique(year_groups)),
                    'n_clusters': self.n_clusters,
                    'has_coords': coords is not None,
                    'device': str(self.device),
                    'mixed_precision': self.mixed_precision
                }
            }

            # 2. åœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡Œèšç±»
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 2: ç«™ç‚¹çº§èšç±»åˆ†æ")
            self.logger.info("=" * 50)

            self.cluster_assignments = self.perform_clustering(X, station_groups)
            results['cluster_assignments'] = self.cluster_assignments

            # 3. å¹´åº¦äº¤å‰éªŒè¯ï¼ˆä½¿ç”¨å›ºå®šèšç±»ï¼‰
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 3: å¹´åº¦äº¤å‰éªŒè¯")
            self.logger.info("=" * 50)

            results['yearly_cv'] = self.cross_validate(X, y, year_groups, coords, 'yearly')

            # 4. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 4: è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
            self.logger.info("=" * 50)

            self.fit(X, y, station_groups, coords)

            results['final_model'] = {
                'kmeans': self.kmeans,
                'cluster_models': self.cluster_models,
                'gnnwr_trainer': self.gnnwr_trainer,
                'cluster_assignments': self.cluster_assignments,
                'feature_columns': self.feature_columns,
                'training_config': {
                    'device': str(self.device),
                    'mixed_precision': self.mixed_precision,
                    'cpu_workers': self.cpu_workers
                }
            }

            # 5. ä¿å­˜ç»“æœ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 5: ä¿å­˜ç»“æœ")
            self.logger.info("=" * 50)

            self._save_results(results, output_dir)

            # 6. ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report(results)
            print(report)
            self.logger.info("ğŸ¯ èšç±»é›†æˆåˆ†æå®Œæˆï¼")
            return results

        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææµç¨‹å¤±è´¥: {str(e)}")
            raise


    def fit(self, X, y, station_groups, coords=None):
        """åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            y (np.array): ç›®æ ‡å˜é‡
            station_groups (np.array): ç«™ç‚¹åˆ†ç»„ä¿¡æ¯
            coords (np.array): åæ ‡æ•°æ®
        """
        self.logger.info("åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒèšç±»é›†æˆæ¨¡å‹...")

        # ç¬¬ä¸€æ­¥ï¼šåœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡Œèšç±»
        self.logger.info(f"åœ¨æ•´ä¸ªæ•°æ®é›†ä¸ŠæŒ‰ç«™ç‚¹è¿›è¡ŒK-meansèšç±»ï¼Œèšç±»æ•°: {self.n_clusters}")
        self.cluster_assignments = self.perform_clustering(X, station_groups)

        # ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªèšç±»è®­ç»ƒæ¨¡å‹
        self.train_cluster_models(X, y, self.cluster_assignments)

        # ç¬¬ä¸‰æ­¥ï¼šè®­ç»ƒGNNWRé›†æˆæ¨¡å‹
        cluster_predictions = self._get_cluster_predictions(X, self.cluster_assignments)
        self.train_gnnwr_model(X, y, cluster_predictions, coords)

        self.logger.info("âœ… èšç±»é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")

    def predict(self, X, coords=None):
        """é¢„æµ‹æ–°æ ·æœ¬

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            coords (np.array): åæ ‡æ•°æ®

        Returns:
            np.array: é¢„æµ‹ç»“æœ
        """
        if self.kmeans is None or not self.cluster_models or self.gnnwr_trainer is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")

        # ç¬¬ä¸€æ­¥ï¼šèšç±»
        if np.isnan(X).any():
            imputer = SimpleImputer(strategy='median')
            X_imputed = imputer.fit_transform(X)
        else:
            X_imputed = X

        cluster_labels = self.kmeans.predict(X_imputed)

        # ç¬¬äºŒæ­¥ï¼šå„èšç±»æ¨¡å‹é¢„æµ‹
        cluster_predictions = np.zeros((len(X), self.n_clusters))

        for cluster_id, model in self.cluster_models.items():
            cluster_mask = cluster_labels == cluster_id
            if np.any(cluster_mask):
                cluster_predictions[cluster_mask, cluster_id] = model.predict(X[cluster_mask])

        # ç¬¬ä¸‰æ­¥ï¼šGNNWRé›†æˆé¢„æµ‹
        return self.predict_with_gnnwr(X, cluster_predictions, coords)

    def _save_results(self, results, output_dir):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶

        Args:
            results (dict): åˆ†æç»“æœ
            output_dir (str): è¾“å‡ºç›®å½•
        """
        self.logger.info("ä¿å­˜åˆ†æç»“æœ...")

        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(output_dir, 'swe_cluster_ensemble_model.pkl')
        joblib.dump({
            'kmeans': self.kmeans,
            'cluster_models': self.cluster_models,
            'gnnwr_trainer': self.gnnwr_trainer,
            'feature_columns': self.feature_columns,
            'params': self.params,
            'gnnwr_params': self.gnnwr_params,
            'n_clusters': self.n_clusters,
            'use_enhanced_gnnwr': self.use_enhanced_gnnwr
        }, model_path)

        # ä¿å­˜ç»“æœæ•°æ®
        results_path = os.path.join(output_dir, 'analysis_results.pkl')
        joblib.dump(results, results_path)

        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        report_path = os.path.join(output_dir, 'analysis_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(self._generate_report(results))

        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._create_visualizations(results, output_dir)

        self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

    def _generate_report(self, results):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š - åŒ…å«GPUä¿¡æ¯"""
        report = []
        report.append("=" * 70)
        report.append("â„ï¸ SWEèšç±»é›†æˆå›å½’åˆ†ææŠ¥å‘Š (GPUä¼˜åŒ–ç‰ˆ)")
        report.append("=" * 70)
        report.append("")

        # æ•°æ®æ¦‚å†µ
        preprocessing = results['preprocessing']
        report.append("ğŸ“Š æ•°æ®æ¦‚å†µ:")
        report.append(f"  æ ·æœ¬æ•°é‡: {preprocessing['samples']}")
        report.append(f"  ç‰¹å¾æ•°é‡: {preprocessing['features']}")
        report.append(f"  ç«™ç‚¹æ•°é‡: {preprocessing['stations']}")
        report.append(f"  å¹´ä»½æ•°é‡: {preprocessing['years']}")
        report.append(f"  èšç±»æ•°é‡: {preprocessing['n_clusters']}")
        report.append(f"  ä½¿ç”¨åæ ‡: {'æ˜¯' if preprocessing['has_coords'] else 'å¦'}")
        report.append(f"  è®­ç»ƒè®¾å¤‡: {preprocessing['device']}")
        report.append(f"  æ··åˆç²¾åº¦: {'æ˜¯' if preprocessing['mixed_precision'] else 'å¦'}")
        report.append(f"  GNNWRç‰ˆæœ¬: {'å¢å¼ºç‰ˆ' if self.use_enhanced_gnnwr else 'åŸºç¡€ç‰ˆ'}")
        report.append("")

        # å¹´åº¦äº¤å‰éªŒè¯ç»“æœ
        yearly_cv = results['yearly_cv']
        yearly_overall = yearly_cv['overall']
        report.append("ğŸ“… å¹´åº¦äº¤å‰éªŒè¯ç»“æœ:")
        report.append(f"  æŠ˜å æ•°é‡: {yearly_cv['folds']}")
        report.append(f"  MAE: {yearly_overall['MAE']:.3f} mm")
        report.append(f"  RMSE: {yearly_overall['RMSE']:.3f} mm")
        report.append(f"  R: {yearly_overall['R']:.3f}")
        report.append(f"  RÂ²: {yearly_overall['R_squared']:.3f}")
        report.append("")

        # èšç±»åˆ†å¸ƒ
        cluster_counts = np.bincount(results['cluster_assignments'])
        report.append("ğŸ” èšç±»åˆ†å¸ƒ:")
        for cluster_id, count in enumerate(cluster_counts):
            report.append(
                f"  èšç±» {cluster_id}: {count} ä¸ªæ ·æœ¬ ({count / len(results['cluster_assignments']) * 100:.1f}%)")
        report.append("")

        report.append("ğŸ¯ æ¨¡å‹é…ç½®:")
        report.append(f"  åŸºç¡€æ¨¡å‹: {'éšæœºæ£®æ—' if self.use_rf else 'XGBoost'}")
        report.append(f"  è®¾å¤‡é…ç½®: {self.device}")
        report.append(f"  CPUçº¿ç¨‹: {self.cpu_workers}")
        if hasattr(self, 'params') and self.params:
            report.append(f"  æ¨¡å‹å‚æ•°: {self.params}")
        report.append(f"  GNNWRå‚æ•°: {self.gnnwr_params}")

        return "\n".join(report)

    def _create_visualizations(self, results, output_dir):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨ - ä½¿ç”¨è‹±æ–‡æ ‡ç­¾"""
        self.logger.info("Generating visualizations...")

        try:
            # æ£€æŸ¥å¿…è¦çš„é”®æ˜¯å¦å­˜åœ¨
            if 'yearly_cv' not in results:
                self.logger.warning("Missing yearly CV results, skipping visualization")
                return

            plt.figure(figsize=(12, 10))

            # 1. å¹´åº¦äº¤å‰éªŒè¯æ•£ç‚¹å›¾
            plt.subplot(2, 2, 1)
            yearly_cv = results['yearly_cv']
            y_true_yearly = yearly_cv['true_values']
            y_pred_yearly = yearly_cv['predictions']

            plt.scatter(y_true_yearly, y_pred_yearly, alpha=0.6, s=20, color='orange')
            plt.plot([y_true_yearly.min(), y_true_yearly.max()],
                     [y_true_yearly.min(), y_true_yearly.max()], 'r--', alpha=0.8)
            plt.xlabel('True SWE (mm)')
            plt.ylabel('Predicted SWE (mm)')
            plt.title(
                f'Yearly Cross-Validation\nMAE={yearly_cv["overall"]["MAE"]:.2f}, R={yearly_cv["overall"]["R"]:.3f}')
            plt.grid(True, alpha=0.3)

            # 2. æ®‹å·®åˆ†å¸ƒå›¾
            plt.subplot(2, 2, 2)
            residuals = y_true_yearly - y_pred_yearly
            plt.hist(residuals, bins=30, alpha=0.7, color='skyblue')
            plt.xlabel('Residuals (mm)')
            plt.ylabel('Frequency')
            plt.title('Residual Distribution')
            plt.grid(True, alpha=0.3)

            # 3. èšç±»åˆ†å¸ƒå›¾
            plt.subplot(2, 2, 3)
            if 'cluster_assignments' in results:
                cluster_assignments = results['cluster_assignments']
                cluster_counts = np.bincount(cluster_assignments)
                colors = plt.cm.Set3(np.linspace(0, 1, len(cluster_counts)))

                bars = plt.bar(range(len(cluster_counts)), cluster_counts, color=colors)
                plt.xlabel('Cluster ID')
                plt.ylabel('Sample Count')
                plt.title('Cluster Distribution')
                plt.xticks(range(len(cluster_counts)))

                # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar, count in zip(bars, cluster_counts):
                    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                             f'{count}', ha='center', va='bottom')
            else:
                plt.text(0.5, 0.5, 'No Cluster Data', ha='center', va='center', transform=plt.gca().transAxes)
                plt.title('Cluster Distribution')

            # 4. æ€§èƒ½å›¾
            plt.subplot(2, 2, 4)
            yearly = results['yearly_cv']['overall']
            metrics = ['MAE', 'RMSE', 'R']
            values = [yearly['MAE'], yearly['RMSE'], yearly['R']]
            colors = ['skyblue', 'lightgreen', 'lightcoral']

            bars = plt.bar(metrics, values, color=colors, alpha=0.7)
            plt.ylabel('Value')
            plt.title('Yearly CV Performance')

            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                         f'{value:.3f}', ha='center', va='bottom')

            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'performance_visualization.png'),
                        dpi=300, bbox_inches='tight')
            plt.close()

            self.logger.info("âœ… Visualization completed")

        except Exception as e:
            self.logger.warning(f"Visualization failed: {e}")


def get_feature_importance(self):
    """è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆåŸºäºå„èšç±»æ¨¡å‹çš„å¹³å‡é‡è¦æ€§ï¼‰"""
    if not self.cluster_models:
        raise ValueError("èšç±»æ¨¡å‹å°šæœªè®­ç»ƒ")

    # æ”¶é›†æ‰€æœ‰ç‰¹å¾çš„é‡è¦æ€§
    all_importances = []
    for cluster_id, model in self.cluster_models.items():
        importance_scores = model.feature_importances_
        all_importances.append(importance_scores)

    # è®¡ç®—å¹³å‡é‡è¦æ€§
    avg_importance = np.mean(all_importances, axis=0)

    # åˆ›å»ºDataFrame
    feature_importance_df = pd.DataFrame({
        'feature': self.feature_columns,
        'importance': avg_importance
    }).sort_values('importance', ascending=False)

    self.logger.info(f"ç‰¹å¾é‡è¦æ€§è®¡ç®—å®Œæˆï¼Œæœ€é«˜é‡è¦æ€§ç‰¹å¾: {feature_importance_df['feature'].iloc[0]}")
    return feature_importance_df


def analyze_cluster_characteristics(self, df):
    """åˆ†æå„èšç±»çš„ç‰¹å¾

    Args:
        df (pd.DataFrame): åŸå§‹æ•°æ®

    Returns:
        dict: èšç±»åˆ†æç»“æœ
    """
    if self.cluster_assignments is None:
        raise ValueError("èšç±»å°šæœªæ‰§è¡Œ")

    self.logger.info("åˆ†æå„èšç±»ç‰¹å¾...")

    cluster_stats = {}
    feature_cols = [col for col in self.feature_columns if col in df.columns]

    for cluster_id in range(self.n_clusters):
        cluster_mask = self.cluster_assignments == cluster_id
        cluster_data = df[cluster_mask]
        cluster_size = len(cluster_data)

        if cluster_size == 0:
            continue

        stats = {
            'size': cluster_size,
            'swe_mean': cluster_data[self.target_column].mean(),
            'swe_std': cluster_data[self.target_column].std(),
            'features': {}
        }

        # è®¡ç®—å„ç‰¹å¾çš„ç»Ÿè®¡é‡
        for feature in feature_cols:
            if feature in cluster_data.columns:
                stats['features'][feature] = {
                    'mean': cluster_data[feature].mean(),
                    'std': cluster_data[feature].std(),
                    'median': cluster_data[feature].median()
                }

        cluster_stats[cluster_id] = stats

        self.logger.info(f"  èšç±» {cluster_id}: {cluster_size}æ ·æœ¬, SWEå‡å€¼={stats['swe_mean']:.2f}mm")

    return cluster_stats


def create_cluster_analysis_report(self, df, output_dir):
    """åˆ›å»ºèšç±»åˆ†ææŠ¥å‘Š

    Args:
        df (pd.DataFrame): åŸå§‹æ•°æ®
        output_dir (str): è¾“å‡ºç›®å½•
    """
    try:
        self.logger.info("åˆ›å»ºèšç±»åˆ†ææŠ¥å‘Š...")

        # è·å–èšç±»ç»Ÿè®¡
        cluster_stats = self.analyze_cluster_characteristics(df)

        # åˆ›å»ºèšç±»ç‰¹å¾å¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. èšç±»å¤§å°åˆ†å¸ƒ
        cluster_sizes = [stats['size'] for stats in cluster_stats.values()]
        cluster_ids = list(cluster_stats.keys())

        axes[0, 0].bar(cluster_ids, cluster_sizes, color=plt.cm.Set3(np.linspace(0, 1, len(cluster_ids))))
        axes[0, 0].set_title('å„èšç±»æ ·æœ¬æ•°é‡')
        axes[0, 0].set_xlabel('èšç±»ID')
        axes[0, 0].set_ylabel('æ ·æœ¬æ•°é‡')
        for i, v in enumerate(cluster_sizes):
            axes[0, 0].text(i, v, str(v), ha='center', va='bottom')

        # 2. å„èšç±»SWEå‡å€¼
        swe_means = [stats['swe_mean'] for stats in cluster_stats.values()]
        axes[0, 1].bar(cluster_ids, swe_means, color=plt.cm.Set3(np.linspace(0, 1, len(cluster_ids))))
        axes[0, 1].set_title('å„èšç±»SWEå‡å€¼')
        axes[0, 1].set_xlabel('èšç±»ID')
        axes[0, 1].set_ylabel('SWEå‡å€¼ (mm)')
        for i, v in enumerate(swe_means):
            axes[0, 1].text(i, v, f'{v:.1f}', ha='center', va='bottom')

        # 3. é‡è¦ç‰¹å¾åœ¨å„èšç±»çš„åˆ†å¸ƒ
        feature_importance = self.get_feature_importance()
        top_features = feature_importance.head(3)['feature'].tolist()

        for i, feature in enumerate(top_features):
            if i >= 2:  # åªæ˜¾ç¤ºå‰ä¸¤ä¸ªç‰¹å¾
                break
            feature_means = []
            for cluster_id, stats in cluster_stats.items():
                if feature in stats['features']:
                    feature_means.append(stats['features'][feature]['mean'])
                else:
                    feature_means.append(0)

            axes[1, i].bar(cluster_ids, feature_means,
                           color=plt.cm.Set3(np.linspace(0, 1, len(cluster_ids))))
            axes[1, i].set_title(f'{feature}åœ¨å„èšç±»çš„å‡å€¼')
            axes[1, i].set_xlabel('èšç±»ID')
            axes[1, i].set_ylabel(f'{feature}å‡å€¼')

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'cluster_analysis.png'),
                    dpi=300, bbox_inches='tight')
        plt.close()

        # ä¿å­˜èšç±»ç»Ÿè®¡åˆ°CSV
        cluster_report = []
        for cluster_id, stats in cluster_stats.items():
            row = {
                'cluster_id': cluster_id,
                'size': stats['size'],
                'swe_mean': stats['swe_mean'],
                'swe_std': stats['swe_std']
            }

            # æ·»åŠ é‡è¦ç‰¹å¾ä¿¡æ¯
            for feature in top_features:
                if feature in stats['features']:
                    row[f'{feature}_mean'] = stats['features'][feature]['mean']
                    row[f'{feature}_std'] = stats['features'][feature]['std']
                else:
                    row[f'{feature}_mean'] = np.nan
                    row[f'{feature}_std'] = np.nan

            cluster_report.append(row)

        cluster_df = pd.DataFrame(cluster_report)
        cluster_df.to_csv(os.path.join(output_dir, 'cluster_statistics.csv'), index=False)

        self.logger.info(f"âœ… èšç±»åˆ†ææŠ¥å‘Šä¿å­˜å®Œæˆ")

    except Exception as e:
        self.logger.warning(f"åˆ›å»ºèšç±»åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")


def compare_with_baseline(self, df, output_dir):
    """ä¸åŸºçº¿æ¨¡å‹æ¯”è¾ƒ

    Args:
        df (pd.DataFrame): åŸå§‹æ•°æ®
        output_dir (str): è¾“å‡ºç›®å½•
    """
    try:
        self.logger.info("ä¸åŸºçº¿æ¨¡å‹æ¯”è¾ƒ...")

        # é¢„å¤„ç†æ•°æ®
        X, y, station_groups, year_groups, coords = self.preprocess_data(df)

        # è®­ç»ƒæ™®é€šXGBoostæ¨¡å‹ä½œä¸ºåŸºçº¿
        from swe_trainer import SWEXGBoostTrainer
        baseline_trainer = SWEXGBoostTrainer(params=self.params)

        # ç«™ç‚¹äº¤å‰éªŒè¯
        baseline_station_results = baseline_trainer.cross_validate(X, y, station_groups, 'station')
        baseline_yearly_results = baseline_trainer.cross_validate(X, y, year_groups, 'yearly')

        # æ¯”è¾ƒç»“æœ
        comparison = {
            'station_cv': {
                'baseline_mae': baseline_station_results['overall']['MAE'],
                'ensemble_mae': self.cross_validate(X, y, station_groups, coords, 'station')['overall']['MAE'],
                'baseline_r': baseline_station_results['overall']['R'],
                'ensemble_r': self.cross_validate(X, y, station_groups, coords, 'station')['overall']['R'],
                'improvement_mae': (baseline_station_results['overall']['MAE'] -
                                    self.cross_validate(X, y, station_groups, coords, 'station')['overall']['MAE']),
                'improvement_r': (self.cross_validate(X, y, station_groups, coords, 'station')['overall']['R'] -
                                  baseline_station_results['overall']['R'])
            },
            'yearly_cv': {
                'baseline_mae': baseline_yearly_results['overall']['MAE'],
                'ensemble_mae': self.cross_validate(X, y, year_groups, coords, 'yearly')['overall']['MAE'],
                'baseline_r': baseline_yearly_results['overall']['R'],
                'ensemble_r': self.cross_validate(X, y, year_groups, coords, 'yearly')['overall']['R'],
                'improvement_mae': (baseline_yearly_results['overall']['MAE'] -
                                    self.cross_validate(X, y, year_groups, coords, 'yearly')['overall']['MAE']),
                'improvement_r': (self.cross_validate(X, y, year_groups, coords, 'yearly')['overall']['R'] -
                                  baseline_yearly_results['overall']['R'])
            }
        }

        # åˆ›å»ºæ¯”è¾ƒå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))

        # MAEæ¯”è¾ƒ
        methods = ['åŸºçº¿', 'èšç±»é›†æˆ']
        station_mae = [comparison['station_cv']['baseline_mae'], comparison['station_cv']['ensemble_mae']]
        yearly_mae = [comparison['yearly_cv']['baseline_mae'], comparison['yearly_cv']['ensemble_mae']]

        x = np.arange(len(methods))
        width = 0.35

        ax1.bar(x - width / 2, station_mae, width, label='ç«™ç‚¹CV', alpha=0.7)
        ax1.bar(x + width / 2, yearly_mae, width, label='å¹´åº¦CV', alpha=0.7)
        ax1.set_xlabel('æ¨¡å‹ç±»å‹')
        ax1.set_ylabel('MAE (mm)')
        ax1.set_title('MAEæ¯”è¾ƒ')
        ax1.set_xticks(x)
        ax1.set_xticklabels(methods)
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # Rå€¼æ¯”è¾ƒ
        station_r = [comparison['station_cv']['baseline_r'], comparison['station_cv']['ensemble_r']]
        yearly_r = [comparison['yearly_cv']['baseline_r'], comparison['yearly_cv']['ensemble_r']]

        ax2.bar(x - width / 2, station_r, width, label='ç«™ç‚¹CV', alpha=0.7)
        ax2.bar(x + width / 2, yearly_r, width, label='å¹´åº¦CV', alpha=0.7)
        ax2.set_xlabel('æ¨¡å‹ç±»å‹')
        ax2.set_ylabel('R')
        ax2.set_title('ç›¸å…³ç³»æ•°æ¯”è¾ƒ')
        ax2.set_xticks(x)
        ax2.set_xticklabels(methods)
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'baseline_comparison.png'),
                    dpi=300, bbox_inches='tight')
        plt.close()

        # ä¿å­˜æ¯”è¾ƒç»“æœ
        comparison_df = pd.DataFrame([
            {
                'method': 'station_cv',
                'baseline_mae': comparison['station_cv']['baseline_mae'],
                'ensemble_mae': comparison['station_cv']['ensemble_mae'],
                'improvement_mae': comparison['station_cv']['improvement_mae'],
                'baseline_r': comparison['station_cv']['baseline_r'],
                'ensemble_r': comparison['station_cv']['ensemble_r'],
                'improvement_r': comparison['station_cv']['improvement_r']
            },
            {
                'method': 'yearly_cv',
                'baseline_mae': comparison['yearly_cv']['baseline_mae'],
                'ensemble_mae': comparison['yearly_cv']['ensemble_mae'],
                'improvement_mae': comparison['yearly_cv']['improvement_mae'],
                'baseline_r': comparison['yearly_cv']['baseline_r'],
                'ensemble_r': comparison['yearly_cv']['ensemble_r'],
                'improvement_r': comparison['yearly_cv']['improvement_r']
            }
        ])

        comparison_df.to_csv(os.path.join(output_dir, 'baseline_comparison.csv'), index=False)

        self.logger.info("âœ… åŸºçº¿æ¯”è¾ƒå®Œæˆ")
        return comparison

    except Exception as e:
        self.logger.warning(f"åŸºçº¿æ¯”è¾ƒå¤±è´¥: {e}")
        return None


# ä¾¿æ·ä½¿ç”¨å‡½æ•°
def train_swe_cluster_ensemble(data_df, output_dir=None, n_clusters=4, params=None, use_rf=False,
                               use_enhanced_gnnwr=True, gnnwr_params=None, device='auto',
                               mixed_precision=True, cpu_workers=24):
    """ä¾¿æ·å‡½æ•°ï¼šè®­ç»ƒSWEèšç±»é›†æˆæ¨¡å‹ - GPUä¼˜åŒ–ç‰ˆæœ¬

    Args:
        data_df (pd.DataFrame): åŒ…å«ç‰¹å¾å’ŒSWEçš„æ•°æ®
        output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„
        n_clusters (int, optional): èšç±»æ•°é‡
        params (dict, optional): XGBoostå‚æ•°
        use_enhanced_gnnwr (bool): æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰ˆGNNWR
        gnnwr_params (dict): GNNWRå‚æ•°
        device (str): è®­ç»ƒè®¾å¤‡
        mixed_precision (bool): æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦
        cpu_workers (int): CPUå·¥ä½œçº¿ç¨‹æ•°
    """
    trainer = SWEClusterEnsemble(
        n_clusters=n_clusters,
        params=params,
        gnnwr_params=gnnwr_params,
        use_enhanced_gnnwr=use_enhanced_gnnwr,
        use_rf=use_rf,
        device=device,
        mixed_precision=mixed_precision,
        cpu_workers=cpu_workers
    )
    return trainer.run_complete_analysis(data_df, output_dir)




def load_swe_cluster_ensemble(model_path):
    """åŠ è½½å·²è®­ç»ƒçš„SWEèšç±»é›†æˆæ¨¡å‹

    Args:
        model_path (str): æ¨¡å‹æ–‡ä»¶è·¯å¾„

    Returns:
        SWEClusterEnsemble: åŠ è½½çš„æ¨¡å‹å®ä¾‹
    """
    model_data = joblib.load(model_path)

    trainer = SWEClusterEnsemble(
        n_clusters=model_data['n_clusters'],
        params=model_data['params'],
        gnnwr_params=model_data['gnnwr_params'],
        use_enhanced_gnnwr=model_data.get('use_enhanced_gnnwr', True)
    )

    trainer.kmeans = model_data['kmeans']
    trainer.cluster_models = model_data['cluster_models']
    trainer.gnnwr_trainer = model_data['gnnwr_trainer']
    trainer.feature_columns = model_data['feature_columns']
    trainer.cluster_assignments = model_data.get('cluster_assignments')

    # æ¢å¤RFå‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'rf_params' in model_data:
        trainer.rf_params = model_data['rf_params']

    return trainer


# æµ‹è¯•å‡½æ•°
def test_cluster_ensemble():
    """æµ‹è¯•èšç±»é›†æˆæ¨¡å‹"""
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    n_features = 10

    # ç”Ÿæˆç©ºé—´åæ ‡
    coords = np.random.uniform(0, 100, (n_samples, 2))

    # ç”Ÿæˆç‰¹å¾
    features = np.random.randn(n_samples, n_features)

    # åˆ›å»ºå…·æœ‰ç©ºé—´ç›¸å…³æ€§çš„ç›®æ ‡å˜é‡
    spatial_effect = np.exp(-0.01 * coords[:, 0]) + np.sin(0.1 * coords[:, 1])
    targets = (features[:, 0] + 2 * features[:, 1] + 0.5 * spatial_effect +
               np.random.normal(0, 0.1, n_samples))

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®æ¡†
    df = pd.DataFrame(features, columns=[f'feature_{i}' for i in range(n_features)])
    df['swe'] = targets
    df['station_id'] = [f'station_{i % 20}' for i in range(n_samples)]
    df['year'] = np.random.randint(2018, 2023, n_samples)
    df['longitude'] = coords[:, 0]
    df['latitude'] = coords[:, 1]

    # è®­ç»ƒæ¨¡å‹
    results = train_swe_cluster_ensemble(
        data_df=df,
        n_clusters=3,
        use_enhanced_gnnwr=True
    )

    return results


class PureGNNWRModel(nn.Module):
    def __init__(self, input_dim, hidden_dims=[256, 128, 64, 32], output_dim=1,  # ä½¿ç”¨æ›´å°çš„ç½‘ç»œ
                 dropout_rate=0.3, use_batch_norm=True, use_attention=True,
                 activation='relu'):
        super(PureGNNWRModel, self).__init__()

        self.use_attention = use_attention

        # é€‰æ‹©æ¿€æ´»å‡½æ•°
        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'leaky_relu':
            self.activation = nn.LeakyReLU(0.1)
        elif activation == 'elu':
            self.activation = nn.ELU()
        else:
            self.activation = nn.ReLU()

        # ç‰¹å¾æå–ç½‘ç»œ - ä¿®å¤æ¢¯åº¦æµåŠ¨
        layers = []
        prev_dim = input_dim

        for i, hidden_dim in enumerate(hidden_dims):
            layers.append(nn.Linear(prev_dim, hidden_dim))

            if use_batch_norm:
                layers.append(nn.BatchNorm1d(hidden_dim))

            layers.append(self.activation)
            layers.append(nn.Dropout(dropout_rate))
            prev_dim = hidden_dim

        self.feature_network = nn.Sequential(*layers)

        # è¾“å‡ºå±‚ - æ·»åŠ æ®‹å·®è¿æ¥é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
        self.output_layer = nn.Linear(prev_dim, output_dim)

        # å…³é”®ä¿®å¤ï¼šæ›´å¥½çš„æƒé‡åˆå§‹åŒ–
        self._initialize_weights()

    def _initialize_weights(self):
        """æƒé‡åˆå§‹åŒ– - ä¿®å¤selfå¼•ç”¨é—®é¢˜"""
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨self.modules()è€Œä¸æ˜¯self.model.modules()
        for module in self.modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨Kaimingåˆå§‹åŒ–ï¼Œé€‚åˆReLU
                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)

    def _initialize_single_module(self, module):
        """åˆå§‹åŒ–å•ä¸ªæ¨¡å—çš„æƒé‡"""
        if isinstance(module, nn.Linear):
            # ä½¿ç”¨Kaimingåˆå§‹åŒ–ï¼Œé€‚åˆReLU
            nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.BatchNorm1d):
            nn.init.constant_(module.weight, 1)
            nn.init.constant_(module.bias, 0)

    def forward(self, x, spatial_weights=None, coords=None):
        # ç‰¹å¾æå–
        features = self.feature_network(x)

        # ç©ºé—´å¹³æ»‘
        if spatial_weights is not None and self.use_attention:
            row_sums = torch.sum(spatial_weights, dim=1, keepdim=True)
            normalized_weights = spatial_weights / torch.where(row_sums > 0, row_sums, torch.tensor(1.0))
            smoothed_features = torch.matmul(normalized_weights, features)
            output = self.output_layer(smoothed_features)
        else:
            output = self.output_layer(features)

        return output.squeeze()


class PureGNNWRTrainer:
    """çº¯å‡€ç‰ˆGNNWRè®­ç»ƒå™¨ - ä¿®å¤autocasté”™è¯¯ç‰ˆæœ¬"""

    def __init__(self, input_dim, coords, hidden_dims=[512, 256, 128, 64],
                 learning_rate=0.001, bandwidth=10.0, dropout_rate=0.3,
                 weight_decay=1e-4, device='auto', output_std_penalty=0.01,
                 mixed_precision=True, cpu_workers=24, gradient_clip=1.0):

        # é¦–å…ˆåˆå§‹åŒ–logger - è¿™æ˜¯å…³é”®ä¿®å¤ï¼
        self.logger = logging.getLogger("PureGNNWRTrainer")

        # è®¾å¤‡è®¾ç½®
        if device == 'auto':
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
                # GPUä¼˜åŒ–è®¾ç½®
                torch.backends.cudnn.benchmark = True
                torch.backends.cuda.matmul.allow_tf32 = True
                torch.backends.cudnn.allow_tf32 = True
                self.device_type = 'cuda'
            else:
                self.device = torch.device('cpu')
                torch.set_num_threads(cpu_workers)
                self.device_type = 'cpu'
        else:
            self.device = torch.device(device)
            self.device_type = 'cuda' if device == 'cuda' else 'cpu'
            if device == 'cpu':
                torch.set_num_threads(cpu_workers)

        # æ··åˆç²¾åº¦è®­ç»ƒ - ä¿®å¤ï¼šåªåœ¨CUDAè®¾å¤‡ä¸Šå¯ç”¨
        self.mixed_precision = mixed_precision and self.device_type == 'cuda'
        if self.mixed_precision:
            self.scaler = GradScaler()
            self.logger.info(f"å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œè®¾å¤‡ç±»å‹: {self.device_type}")
        else:
            self.logger.info(f"ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œè®¾å¤‡ç±»å‹: {self.device_type}")

        self.output_std_penalty = output_std_penalty
        self.logger = logging.getLogger("PureGNNWR")
        self.logger.info(f"çº¯å‡€ç‰ˆGNNWR - ä½¿ç”¨è®¾å¤‡: {self.device}")
        self.logger.info(f"æ··åˆç²¾åº¦: {self.mixed_precision}")

        # å…³é”®ï¼šæ·»åŠ æ ‡å‡†åŒ–å™¨
        from sklearn.preprocessing import StandardScaler
        self.feature_scaler = StandardScaler()
        self.target_scaler = StandardScaler()

        # æ¨¡å‹åˆå§‹åŒ–
        self.model = PureGNNWRModel(
            input_dim=input_dim,
            hidden_dims=hidden_dims,
            dropout_rate=dropout_rate,
            activation='leaky_relu'  # ä½¿ç”¨LeakyReLUé˜²æ­¢æ­»äº¡ReLU
        ).to(self.device)

        # ä¼˜åŒ–å™¨ - ä½¿ç”¨AdamWï¼Œé’ˆå¯¹æ··åˆç²¾åº¦ä¼˜åŒ–
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=1e-4,
            weight_decay=weight_decay,
            betas=(0.9, 0.99)
        )



        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - OneCycleç­–ç•¥
        self.scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=learning_rate,
            epochs=200,
            steps_per_epoch=1000,
            pct_start=0.1
        )

        self.criterion = nn.HuberLoss()  # ä½¿ç”¨HuberLossæ›´ç¨³å®š

        # ç©ºé—´æƒé‡è®¡ç®—
        self.coords = coords.copy() if coords is not None else None
        self.bandwidth = bandwidth

        # CPUå·¥ä½œçº¿ç¨‹é…ç½®
        self.cpu_workers = cpu_workers

        self.gradient_clip = gradient_clip

    def get_training_info(self):
        """è·å–è®­ç»ƒä¿¡æ¯"""
        return {
            'model_state': self.model.state_dict() if hasattr(self, 'model') else None,
            'training_loss': getattr(self, 'training_loss', []),
            'validation_loss': getattr(self, 'validation_loss', []),
            'epochs_completed': getattr(self, 'epochs_completed', 0),
            'current_learning_rate': getattr(self, 'current_lr', 0.0)
        }

    def safe_get_training_info(trainer):
        """å®‰å…¨åœ°è·å–è®­ç»ƒä¿¡æ¯"""
        try:
            if hasattr(trainer, 'get_training_info'):
                return trainer.get_training_info()
            else:
                # å°è¯•ä»trainerçš„å…¶ä»–å±æ€§ä¸­æå–ä¿¡æ¯
                info = {}
                for attr in ['model', 'training_loss', 'validation_loss', 'epoch']:
                    if hasattr(trainer, attr):
                        info[attr] = getattr(trainer, attr)
                return info
        except Exception as e:
            return {'error': f'Failed to get training info: {str(e)}'}

    def _initialize_model(self):
        """ç¡®ä¿æ¨¡å‹æ­£ç¡®åˆå§‹åŒ– - ä¿®å¤applyè°ƒç”¨"""
        # å…³é”®ä¿®å¤ï¼šæ­£ç¡®ä½¿ç”¨applyæ–¹æ³•
        self.model.apply(self._initialize_weights)

        # éªŒè¯åˆå§‹åŒ–
        self.logger.info("=== æ¨¡å‹åˆå§‹åŒ–éªŒè¯ ===")
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        self.logger.info(f"æ€»å‚æ•°: {total_params:,}, å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

        # æ£€æŸ¥æƒé‡èŒƒå›´
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.logger.info(f"{name}: mean={param.data.mean():.6f}, std={param.data.std():.6f}")

        self.logger.info("=====================")

    def _initialize_weights(self):
        """æ›´å¥½çš„æƒé‡åˆå§‹åŒ–"""
        for module in self.model.modules():
            if isinstance(module, nn.Linear):
                # ä½¿ç”¨Xavieråˆå§‹åŒ–
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)

    def debug_model_output(self, X_sample, y_sample, coords_sample=None):
        """è°ƒè¯•æ¨¡å‹è¾“å‡º"""
        self.model.eval()
        with torch.no_grad():
            # æµ‹è¯•ä¸åŒè¾“å…¥
            outputs = []
            print("=== æ¨¡å‹è¾“å‡ºè°ƒè¯• ===")

            for i in range(min(5, len(X_sample))):
                x = torch.tensor(X_sample[i:i + 1], dtype=torch.float32, device=self.device)
                c = torch.tensor(coords_sample[i:i + 1], dtype=torch.float32,
                                 device=self.device) if coords_sample is not None else None

                if self.mixed_precision:
                    with autocast(device_type=self.device_type):
                        output = self.model(x, None, c)
                else:
                    output = self.model(x, None, c)

                outputs.append(output.item())
                print(f"æ ·æœ¬ {i}: è¾“å…¥å‡å€¼ä¸º {x.mean().item():.3f}, è¾“å‡ºä¸º {output.item():.3f}")

            print(f"æ¨¡å‹è¾“å‡ºèŒƒå›´: [{min(outputs):.3f}, {max(outputs):.3f}]")
            print(f"æ¨¡å‹è¾“å‡ºæ ‡å‡†å·®: {np.std(outputs):.6f}")
            print("===================")

    def _compute_spatial_weights(self, batch_coords):
        """è®¡ç®—ç©ºé—´æƒé‡çŸ©é˜µ - ä¿®å¤autocastç‰ˆæœ¬"""
        n_batch = batch_coords.shape[0]
        if n_batch <= 1:
            return torch.ones((n_batch, n_batch), device=self.device,
                              dtype=torch.float16 if self.mixed_precision else torch.float32)

        # ä¿®å¤ï¼šæ­£ç¡®ä½¿ç”¨autocast
        if self.mixed_precision:
            with autocast(device_type=self.device_type):
                # è®¡ç®—æ¬§æ°è·ç¦»
                diff = batch_coords.unsqueeze(1) - batch_coords.unsqueeze(0)
                distances = torch.sqrt(torch.sum(diff ** 2, dim=2) + 1e-8)

                # é«˜æ–¯æ ¸å‡½æ•°
                weights = torch.exp(-0.5 * (distances / self.bandwidth) ** 2)
        else:
            # éæ··åˆç²¾åº¦ç‰ˆæœ¬
            diff = batch_coords.unsqueeze(1) - batch_coords.unsqueeze(0)
            distances = torch.sqrt(torch.sum(diff ** 2, dim=2) + 1e-8)
            weights = torch.exp(-0.5 * (distances / self.bandwidth) ** 2)

        return weights

    def train_epoch_mixed_precision(self, train_loader):
        """ä¿®å¤å­¦ä¹ ç‡è°ƒåº¦é¡ºåºçš„è®­ç»ƒepoch"""
        self.model.train()
        epoch_train_loss = 0.0
        batch_count = 0

        # æ·»åŠ æ¢¯åº¦è£å‰ªé˜ˆå€¼
        gradient_clip = 1.0  # å…³é”®ä¿®å¤ï¼šæ·»åŠ æ¢¯åº¦è£å‰ªé˜ˆå€¼

        for batch_idx, batch in enumerate(train_loader):
            try:
                if len(batch) == 3:
                    batch_features, batch_targets, batch_coords = batch
                    batch_features = batch_features.to(self.device, non_blocking=True)
                    batch_targets = batch_targets.to(self.device, non_blocking=True)
                    batch_coords = batch_coords.to(self.device, non_blocking=True) if batch_coords is not None else None
                else:
                    batch_features, batch_targets = batch
                    batch_features = batch_features.to(self.device, non_blocking=True)
                    batch_targets = batch_targets.to(self.device, non_blocking=True)
                    batch_coords = None

                self.optimizer.zero_grad(set_to_none=True)

                # è®¡ç®—ç©ºé—´æƒé‡
                spatial_weights = None
                if batch_coords is not None:
                    spatial_weights = self._compute_spatial_weights(batch_coords)

                if self.mixed_precision:
                    with autocast(device_type=self.device_type):
                        outputs = self.model(batch_features, spatial_weights, batch_coords)
                        loss = self.criterion(outputs, batch_targets)

                    self.scaler.scale(loss).backward()

                    # å…³é”®ä¿®å¤ï¼šæ·»åŠ æ¢¯åº¦è£å‰ª
                    self.scaler.unscale_(self.optimizer)  # å¿…é¡»å…ˆunscaleæ¢¯åº¦
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        max_norm=gradient_clip,
                        norm_type=2.0
                    )

                    # å…³é”®ï¼šæ·»åŠ æ¢¯åº¦ç›‘æ§
                    total_grad_norm = 0.0
                    grad_norms = []
                    for name, param in self.model.named_parameters():
                        if param.grad is not None:
                            param_grad_norm = param.grad.data.norm(2).item()
                            total_grad_norm += param_grad_norm ** 2  # ä¿®æ­£ï¼šåº”è¯¥å¹³æ–¹å’Œå†å¼€æ–¹
                            grad_norms.append((name, param_grad_norm))

                    total_grad_norm = total_grad_norm ** 0.5  # è®¡ç®—çœŸå®çš„æ¢¯åº¦èŒƒæ•°

                    self.scaler.step(self.optimizer)  # å…ˆæ‰§è¡Œä¼˜åŒ–å™¨
                    self.scaler.update()
                    self.scheduler.step()  # åæ‰§è¡Œå­¦ä¹ ç‡è°ƒåº¦

                else:
                    outputs = self.model(batch_features, spatial_weights, batch_coords)
                    loss = self.criterion(outputs, batch_targets)

                    loss.backward()

                    # å…³é”®ä¿®å¤ï¼šæ·»åŠ æ¢¯åº¦è£å‰ªï¼ˆéæ··åˆç²¾åº¦ç‰ˆæœ¬ï¼‰
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        max_norm=gradient_clip,
                        norm_type=2.0
                    )

                    # å…³é”®ï¼šæ·»åŠ æ¢¯åº¦ç›‘æ§
                    total_grad_norm = 0.0
                    grad_norms = []
                    for name, param in self.model.named_parameters():
                        if param.grad is not None:
                            param_grad_norm = param.grad.data.norm(2).item()
                            total_grad_norm += param_grad_norm ** 2
                            grad_norms.append((name, param_grad_norm))

                    total_grad_norm = total_grad_norm ** 0.5

                    self.optimizer.step()  # å…ˆæ‰§è¡Œä¼˜åŒ–å™¨
                    self.scheduler.step()  # åæ‰§è¡Œå­¦ä¹ ç‡è°ƒåº¦

                epoch_train_loss += loss.item()
                batch_count += 1

                # ç›‘æ§è¾“å‡ºå’Œæ¢¯åº¦
                if batch_idx % 10 == 0:
                    current_lr = self.optimizer.param_groups[0]['lr']
                    output_std = torch.std(outputs).item()
                    output_range = f"[{outputs.min().item():.3f}, {outputs.max().item():.3f}]"

                    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦è¢«è£å‰ª
                    grad_clipped = total_grad_norm > gradient_clip
                    clip_info = " (å·²è£å‰ª)" if grad_clipped else ""

                    self.logger.info(f'Batch {batch_idx}: Loss={loss.item():.6f}, '
                                     f'Output STD={output_std:.6f}, Output Range={output_range}, '
                                     f'Grad Norm={total_grad_norm:.6f}{clip_info}, LR={current_lr:.2e}')

                    # å¦‚æœæ¢¯åº¦å¾ˆå°ï¼Œæ˜¾ç¤ºå…·ä½“å“ªäº›å±‚çš„æ¢¯åº¦å°
                    if total_grad_norm < 1e-6:
                        self.logger.warning("æ¢¯åº¦æ¶ˆå¤±ï¼å„å±‚æ¢¯åº¦:")
                        for name, grad_norm in grad_norms[:5]:  # æ˜¾ç¤ºå‰5å±‚
                            self.logger.warning(f"  {name}: {grad_norm:.6f}")

                    # å¦‚æœæ¢¯åº¦å¾ˆå¤§ï¼Œæ˜¾ç¤ºå…·ä½“å“ªäº›å±‚çš„æ¢¯åº¦å¤§
                    elif total_grad_norm > 1000:
                        self.logger.warning("æ¢¯åº¦çˆ†ç‚¸é£é™©ï¼å„å±‚æ¢¯åº¦:")
                        for name, grad_norm in sorted(grad_norms, key=lambda x: x[1], reverse=True)[:3]:
                            self.logger.warning(f"  {name}: {grad_norm:.6f}")

                # ç›‘æ§è¾“å‡ºå˜åŒ–ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
                if batch_idx % 10 == 0:
                    output_std = torch.std(outputs).item()
                    current_lr = self.optimizer.param_groups[0]['lr']
                    self.logger.info(
                        f'Batch {batch_idx}, Loss: {loss.item():.6f}, Output STD: {output_std:.6f}, LR: {current_lr:.2e}')

            except Exception as e:
                self.logger.error(f"Batch {batch_idx} å¤±è´¥: {e}")
                # å…³é”®ä¿®å¤ï¼šåœ¨å¼‚å¸¸æ—¶æ¸…ç†æ¢¯åº¦
                self.optimizer.zero_grad(set_to_none=True)
                continue

        return epoch_train_loss / max(batch_count, 1)

    def _has_valid_gradients(self):
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœ‰æ•ˆæ¢¯åº¦"""
        has_valid_grad = False
        for param in self.model.parameters():
            if param.grad is not None:
                if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                    self.logger.warning("æ£€æµ‹åˆ°æ— æ•ˆæ¢¯åº¦ï¼Œæ¸…é›¶")
                    param.grad.zero_()
                elif torch.sum(torch.abs(param.grad)) > 0:
                    has_valid_grad = True

        return has_valid_grad

    def _check_and_clip_gradients(self):
        """æ£€æŸ¥å¹¶è£å‰ªæ¢¯åº¦"""
        # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨
        has_valid_grad = False
        for param in self.model.parameters():
            if param.grad is not None:
                if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                    self.logger.warning("æ£€æµ‹åˆ°æ— æ•ˆæ¢¯åº¦ï¼Œæ¸…é›¶")
                    param.grad.zero_()
                else:
                    has_valid_grad = True

        if not has_valid_grad:
            self.logger.warning("æ²¡æœ‰æœ‰æ•ˆæ¢¯åº¦")
            return False

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(
            self.model.parameters(),
            max_norm=self.gradient_clip,
            norm_type=2
        )

        return True

    def train(self, train_loader, val_loader=None, epochs=200, early_stopping_patience=20):
        """å®Œæ•´æ·±åº¦å­¦ä¹ è®­ç»ƒæµç¨‹ - ä¿®å¤æ¨¡å‹ä¿å­˜é€»è¾‘"""
        self.model.train()
        best_val_loss = float('inf')
        patience_counter = 0
        train_losses = []
        val_losses = []

        # å…³é”®ä¿®å¤ï¼šä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€è€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹
        best_model_state = None

        # GPUé¢„çƒ­
        if self.device_type == 'cuda':
            self._warmup_gpu()

        self.logger.info(f"å¼€å§‹è®­ç»ƒï¼Œæ€»è½®æ¬¡: {epochs}ï¼Œæ—©åœè€å¿ƒ: {early_stopping_patience}")

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            try:
                train_loss = self.train_epoch_mixed_precision(train_loader)
                train_losses.append(train_loss)
            except Exception as e:
                self.logger.error(f"Epoch {epoch} è®­ç»ƒå¤±è´¥: {e}")
                break

            # éªŒè¯é˜¶æ®µ
            if val_loader is not None:
                try:
                    val_loss = self.validate(val_loader)
                    val_losses.append(val_loss)

                    # æ—©åœé€»è¾‘
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        patience_counter = 0
                        # å…³é”®ä¿®å¤ï¼šä¿å­˜æ¨¡å‹çŠ¶æ€è€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹
                        best_model_state = {
                            'epoch': epoch,
                            'model_state_dict': self.model.state_dict().copy(),
                            'optimizer_state_dict': self.optimizer.state_dict().copy(),
                            'scheduler_state_dict': self.scheduler.state_dict().copy(),
                            'val_loss': val_loss,
                            'train_loss': train_loss
                        }
                        self.logger.info(f"Epoch {epoch}: ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€ï¼ŒéªŒè¯æŸå¤±: {val_loss:.6f}")
                    else:
                        patience_counter += 1

                    if patience_counter >= early_stopping_patience:
                        self.logger.info(f"æ—©åœåœ¨epoch {epoch}, æœ€ä½³éªŒè¯loss: {best_val_loss:.6f}")
                        break
                except Exception as e:
                    self.logger.error(f"Epoch {epoch} éªŒè¯å¤±è´¥: {e}")
                    val_losses.append(float('inf'))
            else:
                # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œä½¿ç”¨è®­ç»ƒloss
                if train_loss < best_val_loss:
                    best_val_loss = train_loss
                    patience_counter = 0
                    best_model_state = {
                        'epoch': epoch,
                        'model_state_dict': self.model.state_dict().copy(),
                        'optimizer_state_dict': self.optimizer.state_dict().copy(),
                        'scheduler_state_dict': self.scheduler.state_dict().copy(),
                        'train_loss': train_loss
                    }
                else:
                    patience_counter += 1

                if patience_counter >= early_stopping_patience:
                    self.logger.info(f"æ—©åœåœ¨epoch {epoch}, æœ€ä½³è®­ç»ƒloss: {best_val_loss:.6f}")
                    break

            # æ—¥å¿—è¾“å‡º
            if epoch % 10 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                if val_loader is not None and len(val_losses) > epoch:
                    self.logger.info(f"Epoch {epoch:3d} | Train Loss: {train_loss:.6f} | "
                                     f"Val Loss: {val_losses[epoch]:.6f} | LR: {current_lr:.2e}")
                else:
                    self.logger.info(f"Epoch {epoch:3d} | Train Loss: {train_loss:.6f} | "
                                     f"LR: {current_lr:.2e}")

        # å…³é”®ä¿®å¤ï¼šæ­£ç¡®åŠ è½½æœ€ä½³æ¨¡å‹çŠ¶æ€
        if best_model_state is not None:
            self.logger.info(f"åŠ è½½æœ€ä½³æ¨¡å‹çŠ¶æ€ (epoch {best_model_state['epoch']})")
            self.model.load_state_dict(best_model_state['model_state_dict'])
            # å¯é€‰ï¼šæ¢å¤ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨çŠ¶æ€
            # self.optimizer.load_state_dict(best_model_state['optimizer_state_dict'])
            # self.scheduler.load_state_dict(best_model_state['scheduler_state_dict'])
        else:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ€ä½³æ¨¡å‹çŠ¶æ€ï¼Œä½¿ç”¨æœ€ç»ˆè®­ç»ƒçŠ¶æ€")

        return train_losses, val_losses if val_loader is not None else train_losses

    def fit(self, X, y, coords=None):
        """è®­ç»ƒæ¨¡å‹ - æ·»åŠ è®­ç»ƒçŠ¶æ€æ£€æŸ¥"""
        self.logger.info("å¼€å§‹è®­ç»ƒï¼Œæ·»åŠ è®­ç»ƒçŠ¶æ€æ£€æŸ¥...")

        # æ ‡å‡†åŒ–æ•°æ®
        if y.ndim == 1:
            y_2d = y.reshape(-1, 1)
        else:
            y_2d = y

        X_normalized = self.feature_scaler.fit_transform(X)
        y_normalized = self.target_scaler.fit_transform(y_2d).flatten()

        self.logger.info(f"è®­ç»ƒæ•°æ®ç»Ÿè®¡:")
        self.logger.info(f"  XèŒƒå›´: [{X_normalized.min():.3f}, {X_normalized.max():.3f}]")
        self.logger.info(f"  yèŒƒå›´: [{y_normalized.min():.3f}, {y_normalized.max():.3f}]")
        self.logger.info(f"  yæ ‡å‡†å·®: {y_normalized.std():.3f}")

        # åˆ›å»ºæ•°æ®é›†
        dataset = EnhancedSpatialDataset(X_normalized, y_normalized, coords)
        train_loader = self.create_optimized_dataloader(dataset, batch_size=512, shuffle=True)

        # è®­ç»ƒå‰æ£€æŸ¥æ¨¡å‹åˆå§‹çŠ¶æ€
        self._check_model_initial_state(X_normalized, coords)

        # è®­ç»ƒ
        train_losses, val_losses = self.train(train_loader, epochs=100, early_stopping_patience=15)

        # è®­ç»ƒåæ£€æŸ¥æ¨¡å‹æœ€ç»ˆçŠ¶æ€
        self._check_model_final_state(X_normalized, coords)

        return train_losses, val_losses

    def _check_model_initial_state(self, X, coords):
        """æ£€æŸ¥æ¨¡å‹åˆå§‹çŠ¶æ€"""
        self.model.eval()
        with torch.no_grad():
            sample_outputs = []
            for i in range(min(10, len(X))):
                x = torch.tensor(X[i:i + 1], dtype=torch.float32, device=self.device)
                c = torch.tensor(coords[i:i + 1], dtype=torch.float32,
                                 device=self.device) if coords is not None else None

                if self.mixed_precision:
                    with autocast(device_type=self.device_type):
                        output = self.model(x, None, c)
                else:
                    output = self.model(x, None, c)

                sample_outputs.append(output.item())

            self.logger.info("=== æ¨¡å‹åˆå§‹çŠ¶æ€æ£€æŸ¥ ===")
            self.logger.info(f"åˆå§‹è¾“å‡ºèŒƒå›´: [{min(sample_outputs):.6f}, {max(sample_outputs):.6f}]")
            self.logger.info(f"åˆå§‹è¾“å‡ºæ ‡å‡†å·®: {np.std(sample_outputs):.6f}")
            self.logger.info("=====================")

    def _check_model_final_state(self, X, coords):
        """æ£€æŸ¥æ¨¡å‹æœ€ç»ˆçŠ¶æ€ - ä¿®å¤æ£€æŸ¥é€»è¾‘"""
        self.model.eval()
        with torch.no_grad():
            sample_outputs = []

            # æ£€æŸ¥å¤šä¸ªæ ·æœ¬
            for i in range(min(20, len(X))):  # æ£€æŸ¥æ›´å¤šæ ·æœ¬
                x = torch.tensor(X[i:i + 1], dtype=torch.float32, device=self.device)
                c = torch.tensor(coords[i:i + 1], dtype=torch.float32,
                                 device=self.device) if coords is not None else None

                if self.mixed_precision:
                    with autocast(device_type=self.device_type):
                        output = self.model(x, None, c)
                else:
                    output = self.model(x, None, c)

                sample_outputs.append(output.item())

            self.logger.info("=== æ¨¡å‹æœ€ç»ˆçŠ¶æ€è¯¦ç»†æ£€æŸ¥ ===")
            self.logger.info(f"æœ€ç»ˆè¾“å‡ºèŒƒå›´: [{min(sample_outputs):.6f}, {max(sample_outputs):.6f}]")
            self.logger.info(f"æœ€ç»ˆè¾“å‡ºæ ‡å‡†å·®: {np.std(sample_outputs):.6f}")
            self.logger.info(f"è¾“å‡ºå”¯ä¸€å€¼æ•°é‡: {len(np.unique(sample_outputs))}")

            # æ£€æŸ¥è¾“å‡ºæ˜¯å¦æ’å®š
            if np.std(sample_outputs) < 1e-6:
                self.logger.error("âŒ æ¨¡å‹æœ€ç»ˆçŠ¶æ€è¾“å‡ºæ’å®šï¼")
                self.logger.error("å¯èƒ½åŸå› :")
                self.logger.error("1. æ¨¡å‹æƒé‡å…¨éƒ¨ç›¸åŒ")
                self.logger.error("2. æ¢¯åº¦æ¶ˆå¤±å¯¼è‡´æ‰€æœ‰æƒé‡æ”¶æ•›åˆ°ç›¸åŒå€¼")
                self.logger.error("3. æ¨¡å‹ä¿å­˜/åŠ è½½é—®é¢˜")
            else:
                self.logger.info("âœ… æ¨¡å‹æœ€ç»ˆçŠ¶æ€è¾“å‡ºæ­£å¸¸")

            self.logger.info("=====================")

    def predict(self, features, coords=None, batch_size=1024):
        """é‡å†™é¢„æµ‹æ–¹æ³• - ç¡®ä¿ç¨³å®šå¯é çš„é¢„æµ‹"""
        self.model.eval()

        self.logger.info("ğŸš€ å¼€å§‹é¢„æµ‹æµç¨‹...")
        self.logger.info(f"è¾“å…¥ç‰¹å¾å½¢çŠ¶: {features.shape}, åæ ‡å½¢çŠ¶: {coords.shape if coords is not None else 'None'}")

        # ==================== 1. é¢„æµ‹å‰æ¨¡å‹çŠ¶æ€éªŒè¯ ====================
        self.logger.info("=== é¢„æµ‹å‰æ¨¡å‹çŠ¶æ€éªŒè¯ ===")

        # ä½¿ç”¨è®­ç»ƒæ•°æ®çš„å­é›†éªŒè¯æ¨¡å‹çŠ¶æ€
        validation_samples = min(10, len(features))
        validation_outputs = []

        with torch.no_grad():
            for i in range(validation_samples):
                # åˆ›å»ºå•ä¸ªæ ·æœ¬çš„tensor
                x_sample = torch.tensor(features[i:i + 1], dtype=torch.float32, device=self.device)
                c_sample = torch.tensor(coords[i:i + 1], dtype=torch.float32,
                                        device=self.device) if coords is not None else None

                # ä½¿ç”¨ä¸è®­ç»ƒå®Œå…¨ç›¸åŒçš„è·¯å¾„
                spatial_weights = None
                if c_sample is not None:
                    spatial_weights = self._compute_spatial_weights(c_sample)

                # å…³é”®ï¼šå¼ºåˆ¶ä½¿ç”¨éæ··åˆç²¾åº¦è¿›è¡ŒéªŒè¯
                output = self.model(x_sample, spatial_weights, c_sample)
                validation_outputs.append(output.item())

        val_std = np.std(validation_outputs)
        self.logger.info(f"éªŒè¯è¾“å‡º - èŒƒå›´: [{min(validation_outputs):.3f}, {max(validation_outputs):.3f}]")
        self.logger.info(f"éªŒè¯è¾“å‡º - æ ‡å‡†å·®: {val_std:.6f}")
        self.logger.info(f"éªŒè¯è¾“å‡º - å”¯ä¸€å€¼æ•°é‡: {len(np.unique(validation_outputs))}")

        if val_std < 1e-6:
            self.logger.error("âŒ æ¨¡å‹çŠ¶æ€éªŒè¯å¤±è´¥ï¼šè¾“å‡ºæ’å®šï¼")
            self.logger.error("å¯èƒ½åŸå› ï¼šæ¨¡å‹æƒé‡é—®é¢˜ã€æ¢¯åº¦æ¶ˆå¤±ã€æˆ–è®­ç»ƒè¿‡ç¨‹å¼‚å¸¸")

            # ç´§æ€¥ä¿®å¤ï¼šå°è¯•é‡æ–°åˆå§‹åŒ–è¾“å‡ºå±‚
            self.logger.warning("å°è¯•ç´§æ€¥ä¿®å¤ï¼šé‡æ–°åˆå§‹åŒ–è¾“å‡ºå±‚...")
            for name, module in self.model.named_modules():
                if isinstance(module, nn.Linear) and module.out_features == 1:
                    nn.init.kaiming_normal_(module.weight)
                    if module.bias is not None:
                        nn.init.constant_(module.bias, 0.0)
                    self.logger.info(f"é‡æ–°åˆå§‹åŒ–å±‚: {name}")

            # é‡æ–°éªŒè¯
            validation_outputs = []
            with torch.no_grad():
                for i in range(validation_samples):
                    x_sample = torch.tensor(features[i:i + 1], dtype=torch.float32, device=self.device)
                    c_sample = torch.tensor(coords[i:i + 1], dtype=torch.float32,
                                            device=self.device) if coords is not None else None
                    spatial_weights = self._compute_spatial_weights(c_sample) if c_sample is not None else None
                    output = self.model(x_sample, spatial_weights, c_sample)
                    validation_outputs.append(output.item())

            val_std = np.std(validation_outputs)
            self.logger.info(f"ä¿®å¤åéªŒè¯è¾“å‡ºæ ‡å‡†å·®: {val_std:.6f}")

            if val_std < 1e-6:
                self.logger.error("âŒ ç´§æ€¥ä¿®å¤å¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰é¢„æµ‹æ–¹æ¡ˆ")
                return self._fallback_prediction(features, coords)

        self.logger.info("âœ… æ¨¡å‹çŠ¶æ€éªŒè¯é€šè¿‡")

        # ==================== 2. ç‰¹å¾æ ‡å‡†åŒ– ====================
        self.logger.info("=== ç‰¹å¾æ ‡å‡†åŒ– ===")

        if not hasattr(self, 'feature_scaler') or self.feature_scaler is None:
            self.logger.error("ç‰¹å¾æ ‡å‡†åŒ–å™¨æœªåˆå§‹åŒ–")
            return self._fallback_prediction(features, coords)

        try:
            features_normalized = self.feature_scaler.transform(features)
            self.logger.info(
                f"ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ - å‡å€¼: {features_normalized.mean():.3f}, æ ‡å‡†å·®: {features_normalized.std():.3f}")
        except Exception as e:
            self.logger.error(f"ç‰¹å¾æ ‡å‡†åŒ–å¤±è´¥: {e}")
            return self._fallback_prediction(features, coords)

        # ==================== 3. æ‰¹é‡é¢„æµ‹ ====================
        self.logger.info("=== æ‰¹é‡é¢„æµ‹ ===")

        all_predictions_normalized = []
        successful_batches = 0
        failed_batches = 0

        with torch.no_grad():
            for i in range(0, len(features_normalized), batch_size):
                batch_start = i
                batch_end = min(i + batch_size, len(features_normalized))
                batch_size_actual = batch_end - batch_start

                try:
                    # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
                    batch_features = torch.tensor(
                        features_normalized[batch_start:batch_end],
                        dtype=torch.float32,
                        device=self.device
                    )

                    batch_coords = None
                    if coords is not None:
                        batch_coords = torch.tensor(
                            coords[batch_start:batch_end],
                            dtype=torch.float32,
                            device=self.device
                        )

                    # è®¡ç®—ç©ºé—´æƒé‡
                    spatial_weights = None
                    if batch_coords is not None and len(batch_coords) > 1:
                        spatial_weights = self._compute_spatial_weights(batch_coords)

                    # æ¨¡å‹é¢„æµ‹ - å¼ºåˆ¶ä½¿ç”¨float32é¿å…æ··åˆç²¾åº¦é—®é¢˜
                    batch_predictions = self.model(batch_features, spatial_weights, batch_coords)

                    # æ£€æŸ¥æ‰¹æ¬¡é¢„æµ‹ç»“æœ
                    batch_predictions_np = batch_predictions.cpu().numpy()
                    batch_std = np.std(batch_predictions_np)

                    if batch_std < 1e-6 and batch_size_actual > 1:
                        self.logger.warning(f"æ‰¹æ¬¡ {i // batch_size} è¾“å‡ºæ’å®š (std={batch_std:.6f})")
                        # å°è¯•ä¸ä½¿ç”¨ç©ºé—´æƒé‡é‡æ–°é¢„æµ‹
                        self.logger.info("å°è¯•ä¸ä½¿ç”¨ç©ºé—´æƒé‡é‡æ–°é¢„æµ‹...")
                        batch_predictions_fallback = self.model(batch_features, None, batch_coords)
                        batch_predictions_fallback_np = batch_predictions_fallback.cpu().numpy()
                        fallback_std = np.std(batch_predictions_fallback_np)

                        if fallback_std > batch_std:
                            self.logger.info(f"âœ… æ— ç©ºé—´æƒé‡é¢„æµ‹æ”¹å–„: std={fallback_std:.6f}")
                            batch_predictions_np = batch_predictions_fallback_np
                    # å­˜å‚¨é¢„æµ‹ç»“æœ
                    all_predictions_normalized.append(batch_predictions_np)
                    successful_batches += 1

                    # è¿›åº¦æ—¥å¿—
                    if (i // batch_size) % 10 == 0:
                        self.logger.info(f"è¿›åº¦: {batch_end}/{len(features)} samples, å½“å‰æ‰¹æ¬¡æ ‡å‡†å·®: {batch_std:.6f}")

                except Exception as e:
                    self.logger.error(f"æ‰¹æ¬¡ {i // batch_size} é¢„æµ‹å¤±è´¥: {e}")
                    # ä½¿ç”¨å‡å€¼çš„å¤‡é€‰é¢„æµ‹
                    fallback_batch = np.full(batch_size_actual, 0.0)  # æ ‡å‡†åŒ–ç©ºé—´çš„å‡å€¼
                    all_predictions_normalized.append(fallback_batch)
                    failed_batches += 1
                    continue

        self.logger.info(f"æ‰¹é‡é¢„æµ‹å®Œæˆ - æˆåŠŸ: {successful_batches}, å¤±è´¥: {failed_batches}")

        if len(all_predictions_normalized) == 0:
            self.logger.error("æ‰€æœ‰æ‰¹æ¬¡é¢„æµ‹å¤±è´¥")
            return self._fallback_prediction(features, coords)

        # ==================== 4. ç»“æœåˆå¹¶å’Œé€†æ ‡å‡†åŒ– ====================
        self.logger.info("=== ç»“æœåå¤„ç† ===")

        try:
            # åˆå¹¶æ‰€æœ‰é¢„æµ‹ç»“æœ
            predictions_normalized = np.concatenate(all_predictions_normalized)

            self.logger.info(f"æ ‡å‡†åŒ–é¢„æµ‹ç»“æœç»Ÿè®¡:")
            self.logger.info(f"  èŒƒå›´: [{predictions_normalized.min():.3f}, {predictions_normalized.max():.3f}]")
            self.logger.info(f"  å‡å€¼: {predictions_normalized.mean():.3f}")
            self.logger.info(f"  æ ‡å‡†å·®: {predictions_normalized.std():.3f}")
            self.logger.info(f"  å”¯ä¸€å€¼æ•°é‡: {len(np.unique(predictions_normalized))}")

            # é€†æ ‡å‡†åŒ–åˆ°åŸå§‹å°ºåº¦
            if predictions_normalized.ndim == 1:
                predictions_normalized_2d = predictions_normalized.reshape(-1, 1)
            else:
                predictions_normalized_2d = predictions_normalized

            predictions_original = self.target_scaler.inverse_transform(predictions_normalized_2d).flatten()

            self.logger.info(f"åŸå§‹å°ºåº¦é¢„æµ‹ç»“æœç»Ÿè®¡:")
            self.logger.info(f"  èŒƒå›´: [{predictions_original.min():.1f}, {predictions_original.max():.1f}]")
            self.logger.info(f"  å‡å€¼: {predictions_original.mean():.1f}")
            self.logger.info(f"  æ ‡å‡†å·®: {predictions_original.std():.1f}")

            # æœ€ç»ˆæ£€æŸ¥
            if np.std(predictions_original) < 1e-6:
                self.logger.warning("âš ï¸ æœ€ç»ˆé¢„æµ‹ç»“æœæ ‡å‡†å·®å¾ˆå°ï¼Œä½†ä»åœ¨å¯æ¥å—èŒƒå›´å†…")

            self.logger.info("ğŸ¯ é¢„æµ‹æµç¨‹å®Œæˆï¼")
            return predictions_original

        except Exception as e:
            self.logger.error(f"ç»“æœåå¤„ç†å¤±è´¥: {e}")
            return self._fallback_prediction(features, coords)

    def _fallback_prediction(self, features, coords):
        """å¤‡é€‰é¢„æµ‹æ–¹æ¡ˆ"""
        self.logger.warning("ä½¿ç”¨å¤‡é€‰é¢„æµ‹æ–¹æ¡ˆ")

        if hasattr(self, 'target_scaler') and self.target_scaler is not None:
            # ä½¿ç”¨ç›®æ ‡å˜é‡çš„å‡å€¼ä½œä¸ºé¢„æµ‹
            fallback_value = self.target_scaler.mean_[0] if hasattr(self.target_scaler, 'mean_') else 0.0
        else:
            fallback_value = 0.0

        self.logger.info(f"å¤‡é€‰é¢„æµ‹å€¼: {fallback_value}")
        return np.full(len(features), fallback_value)

    def debug_output_range(self, batch_features, batch_targets):
        """è°ƒè¯•è¾“å‡ºèŒƒå›´é—®é¢˜"""
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(batch_features, None, None)

            self.logger.info("=== è¾“å‡ºèŒƒå›´è¯Šæ–­ ===")
            self.logger.info(f"è¾“å…¥ç‰¹å¾èŒƒå›´: [{batch_features.min():.3f}, {batch_features.max():.3f}]")
            self.logger.info(f"ç›®æ ‡å€¼èŒƒå›´: [{batch_targets.min():.3f}, {batch_targets.max():.3f}]")
            self.logger.info(f"æ¨¡å‹è¾“å‡ºèŒƒå›´: [{outputs.min():.3f}, {outputs.max():.3f}]")
            self.logger.info(f"è¾“å‡º/ç›®æ ‡æ¯”ä¾‹: {outputs.std() / batch_targets.std():.3f}")
            self.logger.info("===================")

    def _compute_spatial_weights(self, batch_coords):
        """è®¡ç®—ç©ºé—´æƒé‡çŸ©é˜µ - ç¡®ä¿æ•°å€¼ç¨³å®šæ€§"""
        n_batch = batch_coords.shape[0]

        if n_batch <= 1:
            return torch.eye(n_batch, device=self.device, dtype=torch.float32)

        try:
            # ç¡®ä¿ä½¿ç”¨float32
            batch_coords_float32 = batch_coords.float()

            # è®¡ç®—æ¬§æ°è·ç¦»
            diff = batch_coords_float32.unsqueeze(1) - batch_coords_float32.unsqueeze(0)
            distances = torch.sqrt(torch.sum(diff ** 2, dim=2) + 1e-8)

            # é«˜æ–¯æ ¸å‡½æ•°
            weights = torch.exp(-0.5 * (distances / self.bandwidth) ** 2)

            # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            if torch.any(torch.isnan(weights)) or torch.any(torch.isinf(weights)):
                self.logger.warning("ç©ºé—´æƒé‡åŒ…å«æ— æ•ˆå€¼ï¼Œä½¿ç”¨å•ä½çŸ©é˜µ")
                return torch.eye(n_batch, device=self.device, dtype=torch.float32)

            # ç¡®ä¿å¯¹è§’çº¿ä¸º1
            weights = weights.fill_diagonal_(1.0)

            return weights

        except Exception as e:
            self.logger.error(f"ç©ºé—´æƒé‡è®¡ç®—å¤±è´¥: {e}")
            return torch.eye(n_batch, device=self.device, dtype=torch.float32)

    def create_optimized_dataloader(self, dataset, batch_size=512, shuffle=True, is_train=True):
        """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
        num_workers = min(16, self.cpu_workers // 2) if is_train else min(8, self.cpu_workers // 4)

        self.logger.info(f"åˆ›å»ºæ•°æ®åŠ è½½å™¨ - æ‰¹æ¬¡å¤§å°: {batch_size}, å·¥ä½œè¿›ç¨‹: {num_workers}")

        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=self.device.type == 'cuda',
            persistent_workers=num_workers > 0 and is_train,
            prefetch_factor=2 if num_workers > 0 else None
        )

    def validate(self, val_loader):
        """éªŒè¯é›†è¯„ä¼° - ä¿®å¤ç‰ˆæœ¬"""
        self.model.eval()
        val_loss = 0.0

        with torch.no_grad():
            for batch in val_loader:
                if len(batch) == 3:
                    batch_features, batch_targets, batch_coords = batch
                    batch_features = batch_features.to(self.device, non_blocking=True)
                    batch_targets = batch_targets.to(self.device, non_blocking=True)
                    batch_coords = batch_coords.to(self.device, non_blocking=True) if batch_coords is not None else None
                else:
                    batch_features, batch_targets = batch
                    batch_features = batch_features.to(self.device, non_blocking=True)
                    batch_targets = batch_targets.to(self.device, non_blocking=True)
                    batch_coords = None

                spatial_weights = None
                if batch_coords is not None:
                    spatial_weights = self._compute_spatial_weights(batch_coords)

                # ä¿®å¤ï¼šæ­£ç¡®ä½¿ç”¨autocast
                if self.mixed_precision:
                    with autocast(device_type=self.device_type):
                        outputs = self.model(batch_features, spatial_weights, batch_coords)
                        loss = self.criterion(outputs, batch_targets)
                else:
                    outputs = self.model(batch_features, spatial_weights, batch_coords)
                    loss = self.criterion(outputs, batch_targets)

                val_loss += loss.item()

        return val_loss / len(val_loader)

    def debug_standardization(self, X, y):
        """è°ƒè¯•æ ‡å‡†åŒ–å™¨çŠ¶æ€"""
        self.logger.info("=== æ ‡å‡†åŒ–å™¨è°ƒè¯• ===")

        # æ£€æŸ¥ç‰¹å¾æ ‡å‡†åŒ–å™¨
        if hasattr(self.feature_scaler, 'mean_'):
            self.logger.info(f"ç‰¹å¾æ ‡å‡†åŒ–å™¨ - mean: {self.feature_scaler.mean_[:3]}...")
            self.logger.info(f"ç‰¹å¾æ ‡å‡†åŒ–å™¨ - scale: {self.feature_scaler.scale_[:3]}...")

        # æ£€æŸ¥ç›®æ ‡æ ‡å‡†åŒ–å™¨
        if hasattr(self.target_scaler, 'mean_'):
            self.logger.info(f"ç›®æ ‡æ ‡å‡†åŒ–å™¨ - mean: {self.target_scaler.mean_}")
            self.logger.info(f"ç›®æ ‡æ ‡å‡†åŒ–å™¨ - scale: {self.target_scaler.scale_}")

        # æµ‹è¯•é€†æ ‡å‡†åŒ–
        test_output = np.array([-1.0, 0.0, 1.0])
        try:
            restored = self.target_scaler.inverse_transform(test_output.reshape(-1, 1)).flatten()
            self.logger.info(f"é€†æ ‡å‡†åŒ–æµ‹è¯•: {test_output} -> {restored}")
        except Exception as e:
            self.logger.error(f"é€†æ ‡å‡†åŒ–æµ‹è¯•å¤±è´¥: {e}")

        self.logger.info("===================")

    def _warmup_gpu(self):
        """GPUé¢„çƒ­ - ä¿®å¤ç‰ˆæœ¬"""
        if self.device_type == 'cuda':
            self.logger.info("è¿›è¡ŒGPUé¢„çƒ­...")
            # è¿è¡Œä¸€ä¸ªå°çš„è™šæ‹Ÿè®¡ç®—æ¥é¢„çƒ­GPU
            dummy_input = torch.randn(64, self.model.feature_network[0].in_features,
                                      device=self.device,
                                      dtype=torch.float16 if self.mixed_precision else torch.float32)
            dummy_coords = torch.randn(64, 2, device=self.device,
                                       dtype=torch.float16 if self.mixed_precision else torch.float32)

            # ä¿®å¤ï¼šæ­£ç¡®ä½¿ç”¨autocast
            if self.mixed_precision:
                with autocast(device_type=self.device_type):
                    for _ in range(20):
                        spatial_weights = self._compute_spatial_weights(dummy_coords)
                        _ = self.model(dummy_input, spatial_weights, dummy_coords)
            else:
                for _ in range(20):
                    spatial_weights = self._compute_spatial_weights(dummy_coords)
                    _ = self.model(dummy_input, spatial_weights, dummy_coords)

            torch.cuda.synchronize()
            self.logger.info("GPUé¢„çƒ­å®Œæˆ")


def train_pure_gnnwr_annual_only(df, output_dir=None, random_state=42,
                                 device='auto', mixed_precision=True, cpu_workers=24):
    """
    ä¿®å¤ç‰ˆçš„çº¯å‡€ç‰ˆGNNWRåˆ†æ - ä»…è¿›è¡Œå¹´åº¦äº¤å‰éªŒè¯ç‰ˆæœ¬
    """
    import numpy as np
    import pandas as pd
    import os
    import joblib
    from datetime import datetime

    logger = logging.getLogger("PureGNNWRAnalysis")
    logger.info("=" * 60)
    logger.info("ğŸ“Š å¼€å§‹çº¯å‡€ç‰ˆGNNWRå¹´åº¦äº¤å‰éªŒè¯åˆ†æ")
    logger.info("=" * 60)

    try:
        # ä½¿ç”¨SWEClusterEnsembleçš„æ•°æ®é¢„å¤„ç†
        ensemble = SWEClusterEnsemble(n_clusters=1)
        X, y, station_groups, year_groups, coords = ensemble.preprocess_data(df)

        logger.info(f"æ•°æ®åŠ è½½: {len(X)}æ ·æœ¬, {X.shape[1]}ç‰¹å¾")
        logger.info(f"å¹´åº¦åˆ†å¸ƒ: {len(np.unique(year_groups))}ä¸ªå¹´ä»½")
        logger.info(f"ç«™ç‚¹åˆ†å¸ƒ: {len(np.unique(station_groups))}ä¸ªç«™ç‚¹")

        # 1. ä»…è¿›è¡Œå¹´åº¦äº¤å‰éªŒè¯
        logger.info("\n" + "=" * 50)
        logger.info("æ­¥éª¤ 1: å¹´åº¦äº¤å‰éªŒè¯ (å”¯ä¸€éªŒè¯æ­¥éª¤)")
        logger.info("=" * 50)

        yearly_cv_results = pure_gnnwr_cross_validate_fixed(
            X, y, year_groups, coords, 'yearly', logger,
            device=device, mixed_precision=mixed_precision, cpu_workers=cpu_workers
        )

        # ç¡®ä¿fold_metricså­˜åœ¨
        if 'fold_metrics' not in yearly_cv_results:
            logger.warning("fold_metricsä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºçš„fold_metrics")
            yearly_cv_results['fold_metrics'] = {}

        if 'overall_metrics' not in yearly_cv_results:
            logger.warning("overall_metricsä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤å€¼")
            yearly_cv_results['overall_metrics'] = {
                'r2': 0.0, 'rmse': 1.0, 'mae': 1.0, 'explained_variance': 0.0
            }

        # æ•´åˆæ‰€æœ‰ç»“æœ
        results = {
            'yearly_cv': yearly_cv_results,
            'data_info': {
                'total_samples': len(X),
                'n_features': X.shape[1],
                'n_stations': len(np.unique(station_groups)),
                'n_years': len(np.unique(year_groups)),
                'device': str(device),
                'mixed_precision': mixed_precision
            }
        }

        # ä¿å­˜ç»“æœå’Œç”Ÿæˆå›¾è¡¨
        if output_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = f"./pure_gnnwr_annual_only_{timestamp}"

        os.makedirs(output_dir, exist_ok=True)
        logger.info(f"ä¿å­˜ç»“æœåˆ°: {output_dir}")

        # ä¿å­˜ç»“æœæ•°æ®
        results_path = os.path.join(output_dir, 'pure_gnnwr_results_annual.pkl')
        joblib.dump(results, results_path)

        # ç”Ÿæˆä¸“é—¨é’ˆå¯¹å¹´åº¦éªŒè¯çš„å¯è§†åŒ–å›¾è¡¨
        create_annual_only_visualizations(results, output_dir)

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report_path = os.path.join(output_dir, 'pure_gnnwr_report_annual.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(generate_annual_report(results))

        # è¾“å‡ºç»¼åˆæŠ¥å‘Š
        print_annual_report(results)

        logger.info("âœ… çº¯å‡€ç‰ˆGNNWRå¹´åº¦äº¤å‰éªŒè¯åˆ†æå®Œæˆ!")
        return results

    except Exception as e:
        logger.error(f"çº¯å‡€ç‰ˆGNNWRå¹´åº¦åˆ†æå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise


def create_annual_only_visualizations(results, output_dir):
    """ä¸“é—¨ä¸ºå¹´åº¦äº¤å‰éªŒè¯ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
    import matplotlib.pyplot as plt
    import seaborn as sns
    from matplotlib.gridspec import GridSpec

    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

    # åˆ›å»ºå›¾è¡¨
    fig = plt.figure(figsize=(20, 16))
    gs = GridSpec(3, 3, figure=fig)

    yearly_cv = results['yearly_cv']

    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
    if not yearly_cv['fold_metrics']:
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œåˆ›å»ºç©ºå›¾è¡¨
        ax = fig.add_subplot(gs[:, :])
        ax.text(0.5, 0.5, 'æ— æœ‰æ•ˆæ•°æ®å¯ç”¨\nè¯·æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹',
                horizontalalignment='center', verticalalignment='center',
                transform=ax.transAxes, fontsize=16)
        ax.set_title('å¹´åº¦äº¤å‰éªŒè¯åˆ†æ', fontsize=18)
        plt.savefig(os.path.join(output_dir, 'annual_cross_validation_analysis.png'),
                    dpi=300, bbox_inches='tight')
        plt.close()
        return

    try:
        # 1. å¹´åº¦äº¤å‰éªŒè¯æ€§èƒ½å¯¹æ¯”
        ax1 = fig.add_subplot(gs[0, 0])
        years = list(yearly_cv['fold_metrics'].keys())
        r2_scores = [yearly_cv['fold_metrics'][year]['r2'] for year in years]
        rmse_scores = [yearly_cv['fold_metrics'][year]['rmse'] for year in years]

        x = np.arange(len(years))
        width = 0.35

        ax1.bar(x - width / 2, r2_scores, width, label='RÂ²', alpha=0.7, color='skyblue')
        ax1.bar(x + width / 2, rmse_scores, width, label='RMSE', alpha=0.7, color='lightcoral')

        ax1.set_xlabel('å¹´ä»½')
        ax1.set_ylabel('æŒ‡æ ‡å€¼')
        ax1.set_title('å¹´åº¦äº¤å‰éªŒè¯æ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax1.set_xticks(x)
        ax1.set_xticklabels(years, rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # 2. å¹´åº¦æ€§èƒ½çƒ­åŠ›å›¾
        ax2 = fig.add_subplot(gs[0, 1:])
        metrics_data = []
        for year, metrics in yearly_cv['fold_metrics'].items():
            metrics_data.append({
                'Year': year,
                'RÂ²': metrics['r2'],
                'RMSE': metrics['rmse'],
                'MAE': metrics['mae'],
                'æ ·æœ¬æ•°': metrics.get('n_samples', 0)
            })

        metrics_df = pd.DataFrame(metrics_data)
        metrics_pivot = metrics_df.pivot_table(values=['RÂ²', 'RMSE'], index='Year')

        sns.heatmap(metrics_pivot, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax2)
        ax2.set_title('å¹´åº¦äº¤å‰éªŒè¯æ€§èƒ½çƒ­åŠ›å›¾', fontsize=14, fontweight='bold')

        # 3. æ¨¡å‹æ¶æ„ä¿¡æ¯
        ax3 = fig.add_subplot(gs[1, 0])
        ax3.axis('off')
        info_text = f"""
        æ¨¡å‹æ¶æ„ä¿¡æ¯:
        - è¾“å…¥ç»´åº¦: {results['data_info']['n_features']}
        - éšè—å±‚: [512, 256, 128, 64]
        - Dropout: 0.3
        - å­¦ä¹ ç‡: 0.001
        - è®¾å¤‡: {results['data_info']['device']}
        - æ··åˆç²¾åº¦: {results['data_info']['mixed_precision']}

        æ•°æ®ç»Ÿè®¡:
        - æ€»æ ·æœ¬: {results['data_info']['total_samples']}
        - å¹´ä»½æ•°: {results['data_info']['n_years']}
        - ç«™ç‚¹æ•°: {results['data_info']['n_stations']}
        """
        ax3.text(0.1, 0.9, info_text, transform=ax3.transAxes, fontsize=10,
                 verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))

        # 4. æ€§èƒ½æ±‡æ€»
        ax4 = fig.add_subplot(gs[1, 1:])
        ax4.axis('off')

        yearly_avg_r2 = yearly_cv['overall_metrics']['r2']
        yearly_avg_rmse = yearly_cv['overall_metrics']['rmse']

        summary_text = f"""
        æ€§èƒ½æ±‡æ€»:

        å¹´åº¦äº¤å‰éªŒè¯:
        - å¹³å‡ RÂ²: {yearly_avg_r2:.4f}
        - å¹³å‡ RMSE: {yearly_avg_rmse:.4f}
        - å¹³å‡ MAE: {yearly_cv['overall_metrics']['mae']:.4f}
        - å¹³å‡è§£é‡Šæ–¹å·®: {yearly_cv['overall_metrics']['explained_variance']:.4f}
        """

        # æ·»åŠ æœ€ä½³å’Œæœ€å·®å¹´ä»½ä¿¡æ¯
        if yearly_cv['fold_metrics']:
            best_year = max(yearly_cv['fold_metrics'].items(), key=lambda x: x[1]['r2'])
            worst_year = min(yearly_cv['fold_metrics'].items(), key=lambda x: x[1]['r2'])
            summary_text += f"\næœ€ä½³å¹´ä»½: {best_year[0]} (RÂ² = {best_year[1]['r2']:.4f})"
            summary_text += f"\næœ€å·®å¹´ä»½: {worst_year[0]} (RÂ² = {worst_year[1]['r2']:.4f})"

        ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, fontsize=12,
                 verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))

        # 5. æ®‹å·®åˆ†æï¼ˆä½¿ç”¨æ‰€æœ‰å¹´ä»½çš„æ®‹å·®ï¼‰
        ax5 = fig.add_subplot(gs[2, 0])
        all_residuals = []
        for year_data in yearly_cv['fold_results']:
            all_residuals.extend(year_data['residuals'])

        ax5.hist(all_residuals, bins=50, alpha=0.7, color='orange', edgecolor='black')
        ax5.axvline(0, color='red', linestyle='--', linewidth=2)
        ax5.set_xlabel('æ®‹å·®')
        ax5.set_ylabel('é¢‘æ•°')
        ax5.set_title('æ‰€æœ‰å¹´ä»½æ®‹å·®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax5.grid(True, alpha=0.3)

        # 6. é¢„æµ‹vsçœŸå®å€¼æ•£ç‚¹å›¾ï¼ˆä½¿ç”¨æ‰€æœ‰å¹´ä»½æ•°æ®ï¼‰
        ax6 = fig.add_subplot(gs[2, 1:])
        all_y_true = []
        all_y_pred = []
        for year_data in yearly_cv['fold_results']:
            all_y_true.extend(year_data['y_true'])
            all_y_pred.extend(year_data['y_pred'])

        ax6.scatter(all_y_true, all_y_pred, alpha=0.6, color='blue', s=20)
        ax6.plot([min(all_y_true), max(all_y_true)], [min(all_y_true), max(all_y_true)],
                 'r--', linewidth=2)
        ax6.set_xlabel('çœŸå®å€¼')
        ax6.set_ylabel('é¢„æµ‹å€¼')
        ax6.set_title(f'æ‰€æœ‰å¹´ä»½é¢„æµ‹ vs çœŸå®å€¼\næ€»ä½“RÂ² = {yearly_avg_r2:.3f}, RMSE = {yearly_avg_rmse:.3f}',
                      fontsize=14, fontweight='bold')
        ax6.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'annual_cross_validation_analysis.png'),
                    dpi=300, bbox_inches='tight')
        plt.close()

        # é¢å¤–ä¿å­˜å¹´åº¦è¯¦ç»†å›¾è¡¨
        create_detailed_year_analysis(yearly_cv, output_dir)

    except Exception as e:
        logger.error(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        # åˆ›å»ºé”™è¯¯å›¾è¡¨
        ax = fig.add_subplot(gs[:, :])
        ax.text(0.5, 0.5, f'å¯è§†åŒ–ç”Ÿæˆå¤±è´¥:\n{str(e)}',
                horizontalalignment='center', verticalalignment='center',
                transform=ax.transAxes, fontsize=12, color='red')
        ax.set_title('å¹´åº¦äº¤å‰éªŒè¯åˆ†æ - é”™è¯¯', fontsize=16)
        plt.savefig(os.path.join(output_dir, 'annual_cross_validation_analysis.png'),
                    dpi=300, bbox_inches='tight')
        plt.close()


def create_detailed_year_analysis(yearly_cv, output_dir):
    """ä¸ºæ¯ä¸ªå¹´ä»½åˆ›å»ºè¯¦ç»†çš„åˆ†æå›¾è¡¨"""
    import matplotlib.pyplot as plt

    if not yearly_cv['fold_results']:
        return

    # ä¸ºæ¯ä¸ªå¹´ä»½åˆ›å»ºå•ç‹¬çš„å›¾è¡¨
    for fold_result in yearly_cv['fold_results']:
        year = fold_result['test_group']

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'å¹´ä»½ {year} è¯¦ç»†åˆ†æ', fontsize=16, fontweight='bold')

        # 1. é¢„æµ‹ vs çœŸå®å€¼
        y_true = fold_result['y_true']
        y_pred = fold_result['y_pred']
        metrics = fold_result['metrics']

        ax1.scatter(y_true, y_pred, alpha=0.6, color='blue')
        ax1.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', linewidth=2)
        ax1.set_xlabel('çœŸå®å€¼')
        ax1.set_ylabel('é¢„æµ‹å€¼')
        ax1.set_title(f'é¢„æµ‹ vs çœŸå®å€¼ (RÂ² = {metrics["r2"]:.3f})')
        ax1.grid(True, alpha=0.3)

        # 2. æ®‹å·®åˆ†å¸ƒ
        residuals = fold_result['residuals']
        ax2.hist(residuals, bins=30, alpha=0.7, color='orange', edgecolor='black')
        ax2.axvline(0, color='red', linestyle='--', linewidth=2)
        ax2.set_xlabel('æ®‹å·®')
        ax2.set_ylabel('é¢‘æ•°')
        ax2.set_title(f'æ®‹å·®åˆ†å¸ƒ (å‡å€¼ = {np.mean(residuals):.3f})')
        ax2.grid(True, alpha=0.3)

        # 3. æ®‹å·® vs é¢„æµ‹å€¼
        ax3.scatter(y_pred, residuals, alpha=0.6, color='green')
        ax3.axhline(0, color='red', linestyle='--', linewidth=2)
        ax3.set_xlabel('é¢„æµ‹å€¼')
        ax3.set_ylabel('æ®‹å·®')
        ax3.set_title('æ®‹å·® vs é¢„æµ‹å€¼')
        ax3.grid(True, alpha=0.3)

        # 4. è®­ç»ƒå†å²ï¼ˆå¦‚æœæœ‰ï¼‰
        ax4.axis('off')
        if fold_result['train_losses'] and fold_result['val_losses']:
            ax4.plot(fold_result['train_losses'], label='è®­ç»ƒæŸå¤±', alpha=0.7)
            ax4.plot(fold_result['val_losses'], label='éªŒè¯æŸå¤±', alpha=0.7)
            ax4.set_xlabel('Epoch')
            ax4.set_ylabel('æŸå¤±')
            ax4.set_title('è®­ç»ƒå†å²')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        else:
            info_text = f"""
                æ¨¡å‹æŒ‡æ ‡:
                - RÂ²: {metrics['r2']:.4f}
                - RMSE: {metrics['rmse']:.4f}
                - MAE: {metrics['mae']:.4f}
                - è§£é‡Šæ–¹å·®: {metrics['explained_variance']:.4f}
                - æ ·æœ¬æ•°: {fold_result['n_samples']}
                """
            if fold_result.get('fallback', False):
                info_text += "\nâš ï¸ ä½¿ç”¨å¤‡é€‰é¢„æµ‹æ–¹æ¡ˆ"

            ax4.text(0.1, 0.9, info_text, transform=ax4.transAxes, fontsize=11,
                     verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.7))

        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, f'year_{year}_detailed_analysis.png'),
                    dpi=300, bbox_inches='tight')
        plt.close()


def generate_annual_report(results):
    """ç”Ÿæˆå¹´åº¦éªŒè¯è¯¦ç»†æŠ¥å‘Š"""
    import numpy as np
    from datetime import datetime

    report = []
    report.append("=" * 60)
    report.append("          çº¯å‡€ç‰ˆGNNWRå¹´åº¦äº¤å‰éªŒè¯åˆ†ææŠ¥å‘Š")
    report.append("=" * 60)
    report.append("")

    # æ•°æ®ä¿¡æ¯
    data_info = results['data_info']
    report.append("ğŸ“Š æ•°æ®ä¿¡æ¯:")
    report.append(f"  æ€»æ ·æœ¬æ•°: {data_info['total_samples']}")
    report.append(f"  ç‰¹å¾ç»´åº¦: {data_info['n_features']}")
    report.append(f"  å¹´ä»½æ•°é‡: {data_info['n_years']}")
    report.append(f"  ç«™ç‚¹æ•°é‡: {data_info['n_stations']}")
    report.append(f"  è®¡ç®—è®¾å¤‡: {data_info['device']}")
    report.append(f"  æ··åˆç²¾åº¦: {data_info['mixed_precision']}")
    report.append("")

    # å¹´åº¦äº¤å‰éªŒè¯æ€»ä½“æ€§èƒ½
    yearly_cv = results['yearly_cv']
    overall_metrics = yearly_cv['overall_metrics']
    report.append("ğŸ“ˆ å¹´åº¦äº¤å‰éªŒè¯æ€»ä½“æ€§èƒ½:")
    report.append(f"  å¹³å‡ RÂ²: {overall_metrics['r2']:.4f}")
    report.append(f"  å¹³å‡ RMSE: {overall_metrics['rmse']:.4f}")
    report.append(f"  å¹³å‡ MAE: {overall_metrics['mae']:.4f}")
    report.append(f"  å¹³å‡è§£é‡Šæ–¹å·®: {overall_metrics['explained_variance']:.4f}")
    report.append("")

    # å„å¹´ä»½è¯¦ç»†æ€§èƒ½
    report.append("ğŸ“… å„å¹´ä»½è¯¦ç»†æ€§èƒ½:")
    report.append("-" * 60)
    report.append("å¹´ä»½       æ ·æœ¬æ•°     RÂ²        RMSE       MAE       è§£é‡Šæ–¹å·®")
    report.append("-" * 60)

    fold_metrics = yearly_cv['fold_metrics']
    if fold_metrics:
        for year in sorted(fold_metrics.keys()):
            metrics = fold_metrics[year]
            n_samples = metrics.get('n_samples', 0)
            report.append(
                f"{year:<12}{n_samples:<10}{metrics['r2']:.4f}    {metrics['rmse']:.4f}    {metrics['mae']:.4f}    {metrics['explained_variance']:.4f}")
    else:
        report.append("          æ— æœ‰æ•ˆæ•°æ®")

    report.append("")

    # æ€§èƒ½åˆ†æ
    report.append("ğŸ” æ€§èƒ½åˆ†æ:")
    if fold_metrics:
        best_year = max(fold_metrics.items(), key=lambda x: x[1]['r2'])
        worst_year = min(fold_metrics.items(), key=lambda x: x[1]['r2'])

        report.append(f"  æœ€ä½³å¹´ä»½: {best_year[0]} (RÂ² = {best_year[1]['r2']:.4f})")
        report.append(f"  æœ€å·®å¹´ä»½: {worst_year[0]} (RÂ² = {worst_year[1]['r2']:.4f})")

        # ç¨³å®šæ€§åˆ†æ
        r2_scores = [metrics['r2'] for metrics in fold_metrics.values()]
        r2_std = np.std(r2_scores)
        report.append(f"  RÂ²æ ‡å‡†å·®: {r2_std:.4f} (ç¨³å®šæ€§æŒ‡æ ‡)")

        if r2_std < 0.1:
            stability = "ä¼˜ç§€"
        elif r2_std < 0.15:
            stability = "è‰¯å¥½"
        elif r2_std < 0.2:
            stability = "ä¸€èˆ¬"
        else:
            stability = "è¾ƒå·®"

        report.append(f"  æ¨¡å‹ç¨³å®šæ€§: {stability}")
    else:
        report.append("  æ— æ³•è¿›è¡Œæ€§èƒ½åˆ†æ - æ— æœ‰æ•ˆæ•°æ®")

    report.append("")

    # æ®‹å·®åˆ†æ
    all_residuals = []
    fallback_count = 0

    for year_data in yearly_cv['fold_results']:
        all_residuals.extend(year_data['residuals'])
        if year_data.get('fallback', False):
            fallback_count += 1

    if all_residuals:
        residual_mean = np.mean(all_residuals)
        residual_std = np.std(all_residuals)
        report.append("ğŸ“Š æ®‹å·®åˆ†æ:")
        report.append(f"  æ®‹å·®å‡å€¼: {residual_mean:.4f} (æ¥è¿‘0è¡¨ç¤ºæ— å)")
        report.append(f"  æ®‹å·®æ ‡å‡†å·®: {residual_std:.4f}")

        if fallback_count > 0:
            report.append(f"  âš ï¸  {fallback_count}ä¸ªå¹´ä»½ä½¿ç”¨äº†å¤‡é€‰é¢„æµ‹æ–¹æ¡ˆ")
    else:
        report.append("ğŸ“Š æ®‹å·®åˆ†æ: æ— æœ‰æ•ˆæ•°æ®")

    report.append("")

    # å»ºè®®å’Œæ”¹è¿›æ–¹å‘
    report.append("ğŸ’¡ å»ºè®®å’Œæ”¹è¿›æ–¹å‘:")
    if fold_metrics:
        if overall_metrics['r2'] < 0.7:
            report.append("  â€¢ è€ƒè™‘å¢åŠ æ¨¡å‹å¤æ‚åº¦æˆ–ç‰¹å¾å·¥ç¨‹")
        if len(fold_metrics) > 1:
            r2_scores = [metrics['r2'] for metrics in fold_metrics.values()]
            r2_std = np.std(r2_scores)
            if r2_std > 0.15:
                report.append("  â€¢ æ¨¡å‹åœ¨ä¸åŒå¹´ä»½é—´ç¨³å®šæ€§æœ‰å¾…æå‡")
        if overall_metrics['rmse'] > 1.0:
            report.append("  â€¢ é¢„æµ‹è¯¯å·®è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®æˆ–æ­£åˆ™åŒ–")

        if fallback_count > 0:
            report.append("  â€¢ éƒ¨åˆ†å¹´ä»½è®­ç»ƒå¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è°ƒæ•´è¶…å‚æ•°")
    else:
        report.append("  â€¢ æ‰€æœ‰å¹´ä»½è®­ç»ƒå¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®é¢„å¤„ç†å’Œæ¨¡å‹é…ç½®")

    report.append("  â€¢ å¯ä»¥å°è¯•è°ƒæ•´å­¦ä¹ ç‡æˆ–ä¼˜åŒ–å™¨å‚æ•°")
    report.append("  â€¢ è€ƒè™‘ä½¿ç”¨æ›´å¤æ‚çš„ç©ºé—´æƒé‡æœºåˆ¶")
    report.append("")

    report.append("=" * 60)
    report.append("æŠ¥å‘Šç”Ÿæˆå®Œæˆ - " + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    report.append("=" * 60)

    return "\n".join(report)


def print_annual_report(results):
    """åœ¨æ§åˆ¶å°è¾“å‡ºå¹´åº¦éªŒè¯æŠ¥å‘Šæ‘˜è¦"""
    yearly_cv = results['yearly_cv']

    print("\n" + "=" * 70)
    print("             çº¯å‡€ç‰ˆGNNWRå¹´åº¦äº¤å‰éªŒè¯ç»“æœæ‘˜è¦")
    print("=" * 70)

    print(f"\nğŸ“Š æ•°æ®æ¦‚å†µ:")
    print(f"  æ€»æ ·æœ¬: {results['data_info']['total_samples']}")
    print(f"  å¹´ä»½æ•°: {results['data_info']['n_years']}")
    print(f"  ç«™ç‚¹æ•°: {results['data_info']['n_stations']}")

    if yearly_cv['fold_metrics']:
        print(f"\nğŸ“ˆ å¹´åº¦äº¤å‰éªŒè¯æ€§èƒ½:")
        print(f"  å¹³å‡ RÂ²: {yearly_cv['overall_metrics']['r2']:.4f}")
        print(f"  å¹³å‡ RMSE: {yearly_cv['overall_metrics']['rmse']:.4f}")
        print(f"  å¹³å‡ MAE: {yearly_cv['overall_metrics']['mae']:.4f}")

        # æ˜¾ç¤ºæœ€ä½³å’Œæœ€å·®å¹´ä»½
        fold_metrics = yearly_cv['fold_metrics']
        best_year = max(fold_metrics.items(), key=lambda x: x[1]['r2'])
        worst_year = min(fold_metrics.items(), key=lambda x: x[1]['r2'])

        print(f"\nâ­ æœ€ä½³å¹´ä»½: {best_year[0]} (RÂ² = {best_year[1]['r2']:.4f})")
        print(f"âš ï¸  æœ€å·®å¹´ä»½: {worst_year[0]} (RÂ² = {worst_year[1]['r2']:.4f})")

        # æ€§èƒ½ç¨³å®šæ€§
        r2_scores = [metrics['r2'] for metrics in fold_metrics.values()]
        r2_std = np.std(r2_scores)
        print(f"ğŸ“Š æ€§èƒ½ç¨³å®šæ€§: RÂ²æ ‡å‡†å·® = {r2_std:.4f}")

        # æ£€æŸ¥æ˜¯å¦æœ‰å¤‡é€‰æ–¹æ¡ˆ
        fallback_count = sum(1 for result in yearly_cv['fold_results'] if result.get('fallback', False))
        if fallback_count > 0:
            print(f"âš ï¸  è­¦å‘Š: {fallback_count}ä¸ªå¹´ä»½ä½¿ç”¨äº†å¤‡é€‰é¢„æµ‹æ–¹æ¡ˆ")

        print(f"\nğŸ’¡ å»ºè®®:")
        if yearly_cv['overall_metrics']['r2'] > 0.8:
            print("  æ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼Œå¯ä»¥è€ƒè™‘è¿›è¡Œç«™ç‚¹éªŒè¯")
        elif yearly_cv['overall_metrics']['r2'] > 0.6:
            print("  æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œå¯ä»¥å°è¯•ä¼˜åŒ–è¶…å‚æ•°")
        else:
            print("  æ¨¡å‹æ€§èƒ½æœ‰å¾…æå‡ï¼Œå»ºè®®æ£€æŸ¥ç‰¹å¾å·¥ç¨‹")
    else:
        print(f"\nâŒ å¹´åº¦äº¤å‰éªŒè¯å¤±è´¥:")
        print("  æ‰€æœ‰å¹´ä»½è®­ç»ƒå‡æœªæˆåŠŸå®Œæˆ")
        print("  å»ºè®®æ£€æŸ¥æ•°æ®é¢„å¤„ç†å’Œæ¨¡å‹é…ç½®")

    print("=" * 70)




def create_pure_gnnwr_visualizations_optimized(results, output_dir):
    """ç”Ÿæˆçº¯å‡€ç‰ˆGNNWRçš„ä¼˜åŒ–å¯è§†åŒ–å›¾è¡¨"""
    import matplotlib.pyplot as plt
    import seaborn as sns

    plt.figure(figsize=(20, 15))

    # 1. è®­ç»ƒä¿¡æ¯å±•ç¤º
    plt.subplot(3, 4, 1)
    training_info = results['training_info']
    info_text = f"è®¾å¤‡: {training_info['device']}\n"
    info_text += f"æ··åˆç²¾åº¦: {training_info['mixed_precision']}\n"
    info_text += f"æ¨¡å‹å‚æ•°: {training_info['model_parameters']:,}\n"
    if 'gpu_name' in training_info:
        info_text += f"GPU: {training_info['gpu_name']}\n"
        info_text += f"GPUå†…å­˜: {training_info['gpu_memory']}"

    plt.text(0.1, 0.5, info_text, fontsize=10, verticalalignment='center')
    plt.axis('off')
    plt.title('è®­ç»ƒé…ç½®ä¿¡æ¯', fontsize=12, fontweight='bold')

    # 2. ç«™ç‚¹äº¤å‰éªŒè¯æ•£ç‚¹å›¾
    plt.subplot(3, 4, 2)
    station_cv = results['station_cv']
    y_true_station = station_cv['true_values']
    y_pred_station = station_cv['predictions']

    plt.scatter(y_true_station, y_pred_station, alpha=0.6, s=20, color='blue')
    plt.plot([y_true_station.min(), y_true_station.max()],
             [y_true_station.min(), y_true_station.max()], 'r--', alpha=0.8)
    plt.xlabel('True SWE (mm)')
    plt.ylabel('Predicted SWE (mm)')
    plt.title(f'Station CV\nMAE={station_cv["overall"]["MAE"]:.2f}, R={station_cv["overall"]["R"]:.3f}')
    plt.grid(True, alpha=0.3)

    # 3. å¹´åº¦äº¤å‰éªŒè¯æ•£ç‚¹å›¾
    plt.subplot(3, 4, 3)
    yearly_cv = results['yearly_cv']
    y_true_yearly = yearly_cv['true_values']
    y_pred_yearly = yearly_cv['predictions']

    plt.scatter(y_true_yearly, y_pred_yearly, alpha=0.6, s=20, color='green')
    plt.plot([y_true_yearly.min(), y_true_yearly.max()],
             [y_true_yearly.min(), y_true_yearly.max()], 'r--', alpha=0.8)
    plt.xlabel('True SWE (mm)')
    plt.ylabel('Predicted SWE (mm)')
    plt.title(f'Yearly CV\nMAE={yearly_cv["overall"]["MAE"]:.2f}, R={yearly_cv["overall"]["R"]:.3f}')
    plt.grid(True, alpha=0.3)

    # 4. æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
    plt.subplot(3, 4, 4)
    methods = ['Station CV', 'Yearly CV', 'Standard Test']
    mae_values = [
        station_cv['overall']['MAE'],
        yearly_cv['overall']['MAE'],
        results['standard_test']['MAE']
    ]

    bars = plt.bar(methods, mae_values, color=['skyblue', 'lightgreen', 'lightcoral'])
    plt.ylabel('MAE (mm)')
    plt.title('Performance Comparison (MAE)')
    for bar, value in zip(bars, mae_values):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                 f'{value:.2f}', ha='center', va='bottom')

    # 5. Rå€¼å¯¹æ¯”æŸ±çŠ¶å›¾
    plt.subplot(3, 4, 5)
    r_values = [
        station_cv['overall']['R'],
        yearly_cv['overall']['R'],
        results['standard_test']['R']
    ]

    bars = plt.bar(methods, r_values, color=['skyblue', 'lightgreen', 'lightcoral'])
    plt.ylabel('R')
    plt.title('Performance Comparison (R)')
    for bar, value in zip(bars, r_values):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                 f'{value:.3f}', ha='center', va='bottom')

    # 6. æŠ˜å ç»Ÿè®¡
    plt.subplot(3, 4, 6)
    fold_stats = {
        'æ€»ç«™ç‚¹æŠ˜å ': station_cv['total_folds'],
        'æˆåŠŸç«™ç‚¹æŠ˜å ': station_cv['folds'],
        'æ€»å¹´åº¦æŠ˜å ': yearly_cv['total_folds'],
        'æˆåŠŸå¹´åº¦æŠ˜å ': yearly_cv['folds']
    }

    plt.bar(fold_stats.keys(), fold_stats.values(), color='lightgray')
    plt.xticks(rotation=45, ha='right')
    plt.ylabel('æ•°é‡')
    plt.title('äº¤å‰éªŒè¯æŠ˜å ç»Ÿè®¡')

    # 7. æ•°æ®æ¦‚å†µ
    plt.subplot(3, 4, 7)
    data_info = results['data_info']
    data_text = f"æ€»æ ·æœ¬: {data_info['total_samples']}\n"
    data_text += f"ç‰¹å¾æ•°: {data_info['n_features']}\n"
    data_text += f"ç«™ç‚¹æ•°: {data_info['n_stations']}\n"
    data_text += f"å¹´ä»½æ•°: {data_info['n_years']}\n"
    data_text += f"è®­ç»ƒé›†: {data_info['train_size']}\n"
    data_text += f"æµ‹è¯•é›†: {data_info['test_size']}"

    plt.text(0.1, 0.5, data_text, fontsize=10, verticalalignment='center')
    plt.axis('off')
    plt.title('æ•°æ®æ¦‚å†µ', fontsize=12, fontweight='bold')

    # 8. æ®‹å·®åˆ†å¸ƒ
    plt.subplot(3, 4, 8)
    residuals = y_true_station - y_pred_station
    plt.hist(residuals, bins=30, alpha=0.7, color='orange', density=True)
    plt.xlabel('æ®‹å·® (mm)')
    plt.ylabel('å¯†åº¦')
    plt.title('ç«™ç‚¹CVæ®‹å·®åˆ†å¸ƒ')

    # æ·»åŠ æ­£æ€åˆ†å¸ƒæ›²çº¿
    from scipy.stats import norm
    mu, std = norm.fit(residuals)
    x = np.linspace(residuals.min(), residuals.max(), 100)
    p = norm.pdf(x, mu, std)
    plt.plot(x, p, 'k', linewidth=2)

    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'pure_gnnwr_comprehensive_analysis_optimized.png'),
                dpi=300, bbox_inches='tight')
    plt.close()

    print(f"âœ… ä¼˜åŒ–ç‰ˆå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜")


def generate_detailed_report_optimized(results):
    """ç”Ÿæˆä¼˜åŒ–çš„è¯¦ç»†åˆ†ææŠ¥å‘Š"""
    report = []
    report.append("=" * 80)
    report.append("ğŸ¯ çº¯å‡€ç‰ˆGNNWRè¯¦ç»†åˆ†ææŠ¥å‘Š (GPUä¼˜åŒ–ç‰ˆ)")
    report.append("=" * 80)
    report.append("")

    # è®­ç»ƒé…ç½®ä¿¡æ¯
    training_info = results['training_info']
    report.append("âš™ï¸ è®­ç»ƒé…ç½®:")
    report.append(f"  è®¾å¤‡: {training_info['device']}")
    report.append(f"  æ··åˆç²¾åº¦: {'æ˜¯' if training_info['mixed_precision'] else 'å¦'}")
    report.append(f"  æ¨¡å‹å‚æ•°: {training_info['model_parameters']:,}")
    report.append(f"  CPUå·¥ä½œçº¿ç¨‹: {training_info.get('cpu_workers', 'N/A')}")
    if 'gpu_name' in training_info:
        report.append(f"  GPU: {training_info['gpu_name']}")
        report.append(f"  GPUå†…å­˜: {training_info['gpu_memory']}")
    report.append("")

    # æ•°æ®æ¦‚å†µ
    data_info = results['data_info']
    report.append("ğŸ“Š æ•°æ®æ¦‚å†µ:")
    report.append(f"  æ€»æ ·æœ¬æ•°: {data_info['total_samples']}")
    report.append(f"  ç‰¹å¾æ•°é‡: {data_info['n_features']}")
    report.append(f"  ç«™ç‚¹æ•°é‡: {data_info['n_stations']}")
    report.append(f"  å¹´ä»½æ•°é‡: {data_info['n_years']}")
    report.append(f"  è®­ç»ƒé›†å¤§å°: {data_info['train_size']}")
    report.append(f"  æµ‹è¯•é›†å¤§å°: {data_info['test_size']}")
    report.append("")

    # ç«™ç‚¹äº¤å‰éªŒè¯è¯¦ç»†ç»“æœ
    station_cv = results['station_cv']
    station_overall = station_cv['overall']
    report.append("ğŸ”ï¸ ç«™ç‚¹äº¤å‰éªŒè¯è¯¦ç»†ç»“æœ:")
    report.append(f"  æ€»æŠ˜å æ•°: {station_cv['total_folds']}")
    report.append(f"  æˆåŠŸæŠ˜å : {station_cv['folds']}")
    report.append(f"  è·³è¿‡æŠ˜å : {station_cv.get('skipped_folds', 0)}")
    report.append(f"  MAE: {station_overall['MAE']:.3f} mm")
    report.append(f"  RMSE: {station_overall['RMSE']:.3f} mm")
    report.append(f"  R: {station_overall['R']:.3f}")
    report.append(f"  RÂ²: {station_overall['R_squared']:.3f}")
    report.append(f"  æ ·æœ¬æ•°: {station_overall['samples']}")
    report.append("")

    # å¹´åº¦äº¤å‰éªŒè¯è¯¦ç»†ç»“æœ
    yearly_cv = results['yearly_cv']
    yearly_overall = yearly_cv['overall']
    report.append("ğŸ“… å¹´åº¦äº¤å‰éªŒè¯è¯¦ç»†ç»“æœ:")
    report.append(f"  æ€»æŠ˜å æ•°: {yearly_cv['total_folds']}")
    report.append(f"  æˆåŠŸæŠ˜å : {yearly_cv['folds']}")
    report.append(f"  è·³è¿‡æŠ˜å : {yearly_cv.get('skipped_folds', 0)}")
    report.append(f"  MAE: {yearly_overall['MAE']:.3f} mm")
    report.append(f"  RMSE: {yearly_overall['RMSE']:.3f} mm")
    report.append(f"  R: {yearly_overall['R']:.3f}")
    report.append(f"  RÂ²: {yearly_overall['R_squared']:.3f}")
    report.append(f"  æ ·æœ¬æ•°: {yearly_overall['samples']}")
    report.append("")

    # æ ‡å‡†æµ‹è¯•é›†ç»“æœ
    standard_test = results['standard_test']
    report.append("ğŸ§ª æ ‡å‡†æµ‹è¯•é›†ç»“æœ:")
    report.append(f"  MAE: {standard_test['MAE']:.3f} mm")
    report.append(f"  RMSE: {standard_test['RMSE']:.3f} mm")
    report.append(f"  R: {standard_test['R']:.3f}")
    report.append(f"  RÂ²: {standard_test['R_squared']:.3f}")
    report.append(f"  æ ·æœ¬æ•°: {standard_test['samples']}")
    report.append("")

    # æ€§èƒ½æ€»ç»“
    report.append("ğŸ“ˆ æ€§èƒ½æ€»ç»“:")
    best_mae = min(station_overall['MAE'], yearly_overall['MAE'], standard_test['MAE'])
    best_r = max(station_overall['R'], yearly_overall['R'], standard_test['R'])
    report.append(f"  æœ€ä½³MAE: {best_mae:.3f} mm")
    report.append(f"  æœ€ä½³Rå€¼: {best_r:.3f}")

    # è®¡ç®—æ€§èƒ½æå‡ï¼ˆå¦‚æœæœ‰åŸºçº¿ï¼‰
    if 'baseline' in results:
        baseline_mae = results['baseline']['MAE']
        improvement = (baseline_mae - best_mae) / baseline_mae * 100
        report.append(f"  ç›¸æ¯”åŸºçº¿æå‡: {improvement:.1f}%")
    report.append("")

    report.append("=" * 80)
    report.append("æŠ¥å‘Šç”Ÿæˆæ—¶é—´: " + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    report.append("=" * 80)

    return "\n".join(report)


def generate_detailed_report(results):
    """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
    report = []
    report.append("=" * 80)
    report.append("ğŸ¯ çº¯å‡€ç‰ˆGNNWRè¯¦ç»†åˆ†ææŠ¥å‘Š")
    report.append("=" * 80)
    report.append("")

    # æ•°æ®æ¦‚å†µ
    data_info = results['data_info']
    report.append("ğŸ“Š æ•°æ®æ¦‚å†µ:")
    report.append(f"  æ€»æ ·æœ¬æ•°: {data_info['total_samples']}")
    report.append(f"  ç‰¹å¾æ•°é‡: {data_info['n_features']}")
    report.append(f"  ç«™ç‚¹æ•°é‡: {data_info['n_stations']}")
    report.append(f"  å¹´ä»½æ•°é‡: {data_info['n_years']}")
    report.append(f"  è®­ç»ƒé›†å¤§å°: {data_info['train_size']}")
    report.append(f"  æµ‹è¯•é›†å¤§å°: {data_info['test_size']}")
    report.append("")

    # ç«™ç‚¹äº¤å‰éªŒè¯è¯¦ç»†ç»“æœ
    station_cv = results['station_cv']
    station_overall = station_cv['overall']
    report.append("ğŸ”ï¸ ç«™ç‚¹äº¤å‰éªŒè¯è¯¦ç»†ç»“æœ:")
    report.append(f"  æ€»æŠ˜å æ•°: {station_cv['total_folds']}")
    report.append(f"  æˆåŠŸæŠ˜å : {station_cv['folds']}")
    report.append(f"  è·³è¿‡æŠ˜å : {station_cv.get('skipped_folds', 0)}")
    report.append(f"  MAE: {station_overall['MAE']:.3f} mm")
    report.append(f"  RMSE: {station_overall['RMSE']:.3f} mm")
    report.append(f"  R: {station_overall['R']:.3f}")
    report.append(f"  RÂ²: {station_overall['R_squared']:.3f}")
    report.append(f"  æ ·æœ¬æ•°: {station_overall['samples']}")
    report.append("")

    # å¹´åº¦äº¤å‰éªŒè¯è¯¦ç»†ç»“æœ
    yearly_cv = results['yearly_cv']
    yearly_overall = yearly_cv['overall']
    report.append("ğŸ“… å¹´åº¦äº¤å‰éªŒè¯è¯¦ç»†ç»“æœ:")
    report.append(f"  æ€»æŠ˜å æ•°: {yearly_cv['total_folds']}")
    report.append(f"  æˆåŠŸæŠ˜å : {yearly_cv['folds']}")
    report.append(f"  è·³è¿‡æŠ˜å : {yearly_cv.get('skipped_folds', 0)}")
    report.append(f"  MAE: {yearly_overall['MAE']:.3f} mm")
    report.append(f"  RMSE: {yearly_overall['RMSE']:.3f} mm")
    report.append(f"  R: {yearly_overall['R']:.3f}")
    report.append(f"  RÂ²: {yearly_overall['R_squared']:.3f}")
    report.append(f"  æ ·æœ¬æ•°: {yearly_overall['samples']}")
    report.append("")

    # æ ‡å‡†æµ‹è¯•é›†ç»“æœ
    standard_test = results['standard_test']
    report.append("ğŸ§ª æ ‡å‡†æµ‹è¯•é›†ç»“æœ:")
    report.append(f"  MAE: {standard_test['MAE']:.3f} mm")
    report.append(f"  RMSE: {standard_test['RMSE']:.3f} mm")
    report.append(f"  R: {standard_test['R']:.3f}")
    report.append(f"  RÂ²: {standard_test['R_squared']:.3f}")
    report.append(f"  æ ·æœ¬æ•°: {standard_test['samples']}")
    report.append("")

    # æ€§èƒ½æ€»ç»“
    report.append("ğŸ“ˆ æ€§èƒ½æ€»ç»“:")
    best_mae = min(station_overall['MAE'], yearly_overall['MAE'], standard_test['MAE'])
    best_r = max(station_overall['R'], yearly_overall['R'], standard_test['R'])
    report.append(f"  æœ€ä½³MAE: {best_mae:.3f} mm")
    report.append(f"  æœ€ä½³Rå€¼: {best_r:.3f}")
    report.append("")

    report.append("=" * 80)
    report.append("æŠ¥å‘Šç”Ÿæˆæ—¶é—´: " + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    report.append("=" * 80)

    return "\n".join(report)


def pure_gnnwr_cross_validate_fixed(X, y, groups, coords, cv_type, logger,
                                    device='auto', mixed_precision=True, cpu_workers=24):
    """
    ä¿®å¤ç‰ˆçš„çº¯å‡€GNNWRäº¤å‰éªŒè¯å‡½æ•°
    """
    import numpy as np
    from sklearn.model_selection import GroupKFold
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

    # æ ¹æ®äº¤å‰éªŒè¯ç±»å‹è®¾ç½®åˆ†ç»„
    if cv_type == 'yearly':
        unique_groups = np.unique(groups)
        n_splits = len(unique_groups)
        group_kfold = GroupKFold(n_splits=n_splits)
    else:
        n_splits = 5
        group_kfold = GroupKFold(n_splits=n_splits)

    fold_results = []
    fold_metrics = {}

    logger.info(f"å¼€å§‹{cv_type}äº¤å‰éªŒè¯ï¼Œå…±{n_splits}æŠ˜")

    for fold_idx, (train_idx, test_idx) in enumerate(group_kfold.split(X, y, groups)):
        try:
            # è·å–å½“å‰æµ‹è¯•ç»„çš„æ ‡è¯†
            test_group = np.unique(groups[test_idx])[0]
            logger.info(f"è®­ç»ƒç¬¬{fold_idx + 1}/{n_splits}æŠ˜ï¼Œæµ‹è¯•ç»„: {test_group}")

            # æ•°æ®åˆ†å‰²
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]
            coords_train, coords_test = coords[train_idx], coords[test_idx]

            # ç‰¹å¾æ ‡å‡†åŒ– - ç¡®ä¿æ­£ç¡®æ‹Ÿåˆ
            scaler_X = StandardScaler()
            X_train_scaled = scaler_X.fit_transform(X_train)
            X_test_scaled = scaler_X.transform(X_test)

            scaler_y = StandardScaler()
            y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()

            # è®­ç»ƒæ¨¡å‹
            model = PureGNNWRTrainer(
                input_dim=X_train_scaled.shape[1],
                coords=coords_train,  # æ·»åŠ åæ ‡æ•°æ®
                hidden_dims=[512, 256, 128, 64],
                dropout_rate=0.3,
                learning_rate=0.001,
                device=device,
                mixed_precision=mixed_precision
            )

            # è®­ç»ƒæ¨¡å‹
            train_losses, val_losses = model.fit(
                X_train_scaled, y_train_scaled, coords_train
            )

            # é¢„æµ‹ - ä½¿ç”¨è®­ç»ƒæ—¶çš„æ ‡å‡†åŒ–å™¨
            y_pred_scaled = model.predict(X_test_scaled, coords_test)

            # åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
            y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()

            # è®¡ç®—æŒ‡æ ‡
            r2 = r2_score(y_test, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            mae = mean_absolute_error(y_test, y_pred)
            explained_variance = 1 - np.var(y_test - y_pred) / np.var(y_test)

            # å­˜å‚¨ç»“æœ
            fold_result = {
                'fold': fold_idx,
                'test_group': test_group,
                'y_true': y_test,
                'y_pred': y_pred,
                'residuals': y_test - y_pred,
                'n_samples': len(y_test),
                'metrics': {
                    'r2': r2,
                    'rmse': rmse,
                    'mae': mae,
                    'explained_variance': explained_variance
                },
                'train_losses': train_losses,
                'val_losses': val_losses
            }

            fold_results.append(fold_result)
            fold_metrics[test_group] = fold_result['metrics']

            logger.info(f"  ç¬¬{fold_idx + 1}æŠ˜å®Œæˆ - RÂ²: {r2:.4f}, RMSE: {rmse:.4f}")

        except Exception as e:
            logger.error(f"ç¬¬{fold_idx + 1}æŠ˜è®­ç»ƒå¤±è´¥: {e}")
            # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨ç®€å•å‡å€¼ä½œä¸ºé¢„æµ‹
            y_pred_fallback = np.full_like(y_test, np.mean(y_train))

            r2 = r2_score(y_test, y_pred_fallback)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred_fallback))
            mae = mean_absolute_error(y_test, y_pred_fallback)
            explained_variance = 1 - np.var(y_test - y_pred_fallback) / np.var(y_test)

            fallback_result = {
                'fold': fold_idx,
                'test_group': test_group,
                'y_true': y_test,
                'y_pred': y_pred_fallback,
                'residuals': y_test - y_pred_fallback,
                'n_samples': len(y_test),
                'metrics': {
                    'r2': r2,
                    'rmse': rmse,
                    'mae': mae,
                    'explained_variance': explained_variance
                },
                'train_losses': [],
                'val_losses': [],
                'fallback': True
            }

            fold_results.append(fallback_result)
            fold_metrics[test_group] = fallback_result['metrics']
            logger.warning(f"  ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆå®Œæˆç¬¬{fold_idx + 1}æŠ˜")

    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    all_y_true = np.concatenate([result['y_true'] for result in fold_results])
    all_y_pred = np.concatenate([result['y_pred'] for result in fold_results])

    overall_metrics = {
        'r2': r2_score(all_y_true, all_y_pred),
        'rmse': np.sqrt(mean_squared_error(all_y_true, all_y_pred)),
        'mae': mean_absolute_error(all_y_true, all_y_pred),
        'explained_variance': 1 - np.var(all_y_true - all_y_pred) / np.var(all_y_true)
    }

    logger.info(f"{cv_type}äº¤å‰éªŒè¯å®Œæˆ - æ€»ä½“RÂ²: {overall_metrics['r2']:.4f}")

    return {
        'fold_results': fold_results,
        'fold_metrics': fold_metrics,
        'overall_metrics': overall_metrics
    }


def _is_constant_data(data, axis=None):
    """æ£€æŸ¥æ•°æ®æ˜¯å¦æ’å®šï¼ˆæ‰€æœ‰å€¼ç›¸åŒï¼‰"""
    if data is None or len(data) == 0:
        return True

    if axis is not None:
        # å¯¹äºå¤šç»´æ•°æ®ï¼Œæ£€æŸ¥æ¯ä¸ªç‰¹å¾æ˜¯å¦æ’å®š
        return np.all(np.std(data, axis=axis) == 0)
    else:
        # å¯¹äºä¸€ç»´æ•°æ®
        return np.std(data) == 0


def evaluate_predictions(y_true, y_pred):
    """è¯„ä¼°é¢„æµ‹æ€§èƒ½ - ä¿®å¤å¸¸æ•°è¾“å…¥é—®é¢˜"""
    from sklearn.metrics import mean_absolute_error, mean_squared_error
    from scipy.stats import pearsonr
    import numpy as np
    import warnings

    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))

    # æ£€æŸ¥æ•°æ®æ˜¯å¦æ’å®š
    y_true_std = np.std(y_true)
    y_pred_std = np.std(y_pred)

    # å¦‚æœä»»ä¸€æ•°ç»„æ˜¯å¸¸æ•°ï¼Œç›¸å…³ç³»æ•°è®¾ä¸º0
    if y_true_std == 0 or y_pred_std == 0:
        r_value = 0.0
        warnings.warn(f"æ£€æµ‹åˆ°å¸¸æ•°è¾“å…¥: y_true_std={y_true_std:.6f}, y_pred_std={y_pred_std:.6f}ï¼Œç›¸å…³ç³»æ•°è®¾ä¸º0")
    else:
        try:
            r_value, _ = pearsonr(y_true, y_pred)
        except:
            r_value = 0.0

    return {
        'MAE': mae,
        'RMSE': rmse,
        'R': r_value,
        'R_squared': r_value ** 2,
        'samples': len(y_true),
        'y_true_std': y_true_std,
        'y_pred_std': y_pred_std
    }


def print_comprehensive_report_optimized(results):
    """æ‰“å°ä¼˜åŒ–çš„ç»¼åˆæŠ¥å‘Š"""
    print("\n" + "=" * 70)
    print("ğŸ¯ çº¯å‡€ç‰ˆGNNWRå®Œæ•´åˆ†ææŠ¥å‘Š (GPUä¼˜åŒ–ç‰ˆ)")
    print("=" * 70)

    # è®­ç»ƒé…ç½®
    training_info = results['training_info']
    print(f"\nâš™ï¸ è®­ç»ƒé…ç½®:")
    print(f"  è®¾å¤‡: {training_info['device']}")
    print(f"  æ··åˆç²¾åº¦: {'æ˜¯' if training_info['mixed_precision'] else 'å¦'}")
    if 'gpu_name' in training_info:
        print(f"  GPU: {training_info['gpu_name']}")

    # æ•°æ®æ¦‚å†µ
    data_info = results['data_info']
    print(f"\nğŸ“Š æ•°æ®æ¦‚å†µ:")
    print(f"  æ€»æ ·æœ¬æ•°: {data_info['total_samples']}")
    print(f"  ç‰¹å¾æ•°é‡: {data_info['n_features']}")
    print(f"  ç«™ç‚¹æ•°é‡: {data_info['n_stations']}")
    print(f"  å¹´ä»½æ•°é‡: {data_info['n_years']}")

    # ç«™ç‚¹äº¤å‰éªŒè¯ç»“æœ
    station_cv = results['station_cv']['overall']
    print(f"\nğŸ”ï¸ ç«™ç‚¹äº¤å‰éªŒè¯:")
    print(f"  æŠ˜å æ•°é‡: {results['station_cv']['folds']}")
    print(f"  MAE: {station_cv['MAE']:.3f} mm")
    print(f"  RMSE: {station_cv['RMSE']:.3f} mm")
    print(f"  R: {station_cv['R']:.3f}")
    print(f"  RÂ²: {station_cv['R_squared']:.3f}")

    # å¹´åº¦äº¤å‰éªŒè¯ç»“æœ
    yearly_cv = results['yearly_cv']['overall']
    print(f"\nğŸ“… å¹´åº¦äº¤å‰éªŒè¯:")
    print(f"  æŠ˜å æ•°é‡: {results['yearly_cv']['folds']}")
    print(f"  MAE: {yearly_cv['MAE']:.3f} mm")
    print(f"  RMSE: {yearly_cv['RMSE']:.3f} mm")
    print(f"  R: {yearly_cv['R']:.3f}")
    print(f"  RÂ²: {yearly_cv['R_squared']:.3f}")

    # æ ‡å‡†æµ‹è¯•é›†ç»“æœ
    standard_test = results['standard_test']
    print(f"\nğŸ§ª æ ‡å‡†æµ‹è¯•é›†:")
    print(f"  æ ·æœ¬æ•°é‡: {standard_test['samples']}")
    print(f"  MAE: {standard_test['MAE']:.3f} mm")
    print(f"  RMSE: {standard_test['RMSE']:.3f} mm")
    print(f"  R: {standard_test['R']:.3f}")
    print(f"  RÂ²: {standard_test['R_squared']:.3f}")

    print("=" * 70)



# åœ¨SWEClusterEnsembleç±»ä¸­æ·»åŠ ä¸€ä¸ªä¾¿æ·æ–¹æ³•
# def SWEClusterEnsemble_run_pure_comparison_optimized(self, df, device='auto', mixed_precision=True, cpu_workers=24):
#     """
#     åœ¨SWEClusterEnsembleç±»ä¸­æ·»åŠ çš„æ–¹æ³•
#     ç”¨äºå¿«é€Ÿè¿è¡Œçº¯å‡€ç‰ˆå¯¹æ¯”å®éªŒ - ä¼˜åŒ–ç‰ˆæœ¬
#     """
#     return train_pure_gnnwr_analysis(
#         df,
#         device=device,
#         mixed_precision=mixed_precision,
#         cpu_workers=cpu_workers
#     )





if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("æµ‹è¯•èšç±»é›†æˆæ¨¡å‹...")


    # ç›´æ¥è¿è¡Œçº¯å‡€ç‰ˆå¯¹æ¯”ï¼ˆéœ€è¦å…ˆæœ‰æ•°æ®æ–‡ä»¶ï¼‰
    try:
        import pandas as pd
        df = pd.read_excel("lu_onehot.xlsx")  # æ›¿æ¢ä¸ºæ‚¨çš„æ•°æ®æ–‡ä»¶
        results, trainer =  train_pure_gnnwr_annual_only(df)
    except Exception as e:
        print(f"ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        print("è¯·ç¡®ä¿æœ‰æ•°æ®æ–‡ä»¶å¹¶ä¿®æ”¹æ–‡ä»¶è·¯å¾„")
    print("æµ‹è¯•å®Œæˆï¼")