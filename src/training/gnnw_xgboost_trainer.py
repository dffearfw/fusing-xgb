import logging
import pandas as pd
import numpy as np
import xgboost as xgb
from matplotlib import pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
from scipy.stats import pearsonr
from sklearn.model_selection import LeaveOneGroupOut
import os
import joblib
import json
from datetime import datetime
import seaborn as sns
from scipy import stats

# æ–°å¢å¯¼å…¥
import torch
import torch.nn as nn
from gnnwr import models, datasets
import warnings

warnings.filterwarnings('ignore')

logger = logging.getLogger("GNNW_XGBoostTrainer")


class GNNW_XGBoostTrainer:
    """GNNW-XGBoostè®­ç»ƒå™¨ - é›†æˆGNNWRæƒé‡çŸ©é˜µä¸XGBoost"""

    # é»˜è®¤XGBoostå‚æ•°
    DEFAULT_PARAMS = {
        'n_estimators': 60,
        'learning_rate': 0.17,
        'max_depth': 5,
        'min_child_weight': 5,
        'gamma': 0,
        'subsample': 0.8,
        'colsample_bytree': 0.5,
        'reg_alpha': 0.05,
        'random_state': 42,
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse'
    }

    # GNNWRå‚æ•°
    DEFAULT_GNNWR_PARAMS = {
        'dense_layers': [1024, 512, 256],
        'activate_func': nn.PReLU(init=0.4),
        'start_lr': 0.1,
        'optimizer': "Adadelta",
        'max_epoch': 3000,  # äº¤å‰éªŒè¯ä¸­å‡å°‘è®­ç»ƒè½®æ•°
        'early_stop': 1000,
        'print_frequency': 100
    }

    def __init__(self, params=None, gnnwr_params=None, use_gnnwr=True):
        """åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            params (dict, optional): XGBoostå‚æ•°
            gnnwr_params (dict, optional): GNNWRå‚æ•°
            use_gnnwr (bool): æ˜¯å¦ä½¿ç”¨GNNWRæƒé‡å¢å¼º
        """
        self.logger = logger
        self.model = None
        self.feature_columns = None
        self.target_column = 'swe'
        self.use_gnnwr = use_gnnwr

        # å®šä¹‰GNNWRç‰¹å¾åˆ—ï¼ˆä¸åŸå§‹GNNWRè®­ç»ƒä¿æŒä¸€è‡´ï¼‰
        self.gnnwr_x_columns = ['aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation',
                                'std_slope',
                                'std_eastness', 'std_tpi', 'std_curvature1', 'std_curvature2', 'std_high', 'std_aspect',
                                'glsnow', 'cswe', 'snow_depth_snow_depth', 'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'doy',
                                'gldas',
                                'year', 'month', 'scp_start', 'scp_end', 'd1', 'd2', 'X', 'Y', 'Z','da', 'db', 'dc',
                                'dd']
        self.gnnwr_y_column = ['swe']
        self.gnnwr_spatial_columns = ['X', 'Y', 'Z']

        # æ›´æ–°å‚æ•°
        self.params = self.DEFAULT_PARAMS.copy()
        if params:
            self.params.update(params)

        self.gnnwr_params = self.DEFAULT_GNNWR_PARAMS.copy()
        if gnnwr_params:
            self.gnnwr_params.update(gnnwr_params)

        self.logger.info(f"åˆå§‹åŒ–GNNW-XGBoostè®­ç»ƒå™¨")
        self.logger.info(f"XGBoostå‚æ•°: {self.params}")
        self.logger.info(f"ä½¿ç”¨GNNWRæƒé‡å¢å¼º: {self.use_gnnwr}")

    def preprocess_data(self, df, for_gnnwr=False):
        """æ•°æ®é¢„å¤„ç†

        Args:
            df (pd.DataFrame): åŸå§‹æ•°æ®
            for_gnnwr (bool): æ˜¯å¦ä¸ºGNNWRå¤„ç†æ•°æ®

        Returns:
            tuple: å¤„ç†åçš„ç‰¹å¾çŸ©é˜µã€ç›®æ ‡å‘é‡ã€åˆ†ç»„ä¿¡æ¯
        """
        self.logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")

        # åˆ›å»ºæ•°æ®å‰¯æœ¬
        df_clean = df.copy()

        # éªŒè¯å¿…è¦åˆ—
        required_columns = ['station_id', 'date', self.target_column]
        missing_columns = [col for col in required_columns if col not in df_clean.columns]
        if missing_columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")

        # ç¡®ä¿GNNWRéœ€è¦çš„åˆ—éƒ½å­˜åœ¨
        if self.use_gnnwr:
            gnnwr_required = self.gnnwr_x_columns + self.gnnwr_spatial_columns
            missing_gnnwr = [col for col in gnnwr_required if col not in df_clean.columns]
            if missing_gnnwr:
                self.logger.warning(f"GNNWRç¼ºå°‘ä»¥ä¸‹åˆ—: {missing_gnnwr}")
                # å°è¯•å¡«å……ç¼ºå¤±åˆ—ä¸º0
                for col in missing_gnnwr:
                    df_clean[col] = 0.0

        # å¤„ç†CSWEæ— æ•ˆå€¼
        if 'cswe' in df_clean.columns:
            cswe_invalid_mask = df_clean['cswe'] > 200
            if cswe_invalid_mask.sum() > 0:
                df_clean.loc[cswe_invalid_mask, 'cswe'] = np.nan

        # ç¡®å®šç‰¹å¾åˆ—
        exclude_columns = ['station_id', 'date', self.target_column, 'hydrological_doy']
        exclude_columns.extend([col for col in df_clean.columns if col.startswith('landuse_hash_')])

        # ä¿ç•™GNNWRç‰¹å¾åˆ—ç”¨äºåŠ æƒ
        if self.use_gnnwr:
            # ç¡®ä¿GNNWRç‰¹å¾åˆ—åœ¨ç‰¹å¾åˆ—ä¸­
            for col in self.gnnwr_x_columns:
                if col not in exclude_columns and col not in df_clean.columns:
                    df_clean[col] = 0.0

        self.feature_columns = [col for col in df_clean.columns if col not in exclude_columns]

        if not self.feature_columns:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç‰¹å¾åˆ—")

        # å‡†å¤‡æ•°æ®
        X = df_clean[self.feature_columns].values
        y = df_clean[self.target_column].values

        # åˆ†ç»„ä¿¡æ¯
        df_clean['year'] = pd.to_datetime(df_clean['date']).dt.year
        station_groups = df_clean['station_id'].values
        year_groups = df_clean['year'].values

        # ä¸ºGNNWRå‡†å¤‡æ•°æ®
        gnnwr_data = None
        if self.use_gnnwr:
            gnnwr_data = df_clean.copy()
            # ç¡®ä¿æ‰€æœ‰GNNWRéœ€è¦çš„åˆ—éƒ½å­˜åœ¨
            for col in self.gnnwr_x_columns + self.gnnwr_spatial_columns:
                if col not in gnnwr_data.columns:
                    gnnwr_data[col] = 0.0

        self.logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        self.logger.info(f"  æ ·æœ¬æ•°: {len(X)}, ç‰¹å¾æ•°: {len(self.feature_columns)}")

        return X, y, station_groups, year_groups, gnnwr_data

    def _train_gnnwr_for_fold(self, train_data, val_data):
        """ä¸ºå•ä¸ªæŠ˜å è®­ç»ƒGNNWRæ¨¡å‹å¹¶æå–æƒé‡

        Args:
            train_data (pd.DataFrame): è®­ç»ƒæ•°æ®
            val_data (pd.DataFrame): éªŒè¯æ•°æ®

        Returns:
            tuple: (è®­ç»ƒé›†æƒé‡çŸ©é˜µ, éªŒè¯é›†æƒé‡çŸ©é˜µ)
        """
        self.logger.debug("ä¸ºå½“å‰æŠ˜å è®­ç»ƒGNNWRæ¨¡å‹...")

        try:
            # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„åˆ—éƒ½å­˜åœ¨
            for col in self.gnnwr_x_columns + self.gnnwr_spatial_columns + self.gnnwr_y_column:
                if col not in train_data.columns:
                    train_data[col] = 0.0
                if col not in val_data.columns:
                    val_data[col] = 0.0

            # åˆå§‹åŒ–GNNWRæ•°æ®é›†
            train_set, val_set, _ = datasets.init_dataset_split(
                train_data=train_data,
                val_data=val_data,
                test_data=val_data.head(1),  # æµ‹è¯•é›†ç”¨éªŒè¯é›†å¤´1è¡Œå ä½
                x_column=self.gnnwr_x_columns,
                y_column=self.gnnwr_y_column,
                spatial_column=self.gnnwr_spatial_columns,
                batch_size=128,
                shuffle=False,
                use_model="gnnwr"
            )

            # è®­ç»ƒGNNWRæ¨¡å‹
            gnnwr = models.GNNWR(
                train_dataset=train_set,
                valid_dataset=val_set,
                test_dataset=train_set,  # ä½¿ç”¨è®­ç»ƒé›†ä½œä¸ºæµ‹è¯•é›†å ä½
                dense_layers=self.gnnwr_params['dense_layers'],
                activate_func=self.gnnwr_params['activate_func'],
                start_lr=self.gnnwr_params['start_lr'],
                optimizer=self.gnnwr_params['optimizer'],
                model_name=f"GNNWR_Fold",
                model_save_path="result/gnnwr_models_temp",
                log_path="result/gnnwr_logs_temp",
                write_path="result/gnnwr_runs_temp"
            )

            # ç®€çŸ­è®­ç»ƒ
            gnnwr.run(
                max_epoch=self.gnnwr_params['max_epoch'],
                early_stop=self.gnnwr_params['early_stop'],
                print_frequency=self.gnnwr_params['print_frequency']
            )

            # æå–æƒé‡çŸ©é˜µ
            def extract_weights(gnnwr_instance, dataset):
                model = gnnwr_instance._model
                model.eval()
                device = gnnwr_instance._device

                all_weights = []
                with torch.no_grad():
                    for batch in dataset.dataloader:
                        if len(batch) >= 2:
                            distances, features = batch[:2]
                            distances = distances.to(device)
                            weights = model(distances)
                            all_weights.append(weights.cpu().numpy())

                if all_weights:
                    return np.concatenate(all_weights, axis=0)
                return None

            train_weights = extract_weights(gnnwr, train_set)
            val_weights = extract_weights(gnnwr, val_set)

            if train_weights is not None and val_weights is not None:
                self.logger.debug(f"  æå–åˆ°æƒé‡çŸ©é˜µ: è®­ç»ƒé›†{train_weights.shape}, éªŒè¯é›†{val_weights.shape}")
                return train_weights, val_weights
            else:
                self.logger.warning("  æœªèƒ½æå–åˆ°æƒé‡çŸ©é˜µ")
                return None, None

        except Exception as e:
            self.logger.warning(f"  GNNWRè®­ç»ƒå¤±è´¥: {str(e)}")
            return None, None

    def _apply_gnnwr_weights(self, X, weights, feature_columns, gnnwr_x_columns):
        """åº”ç”¨GNNWRæƒé‡åˆ°ç‰¹å¾çŸ©é˜µ"""
        if weights is None:
            return X

        # ç¡®ä¿æƒé‡çŸ©é˜µå½¢çŠ¶åŒ¹é…
        if weights.shape[1] != len(gnnwr_x_columns):
            self.logger.warning(f"æƒé‡çŸ©é˜µç‰¹å¾æ•°({weights.shape[1]})ä¸GNNWRç‰¹å¾æ•°({len(gnnwr_x_columns)})ä¸åŒ¹é…")
            return X

        # åˆ›å»ºç‰¹å¾æ˜ å°„ï¼šç‰¹å¾åˆ—åˆ°GNNWRç‰¹å¾åˆ—çš„ç´¢å¼•
        feature_to_gnnwr = {}
        for i, feat in enumerate(feature_columns):
            if feat in gnnwr_x_columns:
                feature_to_gnnwr[i] = gnnwr_x_columns.index(feat)

        # ğŸ” éªŒè¯ï¼šæ‰“å°åº”ç”¨æƒé‡å‰çš„ä¿¡æ¯
        print("\n" + "=" * 80)
        print("ğŸ” GNNWRæƒé‡åº”ç”¨éªŒè¯")
        print("=" * 80)

        # 1. æ‰“å°æƒé‡ç»Ÿè®¡
        print(f"æƒé‡çŸ©é˜µå½¢çŠ¶: {weights.shape}")
        print(f"æƒé‡ç»Ÿè®¡:")
        print(f"  å‡å€¼: {weights.mean():.6f}")
        print(f"  æ ‡å‡†å·®: {weights.std():.6f}")
        print(f"  æœ€å°å€¼: {weights.min():.6f}")
        print(f"  æœ€å¤§å€¼: {weights.max():.6f}")
        print(f"  ä¸­ä½æ•°: {np.median(weights):.6f}")

        # 2. æ£€æŸ¥æƒé‡ä¸1çš„è·ç¦»
        distance_from_one = np.abs(weights - 1).mean()
        print(f"æƒé‡ä¸1çš„å¹³å‡è·ç¦»: {distance_from_one:.6f}")

        # 3. ç»Ÿè®¡æ¥è¿‘1çš„æƒé‡æ¯”ä¾‹
        close_to_one = np.sum(np.abs(weights - 1) < 0.01) / weights.size
        print(f"ä¸1å·®å¼‚å°äº0.01çš„æƒé‡æ¯”ä¾‹: {close_to_one:.2%}")

        # 4. æ‰“å°åŒ¹é…çš„ç‰¹å¾ä¿¡æ¯
        print(f"\nç‰¹å¾åŒ¹é…æƒ…å†µ:")
        print(f"  æ€»ç‰¹å¾æ•°: {len(feature_columns)}")
        print(f"  GNNWRç‰¹å¾æ•°: {len(gnnwr_x_columns)}")
        print(f"  åŒ¹é…çš„ç‰¹å¾æ•°: {len(feature_to_gnnwr)}")

        if len(feature_to_gnnwr) > 0:
            matched_features = [feature_columns[idx] for idx in list(feature_to_gnnwr.keys())[:5]]
            print(f"  å‰5ä¸ªåŒ¹é…ç‰¹å¾: {matched_features}")

        # 5. æ£€æŸ¥å‡ ä¸ªå…³é”®ç‰¹å¾çš„å˜åŒ–
        key_features = ['elevation', 'X', 'Y', 'Z', 'slope', 'doy']
        print(f"\nå…³é”®ç‰¹å¾éªŒè¯ (å‰3ä¸ªæ ·æœ¬):")

        for feat in key_features:
            if feat in feature_columns and feat in gnnwr_x_columns:
                feat_idx = feature_columns.index(feat)
                gnnwr_idx = gnnwr_x_columns.index(feat)

                # è·å–å‰3ä¸ªæ ·æœ¬
                print(f"\n{feat}:")
                for i in range(min(3, X.shape[0])):
                    original = X[i, feat_idx]
                    weight = weights[i, gnnwr_idx]
                    weighted = original * weight
                    change = weighted - original
                    rel_change = change / (abs(original) + 1e-10) * 100

                    print(f"  æ ·æœ¬{i}: {original:.4f} Ã— {weight:.4f} = {weighted:.4f} "
                          f"(å˜åŒ–: {change:+.4f}, ç›¸å¯¹: {rel_change:+.2f}%)")

        # ä¿å­˜åŸå§‹Xç”¨äºæ¯”è¾ƒ
        X_original = X.copy()

        # åº”ç”¨æƒé‡ï¼ˆåªå¯¹åŒ¹é…çš„ç‰¹å¾è¿›è¡ŒåŠ æƒï¼‰
        X_weighted = X.copy()
        for feat_idx, gnnwr_idx in feature_to_gnnwr.items():
            X_weighted[:, feat_idx] = X[:, feat_idx] * weights[:, gnnwr_idx]

        # ğŸ” éªŒè¯ï¼šæ‰“å°åº”ç”¨æƒé‡åçš„ä¿¡æ¯
        print(f"\n" + "=" * 60)
        print("æƒé‡åº”ç”¨ç»“æœç»Ÿè®¡")
        print("=" * 60)

        # 6. è®¡ç®—æ€»ä½“å˜åŒ–
        changes = X_weighted - X_original
        abs_changes = np.abs(changes)

        print(f"æ€»ä½“å˜åŒ–ç»Ÿè®¡:")
        print(f"  æœ€å¤§ç»å¯¹å˜åŒ–: {abs_changes.max():.6f}")
        print(f"  å¹³å‡ç»å¯¹å˜åŒ–: {abs_changes.mean():.6f}")
        print(f"  å˜åŒ– > 0.001 çš„æ¯”ä¾‹: {(abs_changes > 0.001).sum() / abs_changes.size:.2%}")

        # 7. æŒ‰ç‰¹å¾ç»Ÿè®¡å˜åŒ–
        print(f"\næŒ‰ç‰¹å¾å˜åŒ–ç»Ÿè®¡ (å‰5ä¸ªåŒ¹é…ç‰¹å¾):")

        if len(feature_to_gnnwr) > 0:
            # è·å–å‰5ä¸ªåŒ¹é…ç‰¹å¾
            feat_indices = list(feature_to_gnnwr.keys())[:5]

            for feat_idx in feat_indices:
                feat_name = feature_columns[feat_idx]
                feat_changes = changes[:, feat_idx]
                feat_abs_changes = abs_changes[:, feat_idx]

                print(f"\n{feat_name}:")
                print(f"  å¹³å‡å˜åŒ–: {feat_changes.mean():.6f}")
                print(f"  å¹³å‡ç»å¯¹å˜åŒ–: {feat_abs_changes.mean():.6f}")
                print(f"  æœ€å¤§å˜åŒ–: {feat_changes.max():.6f}")
                print(f"  æœ€å°å˜åŒ–: {feat_changes.min():.6f}")

                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å˜åŒ–éƒ½æ¥è¿‘0
                if feat_abs_changes.mean() < 0.0001:
                    print(f"  âš ï¸ è­¦å‘Š: è¯¥ç‰¹å¾å‡ ä¹æ²¡æœ‰å˜åŒ–ï¼")

        # 8. éªŒè¯æ˜¯å¦çœŸçš„æ”¹å˜äº†
        if np.allclose(X_weighted, X_original, atol=1e-10):
            print(f"\nâŒ ä¸¥é‡è­¦å‘Š: åŠ æƒåç‰¹å¾ä¸åŸå§‹ç‰¹å¾å‡ ä¹å®Œå…¨ç›¸åŒï¼")
            print(f"  æœ€å¤§å·®å¼‚: {np.abs(X_weighted - X_original).max():.10f}")
        else:
            print(f"\nâœ… åŠ æƒæˆåŠŸ: ç‰¹å¾å·²è¢«ä¿®æ”¹")
            print(f"  å·®å¼‚èŒƒå›´: [{np.min(changes):.6f}, {np.max(changes):.6f}]")

        self.logger.debug(f"  åº”ç”¨æƒé‡: åŒ¹é…äº†{len(feature_to_gnnwr)}/{len(feature_columns)}ä¸ªç‰¹å¾")
        return X_weighted

    def cross_validate(self, X, y, groups, cv_type='station', gnnwr_data=None):
        """æ‰§è¡Œå¸¦GNNWRæƒé‡çš„äº¤å‰éªŒè¯

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            y (np.array): ç›®æ ‡å˜é‡
            groups (np.array): åˆ†ç»„ä¿¡æ¯
            cv_type (str): äº¤å‰éªŒè¯ç±»å‹ ('station' æˆ– 'yearly')
            gnnwr_data (pd.DataFrame): GNNWRéœ€è¦çš„å®Œæ•´æ•°æ®

        Returns:
            dict: äº¤å‰éªŒè¯ç»“æœ
        """
        logo = LeaveOneGroupOut()
        all_predictions = []
        all_true_values = []
        fold_results = {}

        fold_maes = []
        fold_rmses = []
        fold_rs = []
        fold_samples = []

        unique_groups = np.unique(groups)
        total_folds = len(unique_groups)

        self.logger.info(f"å¼€å§‹{cv_type}äº¤å‰éªŒè¯ï¼Œå…±{total_folds}ä¸ªæŠ˜å ...")
        self.logger.info(f"ä½¿ç”¨GNNWRæƒé‡å¢å¼º: {self.use_gnnwr}")

        for fold, (train_idx, val_idx) in enumerate(logo.split(X, y, groups)):
            group_id = groups[val_idx[0]]
            train_size = len(train_idx)
            val_size = len(val_idx)

            print("\n" + "=" * 100)
            print(f"ğŸ¯ {cv_type} Fold {fold + 1}/{total_folds}: {group_id}")
            print(f"   è®­ç»ƒé›†å¤§å°: {train_size}, éªŒè¯é›†å¤§å°: {val_size}")
            print("=" * 100)

            self.logger.info(
                f"{cv_type} Fold {fold + 1}/{total_folds}: {group_id} (è®­ç»ƒé›†{train_size}, éªŒè¯é›†{val_size})")

            # åˆ†å‰²æ•°æ®
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # GNNWRæƒé‡å¢å¼º
            if self.use_gnnwr and gnnwr_data is not None:
                print(f"\nğŸ“Š GNNWRæƒé‡å¢å¼ºé˜¶æ®µ")

                # è·å–å½“å‰æŠ˜å çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®
                train_data_fold = gnnwr_data.iloc[train_idx].copy()
                val_data_fold = gnnwr_data.iloc[val_idx].copy()

                print(f"  è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_data_fold.shape}")
                print(f"  éªŒè¯æ•°æ®å½¢çŠ¶: {val_data_fold.shape}")

                # è®­ç»ƒGNNWRå¹¶æå–æƒé‡
                print(f"  è®­ç»ƒGNNWRæ¨¡å‹...")
                train_weights, val_weights = self._train_gnnwr_for_fold(
                    train_data_fold,
                    val_data_fold
                )

                if train_weights is not None and val_weights is not None:
                    print(f"\nâœ… GNNWRè®­ç»ƒå®Œæˆï¼Œæå–åˆ°æƒé‡çŸ©é˜µ")
                    print(f"  è®­ç»ƒé›†æƒé‡å½¢çŠ¶: {train_weights.shape}")
                    print(f"  éªŒè¯é›†æƒé‡å½¢çŠ¶: {val_weights.shape}")

                    # æ‰“å°æƒé‡ç»Ÿè®¡
                    print(f"  è®­ç»ƒé›†æƒé‡ç»Ÿè®¡:")
                    print(f"    å‡å€¼: {train_weights.mean():.6f}")
                    print(f"    æ ‡å‡†å·®: {train_weights.std():.6f}")
                    print(f"    èŒƒå›´: [{train_weights.min():.6f}, {train_weights.max():.6f}]")

                    # ğŸ” è¯¦ç»†éªŒè¯ï¼šåº”ç”¨æƒé‡å‰åçš„ç‰¹å¾å˜åŒ–
                    print(f"\n" + "=" * 80)
                    print(f"ğŸ§ª è¯¦ç»†éªŒè¯ï¼šæƒé‡åº”ç”¨æ•ˆæœ")
                    print(f"=" * 80)

                    # 1. ä¿å­˜åŸå§‹ç‰¹å¾
                    X_train_original = X_train.copy()
                    X_val_original = X_val.copy()

                    # 2. åº”ç”¨æƒé‡å‰çš„ç‰¹å¾ç»Ÿè®¡
                    print(f"\nğŸ“ˆ åº”ç”¨æƒé‡å‰çš„ç‰¹å¾ç»Ÿè®¡:")
                    print(f"  è®­ç»ƒé›†ç‰¹å¾èŒƒå›´: [{X_train.min():.4f}, {X_train.max():.4f}]")
                    print(f"  è®­ç»ƒé›†ç‰¹å¾å‡å€¼: {X_train.mean():.4f}")
                    print(f"  éªŒè¯é›†ç‰¹å¾èŒƒå›´: [{X_val.min():.4f}, {X_val.max():.4f}]")
                    print(f"  éªŒè¯é›†ç‰¹å¾å‡å€¼: {X_val.mean():.4f}")

                    # 3. åº”ç”¨æƒé‡
                    print(f"\nğŸ”„ åº”ç”¨æƒé‡åˆ°ç‰¹å¾çŸ©é˜µ...")
                    X_train = self._apply_gnnwr_weights(
                        X_train, train_weights,
                        self.feature_columns, self.gnnwr_x_columns
                    )
                    X_val = self._apply_gnnwr_weights(
                        X_val, val_weights,
                        self.feature_columns, self.gnnwr_x_columns
                    )

                    # 4. åº”ç”¨æƒé‡åçš„ç‰¹å¾ç»Ÿè®¡
                    print(f"\nğŸ“Š åº”ç”¨æƒé‡åçš„ç‰¹å¾ç»Ÿè®¡:")
                    print(f"  è®­ç»ƒé›†ç‰¹å¾èŒƒå›´: [{X_train.min():.4f}, {X_train.max():.4f}]")
                    print(f"  è®­ç»ƒé›†ç‰¹å¾å‡å€¼: {X_train.mean():.4f}")
                    print(f"  éªŒè¯é›†ç‰¹å¾èŒƒå›´: [{X_val.min():.4f}, {X_val.max():.4f}]")
                    print(f"  éªŒè¯é›†ç‰¹å¾å‡å€¼: {X_val.mean():.4f}")

                    # 5. è®¡ç®—å˜åŒ–é‡
                    train_changes = X_train - X_train_original
                    val_changes = X_val - X_val_original

                    print(f"\nğŸ“‰ ç‰¹å¾å˜åŒ–åˆ†æ:")
                    print(f"  è®­ç»ƒé›†å˜åŒ–:")
                    print(f"    æœ€å¤§å˜åŒ–: {train_changes.max():.6f}")
                    print(f"    æœ€å°å˜åŒ–: {train_changes.min():.6f}")
                    print(f"    å¹³å‡ç»å¯¹å˜åŒ–: {np.abs(train_changes).mean():.6f}")
                    print(f"    æ˜¾è‘—å˜åŒ–æ¯”ä¾‹(>0.001): {(np.abs(train_changes) > 0.001).sum() / train_changes.size:.2%}")

                    print(f"  éªŒè¯é›†å˜åŒ–:")
                    print(f"    æœ€å¤§å˜åŒ–: {val_changes.max():.6f}")
                    print(f"    æœ€å°å˜åŒ–: {val_changes.min():.6f}")
                    print(f"    å¹³å‡ç»å¯¹å˜åŒ–: {np.abs(val_changes).mean():.6f}")
                    print(f"    æ˜¾è‘—å˜åŒ–æ¯”ä¾‹(>0.001): {(np.abs(val_changes) > 0.001).sum() / val_changes.size:.2%}")

                    # 6. æ£€æŸ¥å‡ ä¸ªå…³é”®ç‰¹å¾çš„å˜åŒ–
                    key_features = ['elevation', 'X', 'Y', 'Z', 'slope', 'doy']
                    print(f"\nğŸ”‘ å…³é”®ç‰¹å¾è¯¦ç»†å˜åŒ– (ç¬¬ä¸€ä¸ªæ ·æœ¬):")

                    for feat in key_features:
                        if feat in self.feature_columns and feat in self.gnnwr_x_columns:
                            feat_idx = self.feature_columns.index(feat)
                            gnnwr_idx = self.gnnwr_x_columns.index(feat)

                            # è®­ç»ƒé›†ç¬¬ä¸€ä¸ªæ ·æœ¬
                            train_original = X_train_original[0, feat_idx]
                            train_weight = train_weights[0, gnnwr_idx]
                            train_weighted = X_train[0, feat_idx]

                            # éªŒè¯é›†ç¬¬ä¸€ä¸ªæ ·æœ¬
                            val_original = X_val_original[0, feat_idx]
                            val_weight = val_weights[0, gnnwr_idx]
                            val_weighted = X_val[0, feat_idx]

                            print(f"\n  {feat}:")
                            print(f"    è®­ç»ƒé›†: {train_original:.4f} Ã— {train_weight:.4f} = {train_weighted:.4f} "
                                  f"(å˜åŒ–: {train_weighted - train_original:+.4f})")
                            print(f"    éªŒè¯é›†: {val_original:.4f} Ã— {val_weight:.4f} = {val_weighted:.4f} "
                                  f"(å˜åŒ–: {val_weighted - val_original:+.4f})")

                    # 7. æ£€æŸ¥æ˜¯å¦çœŸçš„æ”¹å˜äº†
                    train_same = np.allclose(X_train, X_train_original, atol=1e-10)
                    val_same = np.allclose(X_val, X_val_original, atol=1e-10)

                    if train_same and val_same:
                        print(f"\nâš ï¸ è­¦å‘Š: åŠ æƒåç‰¹å¾ä¸åŸå§‹ç‰¹å¾å‡ ä¹å®Œå…¨ç›¸åŒï¼")
                        print(f"  è®­ç»ƒé›†æœ€å¤§å·®å¼‚: {np.abs(X_train - X_train_original).max():.10f}")
                        print(f"  éªŒè¯é›†æœ€å¤§å·®å¼‚: {np.abs(X_val - X_val_original).max():.10f}")
                    else:
                        print(f"\nâœ… éªŒè¯é€šè¿‡: æƒé‡æˆåŠŸåº”ç”¨åˆ°ç‰¹å¾ä¸Š")

                    self.logger.info(f"  âœ… GNNWRæƒé‡åº”ç”¨æˆåŠŸ")
                else:
                    print(f"\nâŒ GNNWRæƒé‡æå–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾")
                    self.logger.info(f"  âš ï¸ GNNWRæƒé‡æå–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾")
            else:
                print(f"\nğŸ“ æœªä½¿ç”¨GNNWRæƒé‡å¢å¼º")

            # è®­ç»ƒXGBoostæ¨¡å‹
            print(f"\nğŸŒ² è®­ç»ƒXGBoostæ¨¡å‹...")
            model = xgb.XGBRegressor(**self.params)

            # æ·»åŠ è®­ç»ƒè¿›åº¦æ˜¾ç¤º
            print(f"  å¼€å§‹æ‹Ÿåˆæ¨¡å‹ (æ ·æœ¬æ•°: {len(X_train)}, ç‰¹å¾æ•°: {X_train.shape[1]})...")

            start_time = datetime.now()
            model.fit(X_train, y_train)
            training_time = (datetime.now() - start_time).total_seconds()

            print(f"  æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")

            # é¢„æµ‹
            print(f"  è¿›è¡Œé¢„æµ‹...")
            y_pred = model.predict(X_val)

            # å­˜å‚¨ç»“æœ
            all_predictions.extend(y_pred)
            all_true_values.extend(y_val)

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            fold_metrics = self.evaluate_predictions(y_val, y_pred)
            fold_results[group_id] = fold_metrics

            fold_maes.append(fold_metrics['MAE'])
            fold_rmses.append(fold_metrics['RMSE'])
            fold_rs.append(fold_metrics['R'])
            fold_samples.append(fold_metrics['æ ·æœ¬æ•°'])

            r_display = fold_metrics['R']
            r_str = f"{r_display:.3f}" if not np.isnan(r_display) else "NaN"

            # æ‰“å°Foldç»“æœ
            print(f"\nğŸ“Š Fold {fold + 1} æ€§èƒ½æŒ‡æ ‡:")
            print(f"  MAE:  {fold_metrics['MAE']:.3f} mm")
            print(f"  RMSE: {fold_metrics['RMSE']:.3f} mm")
            print(f"  R:    {r_str}")
            print(f"  æ ·æœ¬æ•°: {fold_metrics['æ ·æœ¬æ•°']}")

            self.logger.info(
                f"  Fold {fold + 1} æ€§èƒ½: MAE={fold_metrics['MAE']:.3f}, R={r_str}"
            )

        # è®¡ç®—æ€»ä½“æ€§èƒ½
        overall_metrics = self.evaluate_predictions(
            np.array(all_true_values),
            np.array(all_predictions)
        )

        # è®¡ç®—ç»Ÿè®¡é‡
        def safe_statistic(values, func):
            valid_values = [v for v in values if not np.isnan(v)]
            if len(valid_values) == 0:
                return np.nan
            return func(valid_values)

        mean_metrics = {
            'MAE': safe_statistic(fold_maes, np.mean),
            'RMSE': safe_statistic(fold_rmses, np.mean),
            'R': safe_statistic(fold_rs, np.mean),
            'æ ·æœ¬æ•°': np.sum(fold_samples)
        }

        median_metrics = {
            'MAE': safe_statistic(fold_maes, np.median),
            'RMSE': safe_statistic(fold_rmses, np.median),
            'R': safe_statistic(fold_rs, np.median),
            'æ ·æœ¬æ•°': np.sum(fold_samples)
        }

        std_metrics = {
            'MAE': safe_statistic(fold_maes, np.std),
            'RMSE': safe_statistic(fold_rmses, np.std),
            'R': safe_statistic(fold_rs, np.std)
        }

        print("\n" + "=" * 100)
        print(f"ğŸ‰ {cv_type}äº¤å‰éªŒè¯å®Œæˆ!")
        print("=" * 100)
        print(f"ğŸ“ˆ èšåˆæ€§èƒ½æŒ‡æ ‡:")
        print(f"  MAE:  {overall_metrics['MAE']:.3f} mm")
        print(f"  RMSE: {overall_metrics['RMSE']:.3f} mm")
        print(f"  R:    {overall_metrics['R']:.3f}")
        print(f"  æ€»æ ·æœ¬æ•°: {overall_metrics['æ ·æœ¬æ•°']}")
        print(f"\nğŸ“Š æŠ˜å ç»Ÿè®¡:")
        print(f"  æŠ˜å æ•°: {total_folds}")
        print(f"  MAEå‡å€¼: {mean_metrics['MAE']:.3f} Â± {std_metrics['MAE']:.3f} mm")
        print(f"  Rå‡å€¼:   {mean_metrics['R']:.3f} Â± {std_metrics['R']:.3f}")

        self.logger.info(f"âœ… {cv_type}äº¤å‰éªŒè¯å®Œæˆ")
        self.logger.info(f"  èšåˆæ€§èƒ½: MAE={overall_metrics['MAE']:.3f}mm, R={overall_metrics['R']:.3f}")

        return {
            'overall': overall_metrics,
            'mean': mean_metrics,
            'median': median_metrics,
            'std': std_metrics,
            'by_fold': fold_results,
            'predictions': np.array(all_predictions),
            'true_values': np.array(all_true_values),
            'folds': total_folds,
            'fold_metrics': {
                'MAE': fold_maes,
                'RMSE': fold_rmses,
                'R': fold_rs,
                'samples': fold_samples
            }
        }

    def evaluate_predictions(self, y_true, y_pred):
        """è¯„ä¼°é¢„æµ‹ç»“æœ"""
        mask = ~(np.isnan(y_true) | np.isnan(y_pred))
        y_true_clean = y_true[mask]
        y_pred_clean = y_pred[mask]

        if len(y_true_clean) == 0:
            return {
                'MAE': np.nan,
                'RMSE': np.nan,
                'R': np.nan,
                'R_pvalue': np.nan,
                'æ ·æœ¬æ•°': 0,
                'æ€»æ ·æœ¬æ•°': len(y_true),
                'æœ‰æ•ˆæ ·æœ¬æ¯”ä¾‹': 0.0
            }

        mae = mean_absolute_error(y_true_clean, y_pred_clean)
        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))

        def safe_pearsonr(x, y):
            if len(x) <= 1 or np.all(x == x[0]) or np.all(y == y[0]):
                return np.nan, np.nan
            if np.std(x) == 0 or np.std(y) == 0:
                return np.nan, np.nan
            try:
                return pearsonr(x, y)
            except:
                return np.nan, np.nan

        r, p_value = safe_pearsonr(y_true_clean, y_pred_clean)

        return {
            'MAE': mae,
            'RMSE': rmse,
            'R': r,
            'R_pvalue': p_value,
            'æ ·æœ¬æ•°': len(y_true_clean),
            'æ€»æ ·æœ¬æ•°': len(y_true),
            'æœ‰æ•ˆæ ·æœ¬æ¯”ä¾‹': len(y_true_clean) / len(y_true) if len(y_true) > 0 else 0
        }

    def train_final_model(self, X, y, gnnwr_data=None):
        """è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰"""
        self.logger.info("è®­ç»ƒæœ€ç»ˆXGBoostæ¨¡å‹...")

        # GNNWRæƒé‡å¢å¼º
        if self.use_gnnwr and gnnwr_data is not None:
            self.logger.info("ä¸ºæœ€ç»ˆæ¨¡å‹è®­ç»ƒGNNWR...")

            # ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒGNNWR
            train_weights, _ = self._train_gnnwr_for_fold(gnnwr_data, gnnwr_data.head(1))

            if train_weights is not None:
                X = self._apply_gnnwr_weights(
                    X, train_weights,
                    self.feature_columns, self.gnnwr_x_columns
                )
                self.logger.info("âœ… æœ€ç»ˆæ¨¡å‹GNNWRæƒé‡åº”ç”¨æˆåŠŸ")

        # è®­ç»ƒXGBoost
        self.model = xgb.XGBRegressor(**self.params)
        self.model.fit(X, y)

        self.logger.info("âœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
        return self.model

    def run_complete_analysis(self, df, output_dir=None):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹ - å…ˆè¿›è¡Œå¹´åº¦äº¤å‰éªŒè¯"""
        self.logger.info("=" * 70)
        self.logger.info("ğŸš€ å¼€å§‹GNNW-XGBoostå®Œæ•´åˆ†ææµç¨‹")
        self.logger.info("=" * 70)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = f"./gnnw_xgboost_results_{timestamp}"

        os.makedirs(output_dir, exist_ok=True)
        self.logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")

        try:
            # 1. æ•°æ®é¢„å¤„ç†
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 1: æ•°æ®é¢„å¤„ç†")
            self.logger.info("=" * 50)

            X, y, station_groups, year_groups, gnnwr_data = self.preprocess_data(df)

            results = {
                'preprocessing': {
                    'samples': len(X),
                    'features': len(self.feature_columns),
                    'stations': len(np.unique(station_groups)),
                    'years': len(np.unique(year_groups)),
                    'use_gnnwr': self.use_gnnwr
                }
            }

            # 2. å…ˆè¿›è¡Œå¹´åº¦äº¤å‰éªŒè¯ï¼ˆæ•°æ®é‡è¾ƒå°ï¼‰
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 2: å¹´åº¦äº¤å‰éªŒè¯ (æ•°æ®é‡è¾ƒå°ï¼Œå…ˆå¼€å§‹)")
            self.logger.info("=" * 50)

            results['yearly_cv'] = self.cross_validate(
                X, y, year_groups, 'yearly', gnnwr_data
            )

            # 3. å†è¿›è¡Œç«™ç‚¹äº¤å‰éªŒè¯ï¼ˆæ•°æ®é‡è¾ƒå¤§ï¼‰
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 3: ç«™ç‚¹äº¤å‰éªŒè¯ (æ•°æ®é‡è¾ƒå¤§)")
            self.logger.info("=" * 50)

            # å¯¹äºç«™ç‚¹äº¤å‰éªŒè¯ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç®€åŒ–çš„GNNWRè®­ç»ƒï¼ˆå‡å°‘è½®æ•°ï¼‰
            if self.use_gnnwr:
                self.logger.info("ç«™ç‚¹äº¤å‰éªŒè¯ä½¿ç”¨ç®€åŒ–çš„GNNWRè®­ç»ƒï¼ˆå‡å°‘åˆ°3ä¸ªepochï¼‰")
                original_epochs = self.gnnwr_params.get('max_epoch', 5)
                self.gnnwr_params['max_epoch'] = 3  # å‡å°‘è®­ç»ƒè½®æ•°

                results['station_cv'] = self.cross_validate(
                    X, y, station_groups, 'station', gnnwr_data
                )

                # æ¢å¤åŸå§‹è®¾ç½®
                self.gnnwr_params['max_epoch'] = original_epochs
            else:
                results['station_cv'] = self.cross_validate(
                    X, y, station_groups, 'station', gnnwr_data
                )

            # 4. è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 4: è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
            self.logger.info("=" * 50)

            results['final_model'] = self.train_final_model(X, y, gnnwr_data)

            # 5. ç‰¹å¾é‡è¦æ€§åˆ†æ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 5: ç‰¹å¾é‡è¦æ€§åˆ†æ")
            self.logger.info("=" * 50)

            results['feature_importance'] = self.get_feature_importance()

            # 6. ä¿å­˜ç»“æœ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 6: ä¿å­˜ç»“æœ")
            self.logger.info("=" * 50)

            self._save_results(results, output_dir)

            # 7. ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report(results)
            print(report)

            self.logger.info("ğŸ¯ å®Œæ•´åˆ†æå®Œæˆï¼")
            return results

        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææµç¨‹å¤±è´¥: {str(e)}")
            raise

    def _save_results(self, results, output_dir):
        """ä¿å­˜ç»“æœ"""
        try:
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            if 'final_model' in results:
                model_path = f'{output_dir}/final_model.pkl'
                joblib.dump(results['final_model'], model_path)
                self.logger.info(f"âœ… æ¨¡å‹ä¿å­˜: {model_path}")

            # ä¿å­˜è¯¦ç»†ç»“æœ
            eval_results = {
                'training_info': {
                    'timestamp': datetime.now().isoformat(),
                    'feature_columns': self.feature_columns,
                    'gnnwr_x_columns': self.gnnwr_x_columns,
                    'use_gnnwr': self.use_gnnwr,
                    'total_samples': results.get('preprocessing', {}).get('samples', 0)
                },
                'model_parameters': self.params,
                'gnnwr_parameters': self.gnnwr_params,
                'station_cross_validation': results.get('station_cv', {}),
                'yearly_cross_validation': results.get('yearly_cv', {})
            }

            eval_path = f'{output_dir}/evaluation_results.json'
            with open(eval_path, 'w', encoding='utf-8') as f:
                json.dump(eval_results, f, indent=2, ensure_ascii=False, default=float)
            self.logger.info(f"âœ… è¯¦ç»†è¯„ä¼°ç»“æœä¿å­˜: {eval_path}")

            # ç”Ÿæˆå¯è§†åŒ–
            self._create_scatter_plots(results, output_dir)

        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")

    def _create_scatter_plots(self, results, output_dir):
        """åˆ›å»ºæ•£ç‚¹å›¾"""
        try:
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
            plt.rcParams['axes.unicode_minus'] = False

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # ç«™ç‚¹CVæ•£ç‚¹å›¾
            if 'station_cv' in results:
                overall = results['station_cv']['overall']
                self._plot_single_scatter(
                    ax1,
                    results['station_cv']['true_values'],
                    results['station_cv']['predictions'],
                    overall,
                    'Station Cross-Validation'
                )

            # å¹´åº¦CVæ•£ç‚¹å›¾
            if 'yearly_cv' in results:
                overall = results['yearly_cv']['overall']
                self._plot_single_scatter(
                    ax2,
                    results['yearly_cv']['true_values'],
                    results['yearly_cv']['predictions'],
                    overall,
                    'Yearly Cross-Validation'
                )

            plt.tight_layout()
            scatter_path = f'{output_dir}/scatter_plots.png'
            plt.savefig(scatter_path, dpi=300, bbox_inches='tight')
            plt.close()
            self.logger.info(f"âœ… æ•£ç‚¹å›¾ä¿å­˜: {scatter_path}")

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆæ•£ç‚¹å›¾å¤±è´¥: {str(e)}")

    def _plot_single_scatter(self, ax, y_true, y_pred, metrics, title):
        """ç»˜åˆ¶å•ä¸ªæ•£ç‚¹å›¾"""
        mask = ~(np.isnan(y_true) | np.isnan(y_pred))
        y_true_clean = y_true[mask]
        y_pred_clean = y_pred[mask]

        if len(y_true_clean) == 0:
            return

        max_range = 175
        ax.plot([0, max_range], [0, max_range], 'k-', alpha=0.8, linewidth=2)
        ax.scatter(y_true_clean, y_pred_clean, alpha=0.6, s=15, c='blue', edgecolors='none')

        ax.set_xlabel('Observed SWE (mm)', fontsize=14)
        ax.set_ylabel('Predicted SWE (mm)', fontsize=14)
        ax.set_title(title, fontsize=16, fontweight='bold')
        ax.set_xlim([0, max_range])
        ax.set_ylim([0, max_range])
        ax.grid(True, alpha=0.3)

        stats_text = f"MAE = {metrics['MAE']:.2f} mm\nRMSE = {metrics['RMSE']:.2f} mm\nR = {metrics['R']:.3f}\nN = {len(y_true_clean)}"
        ax.text(0.95, 0.95, stats_text, transform=ax.transAxes,
                verticalalignment='top', horizontalalignment='right',
                fontsize=13, fontfamily='monospace', weight='bold',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    def get_feature_importance(self):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")

        importance_scores = self.model.feature_importances_

        if len(importance_scores) != len(self.feature_columns):
            min_length = min(len(importance_scores), len(self.feature_columns))
            importance_scores = importance_scores[:min_length]
            feature_names = self.feature_columns[:min_length]
        else:
            feature_names = self.feature_columns

        feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance_scores
        }).sort_values('importance', ascending=False)

        return feature_importance_df

    def _generate_report(self, results):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("ğŸ“Š GNNW-XGBoostæ¨¡å‹åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"ä½¿ç”¨GNNWRæƒé‡å¢å¼º: {self.use_gnnwr}")
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")

        # ç«™ç‚¹CVç»“æœ
        if 'station_cv' in results:
            station = results['station_cv']
            report_lines.append("ğŸ“ ç«™ç‚¹äº¤å‰éªŒè¯ (ç©ºé—´è¯„ä¼°):")
            report_lines.append(f"  èšåˆMAE: {station['overall']['MAE']:.3f} mm")
            report_lines.append(f"  èšåˆRMSE: {station['overall']['RMSE']:.3f} mm")
            report_lines.append(f"  èšåˆR: {station['overall']['R']:.3f}")
            report_lines.append(f"  æŠ˜å æ•°: {station['folds']}")
            report_lines.append("")

        # å¹´åº¦CVç»“æœ
        if 'yearly_cv' in results:
            yearly = results['yearly_cv']
            report_lines.append("ğŸ“… å¹´åº¦äº¤å‰éªŒè¯ (æ—¶é—´è¯„ä¼°):")
            report_lines.append(f"  èšåˆMAE: {yearly['overall']['MAE']:.3f} mm")
            report_lines.append(f"  èšåˆRMSE: {yearly['overall']['RMSE']:.3f} mm")
            report_lines.append(f"  èšåˆR: {yearly['overall']['R']:.3f}")
            report_lines.append(f"  æŠ˜å æ•°: {yearly['folds']}")
            report_lines.append("")

        # æ€§èƒ½æ¯”è¾ƒ
        if 'station_cv' in results and 'yearly_cv' in results:
            station_r = results['station_cv']['overall']['R']
            yearly_r = results['yearly_cv']['overall']['R']

            report_lines.append("ğŸ’¡ æ€§èƒ½åˆ†æ:")
            if not np.isnan(station_r) and not np.isnan(yearly_r):
                if station_r > yearly_r:
                    report_lines.append(f"  ç«™ç‚¹CVä¼˜äºå¹´åº¦CV (R: {station_r:.3f} > {yearly_r:.3f})")
                else:
                    report_lines.append(f"  å¹´åº¦CVä¼˜äºç«™ç‚¹CV (R: {yearly_r:.3f} > {station_r:.3f})")

        report_lines.append("\n" + "=" * 80)
        return "\n".join(report_lines)


# ä¾¿æ·ä½¿ç”¨å‡½æ•°
def train_gnnw_xgboost_model(data_df, output_dir=None, use_gnnwr=True):
    """ä¾¿æ·å‡½æ•°ï¼šè®­ç»ƒGNNW-XGBoostæ¨¡å‹

    Args:
        data_df (pd.DataFrame): åŒ…å«ç‰¹å¾å’ŒSWEçš„æ•°æ®
        output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„
        use_gnnwr (bool): æ˜¯å¦ä½¿ç”¨GNNWRæƒé‡

    Returns:
        dict: åŒ…å«æ‰€æœ‰è®­ç»ƒç»“æœçš„å­—å…¸
    """
    trainer = GNNW_XGBoostTrainer(use_gnnwr=use_gnnwr)
    return trainer.run_complete_analysis(data_df, output_dir)


# å¯¹æ¯”å®éªŒå‡½æ•°
def compare_models(data_df, output_dir=None):
    """å¯¹æ¯”çº¯XGBoostå’ŒGNNW-XGBoostçš„æ€§èƒ½"""

    print("=" * 80)
    print("ğŸ”¬ å¼€å§‹æ¨¡å‹å¯¹æ¯”å®éªŒ")
    print("=" * 80)

    # 1. çº¯XGBoost
    print("\n1. è®­ç»ƒçº¯XGBoostæ¨¡å‹...")
    xgb_trainer = GNNW_XGBoostTrainer(use_gnnwr=False)
    xgb_results = xgb_trainer.run_complete_analysis(
        data_df,
        output_dir=os.path.join(output_dir, "xgboost_only") if output_dir else None
    )

    # 2. GNNW-XGBoost
    print("\n2. è®­ç»ƒGNNW-XGBoostæ¨¡å‹...")
    gnnw_trainer = GNNW_XGBoostTrainer(use_gnnwr=True)
    gnnw_results = gnnw_trainer.run_complete_analysis(
        data_df,
        output_dir=os.path.join(output_dir, "gnnw_xgboost") if output_dir else None
    )

    # 3. å¯¹æ¯”åˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ")
    print("=" * 80)

    if 'station_cv' in xgb_results and 'station_cv' in gnnw_results:
        xgb_station_r = xgb_results['station_cv']['overall']['R']
        gnnw_station_r = gnnw_results['station_cv']['overall']['R']

        print("ç«™ç‚¹äº¤å‰éªŒè¯ (ç©ºé—´è¯„ä¼°):")
        print(f"  çº¯XGBoost: R = {xgb_station_r:.3f}")
        print(f"  GNNW-XGBoost: R = {gnnw_station_r:.3f}")

        if not np.isnan(xgb_station_r) and not np.isnan(gnnw_station_r):
            improvement = (gnnw_station_r - xgb_station_r) / abs(xgb_station_r) * 100
            print(f"  GNNW-XGBoostæå‡: {improvement:+.1f}%")

    if 'yearly_cv' in xgb_results and 'yearly_cv' in gnnw_results:
        xgb_yearly_r = xgb_results['yearly_cv']['overall']['R']
        gnnw_yearly_r = gnnw_results['yearly_cv']['overall']['R']

        print("\nå¹´åº¦äº¤å‰éªŒè¯ (æ—¶é—´è¯„ä¼°):")
        print(f"  çº¯XGBoost: R = {xgb_yearly_r:.3f}")
        print(f"  GNNW-XGBoost: R = {gnnw_yearly_r:.3f}")

        if not np.isnan(xgb_yearly_r) and not np.isnan(gnnw_yearly_r):
            improvement = (gnnw_yearly_r - xgb_yearly_r) / abs(xgb_yearly_r) * 100
            print(f"  GNNW-XGBoostæå‡: {improvement:+.1f}%")

    return {
        'xgboost': xgb_results,
        'gnnw_xgboost': gnnw_results
    }