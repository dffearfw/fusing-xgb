class GTNNW_XGBoostTrainer:
    """GTNNW-XGBoostè®­ç»ƒå™¨ - é›†æˆGTNNWRæƒé‡çŸ©é˜µä¸XGBoost"""

    # é»˜è®¤XGBoostå‚æ•°
    DEFAULT_PARAMS = {
        'n_estimators': 60,
        'learning_rate': 0.17,
        'max_depth': 5,
        'min_child_weight': 5,
        'gamma': 0,
        'subsample': 0.8,
        'colsample_bytree': 0.5,
        'reg_alpha': 0.05,
        'random_state': 42,
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse'
    }

    # GTNNWRå‚æ•°
    DEFAULT_GTNNWR_PARAMS = {
        'dense_layers': [[3], [512, 256, 64]],
        'drop_out': 0.4,
        'optimizer': "Adadelta",
        'optimizer_params': {
            "scheduler": "MultiStepLR",
            "scheduler_milestones": [1000, 2000, 3000, 4000],
            "scheduler_gamma": 0.8,
        },
        'max_epoch': 3000,
        'early_stop': 1000,
        'print_frequency': 100
    }

    def __init__(self, params=None, gtnnwr_params=None, use_gtnnwr=True,
                 nan_strategy='median', nan_fill_value=0.0,
                 # æ–°å¢å‚æ•°
                 use_feature_mahalanobis=False,
                 feature_columns_for_distance=None):
        """åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            use_feature_mahalanobis: æ˜¯å¦ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»
            feature_columns_for_distance: ç”¨äºé©¬æ°è·ç¦»è®¡ç®—çš„ç‰¹å¾åˆ—
        """
        self.logger = logger
        self.model = None
        self.feature_columns = None
        self.target_column = 'swe'
        self.use_gtnnwr = use_gtnnwr
        self.nan_strategy = nan_strategy
        self.nan_fill_value = nan_fill_value

        # æ–°å¢ï¼šç‰¹å¾é©¬æ°è·ç¦»ç›¸å…³å‚æ•°
        self.use_feature_mahalanobis = use_feature_mahalanobis
        self.feature_columns_for_distance = feature_columns_for_distance

        # å­˜å‚¨å¡«å……å€¼ç”¨äºåç»­é¢„æµ‹
        self.nan_fill_values = {}
        self.nan_fill_stats = {}

        # å®šä¹‰GTNNWRç‰¹å¾åˆ—
        self.gtnnwr_x_columns = ['aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation',
                                 'std_slope', 'std_eastness', 'std_tpi', 'std_curvature1', 'std_curvature2', 'std_high',
                                 'std_aspect', 'glsnow', 'cswe', 'snow_depth_snow_depth', 'ERA5æ¸©åº¦_ERA5æ¸©åº¦',
                                 'era5_swe', 'doy',
                                 'gldas', 'year', 'month', 'scp_start', 'scp_end', 'd1', 'd2', 'X', 'Y', 'Z', 'da',
                                 'db', 'dc',
                                 'dd']

        # GTNNWRéœ€è¦ç©ºé—´åˆ—å’Œæ—¶é—´åˆ—
        self.gtnnwr_spatial_columns = ['X', 'Y']
        self.gtnnwr_temp_columns = ['year', 'month', 'doy']
        self.gtnnwr_id_column = 'id'
        self.gtnnwr_y_column = ['swe']

        # æ›´æ–°å‚æ•°
        self.params = self.DEFAULT_PARAMS.copy()
        if params:
            self.params.update(params)

        self.gtnnwr_params = self.DEFAULT_GTNNWR_PARAMS.copy()
        if gtnnwr_params:
            self.gtnnwr_params.update(gtnnwr_params)

        self.logger.info(f"åˆå§‹åŒ–GTNNW-XGBoostè®­ç»ƒå™¨")
        self.logger.info(f"XGBoostå‚æ•°: {self.params}")
        self.logger.info(f"ä½¿ç”¨GTNNWRæƒé‡å¢å¼º: {self.use_gtnnwr}")
        self.logger.info(f"ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»: {self.use_feature_mahalanobis}")
        self.logger.info(f"NaNå¤„ç†ç­–ç•¥: {self.nan_strategy}")

    def _train_gtnnwr_for_fold(self, train_data, val_data):
        """ä¸ºå•ä¸ªæŠ˜å è®­ç»ƒGTNNWRæ¨¡å‹å¹¶æå–æƒé‡"""
        self.logger.debug("ä¸ºå½“å‰æŠ˜å è®­ç»ƒGTNNWRæ¨¡å‹...")

        print("\n" + "=" * 80)
        print("ğŸ§  GTNNWRæ¨¡å‹è®­ç»ƒ (å½“å‰æŠ˜å )")
        print("=" * 80)

        try:
            # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„åˆ—éƒ½å­˜åœ¨
            print("ğŸ” æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
            required_columns = (self.gtnnwr_x_columns + self.gtnnwr_spatial_columns +
                                self.gtnnwr_temp_columns + [self.gtnnwr_id_column] + self.gtnnwr_y_column)

            # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
            if len(train_data) < 10 or len(val_data) < 1:
                print(f"âš ï¸  æ•°æ®é‡ä¸è¶³: è®­ç»ƒé›†{len(train_data)}æ ·æœ¬, éªŒè¯é›†{len(val_data)}æ ·æœ¬")
                print("âš ï¸  è·³è¿‡GTNNWRè®­ç»ƒï¼Œè¿”å›Noneæƒé‡")
                return None, None

            for col in required_columns:
                if col not in train_data.columns:
                    if col == 'id':
                        train_data[col] = np.arange(len(train_data))
                    else:
                        train_data[col] = 0.0
                    print(f"  âš ï¸  è®­ç»ƒæ•°æ®ç¼ºå¤±åˆ— '{col}'ï¼Œå·²å¡«å……")
                if col not in val_data.columns:
                    if col == 'id':
                        val_data[col] = np.arange(len(val_data))
                    else:
                        val_data[col] = 0.0
                    print(f"  âš ï¸  éªŒè¯æ•°æ®ç¼ºå¤±åˆ— '{col}'ï¼Œå·²å¡«å……")

            # æ£€æŸ¥æ•°æ®å½¢çŠ¶
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶:")
            print(f"  è®­ç»ƒæ•°æ®: {train_data.shape}")
            print(f"  éªŒè¯æ•°æ®: {val_data.shape}")

            # æ£€æŸ¥NaNå€¼
            train_nan = train_data[self.gtnnwr_x_columns].isna().sum().sum()
            val_nan = val_data[self.gtnnwr_x_columns].isna().sum().sum()
            if train_nan > 0 or val_nan > 0:
                print(f"  âš ï¸  è­¦å‘Š: è®­ç»ƒæ•°æ®æœ‰{train_nan}ä¸ªNaNï¼ŒéªŒè¯æ•°æ®æœ‰{val_nan}ä¸ªNaN")
                # ä½¿ç”¨ä¸­ä½æ•°å¡«å……
                for col in self.gtnnwr_x_columns:
                    if col in train_data.columns:
                        median_val = train_data[col].median()
                        train_data[col] = train_data[col].fillna(median_val)
                        val_data[col] = val_data[col].fillna(median_val)

            # åˆå§‹åŒ–GTNNWRæ•°æ®é›†
            print("ğŸ“¦ åˆå§‹åŒ–GTNNWRæ•°æ®é›†...")

            # ç¡®å®šç”¨äºé©¬æ°è·ç¦»è®¡ç®—çš„ç‰¹å¾åˆ—
            if self.use_feature_mahalanobis and self.feature_columns_for_distance is None:
                # é»˜è®¤ä½¿ç”¨æ‰€æœ‰ç‰¹å¾åˆ—ï¼ˆæ’é™¤ç©ºé—´å’Œæ—¶é—´åˆ—ï¼‰
                feature_columns_for_distance = self.gtnnwr_x_columns.copy()
                # æ’é™¤ç©ºé—´åˆ—
                if self.gtnnwr_spatial_columns:
                    feature_columns_for_distance = [col for col in feature_columns_for_distance
                                                    if col not in self.gtnnwr_spatial_columns]
                # æ’é™¤æ—¶é—´åˆ—
                if self.gtnnwr_temp_columns:
                    feature_columns_for_distance = [col for col in feature_columns_for_distance
                                                    if col not in self.gtnnwr_temp_columns]
                print(f"  ğŸ“Š ç‰¹å¾é©¬æ°è·ç¦»: ä½¿ç”¨ {len(feature_columns_for_distance)} ä¸ªç‰¹å¾")
            else:
                feature_columns_for_distance = self.feature_columns_for_distance

            try:
                # ä½¿ç”¨init_dataset_splitï¼Œä¼ å…¥ç‰¹å¾é©¬æ°è·ç¦»å‚æ•°
                train_set, val_set, test_set = datasets.init_dataset_split(
                    train_data=train_data,
                    val_data=val_data,
                    test_data=val_data.head(max(1, min(5, len(val_data) // 2))),
                    x_column=self.gtnnwr_x_columns,
                    y_column=self.gtnnwr_y_column,
                    spatial_column=self.gtnnwr_spatial_columns,
                    temp_column=self.gtnnwr_temp_columns,
                    batch_size=min(1024, len(train_data)),
                    shuffle=False,
                    use_model="gtnnwr",
                    # æ–°å¢å‚æ•°
                    use_feature_mahalanobis=self.use_feature_mahalanobis,
                    feature_columns_for_distance=feature_columns_for_distance
                )
                print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–æˆåŠŸ")
                print(f"  æ˜¯å¦ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»: {self.use_feature_mahalanobis}")
                if self.use_feature_mahalanobis:
                    print(f"  é©¬æ°è·ç¦»ç‰¹å¾æ•°: {len(feature_columns_for_distance)}")
            except Exception as error:
                print(f"âŒ æ•°æ®é›†åˆå§‹åŒ–å¤±è´¥: {error}")
                print("âš ï¸  è·³è¿‡GTNNWRè®­ç»ƒï¼Œè¿”å›Noneæƒé‡")
                return None, None

            print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ:")
            print(f"  è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_set) if hasattr(train_set, '__len__') else 'N/A'}")
            print(f"  éªŒè¯é›†æ ·æœ¬æ•°: {len(val_set) if hasattr(val_set, '__len__') else 'N/A'}")

            # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
            if (not hasattr(train_set, '__len__') or len(train_set) == 0 or
                    not hasattr(val_set, '__len__') or len(val_set) == 0):
                print(f"âŒ æ•°æ®é›†ä¸ºç©ºæˆ–æ— æ•ˆ")
                print("âš ï¸  è·³è¿‡GTNNWRè®­ç»ƒï¼Œè¿”å›Noneæƒé‡")
                return None, None

            # è®­ç»ƒGTNNWRæ¨¡å‹
            print("\nğŸ‹ï¸ è®­ç»ƒGTNNWRæ¨¡å‹...")
            try:
                gtnnwr = models.GTNNWR(
                    train_dataset=train_set,
                    valid_dataset=val_set,
                    test_dataset=train_set,
                    dense_layers=self.gtnnwr_params.get('dense_layers', [[3], [512, 256, 64]]),
                    drop_out=self.gtnnwr_params.get('drop_out', 0.4),
                    optimizer=self.gtnnwr_params.get('optimizer', "Adadelta"),
                    optimizer_params=self.gtnnwr_params.get('optimizer_params', {}),
                    model_name=f"GTNNWR_Fold",
                    model_save_path="result/gtnnwr_models_temp",
                    log_path="result/gtnnwr_logs_temp",
                    write_path="result/gtnnwr_runs_temp"
                )

                # æ·»åŠ å›¾ç»“æ„
                print("ğŸ•¸ï¸ æ·»åŠ å›¾ç»“æ„...")
                gtnnwr.add_graph()

                # è®­ç»ƒ
                print(f"âš™ï¸ è®­ç»ƒå‚æ•°: {self.gtnnwr_params.get('max_epoch', 3000)}è½®, "
                      f"æ—©åœ{self.gtnnwr_params.get('early_stop', 1000)}è½®")

                gtnnwr.run(
                    max_epoch=self.gtnnwr_params.get('max_epoch', 3000),
                    early_stop=self.gtnnwr_params.get('early_stop', 1000),
                    print_frequency=self.gtnnwr_params.get('print_frequency', 100)
                )
            except Exception as model_error:
                print(f"âŒ GTNNWRæ¨¡å‹åˆ›å»ºæˆ–è®­ç»ƒå¤±è´¥: {model_error}")
                print("âš ï¸  è·³è¿‡GTNNWRè®­ç»ƒï¼Œè¿”å›Noneæƒé‡")
                return None, None

            # æå–æƒé‡çŸ©é˜µ
            def extract_weights(gtnnwr_instance, dataset, dataset_name="æ•°æ®é›†"):
                """æå–GTNNWRæ¨¡å‹è¾“å‡ºçš„æƒé‡çŸ©é˜µ"""
                if dataset is None or not hasattr(dataset, 'dataloader'):
                    print(f"  âŒ {dataset_name}æ— æ•ˆæˆ–æ²¡æœ‰dataloader")
                    return None

                model = gtnnwr_instance._model
                model.eval()
                device = gtnnwr_instance._device

                all_weights = []
                sample_count = 0

                print(f"\nğŸ“¥ ä»{dataset_name}æå–æƒé‡...")

                with torch.no_grad():
                    try:
                        total_batches = 0
                        for batch_idx, batch in enumerate(dataset.dataloader):
                            if batch is None or len(batch) < 2:
                                continue

                            distances, features = batch[:2]
                            distances = distances.to(device)

                            # è·å–æ¨¡å‹è¾“å‡º
                            weights = model(distances)

                            # æ£€æŸ¥æƒé‡ä¸­çš„NaN
                            if torch.isnan(weights).any():
                                print(f"  âš ï¸  æ‰¹æ¬¡{batch_idx}æƒé‡ä¸­åŒ…å«NaNå€¼ï¼Œä½¿ç”¨1å¡«å……")
                                weights = torch.nan_to_num(weights, nan=1.0)

                            all_weights.append(weights.cpu().numpy())
                            sample_count += weights.shape[0]
                            total_batches += 1

                        print(f"  âœ… å®Œæˆ: æ€»å…±å¤„ç†{total_batches}ä¸ªæ‰¹æ¬¡ï¼Œ{sample_count}ä¸ªæ ·æœ¬")

                    except Exception as e:
                        print(f"  âŒ æå–æƒé‡æ—¶å‡ºé”™: {e}")
                        import traceback
                        print(traceback.format_exc())
                        return None

                if all_weights:
                    weights_combined = np.concatenate(all_weights, axis=0)

                    # æ£€æŸ¥å¹¶å¤„ç†NaNå€¼
                    nan_count = np.isnan(weights_combined).sum()
                    if nan_count > 0:
                        print(f"  âš ï¸  æƒé‡çŸ©é˜µä¸­æœ‰{nan_count}ä¸ªNaNå€¼ï¼Œä½¿ç”¨1å¡«å……")
                        weights_combined = np.nan_to_num(weights_combined, nan=1.0)

                    print(f"  âœ… æå–å®Œæˆ: {weights_combined.shape} (æ ·æœ¬æ•°Ã—ç‰¹å¾æ•°)")
                    return weights_combined
                else:
                    print(f"  âŒ æå–å¤±è´¥: æ²¡æœ‰è·å–åˆ°æƒé‡")
                    return None

            # æå–è®­ç»ƒé›†å’ŒéªŒè¯é›†æƒé‡
            train_weights = extract_weights(gtnnwr, train_set, "è®­ç»ƒé›†")
            val_weights = extract_weights(gtnnwr, val_set, "éªŒè¯é›†")

            if train_weights is not None and val_weights is not None:
                # æ£€æŸ¥å¹¶è°ƒæ•´ç»´åº¦
                expected_cols = len(self.gtnnwr_x_columns)

                print(f"\nğŸ”§ ç»´åº¦æ£€æŸ¥ä¸è°ƒæ•´:")
                print(f"  æœŸæœ›ç‰¹å¾æ•°: {expected_cols}")

                # æ£€æŸ¥è®­ç»ƒé›†æƒé‡ç»´åº¦
                if train_weights.shape[1] != expected_cols:
                    print(f"  âš ï¸  è®­ç»ƒæƒé‡ç»´åº¦ä¸åŒ¹é…: {train_weights.shape[1]} != {expected_cols}")
                    if train_weights.shape[1] == expected_cols + 1:
                        train_weights = train_weights[:, :expected_cols]
                        print(f"  âœ… ä¿®å¤ï¼šå»æ‰æœ€åä¸€åˆ—ï¼Œæ–°å½¢çŠ¶: {train_weights.shape}")
                    elif train_weights.shape[1] > expected_cols:
                        train_weights = train_weights[:, :expected_cols]
                        print(f"  âœ… ä¿®å¤ï¼šæˆªæ–­åˆ°æœŸæœ›é•¿åº¦ï¼Œæ–°å½¢çŠ¶: {train_weights.shape}")
                    else:
                        padding = np.ones((train_weights.shape[0], expected_cols - train_weights.shape[1]))
                        train_weights = np.hstack([train_weights, padding])
                        print(f"  âœ… ä¿®å¤ï¼šå¡«å……åˆ°æœŸæœ›é•¿åº¦ï¼Œæ–°å½¢çŠ¶: {train_weights.shape}")

                # æ£€æŸ¥éªŒè¯é›†æƒé‡ç»´åº¦
                if val_weights.shape[1] != expected_cols:
                    print(f"  âš ï¸  éªŒè¯æƒé‡ç»´åº¦ä¸åŒ¹é…: {val_weights.shape[1]} != {expected_cols}")
                    if val_weights.shape[1] == expected_cols + 1:
                        val_weights = val_weights[:, :expected_cols]
                        print(f"  âœ… ä¿®å¤ï¼šå»æ‰æœ€åä¸€åˆ—ï¼Œæ–°å½¢çŠ¶: {val_weights.shape}")
                    elif val_weights.shape[1] > expected_cols:
                        val_weights = val_weights[:, :expected_cols]
                        print(f"  âœ… ä¿®å¤ï¼šæˆªæ–­åˆ°æœŸæœ›é•¿åº¦ï¼Œæ–°å½¢çŠ¶: {val_weights.shape}")
                    else:
                        padding = np.ones((val_weights.shape[0], expected_cols - val_weights.shape[1]))
                        val_weights = np.hstack([val_weights, padding])
                        print(f"  âœ… ä¿®å¤ï¼šå¡«å……åˆ°æœŸæœ›é•¿åº¦ï¼Œæ–°å½¢çŠ¶: {val_weights.shape}")

                self.logger.debug(f"  æå–åˆ°æƒé‡çŸ©é˜µ: è®­ç»ƒé›†{train_weights.shape}, éªŒè¯é›†{val_weights.shape}")
                return train_weights, val_weights
            else:
                print(f"\nâŒ GTNNWRæƒé‡æå–å¤±è´¥")
                self.logger.warning("  æœªèƒ½æå–åˆ°æƒé‡çŸ©é˜µ")
                return None, None

        except Exception as e:
            print(f"\nâŒ GTNNWRè®­ç»ƒå¤±è´¥: {str(e)}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
            self.logger.warning(f"  GTNNWRè®­ç»ƒå¤±è´¥: {str(e)}")
            return None, None

    def run_complete_analysis(self, df, output_dir=None):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        self.logger.info("=" * 70)
        self.logger.info("ğŸš€ å¼€å§‹GTNNW-XGBoostå®Œæ•´åˆ†ææµç¨‹")
        self.logger.info(f"ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»: {self.use_feature_mahalanobis}")
        self.logger.info("=" * 70)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = f"./gtnnw_xgboost_results_{timestamp}"

        os.makedirs(output_dir, exist_ok=True)
        self.logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")

        try:
            # 1. æ•°æ®é¢„å¤„ç†
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 1: æ•°æ®é¢„å¤„ç†")
            self.logger.info("=" * 50)

            X, y, station_groups, year_groups, gtnnwr_data = self.preprocess_data(df, is_training=True)

            results = {
                'preprocessing': {
                    'samples': len(X),
                    'features': len(self.feature_columns),
                    'stations': len(np.unique(station_groups)),
                    'years': len(np.unique(year_groups)),
                    'use_gtnnwr': self.use_gtnnwr,
                    'use_feature_mahalanobis': self.use_feature_mahalanobis,
                    'nan_strategy': self.nan_strategy,
                    'nan_fill_stats': self.nan_fill_stats
                }
            }

            # 2. å¹´åº¦äº¤å‰éªŒè¯
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 2: å¹´åº¦äº¤å‰éªŒè¯")
            self.logger.info("=" * 50)

            results['yearly_cv'] = self.cross_validate(
                X, y, year_groups, 'yearly', gtnnwr_data
            )

            # 3. ç«™ç‚¹äº¤å‰éªŒè¯
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 3: ç«™ç‚¹äº¤å‰éªŒè¯")
            self.logger.info("=" * 50)

            results['station_cv'] = self.cross_validate(
                X, y, station_groups, 'station', gtnnwr_data
            )

            # 4. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 4: è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
            self.logger.info("=" * 50)

            results['final_model'] = self.train_final_model(X, y, gtnnwr_data)

            # 5. ç‰¹å¾é‡è¦æ€§åˆ†æ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 5: ç‰¹å¾é‡è¦æ€§åˆ†æ")
            self.logger.info("=" * 50)

            results['feature_importance'] = self.get_feature_importance()

            # 6. ä¿å­˜ç»“æœ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 6: ä¿å­˜ç»“æœ")
            self.logger.info("=" * 50)

            self._save_results(results, output_dir)

            # 7. ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report(results)
            print(report)

            self.logger.info("ğŸ¯ å®Œæ•´åˆ†æå®Œæˆï¼")
            return results

        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææµç¨‹å¤±è´¥: {str(e)}")
            raise

    def _save_results(self, results, output_dir):
        """ä¿å­˜ç»“æœ"""
        try:
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            if 'final_model' in results:
                model_path = f'{output_dir}/final_model.pkl'
                joblib.dump(results['final_model'], model_path)
                self.logger.info(f"âœ… æ¨¡å‹ä¿å­˜: {model_path}")

            # ä¿å­˜NaNå¤„ç†ä¿¡æ¯
            nan_info_path = f'{output_dir}/nan_handling_info.json'
            nan_info = {
                'strategy': self.nan_strategy,
                'fill_values': self.nan_fill_values,
                'fill_stats': self.nan_fill_stats
            }
            with open(nan_info_path, 'w', encoding='utf-8') as f:
                json.dump(nan_info, f, indent=2, ensure_ascii=False,
                          default=lambda x: float(x) if isinstance(x, (np.float32, np.float64)) else x)
            self.logger.info(f"âœ… NaNå¤„ç†ä¿¡æ¯ä¿å­˜: {nan_info_path}")

            # ä¿å­˜è¯¦ç»†ç»“æœ
            eval_results = {
                'training_info': {
                    'timestamp': datetime.now().isoformat(),
                    'feature_columns': self.feature_columns,
                    'gtnnwr_x_columns': self.gtnnwr_x_columns,
                    'use_gtnnwr': self.use_gtnnwr,
                    'use_feature_mahalanobis': self.use_feature_mahalanobis,
                    'nan_strategy': self.nan_strategy,
                    'total_samples': results.get('preprocessing', {}).get('samples', 0)
                },
                'model_parameters': self.params,
                'gtnnwr_parameters': self.gtnnwr_params,
                'station_cross_validation': results.get('station_cv', {}),
                'yearly_cross_validation': results.get('yearly_cv', {})
            }

            eval_path = f'{output_dir}/evaluation_results.json'
            with open(eval_path, 'w', encoding='utf-8') as f:
                json.dump(eval_results, f, indent=2, ensure_ascii=False, default=float)
            self.logger.info(f"âœ… è¯¦ç»†è¯„ä¼°ç»“æœä¿å­˜: {eval_path}")

            # ç”Ÿæˆå¯è§†åŒ–
            self._create_scatter_plots(results, output_dir)

        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")

    def _generate_report(self, results):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("ğŸ“Š GTNNW-XGBoostæ¨¡å‹åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"ä½¿ç”¨GTNNWRæƒé‡å¢å¼º: {self.use_gtnnwr}")
        report_lines.append(f"ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»: {self.use_feature_mahalanobis}")
        report_lines.append(f"NaNå¤„ç†ç­–ç•¥: {self.nan_strategy}")
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")

        # ç«™ç‚¹CVç»“æœ
        if 'station_cv' in results:
            station = results['station_cv']
            report_lines.append("ğŸ“ ç«™ç‚¹äº¤å‰éªŒè¯ (ç©ºé—´è¯„ä¼°):")
            report_lines.append(f"  èšåˆMAE: {station['overall']['MAE']:.3f} mm")
            report_lines.append(f"  èšåˆRMSE: {station['overall']['RMSE']:.3f} mm")
            report_lines.append(f"  èšåˆR: {station['overall']['R']:.3f}")
            report_lines.append(f"  æŠ˜å æ•°: {station['folds']}")
            report_lines.append("")

        # å¹´åº¦CVç»“æœ
        if 'yearly_cv' in results:
            yearly = results['yearly_cv']
            report_lines.append("ğŸ“… å¹´åº¦äº¤å‰éªŒè¯ (æ—¶é—´è¯„ä¼°):")
            report_lines.append(f"  èšåˆMAE: {yearly['overall']['MAE']:.3f} mm")
            report_lines.append(f"  èšåˆRMSE: {yearly['overall']['RMSE']:.3f} mm")
            report_lines.append(f"  èšåˆR: {yearly['overall']['R']:.3f}")
            report_lines.append(f"  æŠ˜å æ•°: {yearly['folds']}")
            report_lines.append("")

        report_lines.append("\n" + "=" * 80)
        return "\n".join(report_lines)


# ä¾¿æ·ä½¿ç”¨å‡½æ•° - æ–°å¢æ”¯æŒç‰¹å¾é©¬æ°è·ç¦»
def train_gtnnw_xgboost_model(data_df, output_dir=None, use_gtnnwr=True,
                              nan_strategy='median', nan_fill_value=0.0,
                              use_feature_mahalanobis=False,
                              feature_columns_for_distance=None):
    """ä¾¿æ·å‡½æ•°ï¼šè®­ç»ƒGTNNW-XGBoostæ¨¡å‹

    Args:
        data_df (pd.DataFrame): åŒ…å«ç‰¹å¾å’ŒSWEçš„æ•°æ®
        output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„
        use_gtnnwr (bool): æ˜¯å¦ä½¿ç”¨GTNNWRæƒé‡
        nan_strategy (str): NaNå¤„ç†ç­–ç•¥
        nan_fill_value (float): å¡«å……NaNçš„å€¼
        use_feature_mahalanobis (bool): æ˜¯å¦ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»
        feature_columns_for_distance (list): ç”¨äºé©¬æ°è·ç¦»è®¡ç®—çš„ç‰¹å¾åˆ—

    Returns:
        dict: åŒ…å«æ‰€æœ‰è®­ç»ƒç»“æœçš„å­—å…¸
    """
    trainer = GTNNW_XGBoostTrainer(
        use_gtnnwr=use_gtnnwr,
        nan_strategy=nan_strategy,
        nan_fill_value=nan_fill_value,
        use_feature_mahalanobis=use_feature_mahalanobis,
        feature_columns_for_distance=feature_columns_for_distance
    )
    return trainer.run_complete_analysis(data_df, output_dir)


# å¯¹æ¯”å®éªŒå‡½æ•° - æ–°å¢æ”¯æŒç‰¹å¾é©¬æ°è·ç¦»
def compare_models(data_df, output_dir=None):
    """å¯¹æ¯”ä¸åŒé…ç½®çš„æ€§èƒ½"""

    print("=" * 80)
    print("ğŸ”¬ å¼€å§‹æ¨¡å‹å¯¹æ¯”å®éªŒ")
    print("=" * 80)

    # 1. çº¯XGBoost
    print("\n1. è®­ç»ƒçº¯XGBoostæ¨¡å‹...")
    xgb_trainer = GTNNW_XGBoostTrainer(use_gtnnwr=False, nan_strategy='median')
    xgb_results = xgb_trainer.run_complete_analysis(
        data_df,
        output_dir=os.path.join(output_dir, "xgboost_only") if output_dir else None
    )

    # 2. GTNNW-XGBoost (æ— ç‰¹å¾é©¬æ°è·ç¦»)
    print("\n2. è®­ç»ƒGTNNW-XGBoostæ¨¡å‹ (æ— ç‰¹å¾é©¬æ°è·ç¦»)...")
    gtnnw_trainer1 = GTNNW_XGBoostTrainer(use_gtnnwr=True, nan_strategy='median',
                                          use_feature_mahalanobis=False)
    gtnnw_results1 = gtnnw_trainer1.run_complete_analysis(
        data_df,
        output_dir=os.path.join(output_dir, "gtnnw_xgboost_no_mahalanobis") if output_dir else None
    )

    # 3. GTNNW-XGBoost (æœ‰ç‰¹å¾é©¬æ°è·ç¦»)
    print("\n3. è®­ç»ƒGTNNW-XGBoostæ¨¡å‹ (æœ‰ç‰¹å¾é©¬æ°è·ç¦»)...")
    gtnnw_trainer2 = GTNNW_XGBoostTrainer(use_gtnnwr=True, nan_strategy='median',
                                          use_feature_mahalanobis=True)
    gtnnw_results2 = gtnnw_trainer2.run_complete_analysis(
        data_df,
        output_dir=os.path.join(output_dir, "gtnnw_xgboost_with_mahalanobis") if output_dir else None
    )

    # 4. å¯¹æ¯”åˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ")
    print("=" * 80)

    results_to_compare = [
        ("çº¯XGBoost", xgb_results),
        ("GTNNW-XGBoost (æ— é©¬æ°è·ç¦»)", gtnnw_results1),
        ("GTNNW-XGBoost (æœ‰é©¬æ°è·ç¦»)", gtnnw_results2)
    ]

    for name, res in results_to_compare:
        if 'station_cv' in res and 'overall' in res['station_cv']:
            r = res['station_cv']['overall']['R']
            if not np.isnan(r):
                print(f"{name}:")
                print(f"  ç«™ç‚¹CV R = {r:.3f}")
                print(f"  MAE = {res['station_cv']['overall']['MAE']:.3f} mm")
                print(f"  RMSE = {res['station_cv']['overall']['RMSE']:.3f} mm")
                print()

    return {
        'xgboost': xgb_results,
        'gtnnw_xgboost_no_mahalanobis': gtnnw_results1,
        'gtnnw_xgboost_with_mahalanobis': gtnnw_results2
    }