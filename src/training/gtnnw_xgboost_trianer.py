from datetime import datetime  # æ­£ç¡®ï¼šå¯¼å…¥äº†datetimeç±»
# ä½¿ç”¨æ—¶ï¼šdatetime.now() å°±å¯ä»¥
import json
import os
from venv import logger

import gnnwr
import joblib
import numpy as np
import pandas as pd
import torch
from gnnwr import models, datasets
from torch import nn

# â­â­â­ çŒ´å­è¡¥ä¸ä¿®å¤STPNN â­â­â­
original_STPNN_forward = gnnwr.networks.STPNN.forward


def patched_STPNN_forward(self, x):
    x = x.to(torch.float32)
    batch = x.shape[0]
    height = x.shape[1]

    # æ£€æŸ¥å¹¶ä¿®å¤ç»´åº¦
    actual_input_dim = x.shape[2]

    # æ£€æŸ¥ç¬¬ä¸€å±‚æƒé‡
    first_layer = self.fc[0]
    if hasattr(first_layer, 'layer'):
        weight_shape = first_layer.layer.weight.shape

        # å¦‚æœæƒé‡ç»´åº¦ä¸åŒ¹é…ï¼Œä¿®æ­£å®ƒ
        if weight_shape[1] != actual_input_dim:
            print(f"âš ï¸ è‡ªåŠ¨ä¿®å¤STPNNç»´åº¦: {weight_shape[1]} -> {actual_input_dim}")

            # åˆ›å»ºæ–°å±‚
            new_layer = nn.Linear(actual_input_dim, weight_shape[0])
            nn.init.kaiming_uniform_(new_layer.weight, a=0, mode='fan_in')
            if new_layer.bias is not None:
                new_layer.bias.data.fill_(0)

            # æ›¿æ¢
            first_layer.layer = new_layer
            self.insize = actual_input_dim
            if self.dense_layer and len(self.dense_layer) > 0:
                self.dense_layer[0] = actual_input_dim

    # å±•å¹³å¹¶ç»§ç»­
    x = torch.reshape(x, shape=(batch * height, x.shape[2]))
    output = self.fc(x)
    output = torch.reshape(output, shape=(batch, height * self.outsize))
    return output


# åº”ç”¨è¡¥ä¸
gnnwr.networks.STPNN.forward = patched_STPNN_forward

print("âœ… STPNNçŒ´å­è¡¥ä¸å·²åº”ç”¨")

class GTNNW_XGBoostTrainer:
    """GTNNW-XGBoostè®­ç»ƒå™¨ - é›†æˆGTNNWRæƒé‡çŸ©é˜µä¸XGBoost"""

    # é»˜è®¤XGBoostå‚æ•°
    DEFAULT_PARAMS = {
        'n_estimators': 60,
        'learning_rate': 0.17,
        'max_depth': 5,
        'min_child_weight': 5,
        'gamma': 0,
        'subsample': 0.8,
        'colsample_bytree': 0.5,
        'reg_alpha': 0.05,
        'random_state': 42,
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse'
    }

    # GTNNWRå‚æ•°
    DEFAULT_GTNNWR_PARAMS = {
        'dense_layers': [[3, 64, 32], [128, 64, 32]],
        'drop_out': 0.4,
        'optimizer': "Adadelta",
        'optimizer_params': {
            "scheduler": "MultiStepLR",
            "scheduler_milestones": [1000, 2000, 3000, 4000],
            "scheduler_gamma": 0.8,
        },
        'max_epoch': 3000,
        'early_stop': 1000,
        'print_frequency': 100
    }

    def __init__(self, params=None, gtnnwr_params=None, use_gtnnwr=True,
                 nan_strategy='median', nan_fill_value=0.0,
                 # æ–°å¢å‚æ•°
                 use_feature_mahalanobis=True,
                 feature_columns_for_distance=None):
        """åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            use_feature_mahalanobis: æ˜¯å¦ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»
            feature_columns_for_distance: ç”¨äºé©¬æ°è·ç¦»è®¡ç®—çš„ç‰¹å¾åˆ—
        """
        self.logger = logger
        self.model = None
        self.feature_columns = None
        self.target_column = 'swe'
        self.use_gtnnwr = use_gtnnwr
        self.nan_strategy = nan_strategy
        self.nan_fill_value = nan_fill_value

        # æ–°å¢ï¼šç‰¹å¾é©¬æ°è·ç¦»ç›¸å…³å‚æ•°
        self.use_feature_mahalanobis = use_feature_mahalanobis
        self.feature_columns_for_distance = feature_columns_for_distance

        # å­˜å‚¨å¡«å……å€¼ç”¨äºåç»­é¢„æµ‹
        self.nan_fill_values = {}
        self.nan_fill_stats = {}

        # å®šä¹‰GTNNWRç‰¹å¾åˆ—
        self.gtnnwr_x_columns = ['aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation',
                                 'std_slope', 'std_eastness', 'std_tpi', 'std_curvature1', 'std_curvature2', 'std_high',
                                 'std_aspect', 'glsnow', 'cswe', 'snow_depth_snow_depth', 'ERA5æ¸©åº¦_ERA5æ¸©åº¦',
                                 'era5_swe', 'doy',
                                 'gldas', 'year', 'month', 'scp_start', 'scp_end', 'd1', 'd2', 'X', 'Y', 'Z', 'da',
                                 'db', 'dc',
                                 'dd']

        # GTNNWRéœ€è¦ç©ºé—´åˆ—å’Œæ—¶é—´åˆ—
        self.gtnnwr_spatial_columns = ['X', 'Y']
        self.gtnnwr_temp_columns = ['year', 'month', 'doy']
        self.gtnnwr_id_column = 'id'
        self.gtnnwr_y_column = ['swe']

        # æ›´æ–°å‚æ•°
        self.params = self.DEFAULT_PARAMS.copy()
        if params:
            self.params.update(params)

        self.gtnnwr_params = self.DEFAULT_GTNNWR_PARAMS.copy()
        if gtnnwr_params:
            self.gtnnwr_params.update(gtnnwr_params)

        self.logger.info(f"åˆå§‹åŒ–GTNNW-XGBoostè®­ç»ƒå™¨")
        self.logger.info(f"XGBoostå‚æ•°: {self.params}")
        self.logger.info(f"ä½¿ç”¨GTNNWRæƒé‡å¢å¼º: {self.use_gtnnwr}")
        self.logger.info(f"ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»: {self.use_feature_mahalanobis}")
        self.logger.info(f"NaNå¤„ç†ç­–ç•¥: {self.nan_strategy}")

    def _handle_nan_values(self, df, is_training=True, fill_values=None):
        """å¤„ç†NaNå€¼

        Args:
            df (pd.DataFrame): è¾“å…¥æ•°æ®
            is_training (bool): æ˜¯å¦ä¸ºè®­ç»ƒé˜¶æ®µ
            fill_values (dict): é¢„è®¡ç®—çš„å¡«å……å€¼

        Returns:
            pd.DataFrame: å¤„ç†åçš„æ•°æ®
        """
        self.logger.info(f"å¤„ç†NaNå€¼ - é˜¶æ®µ: {'è®­ç»ƒ' if is_training else 'é¢„æµ‹'}")

        df_processed = df.copy()

        # ç»Ÿè®¡NaNå€¼
        nan_stats = df_processed.isna().sum()
        total_nan = nan_stats.sum()
        total_cells = df_processed.size

        if total_nan > 0:
            nan_percentage = (total_nan / total_cells) * 100
            self.logger.info(f"å‘ç°NaNå€¼: {total_nan}/{total_cells} ({nan_percentage:.2f}%)")

            # æŒ‰åˆ—ç»Ÿè®¡NaN
            nan_columns = nan_stats[nan_stats > 0]
            for col, nan_count in nan_columns.items():
                nan_pct = (nan_count / len(df_processed)) * 100
                self.logger.info(f"  åˆ— '{col}': {nan_count} NaN ({nan_pct:.2f}%)")

        # å¤„ç†ä¸åŒåˆ—ç±»å‹çš„NaN
        for col in df_processed.columns:
            if df_processed[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                # æ•°å€¼åˆ—å¤„ç†
                nan_count = df_processed[col].isna().sum()

                if nan_count > 0:
                    if self.nan_strategy == 'drop' and is_training:
                        # åˆ é™¤åŒ…å«NaNçš„è¡Œï¼ˆä»…è®­ç»ƒé˜¶æ®µï¼‰
                        self.logger.warning(f"åˆ é™¤åŒ…å«åˆ— '{col}' NaNçš„ {nan_count} è¡Œ")
                        df_processed = df_processed.dropna(subset=[col])
                    else:
                        # è®¡ç®—æˆ–ä½¿ç”¨å¡«å……å€¼
                        if is_training:
                            if self.nan_strategy == 'mean':
                                fill_value = df_processed[col].mean()
                                self.nan_fill_values[col] = fill_value
                                self.logger.info(f"åˆ— '{col}' ä½¿ç”¨å‡å€¼å¡«å……: {fill_value:.4f}")
                            elif self.nan_strategy == 'median':
                                fill_value = df_processed[col].median()
                                self.nan_fill_values[col] = fill_value
                                self.logger.info(f"åˆ— '{col}' ä½¿ç”¨ä¸­ä½æ•°å¡«å……: {fill_value:.4f}")
                            elif self.nan_strategy == 'zero':
                                fill_value = 0
                                self.nan_fill_values[col] = fill_value
                                self.logger.info(f"åˆ— '{col}' ä½¿ç”¨0å¡«å……")
                            else:  # è‡ªå®šä¹‰å€¼
                                fill_value = self.nan_fill_value
                                self.nan_fill_values[col] = fill_value
                                self.logger.info(f"åˆ— '{col}' ä½¿ç”¨è‡ªå®šä¹‰å€¼å¡«å……: {fill_value}")

                            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
                            self.nan_fill_stats[col] = {
                                'strategy': self.nan_strategy,
                                'fill_value': fill_value,
                                'original_nan_count': nan_count,
                                'original_mean': df_processed[col].mean(),
                                'original_median': df_processed[col].median(),
                                'original_std': df_processed[col].std()
                            }
                        else:
                            # é¢„æµ‹é˜¶æ®µä½¿ç”¨è®­ç»ƒé˜¶æ®µè®¡ç®—çš„å¡«å……å€¼
                            fill_value = self.nan_fill_values.get(col, self.nan_fill_value)
                            self.logger.debug(f"åˆ— '{col}' ä½¿ç”¨è®­ç»ƒé˜¶æ®µè®¡ç®—çš„å¡«å……å€¼: {fill_value}")

                        # å¡«å……NaNå€¼
                        df_processed[col] = df_processed[col].fillna(fill_value)

            elif df_processed[col].dtype == 'object':
                # å¯¹è±¡ç±»å‹åˆ—å¤„ç†ï¼ˆå­—ç¬¦ä¸²ï¼‰
                nan_count = df_processed[col].isna().sum()

                if nan_count > 0:
                    if is_training:
                        # å¯¹äºç±»åˆ«åˆ—ï¼Œä½¿ç”¨ä¼—æ•°å¡«å……æˆ–åˆ›å»ºæ–°ç±»åˆ«
                        if len(df_processed[col].unique()) < 50:  # å‡è®¾æ˜¯ç±»åˆ«åˆ—
                            mode_value = df_processed[col].mode()
                            if not mode_value.empty:
                                fill_value = mode_value.iloc[0]
                                self.nan_fill_values[col] = fill_value
                                self.logger.info(f"ç±»åˆ«åˆ— '{col}' ä½¿ç”¨ä¼—æ•°å¡«å……: {fill_value}")
                            else:
                                fill_value = 'MISSING'
                                self.nan_fill_values[col] = fill_value
                                self.logger.info(f"ç±»åˆ«åˆ— '{col}' ä½¿ç”¨'MISSING'å¡«å……")
                        else:
                            fill_value = 'MISSING'
                            self.nan_fill_values[col] = fill_value
                            self.logger.info(f"æ–‡æœ¬åˆ— '{col}' ä½¿ç”¨'MISSING'å¡«å……")
                    else:
                        fill_value = self.nan_fill_values.get(col, 'MISSING')

                    df_processed[col] = df_processed[col].fillna(fill_value)

        # éªŒè¯å¤„ç†åæ˜¯å¦è¿˜æœ‰NaN
        remaining_nan = df_processed.isna().sum().sum()
        if remaining_nan > 0:
            self.logger.warning(f"å¤„ç†åä»æœ‰ {remaining_nan} ä¸ªNaNå€¼")
        else:
            self.logger.info("âœ… NaNå€¼å¤„ç†å®Œæˆï¼Œæ— å‰©ä½™NaNå€¼")

        return df_processed

    def preprocess_data(self, df, for_gtnnwr=False, is_training=True):
        """æ•°æ®é¢„å¤„ç†ï¼ˆè¿™æ˜¯åŸå§‹çš„æ–¹æ³•ï¼Œéœ€è¦æ¢å¤ï¼‰"""
        self.logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")

        # åˆ›å»ºæ•°æ®å‰¯æœ¬
        df_clean = df.copy()

        # éªŒè¯å¿…è¦åˆ—
        required_columns = ['station_id', 'date', self.target_column]
        missing_columns = [col for col in required_columns if col not in df_clean.columns]
        if missing_columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")

        # å¤„ç†NaNå€¼
        df_clean = self._handle_nan_values(df_clean, is_training=is_training)

        # ç¡®ä¿GTNNWRéœ€è¦çš„åˆ—éƒ½å­˜åœ¨
        if self.use_gtnnwr:
            # åˆ›å»ºIDåˆ—ï¼ˆGTNNWRéœ€è¦ï¼‰
            df_clean['id'] = np.arange(len(df_clean))

            # æ£€æŸ¥å¹¶ç¡®ä¿æ‰€æœ‰éœ€è¦çš„åˆ—éƒ½å­˜åœ¨
            gtnnwr_required = (self.gtnnwr_x_columns + self.gtnnwr_spatial_columns +
                               self.gtnnwr_temp_columns + [self.gtnnwr_id_column])
            missing_gtnnwr = [col for col in gtnnwr_required if col not in df_clean.columns]
            if missing_gtnnwr:
                self.logger.warning(f"GTNNWRç¼ºå°‘ä»¥ä¸‹åˆ—: {missing_gtnnwr}")
                # å°è¯•å¡«å……ç¼ºå¤±åˆ—ä¸º0æˆ–åˆé€‚çš„é»˜è®¤å€¼
                for col in missing_gtnnwr:
                    if col == 'id':
                        df_clean[col] = np.arange(len(df_clean))
                    elif col in ['year', 'month', 'doy']:
                        # å¦‚æœæ˜¯æ—¶é—´åˆ—ï¼Œå°è¯•ä»dateåˆ—æå–
                        if 'date' in df_clean.columns and pd.api.types.is_datetime64_any_dtype(df_clean['date']):
                            df_clean['date'] = pd.to_datetime(df_clean['date'])
                            if col == 'year':
                                df_clean[col] = df_clean['date'].dt.year
                            elif col == 'month':
                                df_clean[col] = df_clean['date'].dt.month
                            elif col == 'doy':
                                df_clean[col] = df_clean['date'].dt.dayofyear
                        else:
                            df_clean[col] = 0.0
                    else:
                        df_clean[col] = 0.0

        # å¤„ç†CSWEæ— æ•ˆå€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'cswe' in df_clean.columns:
            cswe_invalid_mask = df_clean['cswe'] > 200
            if cswe_invalid_mask.sum() > 0:
                df_clean.loc[cswe_invalid_mask, 'cswe'] = np.nan
                # é‡æ–°å¤„ç†NaNå€¼
                df_clean = self._handle_nan_values(df_clean, is_training=is_training)

        # ç¡®å®šç‰¹å¾åˆ—
        exclude_columns = ['station_id', 'date', self.target_column, 'hydrological_doy', 'id']
        exclude_columns.extend([col for col in df_clean.columns if col.startswith('landuse_hash_')])

        # ä¿ç•™GTNNWRç‰¹å¾åˆ—ç”¨äºåŠ æƒ
        if self.use_gtnnwr:
            # ç¡®ä¿GTNNWRç‰¹å¾åˆ—åœ¨ç‰¹å¾åˆ—ä¸­
            for col in self.gtnnwr_x_columns:
                if col not in exclude_columns and col not in df_clean.columns:
                    df_clean[col] = 0.0

        self.feature_columns = [col for col in df_clean.columns if col not in exclude_columns]

        if not self.feature_columns:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç‰¹å¾åˆ—")

        # å†æ¬¡æ£€æŸ¥ç‰¹å¾åˆ—ä¸­çš„NaNå€¼
        feature_nan_counts = df_clean[self.feature_columns].isna().sum()
        if feature_nan_counts.sum() > 0:
            self.logger.warning(f"ç‰¹å¾åˆ—ä¸­ä»æœ‰ {feature_nan_counts.sum()} ä¸ªNaNå€¼")
            for col, count in feature_nan_counts[feature_nan_counts > 0].items():
                self.logger.warning(f"  ç‰¹å¾åˆ— '{col}': {count} ä¸ªNaN")
            # ä½¿ç”¨æœ€åä¸€æ¬¡å¡«å……
            df_clean[self.feature_columns] = df_clean[self.feature_columns].fillna(
                df_clean[self.feature_columns].median()
            )

        # å‡†å¤‡æ•°æ®
        X = df_clean[self.feature_columns].values
        y = df_clean[self.target_column].values

        # æ£€æŸ¥ç›®æ ‡å˜é‡ä¸­çš„NaN
        y_nan_count = np.isnan(y).sum()
        if y_nan_count > 0:
            self.logger.warning(f"ç›®æ ‡å˜é‡ '{self.target_column}' ä¸­æœ‰ {y_nan_count} ä¸ªNaNå€¼")
            if self.nan_strategy == 'drop' and is_training:
                # åˆ é™¤ç›®æ ‡å˜é‡ä¸ºNaNçš„è¡Œ
                valid_mask = ~np.isnan(y)
                X = X[valid_mask]
                y = y[valid_mask]
                df_clean = df_clean.iloc[valid_mask]
                self.logger.info(f"åˆ é™¤äº† {y_nan_count} ä¸ªç›®æ ‡å˜é‡ä¸ºNaNçš„æ ·æœ¬")
            else:
                # å¡«å……ç›®æ ‡å˜é‡çš„NaN
                y_fill_value = np.nanmedian(y)
                y = np.nan_to_num(y, nan=y_fill_value)
                self.logger.info(f"ç›®æ ‡å˜é‡ä½¿ç”¨ä¸­ä½æ•°å¡«å……: {y_fill_value:.4f}")

        # åˆ†ç»„ä¿¡æ¯
        df_clean['year'] = pd.to_datetime(df_clean['date']).dt.year
        station_groups = df_clean['station_id'].values
        year_groups = df_clean['year'].values

        # ä¸ºGTNNWRå‡†å¤‡æ•°æ®
        gtnnwr_data = None
        if self.use_gtnnwr:
            gtnnwr_data = df_clean.copy()
            # ç¡®ä¿æ‰€æœ‰GTNNWRéœ€è¦çš„åˆ—éƒ½å­˜åœ¨
            for col in self.gtnnwr_x_columns + self.gtnnwr_spatial_columns + self.gtnnwr_temp_columns + [
                self.gtnnwr_id_column]:
                if col not in gtnnwr_data.columns:
                    if col == 'id':
                        gtnnwr_data[col] = np.arange(len(gtnnwr_data))
                    else:
                        gtnnwr_data[col] = 0.0

        # æœ€ç»ˆæ£€æŸ¥
        x_nan_count = np.isnan(X).sum()
        y_nan_count = np.isnan(y).sum()

        self.logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        self.logger.info(f"  æ ·æœ¬æ•°: {len(X)}, ç‰¹å¾æ•°: {len(self.feature_columns)}")
        self.logger.info(f"  Xä¸­NaNæ•°é‡: {x_nan_count}, yä¸­NaNæ•°é‡: {y_nan_count}")

        # æ‰“å°ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
        self.logger.info(f"  ç‰¹å¾ç»Ÿè®¡:")
        for i, col in enumerate(self.feature_columns[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªç‰¹å¾
            col_values = X[:, i]
            self.logger.info(
                f"    {col}: å‡å€¼={col_values.mean():.4f}, æ ‡å‡†å·®={col_values.std():.4f}, èŒƒå›´=[{col_values.min():.4f}, {col_values.max():.4f}]")
        if len(self.feature_columns) > 5:
            self.logger.info(f"    ... å’Œå…¶ä»– {len(self.feature_columns) - 5} ä¸ªç‰¹å¾")

        return X, y, station_groups, year_groups, gtnnwr_data

    def _train_gtnnwr_for_fold(self, train_data, val_data):
        """ä¸ºå•ä¸ªæŠ˜å è®­ç»ƒGTNNWRæ¨¡å‹å¹¶æå–æƒé‡"""
        """ä¸ºå•ä¸ªæŠ˜å è®­ç»ƒGTNNWRæ¨¡å‹å¹¶æå–æƒé‡"""
        self.logger.debug("ä¸ºå½“å‰æŠ˜å è®­ç»ƒGTNNWRæ¨¡å‹...")

        print("\n" + "=" * 80)
        print("ğŸ§  GTNNWRæ¨¡å‹è®­ç»ƒ (å½“å‰æŠ˜å )")
        print("=" * 80)

        try:
            # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„åˆ—éƒ½å­˜åœ¨
            print("ğŸ” æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
            required_columns = (self.gtnnwr_x_columns + self.gtnnwr_spatial_columns +
                                self.gtnnwr_temp_columns + [self.gtnnwr_id_column] + self.gtnnwr_y_column)

            # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
            if len(train_data) < 10 or len(val_data) < 1:
                print(f"âš ï¸  æ•°æ®é‡ä¸è¶³: è®­ç»ƒé›†{len(train_data)}æ ·æœ¬, éªŒè¯é›†{len(val_data)}æ ·æœ¬")
                print("âš ï¸  è·³è¿‡GTNNWRè®­ç»ƒï¼Œè¿”å›Noneæƒé‡")
                return None, None

            for col in required_columns:
                if col not in train_data.columns:
                    if col == 'id':
                        train_data[col] = np.arange(len(train_data))
                    else:
                        train_data[col] = 0.0
                    print(f"  âš ï¸  è®­ç»ƒæ•°æ®ç¼ºå¤±åˆ— '{col}'ï¼Œå·²å¡«å……")
                if col not in val_data.columns:
                    if col == 'id':
                        val_data[col] = np.arange(len(val_data))
                    else:
                        val_data[col] = 0.0
                    print(f"  âš ï¸  éªŒè¯æ•°æ®ç¼ºå¤±åˆ— '{col}'ï¼Œå·²å¡«å……")

            # æ£€æŸ¥æ•°æ®å½¢çŠ¶
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶:")
            print(f"  è®­ç»ƒæ•°æ®: {train_data.shape}")
            print(f"  éªŒè¯æ•°æ®: {val_data.shape}")

            # æ£€æŸ¥NaNå€¼
            train_nan = train_data[self.gtnnwr_x_columns].isna().sum().sum()
            val_nan = val_data[self.gtnnwr_x_columns].isna().sum().sum()
            if train_nan > 0 or val_nan > 0:
                print(f"  âš ï¸  è­¦å‘Š: è®­ç»ƒæ•°æ®æœ‰{train_nan}ä¸ªNaNï¼ŒéªŒè¯æ•°æ®æœ‰{val_nan}ä¸ªNaN")
                # ä½¿ç”¨ä¸­ä½æ•°å¡«å……
                for col in self.gtnnwr_x_columns:
                    if col in train_data.columns:
                        median_val = train_data[col].median()
                        train_data[col] = train_data[col].fillna(median_val)
                        val_data[col] = val_data[col].fillna(median_val)

            # åˆå§‹åŒ–GTNNWRæ•°æ®é›†
            print("ğŸ“¦ åˆå§‹åŒ–GTNNWRæ•°æ®é›†...")

            # ç¡®å®šç”¨äºé©¬æ°è·ç¦»è®¡ç®—çš„ç‰¹å¾åˆ—
            if self.use_feature_mahalanobis and self.feature_columns_for_distance is None:
                # é»˜è®¤ä½¿ç”¨æ‰€æœ‰ç‰¹å¾åˆ—ï¼ˆæ’é™¤ç©ºé—´å’Œæ—¶é—´åˆ—ï¼‰
                feature_columns_for_distance = self.gtnnwr_x_columns.copy()
                # æ’é™¤ç©ºé—´åˆ—
                if self.gtnnwr_spatial_columns:
                    feature_columns_for_distance = [col for col in feature_columns_for_distance
                                                    if col not in self.gtnnwr_spatial_columns]
                # æ’é™¤æ—¶é—´åˆ—
                if self.gtnnwr_temp_columns:
                    feature_columns_for_distance = [col for col in feature_columns_for_distance
                                                    if col not in self.gtnnwr_temp_columns]
                print(f"  ğŸ“Š ç‰¹å¾é©¬æ°è·ç¦»: ä½¿ç”¨ {len(feature_columns_for_distance)} ä¸ªç‰¹å¾")
            else:
                feature_columns_for_distance = self.feature_columns_for_distance

            try:
                # ä½¿ç”¨init_dataset_splitï¼Œä¼ å…¥ç‰¹å¾é©¬æ°è·ç¦»å‚æ•°
                # è¿™é‡Œæˆ‘ä»¬ä¼ å…¥å‚æ•°ï¼Œä½†ä¸æ£€æŸ¥è¿”å›å¯¹è±¡çš„å±æ€§
                train_set, val_set, test_set = datasets.init_dataset_split(
                    train_data=train_data,
                    val_data=val_data,
                    test_data=val_data.head(max(1, min(5, len(val_data) // 2))),
                    x_column=self.gtnnwr_x_columns,
                    y_column=self.gtnnwr_y_column,
                    spatial_column=self.gtnnwr_spatial_columns,
                    temp_column=self.gtnnwr_temp_columns,
                    batch_size=min(1024, len(train_data)),
                    shuffle=False,
                    use_model="gtnnwr",
                    # æ–°å¢å‚æ•° - ä¼ é€’ç»™baseDatasetçš„æ„é€ å‡½æ•°
                    use_feature_mahalanobis=self.use_feature_mahalanobis,
                    feature_columns_for_distance=feature_columns_for_distance
                )
                print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–æˆåŠŸ")

                # ğŸš¨ æ·»åŠ è¯¦ç»†çš„debugä¿¡æ¯
                print(f"\nğŸ” è¯¦ç»†ç»´åº¦æ£€æŸ¥...")
                print(f"  train_set.distanceså½¢çŠ¶: {train_set.distances.shape}")
                print(f"  train_set.distancesç»´åº¦æ•°: {train_set.distances.ndim}")

                # æ£€æŸ¥è·ç¦»çŸ©é˜µå†…å®¹
                if hasattr(train_set, 'distances') and train_set.distances is not None:
                    print(f"  è·ç¦»çŸ©é˜µç¬¬ä¸€ä¸ªæ ·æœ¬çš„å½¢çŠ¶: {train_set.distances[0].shape}")
                    print(f"  è·ç¦»çŸ©é˜µç¬¬ä¸€ä¸ªæ ·æœ¬çš„å‰3ä¸ªå‚è€ƒç‚¹:")
                    for i in range(min(3, len(train_set.distances[0]))):
                        print(f"    å‚è€ƒç‚¹{i}: {train_set.distances[0][i]}")

                # æ£€æŸ¥æ˜¯å¦æœ‰temporalç»´åº¦
                if hasattr(train_set, 'temporal') and train_set.temporal is not None:
                    print(f"  train_set.temporalå½¢çŠ¶: {train_set.temporal.shape}")
                    if hasattr(train_set, 'distances') and train_set.distances is not None:
                        print(f"  æ—¶ç©ºç»“åˆåæ€»ç»´åº¦: {train_set.distances.shape[-1] + train_set.temporal.shape[-1]}")

                # æ£€æŸ¥is_need_STNN
                print(f"  train_set.is_need_STNN: {train_set.is_need_STNN}")
                print(f"  val_set.is_need_STNN: {val_set.is_need_STNN}")

                # æ£€æŸ¥simple_distance
                print(f"  train_set.simple_distance: {train_set.simple_distance}")

                print(f"\nğŸ” æ£€æŸ¥æ•°æ®é›†åˆå§‹åŒ–å‚æ•°...")
                print(f"  ä½¿ç”¨çš„use_model: {'gtnnwr'}")
                print(f"  ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»: {self.use_feature_mahalanobis}")
                if self.use_feature_mahalanobis:
                    print(f"  é©¬æ°è·ç¦»ç‰¹å¾æ•°: {len(feature_columns_for_distance)}")

                # âœ… è¿™é‡Œéœ€è¦æ£€æŸ¥æ­£ç¡®çš„å±æ€§åï¼šis_need_feature_distance
                # ä»ä½ çš„ä»£ç å¯ä»¥çœ‹å‡ºï¼ŒbaseDatasetç±»ä½¿ç”¨çš„æ˜¯is_need_feature_distanceï¼Œè€Œä¸æ˜¯use_feature_mahalanobis
                if hasattr(train_set, 'is_need_feature_distance'):
                    print(f"  æ•°æ®é›†æ˜¯å¦ä½¿ç”¨ç‰¹å¾è·ç¦»: {train_set.is_need_feature_distance}")
                else:
                    print(f"  æ•°æ®é›†æ²¡æœ‰is_need_feature_distanceå±æ€§")

                # åªæ‰“å°æˆ‘ä»¬è‡ªå·±è®¾å®šçš„å‚æ•°
                print(f"  é…ç½®ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»: {self.use_feature_mahalanobis}")
                if self.use_feature_mahalanobis:
                    print(f"  é©¬æ°è·ç¦»ç‰¹å¾æ•°: {len(feature_columns_for_distance)}")
            except Exception as error:
                print(f"âŒ æ•°æ®é›†åˆå§‹åŒ–å¤±è´¥: {error}")
                import traceback
                print(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
                print("âš ï¸  è·³è¿‡GTNNWRè®­ç»ƒï¼Œè¿”å›Noneæƒé‡")
                return None, None

            print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ:")
            print(f"  è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_set) if hasattr(train_set, '__len__') else 'N/A'}")
            print(f"  éªŒè¯é›†æ ·æœ¬æ•°: {len(val_set) if hasattr(val_set, '__len__') else 'N/A'}")

            # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
            if (not hasattr(train_set, '__len__') or len(train_set) == 0 or
                    not hasattr(val_set, '__len__') or len(val_set) == 0):
                print(f"âŒ æ•°æ®é›†ä¸ºç©ºæˆ–æ— æ•ˆ")
                print("âš ï¸  è·³è¿‡GTNNWRè®­ç»ƒï¼Œè¿”å›Noneæƒé‡")
                return None, None

            # ğŸš¨ æ‰‹åŠ¨è®¡ç®—æ­£ç¡®ç»´åº¦
            print(f"\nğŸ”§ æ‰‹åŠ¨è®¡ç®—æ­£ç¡®ç»´åº¦...")

            # æ–¹æ³•1ï¼šæ£€æŸ¥æ•°æ®åŠ è½½åçš„å®é™…ç»´åº¦
            try:
                sample = train_set[0]
                if isinstance(sample, tuple) and len(sample) > 0:
                    actual_input_dim = sample[0].shape[-1]
                    print(f"  ä»train_set[0]è·å–çš„å®é™…è¾“å…¥ç»´åº¦: {actual_input_dim}")

                    # æ˜¾ç¤ºæ›´å¤šä¿¡æ¯
                    print(f"  sample[0]å½¢çŠ¶: {sample[0].shape}")
                    print(f"  å‚è€ƒç‚¹æ•°é‡: {sample[0].shape[0]}")

                    # æ£€æŸ¥å…·ä½“å†…å®¹
                    if sample[0].shape[0] > 0:
                        print(f"  ç¬¬ä¸€ä¸ªå‚è€ƒç‚¹çš„å€¼: {sample[0][0]}")
                        if len(sample[0][0]) == 3:
                            print(f"    â†’ å¯èƒ½æ˜¯[è·ç¦»_x, è·ç¦»_y, é©¬æ°è·ç¦»]")
                else:
                    actual_input_dim = train_set.distances.shape[-1]
                    print(f"  ä»distancesè·å–çš„è¾“å…¥ç»´åº¦: {actual_input_dim}")
            except Exception as e:
                print(f"  æ£€æŸ¥ç»´åº¦æ—¶å‡ºé”™: {e}")
                actual_input_dim = 3  # é»˜è®¤å‡è®¾æ˜¯3ç»´
                print(f"  ä½¿ç”¨é»˜è®¤ç»´åº¦: {actual_input_dim}")

            # ç¡®ä¿simple_distance=False
            train_set.simple_distance = False
            val_set.simple_distance = False

            # æ£€æŸ¥GTNNWRå‚æ•°çš„dense_layers
            print(f"\nğŸ” æ£€æŸ¥å¹¶è°ƒæ•´dense_layers...")
            dense_layers_param = self.gtnnwr_params.get('dense_layers', [[3], [512, 256, 64]])
            print(f"  åŸå§‹dense_layers: {dense_layers_param}")

            # è°ƒæ•´dense_layersä»¥åŒ¹é…å®é™…ç»´åº¦
            if isinstance(dense_layers_param, list) and len(dense_layers_param) >= 2:
                # ç¡®ä¿ç¬¬ä¸€ä¸ªåˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯actual_input_dim
                if not isinstance(dense_layers_param[0], list):
                    # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                    dense_layers_param[0] = [dense_layers_param[0]]

                if dense_layers_param[0][0] != actual_input_dim:
                    print(f"  âš ï¸  ç»´åº¦ä¸åŒ¹é…: dense_layers[0][0]={dense_layers_param[0][0]}, actual={actual_input_dim}")
                    # ä¿®å¤ï¼šåˆ›å»ºä¸€ä¸ªæ–°çš„dense_layers
                    stpnn_layers = [actual_input_dim]
                    if len(dense_layers_param[0]) > 1:
                        stpnn_layers.extend(dense_layers_param[0][1:])  # ä¿æŒå…¶ä»–å±‚

                    adjusted_dense_layers = [stpnn_layers, dense_layers_param[1]]
                    dense_layers_param = adjusted_dense_layers
                    print(f"  è°ƒæ•´åçš„dense_layers: {dense_layers_param}")
            else:
                # å¦‚æœæ ¼å¼ä¸å¯¹ï¼Œåˆ›å»ºæ­£ç¡®çš„æ ¼å¼
                print(f"  âš ï¸  dense_layersæ ¼å¼ä¸æ­£ç¡®ï¼Œåˆ›å»ºæ–°æ ¼å¼")
                dense_layers_param = [[actual_input_dim, 64, 32], [128, 64, 32]]
                print(f"  åˆ›å»ºçš„dense_layers: {dense_layers_param}")

            # è®­ç»ƒGTNNWRæ¨¡å‹
            print("\nğŸ‹ï¸ è®­ç»ƒGTNNWRæ¨¡å‹...")

            try:
                print(f"\nğŸš€ åˆ›å»ºGTNNWRæ¨¡å‹...")
                print(f"  ä½¿ç”¨dense_layers: {dense_layers_param}")
                print(f"  è¾“å…¥ç»´åº¦: {actual_input_dim}")
                print(f"  æ˜¯å¦ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»: {self.use_feature_mahalanobis}")
                print(f"åŸå§‹ simple_distance: {train_set.simple_distance}")
                # âœ… ä¿®å¤ï¼šæ£€æŸ¥æ­£ç¡®çš„å±æ€§å
                if hasattr(train_set, 'is_need_feature_distance'):
                    print(f"æ•°æ®é›†æ˜¯å¦ä½¿ç”¨ç‰¹å¾è·ç¦»: {train_set.is_need_feature_distance}")
                else:
                    print(f"æ•°æ®é›†æ²¡æœ‰is_need_feature_distanceå±æ€§")
                print(f"é…ç½®ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»: {self.use_feature_mahalanobis}")

                # æ–¹æ³•1: è®¾ç½® simple_distance=False
                train_set.simple_distance = False
                val_set.simple_distance = False

                # åˆ›å»ºGTNNWR
                gtnnwr = models.GTNNWR(
                    train_dataset=train_set,
                    valid_dataset=val_set,
                    test_dataset=train_set,
                    dense_layers=dense_layers_param,
                    drop_out=self.gtnnwr_params.get('drop_out', 0.4),
                    optimizer=self.gtnnwr_params.get('optimizer', "Adadelta"),
                    optimizer_params=self.gtnnwr_params.get('optimizer_params', {}),
                    model_name=f"GTNNWR_Fold",
                    model_save_path="result/gtnnwr_models_temp",
                    log_path="result/gtnnwr_logs_temp",
                    write_path="result/gtnnwr_runs_temp"
                )

                # ğŸš¨ å…³é”®ï¼šè·³è¿‡add_graph()ï¼Œç›´æ¥è®­ç»ƒ
                print("ğŸ•¸ï¸ è·³è¿‡å›¾ç»“æ„æ·»åŠ ï¼ˆé¿å…ç»´åº¦é”™è¯¯ï¼‰...")
                # gtnnwr.add_graph()  # æ³¨é‡Šæ‰è¿™è¡Œ

                print(f"âš™ï¸ å¼€å§‹è®­ç»ƒ...")
                gtnnwr.run(
                    max_epoch=min(500, self.gtnnwr_params.get('max_epoch', 3000)),  # å…ˆè®­ç»ƒ500è½®
                    early_stop=min(100, self.gtnnwr_params.get('early_stop', 1000)),  # æ—©åœ100è½®
                    print_frequency=self.gtnnwr_params.get('print_frequency', 100)
                )
            except Exception as model_error:
                print(f"âŒ GTNNWRæ¨¡å‹åˆ›å»ºæˆ–è®­ç»ƒå¤±è´¥: {model_error}")
                import traceback
                print(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
                print("âš ï¸  è·³è¿‡GTNNWRè®­ç»ƒï¼Œè¿”å›Noneæƒé‡")
                return None, None

            # æå–æƒé‡çŸ©é˜µ
            def extract_weights(gtnnwr_instance, dataset, dataset_name="æ•°æ®é›†"):
                """æå–GTNNWRæ¨¡å‹è¾“å‡ºçš„æƒé‡çŸ©é˜µ"""
                if dataset is None or not hasattr(dataset, 'dataloader'):
                    print(f"  âŒ {dataset_name}æ— æ•ˆæˆ–æ²¡æœ‰dataloader")
                    return None

                model = gtnnwr_instance._model
                model.eval()
                device = gtnnwr_instance._device

                all_weights = []
                sample_count = 0

                print(f"\nğŸ“¥ ä»{dataset_name}æå–æƒé‡...")

                with torch.no_grad():
                    try:
                        total_batches = 0
                        for batch_idx, batch in enumerate(dataset.dataloader):
                            if batch is None or len(batch) < 2:
                                continue

                            distances, features = batch[:2]
                            distances = distances.to(device)

                            # è·å–æ¨¡å‹è¾“å‡º
                            weights = model(distances)

                            # æ£€æŸ¥æƒé‡ä¸­çš„NaN
                            if torch.isnan(weights).any():
                                print(f"  âš ï¸  æ‰¹æ¬¡{batch_idx}æƒé‡ä¸­åŒ…å«NaNå€¼ï¼Œä½¿ç”¨1å¡«å……")
                                weights = torch.nan_to_num(weights, nan=1.0)

                            all_weights.append(weights.cpu().numpy())
                            sample_count += weights.shape[0]
                            total_batches += 1

                        print(f"  âœ… å®Œæˆ: æ€»å…±å¤„ç†{total_batches}ä¸ªæ‰¹æ¬¡ï¼Œ{sample_count}ä¸ªæ ·æœ¬")

                    except Exception as e:
                        print(f"  âŒ æå–æƒé‡æ—¶å‡ºé”™: {e}")
                        import traceback
                        print(traceback.format_exc())
                        return None

                if all_weights:
                    weights_combined = np.concatenate(all_weights, axis=0)

                    # æ£€æŸ¥å¹¶å¤„ç†NaNå€¼
                    nan_count = np.isnan(weights_combined).sum()
                    if nan_count > 0:
                        print(f"  âš ï¸  æƒé‡çŸ©é˜µä¸­æœ‰{nan_count}ä¸ªNaNå€¼ï¼Œä½¿ç”¨1å¡«å……")
                        weights_combined = np.nan_to_num(weights_combined, nan=1.0)

                    print(f"  âœ… æå–å®Œæˆ: {weights_combined.shape} (æ ·æœ¬æ•°Ã—ç‰¹å¾æ•°)")
                    return weights_combined
                else:
                    print(f"  âŒ æå–å¤±è´¥: æ²¡æœ‰è·å–åˆ°æƒé‡")
                    return None

            # æå–è®­ç»ƒé›†å’ŒéªŒè¯é›†æƒé‡
            train_weights = extract_weights(gtnnwr, train_set, "è®­ç»ƒé›†")
            val_weights = extract_weights(gtnnwr, val_set, "éªŒè¯é›†")

            if train_weights is not None and val_weights is not None:
                # æ£€æŸ¥å¹¶è°ƒæ•´ç»´åº¦
                expected_cols = len(self.gtnnwr_x_columns)

                print(f"\nğŸ”§ ç»´åº¦æ£€æŸ¥ä¸è°ƒæ•´:")
                print(f"  æœŸæœ›ç‰¹å¾æ•°: {expected_cols}")

                # æ£€æŸ¥è®­ç»ƒé›†æƒé‡ç»´åº¦
                if train_weights.shape[1] != expected_cols:
                    print(f"  âš ï¸  è®­ç»ƒæƒé‡ç»´åº¦ä¸åŒ¹é…: {train_weights.shape[1]} != {expected_cols}")
                    if train_weights.shape[1] == expected_cols + 1:
                        train_weights = train_weights[:, :expected_cols]
                        print(f"  âœ… ä¿®å¤ï¼šå»æ‰æœ€åä¸€åˆ—ï¼Œæ–°å½¢çŠ¶: {train_weights.shape}")
                    elif train_weights.shape[1] > expected_cols:
                        train_weights = train_weights[:, :expected_cols]
                        print(f"  âœ… ä¿®å¤ï¼šæˆªæ–­åˆ°æœŸæœ›é•¿åº¦ï¼Œæ–°å½¢çŠ¶: {train_weights.shape}")
                    else:
                        padding = np.ones((train_weights.shape[0], expected_cols - train_weights.shape[1]))
                        train_weights = np.hstack([train_weights, padding])
                        print(f"  âœ… ä¿®å¤ï¼šå¡«å……åˆ°æœŸæœ›é•¿åº¦ï¼Œæ–°å½¢çŠ¶: {train_weights.shape}")

                # æ£€æŸ¥éªŒè¯é›†æƒé‡ç»´åº¦
                if val_weights.shape[1] != expected_cols:
                    print(f"  âš ï¸  éªŒè¯æƒé‡ç»´åº¦ä¸åŒ¹é…: {val_weights.shape[1]} != {expected_cols}")
                    if val_weights.shape[1] == expected_cols + 1:
                        val_weights = val_weights[:, :expected_cols]
                        print(f"  âœ… ä¿®å¤ï¼šå»æ‰æœ€åä¸€åˆ—ï¼Œæ–°å½¢çŠ¶: {val_weights.shape}")
                    elif val_weights.shape[1] > expected_cols:
                        val_weights = val_weights[:, :expected_cols]
                        print(f"  âœ… ä¿®å¤ï¼šæˆªæ–­åˆ°æœŸæœ›é•¿åº¦ï¼Œæ–°å½¢çŠ¶: {val_weights.shape}")
                    else:
                        padding = np.ones((val_weights.shape[0], expected_cols - val_weights.shape[1]))
                        val_weights = np.hstack([val_weights, padding])
                        print(f"  âœ… ä¿®å¤ï¼šå¡«å……åˆ°æœŸæœ›é•¿åº¦ï¼Œæ–°å½¢çŠ¶: {val_weights.shape}")

                self.logger.debug(f"  æå–åˆ°æƒé‡çŸ©é˜µ: è®­ç»ƒé›†{train_weights.shape}, éªŒè¯é›†{val_weights.shape}")
                return train_weights, val_weights
            else:
                print(f"\nâŒ GTNNWRæƒé‡æå–å¤±è´¥")
                self.logger.warning("  æœªèƒ½æå–åˆ°æƒé‡çŸ©é˜µ")
                return None, None

        except Exception as e:
            print(f"\nâŒ GTNNWRè®­ç»ƒå¤±è´¥: {str(e)}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
            self.logger.warning(f"  GTNNWRè®­ç»ƒå¤±è´¥: {str(e)}")
            return None, None

    def _apply_gtnnwr_weights(self, X, weights, feature_columns, gtnnwr_x_columns):
        """åº”ç”¨GTNNWRæƒé‡åˆ°ç‰¹å¾çŸ©é˜µ

        Args:
            X (np.array): åŸå§‹ç‰¹å¾çŸ©é˜µ
            weights (np.array): æƒé‡çŸ©é˜µ
            feature_columns (list): ç‰¹å¾åˆ—å
            gtnnwr_x_columns (list): GTNNWRç‰¹å¾åˆ—å

        Returns:
            np.array: åŠ æƒåçš„ç‰¹å¾çŸ©é˜µ
        """
        if weights is None:
            self.logger.warning("æƒé‡çŸ©é˜µä¸ºNoneï¼Œè¿”å›åŸå§‹ç‰¹å¾")
            return X

        # âœ… ä¿®å¤1: é¦–å…ˆæ£€æŸ¥æ ·æœ¬æ•°æ˜¯å¦åŒ¹é…
        if X.shape[0] != weights.shape[0]:
            self.logger.error(f"âŒ æ ·æœ¬æ•°ä¸åŒ¹é…: Xæœ‰{X.shape[0]}ä¸ªæ ·æœ¬, æƒé‡æœ‰{weights.shape[0]}ä¸ªæ ·æœ¬")

            # å°è¯•ä¿®å¤æ ·æœ¬æ•°ä¸åŒ¹é…çš„é—®é¢˜
            if weights.shape[0] < X.shape[0]:
                # å¦‚æœæƒé‡æ ·æœ¬æ•°è¾ƒå°‘ï¼Œé‡å¤æƒé‡ä»¥åŒ¹é…Xçš„æ ·æœ¬æ•°
                repeat_times = int(np.ceil(X.shape[0] / weights.shape[0]))
                weights_repeated = np.tile(weights, (repeat_times, 1))
                weights = weights_repeated[:X.shape[0], :]
                self.logger.warning(f"âœ… æƒé‡æ ·æœ¬æ•°ä¸è¶³ï¼Œé‡å¤æƒé‡åˆ°{weights.shape[0]}ä¸ªæ ·æœ¬")
            else:
                # å¦‚æœæƒé‡æ ·æœ¬æ•°è¾ƒå¤šï¼Œæˆªæ–­åˆ°Xçš„æ ·æœ¬æ•°
                weights = weights[:X.shape[0], :]
                self.logger.warning(f"âœ… æƒé‡æ ·æœ¬æ•°è¿‡å¤šï¼Œæˆªæ–­åˆ°{weights.shape[0]}ä¸ªæ ·æœ¬")

        # âœ… ä¿®å¤2: å¤„ç†ç»´åº¦ä¸åŒ¹é…é—®é¢˜
        if weights.shape[1] != len(gtnnwr_x_columns):
            self.logger.warning(f"âš ï¸ æƒé‡çŸ©é˜µç‰¹å¾æ•°({weights.shape[1]})ä¸GTNNWRç‰¹å¾æ•°({len(gtnnwr_x_columns)})ä¸åŒ¹é…")

            # è‡ªåŠ¨è°ƒæ•´æƒé‡ç»´åº¦
            if weights.shape[1] > len(gtnnwr_x_columns):
                # å¦‚æœæƒé‡æ˜¯35åˆ—ï¼ŒGTNNWRç‰¹å¾æ˜¯34åˆ—ï¼Œå»æ‰æœ€åä¸€åˆ—
                weights = weights[:, :len(gtnnwr_x_columns)]
                self.logger.info(f"âœ… è‡ªåŠ¨è°ƒæ•´ï¼šæˆªæ–­æƒé‡çŸ©é˜µåˆ° {weights.shape[1]} åˆ—")
            elif weights.shape[1] < len(gtnnwr_x_columns):
                # å¦‚æœæƒé‡åˆ—æ•°å°‘ï¼Œå¡«å……1.0
                padding = np.ones((weights.shape[0], len(gtnnwr_x_columns) - weights.shape[1]))
                weights = np.hstack([weights, padding])
                self.logger.info(f"âœ… è‡ªåŠ¨è°ƒæ•´ï¼šå¡«å……æƒé‡çŸ©é˜µåˆ° {weights.shape[1]} åˆ—")

        # æ£€æŸ¥è¾“å…¥ä¸­çš„NaN
        x_nan_count = np.isnan(X).sum()
        if x_nan_count > 0:
            self.logger.warning(f"âš ï¸ è¾“å…¥ç‰¹å¾çŸ©é˜µä¸­æœ‰ {x_nan_count} ä¸ªNaNå€¼ï¼Œä½¿ç”¨åˆ—å‡å€¼å¡«å……")
            col_means = np.nanmean(X, axis=0)
            for i in range(X.shape[1]):
                X[:, i] = np.where(np.isnan(X[:, i]), col_means[i], X[:, i])

        # æ£€æŸ¥æƒé‡ä¸­çš„NaN
        weights_nan_count = np.isnan(weights).sum()
        if weights_nan_count > 0:
            self.logger.warning(f"âš ï¸ æƒé‡çŸ©é˜µä¸­æœ‰ {weights_nan_count} ä¸ªNaNå€¼ï¼Œä½¿ç”¨1å¡«å……")
            weights = np.nan_to_num(weights, nan=1.0)

        # åˆ›å»ºç‰¹å¾æ˜ å°„ï¼šç‰¹å¾åˆ—åˆ°GTNNWRç‰¹å¾åˆ—çš„ç´¢å¼•
        feature_to_gtnnwr = {}
        for i, feat in enumerate(feature_columns):
            if feat in gtnnwr_x_columns:
                feature_to_gtnnwr[i] = gtnnwr_x_columns.index(feat)

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        matched_count = len(feature_to_gtnnwr)
        self.logger.info(f"ğŸ” ç‰¹å¾åŒ¹é…: åŒ¹é…äº† {matched_count}/{len(feature_columns)} ä¸ªç‰¹å¾")

        if matched_count == 0:
            self.logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ç‰¹å¾ï¼Œæ— æ³•åº”ç”¨æƒé‡")
            return X

        # âœ… å…³é”®ä¿®å¤ï¼šåº”ç”¨æƒé‡ï¼Œå³ä½¿æœ‰NaN
        X_weighted = X.copy()
        changed_count = 0

        for feat_idx, gtnnwr_idx in feature_to_gtnnwr.items():
            # è·å–åŸå§‹ç‰¹å¾å€¼å’Œæƒé‡
            original_values = X[:, feat_idx]
            weight_values = weights[:, gtnnwr_idx]

            # æ£€æŸ¥å¹¶å¤„ç†NaN
            original_nan = np.isnan(original_values).sum()
            weight_nan = np.isnan(weight_values).sum()

            if original_nan > 0:
                original_values = np.nan_to_num(original_values, nan=0.0)

            if weight_nan > 0:
                weight_values = np.nan_to_num(weight_values, nan=1.0)

            # åº”ç”¨æƒé‡ï¼šX Ã— weight
            weighted_values = original_values * weight_values

            # æ£€æŸ¥æ˜¯å¦çœŸçš„æ”¹å˜äº†ï¼ˆå¿½ç•¥NaNï¼‰
            mask = ~np.isnan(original_values) & ~np.isnan(weighted_values)
            if mask.any():
                if not np.allclose(original_values[mask], weighted_values[mask], rtol=1e-10):
                    changed_count += 1

            X_weighted[:, feat_idx] = weighted_values

        # æ·»åŠ éªŒè¯è¾“å‡º
        change_ratio = changed_count / matched_count if matched_count > 0 else 0
        self.logger.info(f"âœ… æƒé‡åº”ç”¨ç»“æœ: ä¿®æ”¹äº† {changed_count}/{matched_count} ä¸ªç‰¹å¾ ({change_ratio:.1%})")

        # æ£€æŸ¥å‡ ä¸ªå…³é”®ç‰¹å¾çš„å˜åŒ–
        if changed_count > 0:
            key_features = ['elevation', 'X', 'Y', 'Z', 'slope', 'doy']
            for feat in key_features:
                if feat in feature_columns and feat in gtnnwr_x_columns:
                    feat_idx = feature_columns.index(feat)
                    gtnnwr_idx = gtnnwr_x_columns.index(feat)

                    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬
                    if len(X) > 0 and len(weights) > 0:
                        if (not np.isnan(X[0, feat_idx]) and
                                not np.isnan(X_weighted[0, feat_idx]) and
                                feat_idx < weights.shape[1]):
                            original = X[0, feat_idx]
                            weighted = X_weighted[0, feat_idx]
                            weight_val = weights[0, gtnnwr_idx]

                            if abs(weighted - original) > 1e-10:
                                self.logger.info(f"   {feat}: {original:.4f} Ã— {weight_val:.4f} = {weighted:.4f} "
                                                 f"(Î”={weighted - original:+.4f})")

        # æ£€æŸ¥è¾“å‡ºä¸­çš„NaN
        output_nan_count = np.isnan(X_weighted).sum()
        if output_nan_count > 0:
            self.logger.warning(f"âš ï¸ åŠ æƒåçš„ç‰¹å¾çŸ©é˜µä¸­æœ‰ {output_nan_count} ä¸ªNaNå€¼ï¼Œä½¿ç”¨åŸå§‹å€¼")
            X_weighted = np.where(np.isnan(X_weighted), X, X_weighted)

        # éªŒè¯æœ€ç»ˆå½¢çŠ¶
        if X_weighted.shape != X.shape:
            self.logger.error(f"âŒ å½¢çŠ¶ä¸åŒ¹é…: åŠ æƒå{X_weighted.shape} != åŸå§‹{X.shape}")
            return X  # è¿”å›åŸå§‹ç‰¹å¾é¿å…è¿›ä¸€æ­¥é”™è¯¯

        return X_weighted


    def cross_validate(self, X, y, groups, cv_type='station', gtnnwr_data=None):
        """æ‰§è¡Œå¸¦GTNNWRæƒé‡çš„äº¤å‰éªŒè¯

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            y (np.array): ç›®æ ‡å˜é‡
            groups (np.array): åˆ†ç»„ä¿¡æ¯
            cv_type (str): äº¤å‰éªŒè¯ç±»å‹ ('station' æˆ– 'yearly')
            gtnnwr_data (pd.DataFrame): GTNNWRéœ€è¦çš„å®Œæ•´æ•°æ®

        Returns:
            dict: äº¤å‰éªŒè¯ç»“æœ
        """
        from sklearn.model_selection import LeaveOneGroupOut

        logo = LeaveOneGroupOut()
        all_predictions = []
        all_true_values = []
        fold_results = {}

        fold_maes = []
        fold_rmses = []
        fold_rs = []
        fold_samples = []

        unique_groups = np.unique(groups)
        total_folds = len(unique_groups)

        print("\n" + "=" * 100)
        print(f"ğŸš€ å¼€å§‹{cv_type}äº¤å‰éªŒè¯ï¼Œå…±{total_folds}ä¸ªæŠ˜å ")
        print(f"ä½¿ç”¨GTNNWRæƒé‡å¢å¼º: {self.use_gtnnwr}")
        print(f"NaNå¤„ç†ç­–ç•¥: {self.nan_strategy}")
        print("=" * 100)

        self.logger.info(f"å¼€å§‹{cv_type}äº¤å‰éªŒè¯ï¼Œå…±{total_folds}ä¸ªæŠ˜å ...")
        self.logger.info(f"ä½¿ç”¨GTNNWRæƒé‡å¢å¼º: {self.use_gtnnwr}")

        for fold, (train_idx, val_idx) in enumerate(logo.split(X, y, groups)):
            group_id = groups[val_idx[0]]
            train_size = len(train_idx)
            val_size = len(val_idx)

            print("\n" + "=" * 80)
            print(f"ğŸ¯ {cv_type} Fold {fold + 1}/{total_folds}: åˆ†ç»„ {group_id}")
            print(f"   è®­ç»ƒé›†: {train_size}æ ·æœ¬, éªŒè¯é›†: {val_size}æ ·æœ¬")
            print("=" * 80)

            self.logger.info(
                f"{cv_type} Fold {fold + 1}/{total_folds}: {group_id} (è®­ç»ƒé›†{train_size}, éªŒè¯é›†{val_size})")

            # åˆ†å‰²æ•°æ®
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # GTNNWRæƒé‡å¢å¼º
            if self.use_gtnnwr and gtnnwr_data is not None:
                print(f"\nğŸ“Š GTNNWRæƒé‡å¢å¼ºé˜¶æ®µ")

                # è·å–å½“å‰æŠ˜å çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®
                train_data_fold = gtnnwr_data.iloc[train_idx].copy()
                val_data_fold = gtnnwr_data.iloc[val_idx].copy()

                print(f"  è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_data_fold.shape}")
                print(f"  éªŒè¯æ•°æ®å½¢çŠ¶: {val_data_fold.shape}")

                # è®­ç»ƒGTNNWRå¹¶æå–æƒé‡
                print(f"\nğŸ§  è®­ç»ƒGTNNWRæ¨¡å‹...")
                train_weights, val_weights = self._train_gtnnwr_for_fold(
                    train_data_fold,
                    val_data_fold
                )

                if train_weights is not None and val_weights is not None:
                    print(f"\nâœ… GTNNWRè®­ç»ƒå®Œæˆï¼Œå‡†å¤‡åº”ç”¨æƒé‡")

                    # åº”ç”¨æƒé‡
                    print(f"\nğŸ”„ åº”ç”¨æƒé‡åˆ°ç‰¹å¾çŸ©é˜µ...")
                    X_train = self._apply_gtnnwr_weights(
                        X_train, train_weights,
                        self.feature_columns, self.gtnnwr_x_columns
                    )
                    X_val = self._apply_gtnnwr_weights(
                        X_val, val_weights,
                        self.feature_columns, self.gtnnwr_x_columns
                    )
                else:
                    print(f"\nâŒ GTNNWRæƒé‡æå–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾")
            else:
                print(f"\nğŸ“ æœªä½¿ç”¨GTNNWRæƒé‡å¢å¼º")

            # è®­ç»ƒXGBoostæ¨¡å‹
            print(f"\nğŸŒ² è®­ç»ƒXGBoostæ¨¡å‹...")
            import xgboost as xgb
            model = xgb.XGBRegressor(**self.params)
            model.fit(X_train, y_train)

            # é¢„æµ‹
            print(f"  è¿›è¡Œé¢„æµ‹...")
            y_pred = model.predict(X_val)

            # å­˜å‚¨ç»“æœ
            all_predictions.extend(y_pred)
            all_true_values.extend(y_val)

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            fold_metrics = self._evaluate_predictions(y_val, y_pred)
            fold_results[group_id] = fold_metrics

            fold_maes.append(fold_metrics['MAE'])
            fold_rmses.append(fold_metrics['RMSE'])
            fold_rs.append(fold_metrics['R'])
            fold_samples.append(fold_metrics['æ ·æœ¬æ•°'])

            r_display = fold_metrics['R']
            r_str = f"{r_display:.3f}" if not np.isnan(r_display) else "NaN"

            # æ‰“å°Foldç»“æœ
            print(f"\nğŸ“Š Fold {fold + 1} æ€§èƒ½æŒ‡æ ‡:")
            print(f"  MAE:  {fold_metrics['MAE']:.3f} mm")
            print(f"  RMSE: {fold_metrics['RMSE']:.3f} mm")
            print(f"  R:    {r_str}")
            print(f"  æ ·æœ¬æ•°: {fold_metrics['æ ·æœ¬æ•°']}")

            self.logger.info(
                f"  Fold {fold + 1} æ€§èƒ½: MAE={fold_metrics['MAE']:.3f}, R={r_str}"
            )

        # è®¡ç®—æ€»ä½“æ€§èƒ½
        overall_metrics = self._evaluate_predictions(
            np.array(all_true_values),
            np.array(all_predictions)
        )

        # è®¡ç®—ç»Ÿè®¡é‡
        def safe_statistic(values, func):
            valid_values = [v for v in values if not np.isnan(v)]
            if len(valid_values) == 0:
                return np.nan
            return func(valid_values)

        mean_metrics = {
            'MAE': safe_statistic(fold_maes, np.mean),
            'RMSE': safe_statistic(fold_rmses, np.mean),
            'R': safe_statistic(fold_rs, np.mean),
            'æ ·æœ¬æ•°': np.sum(fold_samples)
        }

        median_metrics = {
            'MAE': safe_statistic(fold_maes, np.median),
            'RMSE': safe_statistic(fold_rmses, np.median),
            'R': safe_statistic(fold_rs, np.median),
            'æ ·æœ¬æ•°': np.sum(fold_samples)
        }

        std_metrics = {
            'MAE': safe_statistic(fold_maes, np.std),
            'RMSE': safe_statistic(fold_rmses, np.std),
            'R': safe_statistic(fold_rs, np.std)
        }

        print("\n" + "=" * 100)
        print(f"ğŸ‰ {cv_type}äº¤å‰éªŒè¯å®Œæˆ!")
        print("=" * 100)

        print(f"\nğŸ“ˆ èšåˆæ€§èƒ½æŒ‡æ ‡:")
        print(f"  MAE:  {overall_metrics['MAE']:.3f} mm")
        print(f"  RMSE: {overall_metrics['RMSE']:.3f} mm")
        print(f"  R:    {overall_metrics['R']:.3f}")
        print(f"  æ€»æ ·æœ¬æ•°: {overall_metrics['æ ·æœ¬æ•°']}")

        self.logger.info(f"âœ… {cv_type}äº¤å‰éªŒè¯å®Œæˆ")
        self.logger.info(f"  èšåˆæ€§èƒ½: MAE={overall_metrics['MAE']:.3f}mm, R={overall_metrics['R']:.3f}")

        return {
            'overall': overall_metrics,
            'mean': mean_metrics,
            'median': median_metrics,
            'std': std_metrics,
            'by_fold': fold_results,
            'predictions': np.array(all_predictions),
            'true_values': np.array(all_true_values),
            'folds': total_folds,
            'fold_metrics': {
                'MAE': fold_maes,
                'RMSE': fold_rmses,
                'R': fold_rs,
                'samples': fold_samples
            }
        }

    def _evaluate_predictions(self, y_true, y_pred):
        """è¯„ä¼°é¢„æµ‹ç»“æœ"""
        from sklearn.metrics import mean_absolute_error, mean_squared_error
        from scipy.stats import pearsonr

        mask = ~(np.isnan(y_true) | np.isnan(y_pred))
        y_true_clean = y_true[mask]
        y_pred_clean = y_pred[mask]

        if len(y_true_clean) == 0:
            return {
                'MAE': np.nan,
                'RMSE': np.nan,
                'R': np.nan,
                'R_pvalue': np.nan,
                'æ ·æœ¬æ•°': 0,
                'æ€»æ ·æœ¬æ•°': len(y_true),
                'æœ‰æ•ˆæ ·æœ¬æ¯”ä¾‹': 0.0
            }

        mae = mean_absolute_error(y_true_clean, y_pred_clean)
        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))

        def safe_pearsonr(x, y):
            if len(x) <= 1 or np.all(x == x[0]) or np.all(y == y[0]):
                return np.nan, np.nan
            if np.std(x) == 0 or np.std(y) == 0:
                return np.nan, np.nan
            try:
                return pearsonr(x, y)
            except:
                return np.nan, np.nan

        r, p_value = safe_pearsonr(y_true_clean, y_pred_clean)

        return {
            'MAE': mae,
            'RMSE': rmse,
            'R': r,
            'R_pvalue': p_value,
            'æ ·æœ¬æ•°': len(y_true_clean),
            'æ€»æ ·æœ¬æ•°': len(y_true),
            'æœ‰æ•ˆæ ·æœ¬æ¯”ä¾‹': len(y_true_clean) / len(y_true) if len(y_true) > 0 else 0
        }


    def run_complete_analysis(self, df, output_dir=None):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        self.logger.info("=" * 70)
        self.logger.info("ğŸš€ å¼€å§‹GTNNW-XGBoostå®Œæ•´åˆ†ææµç¨‹")
        self.logger.info(f"ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»: {self.use_feature_mahalanobis}")
        self.logger.info("=" * 70)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = f"./gtnnw_xgboost_results_{timestamp}"

        os.makedirs(output_dir, exist_ok=True)
        self.logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")

        try:
            # 1. æ•°æ®é¢„å¤„ç†
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 1: æ•°æ®é¢„å¤„ç†")
            self.logger.info("=" * 50)

            X, y, station_groups, year_groups, gtnnwr_data = self.preprocess_data(df, is_training=True)

            results = {
                'preprocessing': {
                    'samples': len(X),
                    'features': len(self.feature_columns),
                    'stations': len(np.unique(station_groups)),
                    'years': len(np.unique(year_groups)),
                    'use_gtnnwr': self.use_gtnnwr,
                    'use_feature_mahalanobis': self.use_feature_mahalanobis,
                    'nan_strategy': self.nan_strategy,
                    'nan_fill_stats': self.nan_fill_stats
                }
            }

            # 2. å¹´åº¦äº¤å‰éªŒè¯
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 2: å¹´åº¦äº¤å‰éªŒè¯")
            self.logger.info("=" * 50)

            results['yearly_cv'] = self.cross_validate(
                X, y, year_groups, 'yearly', gtnnwr_data
            )

            # 3. ç«™ç‚¹äº¤å‰éªŒè¯
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 3: ç«™ç‚¹äº¤å‰éªŒè¯")
            self.logger.info("=" * 50)

            results['station_cv'] = self.cross_validate(
                X, y, station_groups, 'station', gtnnwr_data
            )

            # 4. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 4: è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
            self.logger.info("=" * 50)

            results['final_model'] = self.train_final_model(X, y, gtnnwr_data)

            # 5. ç‰¹å¾é‡è¦æ€§åˆ†æ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 5: ç‰¹å¾é‡è¦æ€§åˆ†æ")
            self.logger.info("=" * 50)

            results['feature_importance'] = self.get_feature_importance()

            # 6. ä¿å­˜ç»“æœ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 6: ä¿å­˜ç»“æœ")
            self.logger.info("=" * 50)

            self._save_results(results, output_dir)

            # 7. ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report(results)
            print(report)

            self.logger.info("ğŸ¯ å®Œæ•´åˆ†æå®Œæˆï¼")
            return results

        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææµç¨‹å¤±è´¥: {str(e)}")
            raise

    def _save_results(self, results, output_dir):
        """ä¿å­˜ç»“æœ"""
        try:
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            if 'final_model' in results:
                model_path = f'{output_dir}/final_model.pkl'
                joblib.dump(results['final_model'], model_path)
                self.logger.info(f"âœ… æ¨¡å‹ä¿å­˜: {model_path}")

            # ä¿å­˜NaNå¤„ç†ä¿¡æ¯
            nan_info_path = f'{output_dir}/nan_handling_info.json'
            nan_info = {
                'strategy': self.nan_strategy,
                'fill_values': self.nan_fill_values,
                'fill_stats': self.nan_fill_stats
            }
            with open(nan_info_path, 'w', encoding='utf-8') as f:
                json.dump(nan_info, f, indent=2, ensure_ascii=False,
                          default=lambda x: float(x) if isinstance(x, (np.float32, np.float64)) else x)
            self.logger.info(f"âœ… NaNå¤„ç†ä¿¡æ¯ä¿å­˜: {nan_info_path}")

            # ä¿å­˜è¯¦ç»†ç»“æœ
            eval_results = {
                'training_info': {
                    'timestamp': datetime.now().isoformat(),
                    'feature_columns': self.feature_columns,
                    'gtnnwr_x_columns': self.gtnnwr_x_columns,
                    'use_gtnnwr': self.use_gtnnwr,
                    'use_feature_mahalanobis': self.use_feature_mahalanobis,
                    'nan_strategy': self.nan_strategy,
                    'total_samples': results.get('preprocessing', {}).get('samples', 0)
                },
                'model_parameters': self.params,
                'gtnnwr_parameters': self.gtnnwr_params,
                'station_cross_validation': results.get('station_cv', {}),
                'yearly_cross_validation': results.get('yearly_cv', {})
            }

            eval_path = f'{output_dir}/evaluation_results.json'
            with open(eval_path, 'w', encoding='utf-8') as f:
                json.dump(eval_results, f, indent=2, ensure_ascii=False, default=float)
            self.logger.info(f"âœ… è¯¦ç»†è¯„ä¼°ç»“æœä¿å­˜: {eval_path}")

            # ç”Ÿæˆå¯è§†åŒ–
            self._create_scatter_plots(results, output_dir)

        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")

    def _generate_report(self, results):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("ğŸ“Š GTNNW-XGBoostæ¨¡å‹åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"ä½¿ç”¨GTNNWRæƒé‡å¢å¼º: {self.use_gtnnwr}")
        report_lines.append(f"ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»: {self.use_feature_mahalanobis}")
        report_lines.append(f"NaNå¤„ç†ç­–ç•¥: {self.nan_strategy}")
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")

        # ç«™ç‚¹CVç»“æœ
        if 'station_cv' in results:
            station = results['station_cv']
            report_lines.append("ğŸ“ ç«™ç‚¹äº¤å‰éªŒè¯ (ç©ºé—´è¯„ä¼°):")
            report_lines.append(f"  èšåˆMAE: {station['overall']['MAE']:.3f} mm")
            report_lines.append(f"  èšåˆRMSE: {station['overall']['RMSE']:.3f} mm")
            report_lines.append(f"  èšåˆR: {station['overall']['R']:.3f}")
            report_lines.append(f"  æŠ˜å æ•°: {station['folds']}")
            report_lines.append("")

        # å¹´åº¦CVç»“æœ
        if 'yearly_cv' in results:
            yearly = results['yearly_cv']
            report_lines.append("ğŸ“… å¹´åº¦äº¤å‰éªŒè¯ (æ—¶é—´è¯„ä¼°):")
            report_lines.append(f"  èšåˆMAE: {yearly['overall']['MAE']:.3f} mm")
            report_lines.append(f"  èšåˆRMSE: {yearly['overall']['RMSE']:.3f} mm")
            report_lines.append(f"  èšåˆR: {yearly['overall']['R']:.3f}")
            report_lines.append(f"  æŠ˜å æ•°: {yearly['folds']}")
            report_lines.append("")

        report_lines.append("\n" + "=" * 80)
        return "\n".join(report_lines)


# ä¾¿æ·ä½¿ç”¨å‡½æ•° - æ–°å¢æ”¯æŒç‰¹å¾é©¬æ°è·ç¦»
def train_gtnnw_xgboost_model(data_df, output_dir=None, use_gtnnwr=True,
                              nan_strategy='median', nan_fill_value=0.0,
                              use_feature_mahalanobis=False,
                              feature_columns_for_distance=None):
    """ä¾¿æ·å‡½æ•°ï¼šè®­ç»ƒGTNNW-XGBoostæ¨¡å‹

    Args:
        data_df (pd.DataFrame): åŒ…å«ç‰¹å¾å’ŒSWEçš„æ•°æ®
        output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„
        use_gtnnwr (bool): æ˜¯å¦ä½¿ç”¨GTNNWRæƒé‡
        nan_strategy (str): NaNå¤„ç†ç­–ç•¥
        nan_fill_value (float): å¡«å……NaNçš„å€¼
        use_feature_mahalanobis (bool): æ˜¯å¦ä½¿ç”¨ç‰¹å¾é©¬æ°è·ç¦»
        feature_columns_for_distance (list): ç”¨äºé©¬æ°è·ç¦»è®¡ç®—çš„ç‰¹å¾åˆ—

    Returns:
        dict: åŒ…å«æ‰€æœ‰è®­ç»ƒç»“æœçš„å­—å…¸
    """
    trainer = GTNNW_XGBoostTrainer(
        use_gtnnwr=use_gtnnwr,
        nan_strategy=nan_strategy,
        nan_fill_value=nan_fill_value,
        use_feature_mahalanobis=use_feature_mahalanobis,
        feature_columns_for_distance=feature_columns_for_distance
    )
    return trainer.run_complete_analysis(data_df, output_dir)


# å¯¹æ¯”å®éªŒå‡½æ•° - æ–°å¢æ”¯æŒç‰¹å¾é©¬æ°è·ç¦»
def compare_models(data_df, output_dir=None):
    """å¯¹æ¯”ä¸åŒé…ç½®çš„æ€§èƒ½"""

    print("=" * 80)
    print("ğŸ”¬ å¼€å§‹æ¨¡å‹å¯¹æ¯”å®éªŒ")
    print("=" * 80)

    # 1. çº¯XGBoost
    print("\n1. è®­ç»ƒçº¯XGBoostæ¨¡å‹...")
    xgb_trainer = GTNNW_XGBoostTrainer(use_gtnnwr=False, nan_strategy='median')
    xgb_results = xgb_trainer.run_complete_analysis(
        data_df,
        output_dir=os.path.join(output_dir, "xgboost_only") if output_dir else None
    )

    # 2. GTNNW-XGBoost (æ— ç‰¹å¾é©¬æ°è·ç¦»)
    print("\n2. è®­ç»ƒGTNNW-XGBoostæ¨¡å‹ (æ— ç‰¹å¾é©¬æ°è·ç¦»)...")
    gtnnw_trainer1 = GTNNW_XGBoostTrainer(use_gtnnwr=True, nan_strategy='median',
                                          use_feature_mahalanobis=False)
    gtnnw_results1 = gtnnw_trainer1.run_complete_analysis(
        data_df,
        output_dir=os.path.join(output_dir, "gtnnw_xgboost_no_mahalanobis") if output_dir else None
    )

    # 3. GTNNW-XGBoost (æœ‰ç‰¹å¾é©¬æ°è·ç¦»)
    print("\n3. è®­ç»ƒGTNNW-XGBoostæ¨¡å‹ (æœ‰ç‰¹å¾é©¬æ°è·ç¦»)...")
    gtnnw_trainer2 = GTNNW_XGBoostTrainer(use_gtnnwr=True, nan_strategy='median',
                                          use_feature_mahalanobis=True)
    gtnnw_results2 = gtnnw_trainer2.run_complete_analysis(
        data_df,
        output_dir=os.path.join(output_dir, "gtnnw_xgboost_with_mahalanobis") if output_dir else None
    )

    # 4. å¯¹æ¯”åˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ")
    print("=" * 80)

    results_to_compare = [
        ("çº¯XGBoost", xgb_results),
        ("GTNNW-XGBoost (æ— é©¬æ°è·ç¦»)", gtnnw_results1),
        ("GTNNW-XGBoost (æœ‰é©¬æ°è·ç¦»)", gtnnw_results2)
    ]

    for name, res in results_to_compare:
        if 'station_cv' in res and 'overall' in res['station_cv']:
            r = res['station_cv']['overall']['R']
            if not np.isnan(r):
                print(f"{name}:")
                print(f"  ç«™ç‚¹CV R = {r:.3f}")
                print(f"  MAE = {res['station_cv']['overall']['MAE']:.3f} mm")
                print(f"  RMSE = {res['station_cv']['overall']['RMSE']:.3f} mm")
                print()

    return {
        'xgboost': xgb_results,
        'gtnnw_xgboost_no_mahalanobis': gtnnw_results1,
        'gtnnw_xgboost_with_mahalanobis': gtnnw_results2
    }