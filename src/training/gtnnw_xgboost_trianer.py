import logging
import pandas as pd
import numpy as np
import xgboost as xgb
from matplotlib import pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
from scipy.stats import pearsonr
from sklearn.model_selection import LeaveOneGroupOut
import os
import joblib
import json
from datetime import datetime
import seaborn as sns
from scipy import stats

# æ–°å¢å¯¼å…¥
import torch
import torch.nn as nn
from gnnwr import models, datasets
import warnings

warnings.filterwarnings('ignore')

logger = logging.getLogger("GTNNW_XGBoostTrainer")


class GTNNW_XGBoostTrainer:
    """GTNNW-XGBoostè®­ç»ƒå™¨ - é›†æˆGTNNWRæƒé‡çŸ©é˜µä¸XGBoost"""

    # é»˜è®¤XGBoostå‚æ•°
    DEFAULT_PARAMS = {
        'n_estimators': 60,
        'learning_rate': 0.17,
        'max_depth': 5,
        'min_child_weight': 5,
        'gamma': 0,
        'subsample': 0.8,
        'colsample_bytree': 0.5,
        'reg_alpha': 0.05,
        'random_state': 42,
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse'
    }

    # GTNNWRå‚æ•°
    DEFAULT_GTNNWR_PARAMS = {
        'dense_layers': [[3], [512, 256, 64]],  # ä¿®æ”¹ï¼šå°† graph_layers æ”¹ä¸º dense_layers
        'drop_out': 0.4,
        'optimizer': "Adadelta",
        'optimizer_params': {
            "scheduler": "MultiStepLR",
            "scheduler_milestones": [1000, 2000, 3000, 4000],
            "scheduler_gamma": 0.8,
        },
        'max_epoch': 3000,
        'early_stop': 1000,
        'print_frequency': 100
    }

    def __init__(self, params=None, gtnnwr_params=None, use_gtnnwr=True,
                 nan_strategy='median', nan_fill_value=0.0):
        """åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            params (dict, optional): XGBoostå‚æ•°
            gtnnwr_params (dict, optional): GTNNWRå‚æ•°
            use_gtnnwr (bool): æ˜¯å¦ä½¿ç”¨GTNNWRæƒé‡å¢å¼º
            nan_strategy (str): NaNå¤„ç†ç­–ç•¥ ('mean', 'median', 'zero', 'drop')
            nan_fill_value (float): å¡«å……NaNçš„å€¼ï¼ˆå½“nan_strategyä¸ºè‡ªå®šä¹‰å€¼æ—¶ï¼‰
        """
        self.logger = logger
        self.model = None
        self.feature_columns = None
        self.target_column = 'swe'
        self.use_gtnnwr = use_gtnnwr
        self.nan_strategy = nan_strategy
        self.nan_fill_value = nan_fill_value

        # å­˜å‚¨å¡«å……å€¼ç”¨äºåç»­é¢„æµ‹
        self.nan_fill_values = {}
        self.nan_fill_stats = {}

        # å®šä¹‰GTNNWRç‰¹å¾åˆ—ï¼ˆä¸åŸå§‹GTNNWRè®­ç»ƒä¿æŒä¸€è‡´ï¼‰
        self.gtnnwr_x_columns = ['aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation',
                                 'std_slope',
                                 'std_eastness', 'std_tpi', 'std_curvature1', 'std_curvature2', 'std_high',
                                 'std_aspect',
                                 'glsnow', 'cswe', 'snow_depth_snow_depth', 'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'doy',
                                 'gldas',
                                 'year', 'month', 'scp_start', 'scp_end', 'd1', 'd2', 'X', 'Y', 'Z', 'da', 'db', 'dc',
                                 'dd']

        # GTNNWRéœ€è¦ç©ºé—´åˆ—å’Œæ—¶é—´åˆ—
        self.gtnnwr_spatial_columns = ['X', 'Y']  # ä½¿ç”¨X, Yä½œä¸ºç©ºé—´åˆ—
        self.gtnnwr_temp_columns = ['year', 'month', 'doy']  # æ—¶é—´åˆ—
        self.gtnnwr_id_column = 'id'  # IDåˆ—ï¼Œéœ€è¦åœ¨æ•°æ®é¢„å¤„ç†ä¸­åˆ›å»º
        self.gtnnwr_y_column = ['swe']

        # æ›´æ–°å‚æ•°
        self.params = self.DEFAULT_PARAMS.copy()
        if params:
            self.params.update(params)

        self.gtnnwr_params = self.DEFAULT_GTNNWR_PARAMS.copy()
        if gtnnwr_params:
            self.gtnnwr_params.update(gtnnwr_params)

        self.logger.info(f"åˆå§‹åŒ–GTNNW-XGBoostè®­ç»ƒå™¨")
        self.logger.info(f"XGBoostå‚æ•°: {self.params}")
        self.logger.info(f"ä½¿ç”¨GTNNWRæƒé‡å¢å¼º: {self.use_gtnnwr}")
        self.logger.info(f"NaNå¤„ç†ç­–ç•¥: {self.nan_strategy}")

    def _handle_nan_values(self, df, is_training=True, fill_values=None):
        """å¤„ç†NaNå€¼

        Args:
            df (pd.DataFrame): è¾“å…¥æ•°æ®
            is_training (bool): æ˜¯å¦ä¸ºè®­ç»ƒé˜¶æ®µ
            fill_values (dict): é¢„è®¡ç®—çš„å¡«å……å€¼

        Returns:
            pd.DataFrame: å¤„ç†åçš„æ•°æ®
        """
        self.logger.info(f"å¤„ç†NaNå€¼ - é˜¶æ®µ: {'è®­ç»ƒ' if is_training else 'é¢„æµ‹'}")

        df_processed = df.copy()

        # ç»Ÿè®¡NaNå€¼
        nan_stats = df_processed.isna().sum()
        total_nan = nan_stats.sum()
        total_cells = df_processed.size

        if total_nan > 0:
            nan_percentage = (total_nan / total_cells) * 100
            self.logger.info(f"å‘ç°NaNå€¼: {total_nan}/{total_cells} ({nan_percentage:.2f}%)")

            # æŒ‰åˆ—ç»Ÿè®¡NaN
            nan_columns = nan_stats[nan_stats > 0]
            for col, nan_count in nan_columns.items():
                nan_pct = (nan_count / len(df_processed)) * 100
                self.logger.info(f"  åˆ— '{col}': {nan_count} NaN ({nan_pct:.2f}%)")

        # å¤„ç†ä¸åŒåˆ—ç±»å‹çš„NaN
        for col in df_processed.columns:
            if df_processed[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                # æ•°å€¼åˆ—å¤„ç†
                nan_count = df_processed[col].isna().sum()

                if nan_count > 0:
                    if self.nan_strategy == 'drop' and is_training:
                        # åˆ é™¤åŒ…å«NaNçš„è¡Œï¼ˆä»…è®­ç»ƒé˜¶æ®µï¼‰
                        self.logger.warning(f"åˆ é™¤åŒ…å«åˆ— '{col}' NaNçš„ {nan_count} è¡Œ")
                        df_processed = df_processed.dropna(subset=[col])
                    else:
                        # è®¡ç®—æˆ–ä½¿ç”¨å¡«å……å€¼
                        if is_training:
                            if self.nan_strategy == 'mean':
                                fill_value = df_processed[col].mean()
                                self.nan_fill_values[col] = fill_value
                                self.logger.info(f"åˆ— '{col}' ä½¿ç”¨å‡å€¼å¡«å……: {fill_value:.4f}")
                            elif self.nan_strategy == 'median':
                                fill_value = df_processed[col].median()
                                self.nan_fill_values[col] = fill_value
                                self.logger.info(f"åˆ— '{col}' ä½¿ç”¨ä¸­ä½æ•°å¡«å……: {fill_value:.4f}")
                            elif self.nan_strategy == 'zero':
                                fill_value = 0
                                self.nan_fill_values[col] = fill_value
                                self.logger.info(f"åˆ— '{col}' ä½¿ç”¨0å¡«å……")
                            else:  # è‡ªå®šä¹‰å€¼
                                fill_value = self.nan_fill_value
                                self.nan_fill_values[col] = fill_value
                                self.logger.info(f"åˆ— '{col}' ä½¿ç”¨è‡ªå®šä¹‰å€¼å¡«å……: {fill_value}")

                            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
                            self.nan_fill_stats[col] = {
                                'strategy': self.nan_strategy,
                                'fill_value': fill_value,
                                'original_nan_count': nan_count,
                                'original_mean': df_processed[col].mean(),
                                'original_median': df_processed[col].median(),
                                'original_std': df_processed[col].std()
                            }
                        else:
                            # é¢„æµ‹é˜¶æ®µä½¿ç”¨è®­ç»ƒé˜¶æ®µè®¡ç®—çš„å¡«å……å€¼
                            fill_value = self.nan_fill_values.get(col, self.nan_fill_value)
                            self.logger.debug(f"åˆ— '{col}' ä½¿ç”¨è®­ç»ƒé˜¶æ®µè®¡ç®—çš„å¡«å……å€¼: {fill_value}")

                        # å¡«å……NaNå€¼
                        df_processed[col] = df_processed[col].fillna(fill_value)

            elif df_processed[col].dtype == 'object':
                # å¯¹è±¡ç±»å‹åˆ—å¤„ç†ï¼ˆå­—ç¬¦ä¸²ï¼‰
                nan_count = df_processed[col].isna().sum()

                if nan_count > 0:
                    if is_training:
                        # å¯¹äºç±»åˆ«åˆ—ï¼Œä½¿ç”¨ä¼—æ•°å¡«å……æˆ–åˆ›å»ºæ–°ç±»åˆ«
                        if len(df_processed[col].unique()) < 50:  # å‡è®¾æ˜¯ç±»åˆ«åˆ—
                            mode_value = df_processed[col].mode()
                            if not mode_value.empty:
                                fill_value = mode_value.iloc[0]
                                self.nan_fill_values[col] = fill_value
                                self.logger.info(f"ç±»åˆ«åˆ— '{col}' ä½¿ç”¨ä¼—æ•°å¡«å……: {fill_value}")
                            else:
                                fill_value = 'MISSING'
                                self.nan_fill_values[col] = fill_value
                                self.logger.info(f"ç±»åˆ«åˆ— '{col}' ä½¿ç”¨'MISSING'å¡«å……")
                        else:
                            fill_value = 'MISSING'
                            self.nan_fill_values[col] = fill_value
                            self.logger.info(f"æ–‡æœ¬åˆ— '{col}' ä½¿ç”¨'MISSING'å¡«å……")
                    else:
                        fill_value = self.nan_fill_values.get(col, 'MISSING')

                    df_processed[col] = df_processed[col].fillna(fill_value)

        # éªŒè¯å¤„ç†åæ˜¯å¦è¿˜æœ‰NaN
        remaining_nan = df_processed.isna().sum().sum()
        if remaining_nan > 0:
            self.logger.warning(f"å¤„ç†åä»æœ‰ {remaining_nan} ä¸ªNaNå€¼")
        else:
            self.logger.info("âœ… NaNå€¼å¤„ç†å®Œæˆï¼Œæ— å‰©ä½™NaNå€¼")

        return df_processed

    def preprocess_data(self, df, for_gtnnwr=False, is_training=True):
        """æ•°æ®é¢„å¤„ç†

        Args:
            df (pd.DataFrame): åŸå§‹æ•°æ®
            for_gtnnwr (bool): æ˜¯å¦ä¸ºGTNNWRå¤„ç†æ•°æ®
            is_training (bool): æ˜¯å¦ä¸ºè®­ç»ƒé˜¶æ®µ

        Returns:
            tuple: å¤„ç†åçš„ç‰¹å¾çŸ©é˜µã€ç›®æ ‡å‘é‡ã€åˆ†ç»„ä¿¡æ¯
        """
        self.logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")

        # åˆ›å»ºæ•°æ®å‰¯æœ¬
        df_clean = df.copy()

        # éªŒè¯å¿…è¦åˆ—
        required_columns = ['station_id', 'date', self.target_column]
        missing_columns = [col for col in required_columns if col not in df_clean.columns]
        if missing_columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")

        # å¤„ç†NaNå€¼
        df_clean = self._handle_nan_values(df_clean, is_training=is_training)

        # ç¡®ä¿GTNNWRéœ€è¦çš„åˆ—éƒ½å­˜åœ¨
        if self.use_gtnnwr:
            # åˆ›å»ºIDåˆ—ï¼ˆGTNNWRéœ€è¦ï¼‰
            df_clean['id'] = np.arange(len(df_clean))

            # æ£€æŸ¥å¹¶ç¡®ä¿æ‰€æœ‰éœ€è¦çš„åˆ—éƒ½å­˜åœ¨
            gtnnwr_required = (self.gtnnwr_x_columns + self.gtnnwr_spatial_columns +
                               self.gtnnwr_temp_columns + [self.gtnnwr_id_column])
            missing_gtnnwr = [col for col in gtnnwr_required if col not in df_clean.columns]
            if missing_gtnnwr:
                self.logger.warning(f"GTNNWRç¼ºå°‘ä»¥ä¸‹åˆ—: {missing_gtnnwr}")
                # å°è¯•å¡«å……ç¼ºå¤±åˆ—ä¸º0æˆ–åˆé€‚çš„é»˜è®¤å€¼
                for col in missing_gtnnwr:
                    if col == 'id':
                        df_clean[col] = np.arange(len(df_clean))
                    elif col in ['year', 'month', 'doy']:
                        # å¦‚æœæ˜¯æ—¶é—´åˆ—ï¼Œå°è¯•ä»dateåˆ—æå–
                        if 'date' in df_clean.columns and pd.api.types.is_datetime64_any_dtype(df_clean['date']):
                            df_clean['date'] = pd.to_datetime(df_clean['date'])
                            if col == 'year':
                                df_clean[col] = df_clean['date'].dt.year
                            elif col == 'month':
                                df_clean[col] = df_clean['date'].dt.month
                            elif col == 'doy':
                                df_clean[col] = df_clean['date'].dt.dayofyear
                        else:
                            df_clean[col] = 0.0
                    else:
                        df_clean[col] = 0.0

        # å¤„ç†CSWEæ— æ•ˆå€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'cswe' in df_clean.columns:
            cswe_invalid_mask = df_clean['cswe'] > 200
            if cswe_invalid_mask.sum() > 0:
                df_clean.loc[cswe_invalid_mask, 'cswe'] = np.nan
                # é‡æ–°å¤„ç†NaNå€¼
                df_clean = self._handle_nan_values(df_clean, is_training=is_training)

        # ç¡®å®šç‰¹å¾åˆ—
        exclude_columns = ['station_id', 'date', self.target_column, 'hydrological_doy', 'id']
        exclude_columns.extend([col for col in df_clean.columns if col.startswith('landuse_hash_')])

        # ä¿ç•™GTNNWRç‰¹å¾åˆ—ç”¨äºåŠ æƒ
        if self.use_gtnnwr:
            # ç¡®ä¿GTNNWRç‰¹å¾åˆ—åœ¨ç‰¹å¾åˆ—ä¸­
            for col in self.gtnnwr_x_columns:
                if col not in exclude_columns and col not in df_clean.columns:
                    df_clean[col] = 0.0

        self.feature_columns = [col for col in df_clean.columns if col not in exclude_columns]

        if not self.feature_columns:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç‰¹å¾åˆ—")

        # å†æ¬¡æ£€æŸ¥ç‰¹å¾åˆ—ä¸­çš„NaNå€¼
        feature_nan_counts = df_clean[self.feature_columns].isna().sum()
        if feature_nan_counts.sum() > 0:
            self.logger.warning(f"ç‰¹å¾åˆ—ä¸­ä»æœ‰ {feature_nan_counts.sum()} ä¸ªNaNå€¼")
            for col, count in feature_nan_counts[feature_nan_counts > 0].items():
                self.logger.warning(f"  ç‰¹å¾åˆ— '{col}': {count} ä¸ªNaN")
            # ä½¿ç”¨æœ€åä¸€æ¬¡å¡«å……
            df_clean[self.feature_columns] = df_clean[self.feature_columns].fillna(
                df_clean[self.feature_columns].median()
            )

        # å‡†å¤‡æ•°æ®
        X = df_clean[self.feature_columns].values
        y = df_clean[self.target_column].values

        # æ£€æŸ¥ç›®æ ‡å˜é‡ä¸­çš„NaN
        y_nan_count = np.isnan(y).sum()
        if y_nan_count > 0:
            self.logger.warning(f"ç›®æ ‡å˜é‡ '{self.target_column}' ä¸­æœ‰ {y_nan_count} ä¸ªNaNå€¼")
            if self.nan_strategy == 'drop' and is_training:
                # åˆ é™¤ç›®æ ‡å˜é‡ä¸ºNaNçš„è¡Œ
                valid_mask = ~np.isnan(y)
                X = X[valid_mask]
                y = y[valid_mask]
                df_clean = df_clean.iloc[valid_mask]
                self.logger.info(f"åˆ é™¤äº† {y_nan_count} ä¸ªç›®æ ‡å˜é‡ä¸ºNaNçš„æ ·æœ¬")
            else:
                # å¡«å……ç›®æ ‡å˜é‡çš„NaN
                y_fill_value = np.nanmedian(y)
                y = np.nan_to_num(y, nan=y_fill_value)
                self.logger.info(f"ç›®æ ‡å˜é‡ä½¿ç”¨ä¸­ä½æ•°å¡«å……: {y_fill_value:.4f}")

        # åˆ†ç»„ä¿¡æ¯
        df_clean['year'] = pd.to_datetime(df_clean['date']).dt.year
        station_groups = df_clean['station_id'].values
        year_groups = df_clean['year'].values

        # ä¸ºGTNNWRå‡†å¤‡æ•°æ®
        gtnnwr_data = None
        if self.use_gtnnwr:
            gtnnwr_data = df_clean.copy()
            # ç¡®ä¿æ‰€æœ‰GTNNWRéœ€è¦çš„åˆ—éƒ½å­˜åœ¨
            for col in self.gtnnwr_x_columns + self.gtnnwr_spatial_columns + self.gtnnwr_temp_columns + [
                self.gtnnwr_id_column]:
                if col not in gtnnwr_data.columns:
                    if col == 'id':
                        gtnnwr_data[col] = np.arange(len(gtnnwr_data))
                    else:
                        gtnnwr_data[col] = 0.0

        # æœ€ç»ˆæ£€æŸ¥
        x_nan_count = np.isnan(X).sum()
        y_nan_count = np.isnan(y).sum()

        self.logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        self.logger.info(f"  æ ·æœ¬æ•°: {len(X)}, ç‰¹å¾æ•°: {len(self.feature_columns)}")
        self.logger.info(f"  Xä¸­NaNæ•°é‡: {x_nan_count}, yä¸­NaNæ•°é‡: {y_nan_count}")

        # æ‰“å°ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
        self.logger.info(f"  ç‰¹å¾ç»Ÿè®¡:")
        for i, col in enumerate(self.feature_columns[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªç‰¹å¾
            col_values = X[:, i]
            self.logger.info(
                f"    {col}: å‡å€¼={col_values.mean():.4f}, æ ‡å‡†å·®={col_values.std():.4f}, èŒƒå›´=[{col_values.min():.4f}, {col_values.max():.4f}]")
        if len(self.feature_columns) > 5:
            self.logger.info(f"    ... å’Œå…¶ä»– {len(self.feature_columns) - 5} ä¸ªç‰¹å¾")

        return X, y, station_groups, year_groups, gtnnwr_data

    def _train_gtnnwr_for_fold(self, train_data, val_data):
        """ä¸ºå•ä¸ªæŠ˜å è®­ç»ƒGTNNWRæ¨¡å‹å¹¶æå–æƒé‡

        Args:
            train_data (pd.DataFrame): è®­ç»ƒæ•°æ®
            val_data (pd.DataFrame): éªŒè¯æ•°æ®

        Returns:
            tuple: (è®­ç»ƒé›†æƒé‡çŸ©é˜µ, éªŒè¯é›†æƒé‡çŸ©é˜µ)
        """
        self.logger.debug("ä¸ºå½“å‰æŠ˜å è®­ç»ƒGTNNWRæ¨¡å‹...")

        print("\n" + "=" * 80)
        print("ğŸ§  GTNNWRæ¨¡å‹è®­ç»ƒ (å½“å‰æŠ˜å )")
        print("=" * 80)

        try:
            # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„åˆ—éƒ½å­˜åœ¨
            print("ğŸ” æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
            required_columns = (self.gtnnwr_x_columns + self.gtnnwr_spatial_columns +
                                self.gtnnwr_temp_columns + [self.gtnnwr_id_column] + self.gtnnwr_y_column)

            # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿ
            if len(train_data) < 10 or len(val_data) < 1:
                print(f"âš ï¸  æ•°æ®é‡ä¸è¶³: è®­ç»ƒé›†{len(train_data)}æ ·æœ¬, éªŒè¯é›†{len(val_data)}æ ·æœ¬")
                print("âš ï¸  è·³è¿‡GTNNWRè®­ç»ƒï¼Œè¿”å›Noneæƒé‡")
                return None, None

            for col in required_columns:
                if col not in train_data.columns:
                    if col == 'id':
                        train_data[col] = np.arange(len(train_data))
                    else:
                        train_data[col] = 0.0
                    print(f"  âš ï¸  è®­ç»ƒæ•°æ®ç¼ºå¤±åˆ— '{col}'ï¼Œå·²å¡«å……")
                if col not in val_data.columns:
                    if col == 'id':
                        val_data[col] = np.arange(len(val_data))
                    else:
                        val_data[col] = 0.0
                    print(f"  âš ï¸  éªŒè¯æ•°æ®ç¼ºå¤±åˆ— '{col}'ï¼Œå·²å¡«å……")

            # æ£€æŸ¥æ•°æ®å½¢çŠ¶
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶:")
            print(f"  è®­ç»ƒæ•°æ®: {train_data.shape}")
            print(f"  éªŒè¯æ•°æ®: {val_data.shape}")

            # æ£€æŸ¥NaNå€¼
            train_nan = train_data[self.gtnnwr_x_columns].isna().sum().sum()
            val_nan = val_data[self.gtnnwr_x_columns].isna().sum().sum()
            if train_nan > 0 or val_nan > 0:
                print(f"  âš ï¸  è­¦å‘Š: è®­ç»ƒæ•°æ®æœ‰{train_nan}ä¸ªNaNï¼ŒéªŒè¯æ•°æ®æœ‰{val_nan}ä¸ªNaN")
                # ä½¿ç”¨ä¸­ä½æ•°å¡«å……
                for col in self.gtnnwr_x_columns:
                    if col in train_data.columns:
                        median_val = train_data[col].median()
                        train_data[col] = train_data[col].fillna(median_val)
                        val_data[col] = val_data[col].fillna(median_val)

            # åˆå§‹åŒ–GTNNWRæ•°æ®é›† - ä¿®æ”¹æ–¹æ³•é¿å…ç©ºæ•°æ®é›†
            print("ğŸ“¦ åˆå§‹åŒ–GTNNWRæ•°æ®é›†...")

            # æ–¹æ³•1: ä½¿ç”¨init_dataset_splitæ›¿ä»£init_dataset
            # ç»„åˆè®­ç»ƒå’ŒéªŒè¯æ•°æ®
            combined_data = pd.concat([train_data, val_data], ignore_index=True)

            # é‡æ–°è®¡ç®—éªŒè¯é›†æ¯”ä¾‹
            total_samples = len(combined_data)
            train_samples = len(train_data)
            valid_ratio = len(val_data) / total_samples if total_samples > 0 else 0.1

            # ç¡®ä¿éªŒè¯é›†æ¯”ä¾‹åˆç†
            if valid_ratio < 0.05:
                valid_ratio = 0.1  # è‡³å°‘10%ä½œä¸ºéªŒè¯é›†
            elif valid_ratio > 0.5:
                valid_ratio = 0.3  # æœ€å¤š30%ä½œä¸ºéªŒè¯é›†

            # è®¡ç®—æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆä½¿ç”¨å°æ¯”ä¾‹æˆ–0ï¼‰
            test_ratio = 0.05 if total_samples > 20 else 0.0

            print(f"  æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›†{len(train_data)}æ ·æœ¬, éªŒè¯é›†æ¯”ä¾‹{valid_ratio:.2%}, æµ‹è¯•é›†æ¯”ä¾‹{test_ratio:.2%}")

            try:
                # å°è¯•ä½¿ç”¨init_dataset_splitï¼Œæ·»åŠ temp_columnå‚æ•°
                train_set, val_set, test_set = datasets.init_dataset_split(
                    train_data=train_data,
                    val_data=val_data,
                    test_data=val_data.head(max(1, min(5, len(val_data) // 2))),  # ä½¿ç”¨éƒ¨åˆ†éªŒè¯æ•°æ®ä½œä¸ºæµ‹è¯•æ•°æ®
                    x_column=self.gtnnwr_x_columns,
                    y_column=self.gtnnwr_y_column,
                    spatial_column=self.gtnnwr_spatial_columns,
                    temp_column=self.gtnnwr_temp_columns,  # æ·»åŠ æ—¶é—´åˆ—å‚æ•°
                    batch_size=min(1024, len(train_data)),
                    shuffle=False,
                    use_model="gtnnwr"  # ä½¿ç”¨gtnnwræ¨¡å‹
                )
                print(f"âœ… ä½¿ç”¨init_dataset_splitåˆå§‹åŒ–æˆåŠŸ")
            except Exception as split_error:
                print(f"âš ï¸  init_dataset_splitå¤±è´¥: {split_error}")
                print("  å°è¯•ä½¿ç”¨init_dataset...")

                # æ–¹æ³•2: å›é€€åˆ°init_dataset
                try:
                    # åˆ›å»ºåˆå¹¶æ•°æ®å¹¶æ·»åŠ æ ‡è¯†
                    combined_data['fold_source'] = ['train'] * len(train_data) + ['val'] * len(val_data)

                    train_set, val_set, test_set = datasets.init_dataset(
                        data=combined_data,
                        test_ratio=test_ratio,
                        valid_ratio=valid_ratio,
                        x_column=self.gtnnwr_x_columns,
                        y_column=self.gtnnwr_y_column,
                        spatial_column=self.gtnnwr_spatial_columns,
                        temp_column=self.gtnnwr_temp_columns,  # æ·»åŠ æ—¶é—´åˆ—å‚æ•°
                        id_column=[self.gtnnwr_id_column],
                        use_model="gtnnwr",
                        sample_seed=42,
                        batch_size=min(1024, len(combined_data))
                    )
                    print(f"âœ… ä½¿ç”¨init_datasetåˆå§‹åŒ–æˆåŠŸ")
                except Exception as init_error:
                    print(f"âŒ init_datasetä¹Ÿå¤±è´¥: {init_error}")
                    print("âš ï¸  è·³è¿‡GTNNWRè®­ç»ƒï¼Œè¿”å›Noneæƒé‡")
                    return None, None

            print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ:")
            print(f"  è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_set) if hasattr(train_set, '__len__') else 'N/A'}")
            print(f"  éªŒè¯é›†æ ·æœ¬æ•°: {len(val_set) if hasattr(val_set, '__len__') else 'N/A'}")
            print(f"  æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_set) if hasattr(test_set, '__len__') else 'N/A'}")

            # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©º
            if (not hasattr(train_set, '__len__') or len(train_set) == 0 or
                    not hasattr(val_set, '__len__') or len(val_set) == 0):
                print(f"âŒ æ•°æ®é›†ä¸ºç©ºæˆ–æ— æ•ˆ: è®­ç»ƒé›†={len(train_set) if hasattr(train_set, '__len__') else 'N/A'}, "
                      f"éªŒè¯é›†={len(val_set) if hasattr(val_set, '__len__') else 'N/A'}")
                print("âš ï¸  è·³è¿‡GTNNWRè®­ç»ƒï¼Œè¿”å›Noneæƒé‡")
                return None, None

            # è®­ç»ƒGTNNWRæ¨¡å‹
            print("\nğŸ‹ï¸ è®­ç»ƒGTNNWRæ¨¡å‹...")
            try:
                gtnnwr = models.GTNNWR(
                    train_dataset=train_set,
                    valid_dataset=val_set,
                    test_dataset=train_set,  # ä½¿ç”¨è®­ç»ƒé›†ä½œä¸ºæµ‹è¯•é›†å ä½
                    dense_layers=self.gtnnwr_params['graph_layers'],  # ä¿®æ”¹ï¼šä½¿ç”¨dense_layerså‚æ•°å
                    drop_out=self.gtnnwr_params['drop_out'],
                    optimizer=self.gtnnwr_params['optimizer'],
                    optimizer_params=self.gtnnwr_params['optimizer_params'],
                    model_name=f"GTNNWR_Fold",
                    model_save_path="result/gtnnwr_models_temp",
                    log_path="result/gtnnwr_logs_temp",
                    write_path="result/gtnnwr_runs_temp"
                )

                # æ·»åŠ å›¾ç»“æ„
                print("ğŸ•¸ï¸ æ·»åŠ å›¾ç»“æ„...")
                gtnnwr.add_graph()

                # ç®€çŸ­è®­ç»ƒ
                print(f"âš™ï¸ è®­ç»ƒå‚æ•°: {self.gtnnwr_params['max_epoch']}è½®, "
                      f"æ—©åœ{self.gtnnwr_params['early_stop']}è½®")

                gtnnwr.run(
                    max_epoch=self.gtnnwr_params['max_epoch'],
                    early_stop=self.gtnnwr_params['early_stop'],
                    print_frequency=self.gtnnwr_params['print_frequency']
                )
            except Exception as model_error:
                print(f"âŒ GTNNWRæ¨¡å‹åˆ›å»ºæˆ–è®­ç»ƒå¤±è´¥: {model_error}")
                print("âš ï¸  è·³è¿‡GTNNWRè®­ç»ƒï¼Œè¿”å›Noneæƒé‡")
                return None, None

            # æå–æƒé‡çŸ©é˜µ
            def extract_weights(gtnnwr_instance, dataset, dataset_name="æ•°æ®é›†"):
                """æå–GTNNWRæ¨¡å‹è¾“å‡ºçš„æƒé‡çŸ©é˜µ"""
                if dataset is None or not hasattr(dataset, 'dataloader'):
                    print(f"  âŒ {dataset_name}æ— æ•ˆæˆ–æ²¡æœ‰dataloader")
                    return None

                model = gtnnwr_instance._model
                model.eval()
                device = gtnnwr_instance._device

                all_weights = []
                sample_count = 0

                print(f"\nğŸ“¥ ä»{dataset_name}æå–æƒé‡...")
                print(f"  æœŸæœ›æ ·æœ¬æ•°: {len(dataset) if hasattr(dataset, '__len__') else 'æœªçŸ¥'}")

                with torch.no_grad():
                    try:
                        # âœ… ä¿®å¤ï¼šå¤„ç†æ‰€æœ‰æ‰¹æ¬¡ï¼Œä¸åªæ˜¯å‰10ä¸ª
                        total_batches = 0
                        for batch_idx, batch in enumerate(dataset.dataloader):
                            if batch is None or len(batch) < 2:
                                continue

                            distances, features = batch[:2]
                            distances = distances.to(device)

                            # è·å–æ¨¡å‹è¾“å‡º
                            weights = model(distances)

                            # æ£€æŸ¥æƒé‡ä¸­çš„NaN
                            if torch.isnan(weights).any():
                                print(f"  âš ï¸  æ‰¹æ¬¡{batch_idx}æƒé‡ä¸­åŒ…å«NaNå€¼ï¼Œä½¿ç”¨1å¡«å……")
                                weights = torch.nan_to_num(weights, nan=1.0)

                            # âœ… è°ƒè¯•ä¿¡æ¯ï¼šä»…æ‰“å°ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
                            if batch_idx == 0:
                                print(f"  ç¬¬ä¸€æ‰¹æƒé‡å½¢çŠ¶: {weights.shape}")
                                print(f"  ç¬¬ä¸€æ‰¹æƒé‡ç»Ÿè®¡:")
                                print(f"    èŒƒå›´: [{weights.min():.4f}, {weights.max():.4f}]")
                                print(f"    å‡å€¼: {weights.mean():.4f}")
                                print(f"    æ ‡å‡†å·®: {weights.std():.4f}")

                            all_weights.append(weights.cpu().numpy())
                            sample_count += weights.shape[0]
                            total_batches += 1

                            # âœ… é‡è¦ä¿®å¤ï¼šæ˜¾ç¤ºè¿›åº¦ä½†ä¸ä¸­æ–­
                            if batch_idx % 10 == 0 and batch_idx > 0:
                                print(f"  å·²å¤„ç†{total_batches}ä¸ªæ‰¹æ¬¡ï¼Œç´¯è®¡{sample_count}ä¸ªæ ·æœ¬")

                        print(f"  âœ… å®Œæˆ: æ€»å…±å¤„ç†{total_batches}ä¸ªæ‰¹æ¬¡ï¼Œ{sample_count}ä¸ªæ ·æœ¬")

                    except Exception as e:
                        print(f"  âŒ æå–æƒé‡æ—¶å‡ºé”™: {e}")
                        import traceback
                        print(traceback.format_exc())
                        return None

                if all_weights:
                    weights_combined = np.concatenate(all_weights, axis=0)

                    # æ£€æŸ¥å¹¶å¤„ç†NaNå€¼
                    nan_count = np.isnan(weights_combined).sum()
                    if nan_count > 0:
                        print(f"  âš ï¸  æƒé‡çŸ©é˜µä¸­æœ‰{nan_count}ä¸ªNaNå€¼ï¼Œä½¿ç”¨1å¡«å……")
                        weights_combined = np.nan_to_num(weights_combined, nan=1.0)

                    print(f"  âœ… æå–å®Œæˆ: {weights_combined.shape} (æ ·æœ¬æ•°Ã—ç‰¹å¾æ•°)")
                    print(f"    æ ·æœ¬æ•°: {weights_combined.shape[0]}")
                    print(f"    ç‰¹å¾æ•°: {weights_combined.shape[1]}")

                    return weights_combined
                else:
                    print(f"  âŒ æå–å¤±è´¥: æ²¡æœ‰è·å–åˆ°æƒé‡")
                    return None

            # æå–è®­ç»ƒé›†å’ŒéªŒè¯é›†æƒé‡
            train_weights = extract_weights(gtnnwr, train_set, "è®­ç»ƒé›†")
            val_weights = extract_weights(gtnnwr, val_set, "éªŒè¯é›†")

            if train_weights is not None and val_weights is not None:
                # âœ… å…³é”®ä¿®å¤ï¼šæ£€æŸ¥å¹¶è°ƒæ•´ç»´åº¦
                expected_cols = len(self.gtnnwr_x_columns)

                print(f"\nğŸ”§ ç»´åº¦æ£€æŸ¥ä¸è°ƒæ•´:")
                print(f"  æœŸæœ›ç‰¹å¾æ•°: {expected_cols} (GTNNWRç‰¹å¾åˆ—è¡¨é•¿åº¦)")

                # æ£€æŸ¥è®­ç»ƒé›†æƒé‡ç»´åº¦
                if train_weights.shape[1] != expected_cols:
                    print(f"  âš ï¸  è®­ç»ƒæƒé‡ç»´åº¦ä¸åŒ¹é…: {train_weights.shape[1]} != {expected_cols}")
                    if train_weights.shape[1] == expected_cols + 1:
                        # å¸¸è§æƒ…å†µï¼šå¤šäº†ä¸€åˆ—æˆªè·é¡¹
                        train_weights = train_weights[:, :expected_cols]
                        print(f"  âœ… ä¿®å¤ï¼šå»æ‰æœ€åä¸€åˆ—ï¼Œæ–°å½¢çŠ¶: {train_weights.shape}")
                    elif train_weights.shape[1] > expected_cols:
                        # å…¶ä»–æƒ…å†µï¼šæˆªæ–­åˆ°æœŸæœ›é•¿åº¦
                        train_weights = train_weights[:, :expected_cols]
                        print(f"  âœ… ä¿®å¤ï¼šæˆªæ–­åˆ°æœŸæœ›é•¿åº¦ï¼Œæ–°å½¢çŠ¶: {train_weights.shape}")
                    else:
                        # ç»´åº¦å¤ªå°‘ï¼Œå¡«å……1.0
                        padding = np.ones((train_weights.shape[0], expected_cols - train_weights.shape[1]))
                        train_weights = np.hstack([train_weights, padding])
                        print(f"  âœ… ä¿®å¤ï¼šå¡«å……åˆ°æœŸæœ›é•¿åº¦ï¼Œæ–°å½¢çŠ¶: {train_weights.shape}")

                # æ£€æŸ¥éªŒè¯é›†æƒé‡ç»´åº¦
                if val_weights.shape[1] != expected_cols:
                    print(f"  âš ï¸  éªŒè¯æƒé‡ç»´åº¦ä¸åŒ¹é…: {val_weights.shape[1]} != {expected_cols}")
                    if val_weights.shape[1] == expected_cols + 1:
                        val_weights = val_weights[:, :expected_cols]
                        print(f"  âœ… ä¿®å¤ï¼šå»æ‰æœ€åä¸€åˆ—ï¼Œæ–°å½¢çŠ¶: {val_weights.shape}")
                    elif val_weights.shape[1] > expected_cols:
                        val_weights = val_weights[:, :expected_cols]
                        print(f"  âœ… ä¿®å¤ï¼šæˆªæ–­åˆ°æœŸæœ›é•¿åº¦ï¼Œæ–°å½¢çŠ¶: {val_weights.shape}")
                    else:
                        padding = np.ones((val_weights.shape[0], expected_cols - val_weights.shape[1]))
                        val_weights = np.hstack([val_weights, padding])
                        print(f"  âœ… ä¿®å¤ï¼šå¡«å……åˆ°æœŸæœ›é•¿åº¦ï¼Œæ–°å½¢çŠ¶: {val_weights.shape}")

                # æ‰“å°æƒé‡ç»Ÿè®¡
                print(f"\nğŸ“Š æœ€ç»ˆæƒé‡ç»Ÿè®¡:")
                print(f"  è®­ç»ƒé›†æƒé‡:")
                print(f"    å½¢çŠ¶: {train_weights.shape}")
                print(f"    èŒƒå›´: [{train_weights.min():.6f}, {train_weights.max():.6f}]")
                print(f"    å‡å€¼: {train_weights.mean():.6f}")
                print(f"    æ ‡å‡†å·®: {train_weights.std():.6f}")

                # æ£€æŸ¥æƒé‡æ˜¯å¦æ¥è¿‘1ï¼ˆä¹˜æ³•å› å­çš„æœŸæœ›ï¼‰
                distance_from_one = np.abs(train_weights - 1).mean()
                print(f"    ä¸1çš„å¹³å‡è·ç¦»: {distance_from_one:.6f}")

                if distance_from_one < 0.01:
                    print(f"    âš ï¸  è­¦å‘Šï¼šæƒé‡éå¸¸æ¥è¿‘1ï¼ŒåŠ æƒæ•ˆæœå¯èƒ½ä¸æ˜æ˜¾")
                else:
                    print(f"    âœ… æƒé‡ä¸1æœ‰æ˜¾è‘—å·®å¼‚ï¼ŒåŠ æƒä¼šæœ‰æ•ˆæœ")

                print(f"\n  éªŒè¯é›†æƒé‡:")
                print(f"    å½¢çŠ¶: {val_weights.shape}")
                print(f"    èŒƒå›´: [{val_weights.min():.6f}, {val_weights.max():.6f}]")
                print(f"    å‡å€¼: {val_weights.mean():.6f}")

                self.logger.debug(f"  æå–åˆ°æƒé‡çŸ©é˜µ: è®­ç»ƒé›†{train_weights.shape}, éªŒè¯é›†{val_weights.shape}")
                return train_weights, val_weights
            else:
                print(f"\nâŒ GTNNWRæƒé‡æå–å¤±è´¥")
                self.logger.warning("  æœªèƒ½æå–åˆ°æƒé‡çŸ©é˜µ")
                return None, None

        except Exception as e:
            print(f"\nâŒ GTNNWRè®­ç»ƒå¤±è´¥: {str(e)}")
            import traceback
            print(f"è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
            self.logger.warning(f"  GTNNWRè®­ç»ƒå¤±è´¥: {str(e)}")
            return None, None

    def _apply_gtnnwr_weights(self, X, weights, feature_columns, gtnnwr_x_columns):
        """åº”ç”¨GTNNWRæƒé‡åˆ°ç‰¹å¾çŸ©é˜µ

        Args:
            X (np.array): åŸå§‹ç‰¹å¾çŸ©é˜µ
            weights (np.array): æƒé‡çŸ©é˜µ
            feature_columns (list): ç‰¹å¾åˆ—å
            gtnnwr_x_columns (list): GTNNWRç‰¹å¾åˆ—å

        Returns:
            np.array: åŠ æƒåçš„ç‰¹å¾çŸ©é˜µ
        """
        if weights is None:
            self.logger.warning("æƒé‡çŸ©é˜µä¸ºNoneï¼Œè¿”å›åŸå§‹ç‰¹å¾")
            return X

        # âœ… ä¿®å¤1: é¦–å…ˆæ£€æŸ¥æ ·æœ¬æ•°æ˜¯å¦åŒ¹é…
        if X.shape[0] != weights.shape[0]:
            self.logger.error(f"âŒ æ ·æœ¬æ•°ä¸åŒ¹é…: Xæœ‰{X.shape[0]}ä¸ªæ ·æœ¬, æƒé‡æœ‰{weights.shape[0]}ä¸ªæ ·æœ¬")

            # å°è¯•ä¿®å¤æ ·æœ¬æ•°ä¸åŒ¹é…çš„é—®é¢˜
            if weights.shape[0] < X.shape[0]:
                # å¦‚æœæƒé‡æ ·æœ¬æ•°è¾ƒå°‘ï¼Œé‡å¤æƒé‡ä»¥åŒ¹é…Xçš„æ ·æœ¬æ•°
                repeat_times = int(np.ceil(X.shape[0] / weights.shape[0]))
                weights_repeated = np.tile(weights, (repeat_times, 1))
                weights = weights_repeated[:X.shape[0], :]
                self.logger.warning(f"âœ… æƒé‡æ ·æœ¬æ•°ä¸è¶³ï¼Œé‡å¤æƒé‡åˆ°{weights.shape[0]}ä¸ªæ ·æœ¬")
            else:
                # å¦‚æœæƒé‡æ ·æœ¬æ•°è¾ƒå¤šï¼Œæˆªæ–­åˆ°Xçš„æ ·æœ¬æ•°
                weights = weights[:X.shape[0], :]
                self.logger.warning(f"âœ… æƒé‡æ ·æœ¬æ•°è¿‡å¤šï¼Œæˆªæ–­åˆ°{weights.shape[0]}ä¸ªæ ·æœ¬")

        # âœ… ä¿®å¤2: å¤„ç†ç»´åº¦ä¸åŒ¹é…é—®é¢˜
        if weights.shape[1] != len(gtnnwr_x_columns):
            self.logger.warning(f"âš ï¸ æƒé‡çŸ©é˜µç‰¹å¾æ•°({weights.shape[1]})ä¸GTNNWRç‰¹å¾æ•°({len(gtnnwr_x_columns)})ä¸åŒ¹é…")

            # è‡ªåŠ¨è°ƒæ•´æƒé‡ç»´åº¦
            if weights.shape[1] > len(gtnnwr_x_columns):
                # å¦‚æœæƒé‡æ˜¯35åˆ—ï¼ŒGTNNWRç‰¹å¾æ˜¯34åˆ—ï¼Œå»æ‰æœ€åä¸€åˆ—
                weights = weights[:, :len(gtnnwr_x_columns)]
                self.logger.info(f"âœ… è‡ªåŠ¨è°ƒæ•´ï¼šæˆªæ–­æƒé‡çŸ©é˜µåˆ° {weights.shape[1]} åˆ—")
            elif weights.shape[1] < len(gtnnwr_x_columns):
                # å¦‚æœæƒé‡åˆ—æ•°å°‘ï¼Œå¡«å……1.0
                padding = np.ones((weights.shape[0], len(gtnnwr_x_columns) - weights.shape[1]))
                weights = np.hstack([weights, padding])
                self.logger.info(f"âœ… è‡ªåŠ¨è°ƒæ•´ï¼šå¡«å……æƒé‡çŸ©é˜µåˆ° {weights.shape[1]} åˆ—")

        # æ£€æŸ¥è¾“å…¥ä¸­çš„NaN
        x_nan_count = np.isnan(X).sum()
        if x_nan_count > 0:
            self.logger.warning(f"âš ï¸ è¾“å…¥ç‰¹å¾çŸ©é˜µä¸­æœ‰ {x_nan_count} ä¸ªNaNå€¼ï¼Œä½¿ç”¨åˆ—å‡å€¼å¡«å……")
            col_means = np.nanmean(X, axis=0)
            for i in range(X.shape[1]):
                X[:, i] = np.where(np.isnan(X[:, i]), col_means[i], X[:, i])

        # æ£€æŸ¥æƒé‡ä¸­çš„NaN
        weights_nan_count = np.isnan(weights).sum()
        if weights_nan_count > 0:
            self.logger.warning(f"âš ï¸ æƒé‡çŸ©é˜µä¸­æœ‰ {weights_nan_count} ä¸ªNaNå€¼ï¼Œä½¿ç”¨1å¡«å……")
            weights = np.nan_to_num(weights, nan=1.0)

        # åˆ›å»ºç‰¹å¾æ˜ å°„ï¼šç‰¹å¾åˆ—åˆ°GTNNWRç‰¹å¾åˆ—çš„ç´¢å¼•
        feature_to_gtnnwr = {}
        for i, feat in enumerate(feature_columns):
            if feat in gtnnwr_x_columns:
                feature_to_gtnnwr[i] = gtnnwr_x_columns.index(feat)

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        matched_count = len(feature_to_gtnnwr)
        self.logger.info(f"ğŸ” ç‰¹å¾åŒ¹é…: åŒ¹é…äº† {matched_count}/{len(feature_columns)} ä¸ªç‰¹å¾")

        if matched_count == 0:
            self.logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ç‰¹å¾ï¼Œæ— æ³•åº”ç”¨æƒé‡")
            return X

        # âœ… å…³é”®ä¿®å¤ï¼šåº”ç”¨æƒé‡ï¼Œå³ä½¿æœ‰NaN
        X_weighted = X.copy()
        changed_count = 0

        for feat_idx, gtnnwr_idx in feature_to_gtnnwr.items():
            # è·å–åŸå§‹ç‰¹å¾å€¼å’Œæƒé‡
            original_values = X[:, feat_idx]
            weight_values = weights[:, gtnnwr_idx]

            # æ£€æŸ¥å¹¶å¤„ç†NaN
            original_nan = np.isnan(original_values).sum()
            weight_nan = np.isnan(weight_values).sum()

            if original_nan > 0:
                original_values = np.nan_to_num(original_values, nan=0.0)

            if weight_nan > 0:
                weight_values = np.nan_to_num(weight_values, nan=1.0)

            # åº”ç”¨æƒé‡ï¼šX Ã— weight
            weighted_values = original_values * weight_values

            # æ£€æŸ¥æ˜¯å¦çœŸçš„æ”¹å˜äº†ï¼ˆå¿½ç•¥NaNï¼‰
            mask = ~np.isnan(original_values) & ~np.isnan(weighted_values)
            if mask.any():
                if not np.allclose(original_values[mask], weighted_values[mask], rtol=1e-10):
                    changed_count += 1

            X_weighted[:, feat_idx] = weighted_values

        # æ·»åŠ éªŒè¯è¾“å‡º
        change_ratio = changed_count / matched_count if matched_count > 0 else 0
        self.logger.info(f"âœ… æƒé‡åº”ç”¨ç»“æœ: ä¿®æ”¹äº† {changed_count}/{matched_count} ä¸ªç‰¹å¾ ({change_ratio:.1%})")

        # æ£€æŸ¥å‡ ä¸ªå…³é”®ç‰¹å¾çš„å˜åŒ–
        if changed_count > 0:
            key_features = ['elevation', 'X', 'Y', 'Z', 'slope', 'doy']
            for feat in key_features:
                if feat in feature_columns and feat in gtnnwr_x_columns:
                    feat_idx = feature_columns.index(feat)
                    gtnnwr_idx = gtnnwr_x_columns.index(feat)

                    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬
                    if len(X) > 0 and len(weights) > 0:
                        if (not np.isnan(X[0, feat_idx]) and
                                not np.isnan(X_weighted[0, feat_idx]) and
                                feat_idx < weights.shape[1]):
                            original = X[0, feat_idx]
                            weighted = X_weighted[0, feat_idx]
                            weight_val = weights[0, gtnnwr_idx]

                            if abs(weighted - original) > 1e-10:
                                self.logger.info(f"   {feat}: {original:.4f} Ã— {weight_val:.4f} = {weighted:.4f} "
                                                 f"(Î”={weighted - original:+.4f})")

        # æ£€æŸ¥è¾“å‡ºä¸­çš„NaN
        output_nan_count = np.isnan(X_weighted).sum()
        if output_nan_count > 0:
            self.logger.warning(f"âš ï¸ åŠ æƒåçš„ç‰¹å¾çŸ©é˜µä¸­æœ‰ {output_nan_count} ä¸ªNaNå€¼ï¼Œä½¿ç”¨åŸå§‹å€¼")
            X_weighted = np.where(np.isnan(X_weighted), X, X_weighted)

        # éªŒè¯æœ€ç»ˆå½¢çŠ¶
        if X_weighted.shape != X.shape:
            self.logger.error(f"âŒ å½¢çŠ¶ä¸åŒ¹é…: åŠ æƒå{X_weighted.shape} != åŸå§‹{X.shape}")
            return X  # è¿”å›åŸå§‹ç‰¹å¾é¿å…è¿›ä¸€æ­¥é”™è¯¯

        return X_weighted

    def cross_validate(self, X, y, groups, cv_type='station', gtnnwr_data=None):
        """æ‰§è¡Œå¸¦GTNNWRæƒé‡çš„äº¤å‰éªŒè¯

        Args:
            X (np.array): ç‰¹å¾æ•°æ®
            y (np.array): ç›®æ ‡å˜é‡
            groups (np.array): åˆ†ç»„ä¿¡æ¯
            cv_type (str): äº¤å‰éªŒè¯ç±»å‹ ('station' æˆ– 'yearly')
            gtnnwr_data (pd.DataFrame): GTNNWRéœ€è¦çš„å®Œæ•´æ•°æ®

        Returns:
            dict: äº¤å‰éªŒè¯ç»“æœ
        """
        logo = LeaveOneGroupOut()
        all_predictions = []
        all_true_values = []
        fold_results = {}

        fold_maes = []
        fold_rmses = []
        fold_rs = []
        fold_samples = []

        unique_groups = np.unique(groups)
        total_folds = len(unique_groups)

        print("\n" + "=" * 100)
        print(f"ğŸš€ å¼€å§‹{cv_type}äº¤å‰éªŒè¯ï¼Œå…±{total_folds}ä¸ªæŠ˜å ")
        print(f"ä½¿ç”¨GTNNWRæƒé‡å¢å¼º: {self.use_gtnnwr}")
        print(f"NaNå¤„ç†ç­–ç•¥: {self.nan_strategy}")
        print("=" * 100)

        self.logger.info(f"å¼€å§‹{cv_type}äº¤å‰éªŒè¯ï¼Œå…±{total_folds}ä¸ªæŠ˜å ...")
        self.logger.info(f"ä½¿ç”¨GTNNWRæƒé‡å¢å¼º: {self.use_gtnnwr}")

        # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        print(f"\nğŸ” æ•°æ®å®Œæ•´æ€§æ£€æŸ¥:")
        print(f"  ç‰¹å¾çŸ©é˜µXå½¢çŠ¶: {X.shape}")
        print(f"  ç›®æ ‡å˜é‡yå½¢çŠ¶: {y.shape}")
        print(f"  åˆ†ç»„ä¿¡æ¯å½¢çŠ¶: {groups.shape}")
        print(f"  å”¯ä¸€åˆ†ç»„æ•°: {total_folds}")

        if gtnnwr_data is not None:
            print(f"  GTNNWRæ•°æ®å½¢çŠ¶: {gtnnwr_data.shape}")

        # æ£€æŸ¥NaNå€¼
        x_nan = np.isnan(X).sum()
        y_nan = np.isnan(y).sum()
        if x_nan > 0:
            print(f"  âš ï¸  ç‰¹å¾çŸ©é˜µæœ‰{x_nan}ä¸ªNaNå€¼ ({x_nan / X.size:.1%})")
            # ä½¿ç”¨åˆ—å‡å€¼å¡«å……
            col_means = np.nanmean(X, axis=0)
            for i in range(X.shape[1]):
                X[:, i] = np.where(np.isnan(X[:, i]), col_means[i], X[:, i])
            print(f"  âœ… ç‰¹å¾çŸ©é˜µNaNå€¼å·²å¡«å……")

        if y_nan > 0:
            print(f"  âš ï¸  ç›®æ ‡å˜é‡æœ‰{y_nan}ä¸ªNaNå€¼ ({y_nan / len(y):.1%})")
            # ä½¿ç”¨ä¸­ä½æ•°å¡«å……
            y = np.where(np.isnan(y), np.nanmedian(y), y)
            print(f"  âœ… ç›®æ ‡å˜é‡NaNå€¼å·²å¡«å……")

        # æ£€æŸ¥ç‰¹å¾åˆ—
        print(f"\nğŸ” ç‰¹å¾åˆ—æ£€æŸ¥:")
        print(f"  æ€»ç‰¹å¾æ•°: {len(self.feature_columns)}")
        print(f"  GTNNWRç‰¹å¾æ•°: {len(self.gtnnwr_x_columns)}")

        # æ£€æŸ¥ç‰¹å¾åŒ¹é…
        matched_features = [f for f in self.feature_columns if f in self.gtnnwr_x_columns]
        unmatched_features = [f for f in self.gtnnwr_x_columns if f not in self.feature_columns]

        print(f"  åŒ¹é…çš„ç‰¹å¾æ•°: {len(matched_features)}/{len(self.feature_columns)}")
        if len(unmatched_features) > 0:
            print(f"  âš ï¸  æœªåŒ¹é…çš„GTNNWRç‰¹å¾: {unmatched_features[:5]}...")

        for fold, (train_idx, val_idx) in enumerate(logo.split(X, y, groups)):
            group_id = groups[val_idx[0]]
            train_size = len(train_idx)
            val_size = len(val_idx)

            print("\n" + "=" * 80)
            print(f"ğŸ¯ {cv_type} Fold {fold + 1}/{total_folds}: åˆ†ç»„ {group_id}")
            print(f"   è®­ç»ƒé›†: {train_size}æ ·æœ¬, éªŒè¯é›†: {val_size}æ ·æœ¬")
            print("=" * 80)

            self.logger.info(
                f"{cv_type} Fold {fold + 1}/{total_folds}: {group_id} (è®­ç»ƒé›†{train_size}, éªŒè¯é›†{val_size})")

            # åˆ†å‰²æ•°æ®
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # æ£€æŸ¥åˆ†å‰²åçš„NaNå€¼
            train_nan = np.isnan(X_train).sum()
            val_nan = np.isnan(X_val).sum()
            if train_nan > 0 or val_nan > 0:
                print(f"  âš ï¸  åˆ†å‰²åæ•°æ®æœ‰NaN - è®­ç»ƒé›†: {train_nan}, éªŒè¯é›†: {val_nan}")
                # ä½¿ç”¨è®­ç»ƒé›†çš„åˆ—å‡å€¼å¡«å……
                col_means = np.nanmean(X_train, axis=0)
                for i in range(X_train.shape[1]):
                    X_train[:, i] = np.where(np.isnan(X_train[:, i]), col_means[i], X_train[:, i])
                    X_val[:, i] = np.where(np.isnan(X_val[:, i]), col_means[i], X_val[:, i])
                print(f"  âœ… ä½¿ç”¨è®­ç»ƒé›†åˆ—å‡å€¼å¡«å……NaN")

            # ä¿å­˜åŸå§‹ç‰¹å¾ç”¨äºéªŒè¯
            X_train_original = X_train.copy()
            X_val_original = X_val.copy()

            # GTNNWRæƒé‡å¢å¼º
            if self.use_gtnnwr and gtnnwr_data is not None:
                print(f"\nğŸ“Š GTNNWRæƒé‡å¢å¼ºé˜¶æ®µ")

                # è·å–å½“å‰æŠ˜å çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®
                train_data_fold = gtnnwr_data.iloc[train_idx].copy()
                val_data_fold = gtnnwr_data.iloc[val_idx].copy()

                print(f"  è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_data_fold.shape}")
                print(f"  éªŒè¯æ•°æ®å½¢çŠ¶: {val_data_fold.shape}")

                # âœ… é‡è¦ä¿®å¤ï¼šæ£€æŸ¥å¹¶å¤„ç†æ•°æ®æ ·æœ¬æ•°
                if len(train_data_fold) != len(X_train):
                    print(f"  âš ï¸  è®­ç»ƒæ•°æ®æ ·æœ¬æ•°ä¸åŒ¹é…: GTNNWRæ•°æ®{len(train_data_fold)} vs ç‰¹å¾æ•°æ®{len(X_train)}")
                    # å¯¹é½æ•°æ®
                    if len(train_data_fold) < len(X_train):
                        # é‡å¤æ•°æ®
                        repeat_factor = int(np.ceil(len(X_train) / len(train_data_fold)))
                        train_data_fold = pd.concat([train_data_fold] * repeat_factor, ignore_index=True)
                        train_data_fold = train_data_fold.iloc[:len(X_train)]
                        print(f"  âœ… é‡å¤è®­ç»ƒæ•°æ®åˆ°{len(train_data_fold)}ä¸ªæ ·æœ¬")
                    else:
                        train_data_fold = train_data_fold.iloc[:len(X_train)]
                        print(f"  âœ… æˆªæ–­è®­ç»ƒæ•°æ®åˆ°{len(train_data_fold)}ä¸ªæ ·æœ¬")

                if len(val_data_fold) != len(X_val):
                    print(f"  âš ï¸  éªŒè¯æ•°æ®æ ·æœ¬æ•°ä¸åŒ¹é…: GTNNWRæ•°æ®{len(val_data_fold)} vs ç‰¹å¾æ•°æ®{len(X_val)}")
                    # å¯¹é½æ•°æ®
                    if len(val_data_fold) < len(X_val):
                        repeat_factor = int(np.ceil(len(X_val) / len(val_data_fold)))
                        val_data_fold = pd.concat([val_data_fold] * repeat_factor, ignore_index=True)
                        val_data_fold = val_data_fold.iloc[:len(X_val)]
                        print(f"  âœ… é‡å¤éªŒè¯æ•°æ®åˆ°{len(val_data_fold)}ä¸ªæ ·æœ¬")
                    else:
                        val_data_fold = val_data_fold.iloc[:len(X_val)]
                        print(f"  âœ… æˆªæ–­éªŒè¯æ•°æ®åˆ°{len(val_data_fold)}ä¸ªæ ·æœ¬")

                # æ£€æŸ¥GTNNWRæ•°æ®ä¸­çš„NaN
                train_gtnnwr_nan = train_data_fold.isna().sum().sum()
                val_gtnnwr_nan = val_data_fold.isna().sum().sum()
                if train_gtnnwr_nan > 0 or val_gtnnwr_nan > 0:
                    print(f"  âš ï¸  GTNNWRæ•°æ®æœ‰NaN - è®­ç»ƒé›†: {train_gtnnwr_nan}, éªŒè¯é›†: {val_gtnnwr_nan}")
                    # ä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡ä¿¡æ¯å¡«å……
                    for col in train_data_fold.columns:
                        if train_data_fold[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                            fill_value = train_data_fold[col].median()
                            train_data_fold[col] = train_data_fold[col].fillna(fill_value)
                            val_data_fold[col] = val_data_fold[col].fillna(fill_value)

                # è®­ç»ƒGTNNWRå¹¶æå–æƒé‡
                print(f"\nğŸ§  è®­ç»ƒGTNNWRæ¨¡å‹...")
                train_weights, val_weights = self._train_gtnnwr_for_fold(
                    train_data_fold,
                    val_data_fold
                )

                if train_weights is not None and val_weights is not None:
                    print(f"\nâœ… GTNNWRè®­ç»ƒå®Œæˆï¼Œå‡†å¤‡åº”ç”¨æƒé‡")

                    # âœ… å…³é”®ä¿®å¤ï¼šéªŒè¯æƒé‡çŸ©é˜µå½¢çŠ¶
                    print(f"\nğŸ”¬ æƒé‡çŸ©é˜µå½¢çŠ¶éªŒè¯:")
                    print(f"  X_trainå½¢çŠ¶: {X_train.shape}")
                    print(f"  train_weightså½¢çŠ¶: {train_weights.shape}")
                    print(f"  X_valå½¢çŠ¶: {X_val.shape}")
                    print(f"  val_weightså½¢çŠ¶: {val_weights.shape}")

                    # æ£€æŸ¥æ ·æœ¬æ•°æ˜¯å¦åŒ¹é…
                    if train_weights.shape[0] != X_train.shape[0]:
                        print(f"  âš ï¸  è®­ç»ƒé›†æ ·æœ¬æ•°ä¸åŒ¹é…: æƒé‡{train_weights.shape[0]} vs ç‰¹å¾{X_train.shape[0]}")
                        if train_weights.shape[0] < X_train.shape[0]:
                            repeat_factor = int(np.ceil(X_train.shape[0] / train_weights.shape[0]))
                            train_weights = np.repeat(train_weights, repeat_factor, axis=0)[:X_train.shape[0], :]
                            print(f"  âœ… é‡å¤è®­ç»ƒæƒé‡åˆ°{train_weights.shape[0]}ä¸ªæ ·æœ¬")
                        else:
                            train_weights = train_weights[:X_train.shape[0], :]
                            print(f"  âœ… æˆªæ–­è®­ç»ƒæƒé‡åˆ°{train_weights.shape[0]}ä¸ªæ ·æœ¬")

                    if val_weights.shape[0] != X_val.shape[0]:
                        print(f"  âš ï¸  éªŒè¯é›†æ ·æœ¬æ•°ä¸åŒ¹é…: æƒé‡{val_weights.shape[0]} vs ç‰¹å¾{X_val.shape[0]}")
                        if val_weights.shape[0] < X_val.shape[0]:
                            repeat_factor = int(np.ceil(X_val.shape[0] / val_weights.shape[0]))
                            val_weights = np.repeat(val_weights, repeat_factor, axis=0)[:X_val.shape[0], :]
                            print(f"  âœ… é‡å¤éªŒè¯æƒé‡åˆ°{val_weights.shape[0]}ä¸ªæ ·æœ¬")
                        else:
                            val_weights = val_weights[:X_val.shape[0], :]
                            print(f"  âœ… æˆªæ–­éªŒè¯æƒé‡åˆ°{val_weights.shape[0]}ä¸ªæ ·æœ¬")

                    # åº”ç”¨æƒé‡
                    print(f"\nğŸ”„ åº”ç”¨æƒé‡åˆ°ç‰¹å¾çŸ©é˜µ...")
                    X_train = self._apply_gtnnwr_weights(
                        X_train, train_weights,
                        self.feature_columns, self.gtnnwr_x_columns
                    )
                    X_val = self._apply_gtnnwr_weights(
                        X_val, val_weights,
                        self.feature_columns, self.gtnnwr_x_columns
                    )

                    # éªŒè¯æƒé‡åº”ç”¨æ•ˆæœ
                    print(f"\nğŸ” æƒé‡åº”ç”¨éªŒè¯:")
                    if not np.allclose(X_train, X_train_original, rtol=1e-10):
                        changes = np.abs(X_train - X_train_original).mean()
                        print(f"  è®­ç»ƒé›†ç‰¹å¾å¹³å‡å˜åŒ–: {changes:.6f}")
                        if changes > 0.001:
                            print(f"  âœ… æƒé‡æˆåŠŸåº”ç”¨äºè®­ç»ƒé›†")
                        else:
                            print(f"  âš ï¸  æƒé‡å¯¹è®­ç»ƒé›†å½±å“å¾ˆå°")
                    else:
                        print(f"  âš ï¸  è®­ç»ƒé›†ç‰¹å¾æœªå˜åŒ–ï¼Œæƒé‡å¯èƒ½æ— æ•ˆ")

                    if not np.allclose(X_val, X_val_original, rtol=1e-10):
                        changes = np.abs(X_val - X_val_original).mean()
                        print(f"  éªŒè¯é›†ç‰¹å¾å¹³å‡å˜åŒ–: {changes:.6f}")
                        if changes > 0.001:
                            print(f"  âœ… æƒé‡æˆåŠŸåº”ç”¨äºéªŒè¯é›†")
                        else:
                            print(f"  âš ï¸  æƒé‡å¯¹éªŒè¯é›†å½±å“å¾ˆå°")
                    else:
                        print(f"  âš ï¸  éªŒè¯é›†ç‰¹å¾æœªå˜åŒ–ï¼Œæƒé‡å¯èƒ½æ— æ•ˆ")
                else:
                    print(f"\nâŒ GTNNWRæƒé‡æå–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾")
                    self.logger.info(f"  âš ï¸ GTNNWRæƒé‡æå–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾")
            else:
                print(f"\nğŸ“ æœªä½¿ç”¨GTNNWRæƒé‡å¢å¼º")

            # æ£€æŸ¥æœ€ç»ˆæ•°æ®ä¸­çš„NaN
            final_train_nan = np.isnan(X_train).sum()
            final_val_nan = np.isnan(X_val).sum()
            if final_train_nan > 0 or final_val_nan > 0:
                print(f"  âš ï¸  æœ€ç»ˆæ•°æ®ä»æœ‰NaN - è®­ç»ƒé›†: {final_train_nan}, éªŒè¯é›†: {final_val_nan}")
                # ä½¿ç”¨0å¡«å……
                X_train = np.nan_to_num(X_train, nan=0.0)
                X_val = np.nan_to_num(X_val, nan=0.0)
                print(f"  âœ… ä½¿ç”¨0å¡«å……å‰©ä½™NaN")

            # è®­ç»ƒXGBoostæ¨¡å‹
            print(f"\nğŸŒ² è®­ç»ƒXGBoostæ¨¡å‹...")
            model = xgb.XGBRegressor(**self.params)

            print(f"  æ¨¡å‹å‚æ•°: n_estimators={self.params['n_estimators']}, "
                  f"learning_rate={self.params['learning_rate']}, "
                  f"max_depth={self.params['max_depth']}")

            print(f"  å¼€å§‹æ‹Ÿåˆæ¨¡å‹...")

            import time
            start_time = time.time()
            model.fit(X_train, y_train)
            training_time = time.time() - start_time

            print(f"  æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")

            # é¢„æµ‹
            print(f"  è¿›è¡Œé¢„æµ‹...")
            y_pred = model.predict(X_val)

            # æ£€æŸ¥é¢„æµ‹ç»“æœä¸­çš„NaN
            pred_nan = np.isnan(y_pred).sum()
            if pred_nan > 0:
                print(f"  âš ï¸  é¢„æµ‹ç»“æœä¸­æœ‰{pred_nan}ä¸ªNaNå€¼ï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……")
                y_pred = np.where(np.isnan(y_pred), np.median(y_pred[~np.isnan(y_pred)]), y_pred)

            # å­˜å‚¨ç»“æœ
            all_predictions.extend(y_pred)
            all_true_values.extend(y_val)

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            fold_metrics = self.evaluate_predictions(y_val, y_pred)
            fold_results[group_id] = fold_metrics

            fold_maes.append(fold_metrics['MAE'])
            fold_rmses.append(fold_metrics['RMSE'])
            fold_rs.append(fold_metrics['R'])
            fold_samples.append(fold_metrics['æ ·æœ¬æ•°'])

            r_display = fold_metrics['R']
            r_str = f"{r_display:.3f}" if not np.isnan(r_display) else "NaN"

            # æ‰“å°Foldç»“æœ
            print(f"\nğŸ“Š Fold {fold + 1} æ€§èƒ½æŒ‡æ ‡:")
            print(f"  MAE:  {fold_metrics['MAE']:.3f} mm")
            print(f"  RMSE: {fold_metrics['RMSE']:.3f} mm")
            print(f"  R:    {r_str}")
            print(f"  æ ·æœ¬æ•°: {fold_metrics['æ ·æœ¬æ•°']}")

            # æ£€æŸ¥æ˜¯å¦æœ‰NaNé¢„æµ‹å€¼
            nan_predictions = np.isnan(y_pred).sum()
            if nan_predictions > 0:
                print(f"  âš ï¸  è­¦å‘Š: æœ‰{nan_predictions}ä¸ªé¢„æµ‹å€¼ä¸ºNaN")

            self.logger.info(
                f"  Fold {fold + 1} æ€§èƒ½: MAE={fold_metrics['MAE']:.3f}, R={r_str}"
            )

        # è®¡ç®—æ€»ä½“æ€§èƒ½
        overall_metrics = self.evaluate_predictions(
            np.array(all_true_values),
            np.array(all_predictions)
        )

        # è®¡ç®—ç»Ÿè®¡é‡
        def safe_statistic(values, func):
            valid_values = [v for v in values if not np.isnan(v)]
            if len(valid_values) == 0:
                return np.nan
            return func(valid_values)

        mean_metrics = {
            'MAE': safe_statistic(fold_maes, np.mean),
            'RMSE': safe_statistic(fold_rmses, np.mean),
            'R': safe_statistic(fold_rs, np.mean),
            'æ ·æœ¬æ•°': np.sum(fold_samples)
        }

        median_metrics = {
            'MAE': safe_statistic(fold_maes, np.median),
            'RMSE': safe_statistic(fold_rmses, np.median),
            'R': safe_statistic(fold_rs, np.median),
            'æ ·æœ¬æ•°': np.sum(fold_samples)
        }

        std_metrics = {
            'MAE': safe_statistic(fold_maes, np.std),
            'RMSE': safe_statistic(fold_rmses, np.std),
            'R': safe_statistic(fold_rs, np.std)
        }

        print("\n" + "=" * 100)
        print(f"ğŸ‰ {cv_type}äº¤å‰éªŒè¯å®Œæˆ!")
        print("=" * 100)

        print(f"\nğŸ“ˆ èšåˆæ€§èƒ½æŒ‡æ ‡:")
        print(f"  MAE:  {overall_metrics['MAE']:.3f} mm")
        print(f"  RMSE: {overall_metrics['RMSE']:.3f} mm")
        print(f"  R:    {overall_metrics['R']:.3f}")
        print(f"  æ€»æ ·æœ¬æ•°: {overall_metrics['æ ·æœ¬æ•°']}")

        print(f"\nğŸ“Š æŠ˜å ç»Ÿè®¡:")
        print(f"  æŠ˜å æ•°: {total_folds}")
        print(f"  MAEå‡å€¼: {mean_metrics['MAE']:.3f} Â± {std_metrics['MAE']:.3f} mm")
        print(f"  RMSEå‡å€¼: {mean_metrics['RMSE']:.3f} Â± {std_metrics['RMSE']:.3f} mm")
        print(f"  Rå‡å€¼:   {mean_metrics['R']:.3f} Â± {std_metrics['R']:.3f}")

        # ä¸çº¯XGBoostæ¯”è¾ƒï¼ˆå¦‚æœæœ‰å†å²æ•°æ®ï¼‰
        print(f"\nğŸ’¡ æ€§èƒ½åˆ†æ:")
        if self.use_gtnnwr:
            print(f"  æœ¬æ¬¡å®éªŒä½¿ç”¨äº†GTNNWRæƒé‡å¢å¼º")
            if mean_metrics['R'] > 0.6:
                print(f"  âœ… æ¨¡å‹æ€§èƒ½è‰¯å¥½ (R > 0.6)")
            else:
                print(f"  âš ï¸  æ¨¡å‹æ€§èƒ½æœ‰å¾…æå‡ (R = {mean_metrics['R']:.3f})")

        # æ‰“å°NaNå¤„ç†ç»Ÿè®¡
        if hasattr(self, 'nan_fill_stats') and self.nan_fill_stats:
            print(f"\nğŸ“Š NaNå¤„ç†ç»Ÿè®¡:")
            total_filled = sum(stats['original_nan_count'] for stats in self.nan_fill_stats.values())
            print(f"  æ€»å…±å¡«å……äº† {total_filled} ä¸ªNaNå€¼")
            print(f"  ä½¿ç”¨çš„ç­–ç•¥: {self.nan_strategy}")

        self.logger.info(f"âœ… {cv_type}äº¤å‰éªŒè¯å®Œæˆ")
        self.logger.info(f"  èšåˆæ€§èƒ½: MAE={overall_metrics['MAE']:.3f}mm, R={overall_metrics['R']:.3f}")

        return {
            'overall': overall_metrics,
            'mean': mean_metrics,
            'median': median_metrics,
            'std': std_metrics,
            'by_fold': fold_results,
            'predictions': np.array(all_predictions),
            'true_values': np.array(all_true_values),
            'folds': total_folds,
            'fold_metrics': {
                'MAE': fold_maes,
                'RMSE': fold_rmses,
                'R': fold_rs,
                'samples': fold_samples
            }
        }

    def evaluate_predictions(self, y_true, y_pred):
        """è¯„ä¼°é¢„æµ‹ç»“æœ"""
        mask = ~(np.isnan(y_true) | np.isnan(y_pred))
        y_true_clean = y_true[mask]
        y_pred_clean = y_pred[mask]

        if len(y_true_clean) == 0:
            return {
                'MAE': np.nan,
                'RMSE': np.nan,
                'R': np.nan,
                'R_pvalue': np.nan,
                'æ ·æœ¬æ•°': 0,
                'æ€»æ ·æœ¬æ•°': len(y_true),
                'æœ‰æ•ˆæ ·æœ¬æ¯”ä¾‹': 0.0
            }

        mae = mean_absolute_error(y_true_clean, y_pred_clean)
        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))

        def safe_pearsonr(x, y):
            if len(x) <= 1 or np.all(x == x[0]) or np.all(y == y[0]):
                return np.nan, np.nan
            if np.std(x) == 0 or np.std(y) == 0:
                return np.nan, np.nan
            try:
                return pearsonr(x, y)
            except:
                return np.nan, np.nan

        r, p_value = safe_pearsonr(y_true_clean, y_pred_clean)

        return {
            'MAE': mae,
            'RMSE': rmse,
            'R': r,
            'R_pvalue': p_value,
            'æ ·æœ¬æ•°': len(y_true_clean),
            'æ€»æ ·æœ¬æ•°': len(y_true),
            'æœ‰æ•ˆæ ·æœ¬æ¯”ä¾‹': len(y_true_clean) / len(y_true) if len(y_true) > 0 else 0
        }

    def train_final_model(self, X, y, gtnnwr_data=None):
        """è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰"""
        self.logger.info("è®­ç»ƒæœ€ç»ˆXGBoostæ¨¡å‹...")

        # æ£€æŸ¥å¹¶å¤„ç†NaNå€¼
        x_nan = np.isnan(X).sum()
        y_nan = np.isnan(y).sum()
        if x_nan > 0 or y_nan > 0:
            self.logger.info(f"æœ€ç»ˆæ¨¡å‹è®­ç»ƒå‰å¤„ç†NaNå€¼: Xä¸­æœ‰{x_nan}ä¸ªNaN, yä¸­æœ‰{y_nan}ä¸ªNaN")
            # ä½¿ç”¨åˆ—å‡å€¼å¡«å……Xçš„NaN
            col_means = np.nanmean(X, axis=0)
            for i in range(X.shape[1]):
                X[:, i] = np.where(np.isnan(X[:, i]), col_means[i], X[:, i])
            # ä½¿ç”¨ä¸­ä½æ•°å¡«å……yçš„NaN
            y = np.where(np.isnan(y), np.nanmedian(y), y)

        # GTNNWRæƒé‡å¢å¼º
        if self.use_gtnnwr and gtnnwr_data is not None:
            self.logger.info("ä¸ºæœ€ç»ˆæ¨¡å‹è®­ç»ƒGTNNWR...")

            # æ£€æŸ¥GTNNWRæ•°æ®ä¸­çš„NaN
            gtnnwr_nan = gtnnwr_data.isna().sum().sum()
            if gtnnwr_nan > 0:
                self.logger.info(f"GTNNWRæ•°æ®ä¸­æœ‰{gtnnwr_nan}ä¸ªNaNå€¼ï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……")
                for col in gtnnwr_data.columns:
                    if gtnnwr_data[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                        fill_value = gtnnwr_data[col].median()
                        gtnnwr_data[col] = gtnnwr_data[col].fillna(fill_value)

            # ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒGTNNWR
            train_weights, _ = self._train_gtnnwr_for_fold(gtnnwr_data, gtnnwr_data.head(1))

            if train_weights is not None:
                X = self._apply_gtnnwr_weights(
                    X, train_weights,
                    self.feature_columns, self.gtnnwr_x_columns
                )
                self.logger.info("âœ… æœ€ç»ˆæ¨¡å‹GTNNWRæƒé‡åº”ç”¨æˆåŠŸ")

        # è®­ç»ƒXGBoost
        self.model = xgb.XGBRegressor(**self.params)
        self.model.fit(X, y)

        self.logger.info("âœ… æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
        return self.model

    def run_complete_analysis(self, df, output_dir=None):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹ - å…ˆè¿›è¡Œå¹´åº¦äº¤å‰éªŒè¯"""
        self.logger.info("=" * 70)
        self.logger.info("ğŸš€ å¼€å§‹GTNNW-XGBoostå®Œæ•´åˆ†ææµç¨‹")
        self.logger.info("=" * 70)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = f"./gtnnw_xgboost_results_{timestamp}"

        os.makedirs(output_dir, exist_ok=True)
        self.logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")

        try:
            # 1. æ•°æ®é¢„å¤„ç†
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 1: æ•°æ®é¢„å¤„ç†")
            self.logger.info("=" * 50)

            X, y, station_groups, year_groups, gtnnwr_data = self.preprocess_data(df, is_training=True)

            results = {
                'preprocessing': {
                    'samples': len(X),
                    'features': len(self.feature_columns),
                    'stations': len(np.unique(station_groups)),
                    'years': len(np.unique(year_groups)),
                    'use_gtnnwr': self.use_gtnnwr,
                    'nan_strategy': self.nan_strategy,
                    'nan_fill_stats': self.nan_fill_stats
                }
            }

            # 2. å…ˆè¿›è¡Œå¹´åº¦äº¤å‰éªŒè¯ï¼ˆæ•°æ®é‡è¾ƒå°ï¼‰
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 2: å¹´åº¦äº¤å‰éªŒè¯ (æ•°æ®é‡è¾ƒå°ï¼Œå…ˆå¼€å§‹)")
            self.logger.info("=" * 50)

            results['yearly_cv'] = self.cross_validate(
                X, y, year_groups, 'yearly', gtnnwr_data
            )

            # 3. å†è¿›è¡Œç«™ç‚¹äº¤å‰éªŒè¯ï¼ˆæ•°æ®é‡è¾ƒå¤§ï¼‰
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 3: ç«™ç‚¹äº¤å‰éªŒè¯ (æ•°æ®é‡è¾ƒå¤§)")
            self.logger.info("=" * 50)

            # å¯¹äºç«™ç‚¹äº¤å‰éªŒè¯ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç®€åŒ–çš„GTNNWRè®­ç»ƒï¼ˆå‡å°‘è½®æ•°ï¼‰
            if self.use_gtnnwr:
                self.logger.info("ç«™ç‚¹äº¤å‰éªŒè¯ä½¿ç”¨ç®€åŒ–çš„GTNNWRè®­ç»ƒï¼ˆå‡å°‘åˆ°3ä¸ªepochï¼‰")
                original_epochs = self.gtnnwr_params.get('max_epoch', 5)
                self.gtnnwr_params['max_epoch'] = 3  # å‡å°‘è®­ç»ƒè½®æ•°

                results['station_cv'] = self.cross_validate(
                    X, y, station_groups, 'station', gtnnwr_data
                )

                # æ¢å¤åŸå§‹è®¾ç½®
                self.gtnnwr_params['max_epoch'] = original_epochs
            else:
                results['station_cv'] = self.cross_validate(
                    X, y, station_groups, 'station', gtnnwr_data
                )

            # 4. è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼ˆä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 4: è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
            self.logger.info("=" * 50)

            results['final_model'] = self.train_final_model(X, y, gtnnwr_data)

            # 5. ç‰¹å¾é‡è¦æ€§åˆ†æ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 5: ç‰¹å¾é‡è¦æ€§åˆ†æ")
            self.logger.info("=" * 50)

            results['feature_importance'] = self.get_feature_importance()

            # 6. ä¿å­˜ç»“æœ
            self.logger.info("\n" + "=" * 50)
            self.logger.info("æ­¥éª¤ 6: ä¿å­˜ç»“æœ")
            self.logger.info("=" * 50)

            self._save_results(results, output_dir)

            # 7. ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report(results)
            print(report)

            self.logger.info("ğŸ¯ å®Œæ•´åˆ†æå®Œæˆï¼")
            return results

        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææµç¨‹å¤±è´¥: {str(e)}")
            raise

    def _save_results(self, results, output_dir):
        """ä¿å­˜ç»“æœ"""
        try:
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            if 'final_model' in results:
                model_path = f'{output_dir}/final_model.pkl'
                joblib.dump(results['final_model'], model_path)
                self.logger.info(f"âœ… æ¨¡å‹ä¿å­˜: {model_path}")

            # ä¿å­˜NaNå¤„ç†ä¿¡æ¯
            nan_info_path = f'{output_dir}/nan_handling_info.json'
            nan_info = {
                'strategy': self.nan_strategy,
                'fill_values': self.nan_fill_values,
                'fill_stats': self.nan_fill_stats
            }
            with open(nan_info_path, 'w', encoding='utf-8') as f:
                json.dump(nan_info, f, indent=2, ensure_ascii=False,
                          default=lambda x: float(x) if isinstance(x, (np.float32, np.float64)) else x)
            self.logger.info(f"âœ… NaNå¤„ç†ä¿¡æ¯ä¿å­˜: {nan_info_path}")

            # ä¿å­˜è¯¦ç»†ç»“æœ
            eval_results = {
                'training_info': {
                    'timestamp': datetime.now().isoformat(),
                    'feature_columns': self.feature_columns,
                    'gtnnwr_x_columns': self.gtnnwr_x_columns,
                    'gtnnwr_spatial_columns': self.gtnnwr_spatial_columns,
                    'gtnnwr_temp_columns': self.gtnnwr_temp_columns,
                    'use_gtnnwr': self.use_gtnnwr,
                    'nan_strategy': self.nan_strategy,
                    'total_samples': results.get('preprocessing', {}).get('samples', 0)
                },
                'model_parameters': self.params,
                'gtnnwr_parameters': self.gtnnwr_params,
                'station_cross_validation': results.get('station_cv', {}),
                'yearly_cross_validation': results.get('yearly_cv', {})
            }

            eval_path = f'{output_dir}/evaluation_results.json'
            with open(eval_path, 'w', encoding='utf-8') as f:
                json.dump(eval_results, f, indent=2, ensure_ascii=False, default=float)
            self.logger.info(f"âœ… è¯¦ç»†è¯„ä¼°ç»“æœä¿å­˜: {eval_path}")

            # ç”Ÿæˆå¯è§†åŒ–
            self._create_scatter_plots(results, output_dir)

        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")

    def _create_scatter_plots(self, results, output_dir):
        """åˆ›å»ºæ•£ç‚¹å›¾"""
        try:
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
            plt.rcParams['axes.unicode_minus'] = False

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # ç«™ç‚¹CVæ•£ç‚¹å›¾
            if 'station_cv' in results:
                overall = results['station_cv']['overall']
                self._plot_single_scatter(
                    ax1,
                    results['station_cv']['true_values'],
                    results['station_cv']['predictions'],
                    overall,
                    'Station Cross-Validation'
                )

            # å¹´åº¦CVæ•£ç‚¹å›¾
            if 'yearly_cv' in results:
                overall = results['yearly_cv']['overall']
                self._plot_single_scatter(
                    ax2,
                    results['yearly_cv']['true_values'],
                    results['yearly_cv']['predictions'],
                    overall,
                    'Yearly Cross-Validation'
                )

            plt.tight_layout()
            scatter_path = f'{output_dir}/scatter_plots.png'
            plt.savefig(scatter_path, dpi=300, bbox_inches='tight')
            plt.close()
            self.logger.info(f"âœ… æ•£ç‚¹å›¾ä¿å­˜: {scatter_path}")

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆæ•£ç‚¹å›¾å¤±è´¥: {str(e)}")

    def _plot_single_scatter(self, ax, y_true, y_pred, metrics, title):
        """ç»˜åˆ¶å•ä¸ªæ•£ç‚¹å›¾"""
        mask = ~(np.isnan(y_true) | np.isnan(y_pred))
        y_true_clean = y_true[mask]
        y_pred_clean = y_pred[mask]

        if len(y_true_clean) == 0:
            return

        max_range = 175
        ax.plot([0, max_range], [0, max_range], 'k-', alpha=0.8, linewidth=2)
        ax.scatter(y_true_clean, y_pred_clean, alpha=0.6, s=15, c='blue', edgecolors='none')

        ax.set_xlabel('Observed SWE (mm)', fontsize=14)
        ax.set_ylabel('Predicted SWE (mm)', fontsize=14)
        ax.set_title(title, fontsize=16, fontweight='bold')
        ax.set_xlim([0, max_range])
        ax.set_ylim([0, max_range])
        ax.grid(True, alpha=0.3)

        stats_text = f"MAE = {metrics['MAE']:.2f} mm\nRMSE = {metrics['RMSE']:.2f} mm\nR = {metrics['R']:.3f}\nN = {len(y_true_clean)}"
        ax.text(0.95, 0.95, stats_text, transform=ax.transAxes,
                verticalalignment='top', horizontalalignment='right',
                fontsize=13, fontfamily='monospace', weight='bold',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

    def get_feature_importance(self):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")

        importance_scores = self.model.feature_importances_

        if len(importance_scores) != len(self.feature_columns):
            min_length = min(len(importance_scores), len(self.feature_columns))
            importance_scores = importance_scores[:min_length]
            feature_names = self.feature_columns[:min_length]
        else:
            feature_names = self.feature_columns

        feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance_scores
        }).sort_values('importance', ascending=False)

        return feature_importance_df

    def _generate_report(self, results):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("ğŸ“Š GTNNW-XGBoostæ¨¡å‹åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"ä½¿ç”¨GTNNWRæƒé‡å¢å¼º: {self.use_gtnnwr}")
        report_lines.append(f"NaNå¤„ç†ç­–ç•¥: {self.nan_strategy}")
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")

        # NaNå¤„ç†ç»Ÿè®¡
        if 'preprocessing' in results and 'nan_fill_stats' in results['preprocessing']:
            nan_stats = results['preprocessing']['nan_fill_stats']
            total_filled = sum(stats['original_nan_count'] for stats in nan_stats.values())
            report_lines.append(f"NaNå¤„ç†ç»Ÿè®¡: æ€»å…±å¡«å……äº† {total_filled} ä¸ªNaNå€¼")
            report_lines.append("")

        # ç«™ç‚¹CVç»“æœ
        if 'station_cv' in results:
            station = results['station_cv']
            report_lines.append("ğŸ“ ç«™ç‚¹äº¤å‰éªŒè¯ (ç©ºé—´è¯„ä¼°):")
            report_lines.append(f"  èšåˆMAE: {station['overall']['MAE']:.3f} mm")
            report_lines.append(f"  èšåˆRMSE: {station['overall']['RMSE']:.3f} mm")
            report_lines.append(f"  èšåˆR: {station['overall']['R']:.3f}")
            report_lines.append(f"  æŠ˜å æ•°: {station['folds']}")
            report_lines.append("")

        # å¹´åº¦CVç»“æœ
        if 'yearly_cv' in results:
            yearly = results['yearly_cv']
            report_lines.append("ğŸ“… å¹´åº¦äº¤å‰éªŒè¯ (æ—¶é—´è¯„ä¼°):")
            report_lines.append(f"  èšåˆMAE: {yearly['overall']['MAE']:.3f} mm")
            report_lines.append(f"  èšåˆRMSE: {yearly['overall']['RMSE']:.3f} mm")
            report_lines.append(f"  èšåˆR: {yearly['overall']['R']:.3f}")
            report_lines.append(f"  æŠ˜å æ•°: {yearly['folds']}")
            report_lines.append("")

        # æ€§èƒ½æ¯”è¾ƒ
        if 'station_cv' in results and 'yearly_cv' in results:
            station_r = results['station_cv']['overall']['R']
            yearly_r = results['yearly_cv']['overall']['R']

            report_lines.append("ğŸ’¡ æ€§èƒ½åˆ†æ:")
            if not np.isnan(station_r) and not np.isnan(yearly_r):
                if station_r > yearly_r:
                    report_lines.append(f"  ç«™ç‚¹CVä¼˜äºå¹´åº¦CV (R: {station_r:.3f} > {yearly_r:.3f})")
                else:
                    report_lines.append(f"  å¹´åº¦CVä¼˜äºç«™ç‚¹CV (R: {yearly_r:.3f} > {station_r:.3f})")

        report_lines.append("\n" + "=" * 80)
        return "\n".join(report_lines)


# ä¾¿æ·ä½¿ç”¨å‡½æ•°
def train_gtnnw_xgboost_model(data_df, output_dir=None, use_gtnnwr=True,
                              nan_strategy='median', nan_fill_value=0.0):
    """ä¾¿æ·å‡½æ•°ï¼šè®­ç»ƒGTNNW-XGBoostæ¨¡å‹

    Args:
        data_df (pd.DataFrame): åŒ…å«ç‰¹å¾å’ŒSWEçš„æ•°æ®
        output_dir (str, optional): è¾“å‡ºç›®å½•è·¯å¾„
        use_gtnnwr (bool): æ˜¯å¦ä½¿ç”¨GTNNWRæƒé‡
        nan_strategy (str): NaNå¤„ç†ç­–ç•¥
        nan_fill_value (float): å¡«å……NaNçš„å€¼

    Returns:
        dict: åŒ…å«æ‰€æœ‰è®­ç»ƒç»“æœçš„å­—å…¸
    """
    trainer = GTNNW_XGBoostTrainer(
        use_gtnnwr=use_gtnnwr,
        nan_strategy=nan_strategy,
        nan_fill_value=nan_fill_value
    )
    return trainer.run_complete_analysis(data_df, output_dir)


# å¯¹æ¯”å®éªŒå‡½æ•°
def compare_models(data_df, output_dir=None):
    """å¯¹æ¯”çº¯XGBoostå’ŒGTNNW-XGBoostçš„æ€§èƒ½"""

    print("=" * 80)
    print("ğŸ”¬ å¼€å§‹æ¨¡å‹å¯¹æ¯”å®éªŒ")
    print("=" * 80)

    # 1. çº¯XGBoost
    print("\n1. è®­ç»ƒçº¯XGBoostæ¨¡å‹...")
    xgb_trainer = GTNNW_XGBoostTrainer(use_gtnnwr=False, nan_strategy='median')
    xgb_results = xgb_trainer.run_complete_analysis(
        data_df,
        output_dir=os.path.join(output_dir, "xgboost_only") if output_dir else None
    )

    # 2. GTNNW-XGBoost
    print("\n2. è®­ç»ƒGTNNW-XGBoostæ¨¡å‹...")
    gtnnw_trainer = GTNNW_XGBoostTrainer(use_gtnnwr=True, nan_strategy='median')
    gtnnw_results = gtnnw_trainer.run_complete_analysis(
        data_df,
        output_dir=os.path.join(output_dir, "gtnnw_xgboost") if output_dir else None
    )

    # 3. å¯¹æ¯”åˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ")
    print("=" * 80)

    if 'station_cv' in xgb_results and 'station_cv' in gtnnw_results:
        xgb_station_r = xgb_results['station_cv']['overall']['R']
        gtnnw_station_r = gtnnw_results['station_cv']['overall']['R']

        print("ç«™ç‚¹äº¤å‰éªŒè¯ (ç©ºé—´è¯„ä¼°):")
        print(f"  çº¯XGBoost: R = {xgb_station_r:.3f}")
        print(f"  GTNNW-XGBoost: R = {gtnnw_station_r:.3f}")

        if not np.isnan(xgb_station_r) and not np.isnan(gtnnw_station_r):
            improvement = (gtnnw_station_r - xgb_station_r) / abs(xgb_station_r) * 100
            print(f"  GTNNW-XGBoostæå‡: {improvement:+.1f}%")

    if 'yearly_cv' in xgb_results and 'yearly_cv' in gtnnw_results:
        xgb_yearly_r = xgb_results['yearly_cv']['overall']['R']
        gtnnw_yearly_r = gtnnw_results['yearly_cv']['overall']['R']

        print("\nå¹´åº¦äº¤å‰éªŒè¯ (æ—¶é—´è¯„ä¼°):")
        print(f"  çº¯XGBoost: R = {xgb_yearly_r:.3f}")
        print(f"  GTNNW-XGBoost: R = {gtnnw_yearly_r:.3f}")

        if not np.isnan(xgb_yearly_r) and not np.isnan(gtnnw_yearly_r):
            improvement = (gtnnw_yearly_r - xgb_yearly_r) / abs(xgb_yearly_r) * 100
            print(f"  GTNNW-XGBoostæå‡: {improvement:+.1f}%")

    return {
        'xgboost': xgb_results,
        'gtnnw_xgboost': gtnnw_results
    }