import logging
import sys
import os
import argparse

import numpy as np
import pandas as pd




# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# å¯¼å…¥è®­ç»ƒå™¨
try:
    from gtnnw_xgboost_trianer import GTNNW_XGBoostTrainer, train_gtnnw_xgboost_model, compare_models
    from swe_trainer import SWEXGBoostTrainer, train_swe_model
except ImportError as e:
    print(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿ gtnnw_xgboost_trainer.py å’Œ swe_trainer.py åœ¨å½“å‰ç›®å½•")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('swe_training.log', encoding='utf-8')
    ]
)

logger = logging.getLogger("SWETrainingMain")


def build_model_parameters(args):
    """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°æ„å»ºæ¨¡å‹å‚æ•°å­—å…¸

    Args:
        args: argparseå‚æ•°å¯¹è±¡

    Returns:
        dict: XGBoostå‚æ•°å­—å…¸
    """
    params = {
        'n_estimators': args.trees,
        'learning_rate': args.lr,
        'max_depth': args.depth,
        'min_child_weight': getattr(args, 'min_child_weight', 5),
        'gamma': getattr(args, 'gamma', 0),
        'subsample': args.subsample,
        'colsample_bytree': args.colsample,
        'reg_alpha': getattr(args, 'reg_alpha', 0.05),
        'random_state': 42,
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse'
    }

    # å¯é€‰ï¼šæ·»åŠ å…¶ä»–å‚æ•°ï¼Œå¦‚æœç”¨æˆ·åœ¨å‘½ä»¤è¡Œä¸­æŒ‡å®šäº†çš„è¯
    optional_params = ['reg_lambda', 'max_delta_step', 'scale_pos_weight']
    for param in optional_params:
        if hasattr(args, param) and getattr(args, param) is not None:
            params[param] = getattr(args, param)

    return params


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£ - æ”¯æŒGTNNW-XGBoostèåˆ"""
    parser = argparse.ArgumentParser(
        description='SWE XGBoost/GTNNW-XGBoostæ¨¡å‹è®­ç»ƒ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python main.py -d data.csv
  python main.py -d data.csv -o ./results --trees 100 --lr 0.1
  python main.py -d data.csv --use-gtnnwr              # ä½¿ç”¨GTNNWRæƒé‡å¢å¼º
  python main.py -d data.csv --compare-models        # å¯¹æ¯”çº¯XGBoostå’ŒGTNNW-XGBoost
        """
    )

    parser.add_argument('--data', '-d', required=True,
                        help='è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ (æ”¯æŒCSV/Excel/Parquet)')
    parser.add_argument('--output', '-o', default=None,
                        help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤:è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³ç›®å½•)')
    parser.add_argument('--trees', '-n', type=int, default=60,
                        help='æ ‘çš„æ•°é‡ (é»˜è®¤: 60)')
    parser.add_argument('--lr', '--learning-rate', type=float, default=0.17,
                        help='å­¦ä¹ ç‡ (é»˜è®¤: 0.17)')
    parser.add_argument('--depth', type=int, default=5,
                        help='æ ‘çš„æœ€å¤§æ·±åº¦ (é»˜è®¤: 5)')
    parser.add_argument('--subsample', type=float, default=0.8,
                        help='å­é‡‡æ ·æ¯”ä¾‹ (é»˜è®¤: 0.8)')
    parser.add_argument('--colsample', type=float, default=0.5,
                        help='ç‰¹å¾é‡‡æ ·æ¯”ä¾‹ (é»˜è®¤: 0.5)')

    # GTNNWRç›¸å…³å‚æ•°
    parser.add_argument('--use-gtnnwr', action='store_true',
                        help='ä½¿ç”¨GTNNWRæƒé‡å¢å¼ºXGBoost')
    parser.add_argument('--gtnnwr-epochs', type=int, default=5,
                        help='GTNNWRè®­ç»ƒè½®æ•° (é»˜è®¤: 5)')
    parser.add_argument('--compare-models', action='store_true',
                        help='å¯¹æ¯”çº¯XGBoostå’ŒGTNNW-XGBoostæ€§èƒ½')
    parser.add_argument('--no-gtnnwr', action='store_true',
                        help='å¼ºåˆ¶ä¸ä½¿ç”¨GTNNWRï¼ˆç”¨äºå¯¹æ¯”å®éªŒï¼‰')

    # GTNNWRç‰¹æœ‰å‚æ•°
    parser.add_argument('--graph-layers', type=str, default="[[3], [512,256,64]]",
                        help='GTNNWRå›¾å·ç§¯å±‚ç»“æ„ (é»˜è®¤: [[3], [512,256,64]])')
    parser.add_argument('--drop-out', type=float, default=0.4,
                        help='GTNNWR dropoutæ¯”ä¾‹ (é»˜è®¤: 0.4)')

    # å…¶ä»–å‚æ•°
    parser.add_argument('--cluster-mode', action='store_true',
                        help='ä½¿ç”¨èšç±»é›†æˆæ¨¡å¼')
    parser.add_argument('--n-clusters', type=int, default=4,
                        help='èšç±»æ•°é‡ (é»˜è®¤: 4)')
    parser.add_argument('--use-rf', action='store_true',
                        help='åœ¨èšç±»é›†æˆä¸­ä½¿ç”¨éšæœºæ£®æ—ä»£æ›¿XGBoost')
    parser.add_argument('--optimize', choices=['rf', 'xgb', 'gtnnwr', 'all'],
                        help='ä½¿ç”¨Optunaä¼˜åŒ–æŒ‡å®šæ¨¡å‹çš„è¶…å‚æ•°')
    parser.add_argument('--n-trials', type=int, default=50,
                        help='Optunaä¼˜åŒ–è¯•éªŒæ¬¡æ•°')
    parser.add_argument('--pure-gtnnwr', action='store_true',
                        help='è¿è¡Œçº¯å‡€ç‰ˆGTNNWRå¯¹æ¯”å®éªŒ')

    args = parser.parse_args()

    try:
        logger.info("ğŸš€ å¯åŠ¨SWEæ¨¡å‹è®­ç»ƒç¨‹åº")
        logger.info(f"è¾“å…¥æ–‡ä»¶: {args.data}")
        logger.info(f"è¾“å‡ºç›®å½•: {args.output or 'è‡ªåŠ¨ç”Ÿæˆ'}")
        logger.info(f"ä½¿ç”¨GTNNWRæƒé‡å¢å¼º: {args.use_gtnnwr}")

        # æ£€æŸ¥å‚æ•°ä¸€è‡´æ€§
        if args.no_gtnnwr and args.use_gtnnwr:
            logger.warning("--no-gtnnwrå’Œ--use-gtnnwråŒæ—¶æŒ‡å®šï¼Œå°†ç¦ç”¨GTNNWR")
            args.use_gtnnwr = False

        # 1. åŠ è½½æ•°æ®
        logger.info("ğŸ“¥ åŠ è½½æ•°æ®...")
        df = load_data(args.data)

        if df.empty:
            logger.error("æ•°æ®åŠ è½½å¤±è´¥æˆ–æ•°æ®ä¸ºç©º")
            return 1

        logger.info(f"æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")

        # æ£€æŸ¥GTNNWRæ‰€éœ€çš„å…³é”®ç‰¹å¾åˆ—æ˜¯å¦å­˜åœ¨
        if args.use_gtnnwr or args.compare_models:
            # GTNNWRéœ€è¦ç©ºé—´åˆ—å’Œæ—¶é—´åˆ—
            gtnnwr_required_cols = ['X', 'Y', 'year', 'month', 'doy']
            missing_cols = [col for col in gtnnwr_required_cols if col not in df.columns]
            if missing_cols:
                logger.warning(f"GTNNWRéœ€è¦ä½†æ•°æ®ä¸­ç¼ºå°‘çš„åˆ—: {missing_cols}")
                logger.warning("GTNNWRå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œï¼Œå»ºè®®æ£€æŸ¥æ•°æ®")
                if args.use_gtnnwr:
                    response = input("ç»§ç»­ä½¿ç”¨GTNNWRå—ï¼Ÿ(y/n): ").strip().lower()
                    if response != 'y':
                        logger.info("ç¦ç”¨GTNNWRï¼Œä½¿ç”¨çº¯XGBoost")
                        args.use_gtnnwr = False

        # æ„å»ºæ¨¡å‹å‚æ•°
        params = build_model_parameters(args)

        if args.cluster_mode:
            # ä½¿ç”¨èšç±»é›†æˆæ¨¡å¼
            logger.info("ğŸ¯ ä½¿ç”¨èšç±»é›†æˆæ¨¡å¼")
            logger.info(f"èšç±»æ•°é‡: {args.n_clusters}")
            logger.info(f"ä½¿ç”¨{'éšæœºæ£®æ—' if args.use_rf else 'XGBoost'}ä½œä¸ºåŸºç¡€æ¨¡å‹")

            # è¿™é‡Œéœ€è¦å¯¼å…¥èšç±»é›†æˆå‡½æ•°
            try:
                from cluster import train_swe_cluster_ensemble
                results = train_swe_cluster_ensemble(
                    data_df=df,
                    output_dir=args.output,
                    n_clusters=args.n_clusters,
                    params=build_model_parameters(args),
                    use_rf=args.use_rf
                )
            except ImportError:
                logger.error("èšç±»é›†æˆæ¨¡å—æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿cluster.pyåœ¨ç›®å½•ä¸­")
                return 1

        elif args.compare_models:
            # å¯¹æ¯”å®éªŒæ¨¡å¼
            logger.info("ğŸ”¬ å¯åŠ¨æ¨¡å‹å¯¹æ¯”å®éªŒï¼šçº¯XGBoost vs GTNNW-XGBoost")

            if args.output is None:
                timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
                args.output = f"./model_comparison_{timestamp}"

            comparison_results = compare_models(df, args.output)

            # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
            print_comparison_summary(comparison_results)

            return 0

        elif args.use_gtnnwr:
            # GTNNW-XGBoostæ¨¡å¼
            logger.info("ğŸ¯ ä½¿ç”¨GTNNW-XGBoostèåˆæ¨¡å¼")
            logger.info(f"GTNNWRè®­ç»ƒè½®æ•°: {args.gtnnwr_epochs}")
            logger.info(f"GTNNWRå›¾å·ç§¯å±‚: {args.graph_layers}")
            logger.info(f"GTNNWR dropoutæ¯”ä¾‹: {args.drop_out}")

            # è§£æå›¾å·ç§¯å±‚ç»“æ„
            try:
                graph_layers = eval(args.graph_layers)
            except:
                logger.warning(f"æ— æ³•è§£æå›¾å·ç§¯å±‚ç»“æ„: {args.graph_layers}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                graph_layers = [[3], [512, 256, 64]]

            # é…ç½®GTNNWRå‚æ•°
            gtnnwr_params = {
                'max_epoch': args.gtnnwr_epochs,
                'graph_layers': graph_layers,
                'drop_out': args.drop_out,
                'optimizer_params': {
                    "scheduler": "MultiStepLR",
                    "scheduler_milestones": [1000, 2000, 3000, 4000],
                    "scheduler_gamma": 0.8,
                }
            }

            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = GTNNW_XGBoostTrainer(
                params=params,
                gtnnwr_params=gtnnwr_params,
                use_gtnnwr=True
            )

            # è¿è¡Œå®Œæ•´åˆ†æ
            results = trainer.run_complete_analysis(df, args.output)

        elif args.no_gtnnwr:
            # å¼ºåˆ¶çº¯XGBoostæ¨¡å¼
            logger.info("ğŸ¯ ä½¿ç”¨çº¯XGBoostæ¨¡å¼ï¼ˆå¼ºåˆ¶ç¦ç”¨GTNNWRï¼‰")
            results = train_swe_model(df, args.output, params)

        else:
            # é»˜è®¤çº¯XGBoostæ¨¡å¼
            logger.info("ğŸ¯ ä½¿ç”¨æ ‡å‡†XGBoostæ¨¡å¼")
            results = train_swe_model(df, args.output, params)

        # è¶…å‚æ•°ä¼˜åŒ–
        if args.optimize:
            try:
                from optuna_optimizer import optimize_swe_model
                logger.info(f"å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–: {args.optimize}, è¯•éªŒæ¬¡æ•°: {args.n_trials}")

                if args.optimize == 'all':
                    for model_type in ['rf', 'gtnnwr']:
                        best_params = optimize_swe_model(df, model_type, args.n_trials)
                        logger.info(f"{model_type} æœ€ä½³å‚æ•°: {best_params}")
                else:
                    best_params = optimize_swe_model(df, args.optimize, args.n_trials)
                    logger.info(f"æœ€ä½³å‚æ•°: {best_params}")
            except ImportError:
                logger.warning("Optunaä¼˜åŒ–æ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡ä¼˜åŒ–")

        if args.pure_gtnnwr:
            try:
                from cluster import train_pure_gtnnwr_analysis
                results = train_pure_gtnnwr_analysis(df)
            except ImportError:
                logger.warning("çº¯å‡€ç‰ˆGTNNWRåˆ†ææ¨¡å—æœªæ‰¾åˆ°")

        logger.info("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

        # æ˜¾ç¤ºå…³é”®ç»“æœ
        print_summary(results)

        return 0

    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return 1


def print_comparison_summary(comparison_results):
    """æ‰“å°æ¨¡å‹å¯¹æ¯”ç»“æœæ‘˜è¦"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¨¡å‹å¯¹æ¯”å®éªŒç»“æœ")
    print("=" * 80)

    xgb_results = comparison_results.get('xgboost', {})
    gtnnw_results = comparison_results.get('gtnnw_xgboost', {})

    # ç«™ç‚¹äº¤å‰éªŒè¯å¯¹æ¯”
    if 'station_cv' in xgb_results and 'station_cv' in gtnnw_results:
        xgb_station = xgb_results['station_cv']['overall']
        gtnnw_station = gtnnw_results['station_cv']['overall']

        print("\nğŸ“ ç«™ç‚¹äº¤å‰éªŒè¯ (ç©ºé—´è¯„ä¼°):")
        print(f"  {'æŒ‡æ ‡':<10} {'çº¯XGBoost':<12} {'GTNNW-XGBoost':<12} {'æå‡':<10}")
        print("-" * 50)

        # MAEå¯¹æ¯”
        xgb_mae = xgb_station.get('MAE', np.nan)
        gtnnw_mae = gtnnw_station.get('MAE', np.nan)
        if not np.isnan(xgb_mae) and not np.isnan(gtnnw_mae):
            mae_improve = (xgb_mae - gtnnw_mae) / xgb_mae * 100
            print(f"  {'MAE (mm)':<10} {xgb_mae:<12.3f} {gtnnw_mae:<12.3f} {mae_improve:>+8.1f}%")

        # Rå¯¹æ¯”
        xgb_r = xgb_station.get('R', np.nan)
        gtnnw_r = gtnnw_station.get('R', np.nan)
        if not np.isnan(xgb_r) and not np.isnan(gtnnw_r):
            r_improve = (gtnnw_r - xgb_r) / abs(xgb_r) * 100
            print(f"  {'R':<10} {xgb_r:<12.3f} {gtnnw_r:<12.3f} {r_improve:>+8.1f}%")

    # å¹´åº¦äº¤å‰éªŒè¯å¯¹æ¯”
    if 'yearly_cv' in xgb_results and 'yearly_cv' in gtnnw_results:
        xgb_yearly = xgb_results['yearly_cv']['overall']
        gtnnw_yearly = gtnnw_results['yearly_cv']['overall']

        print("\nğŸ“… å¹´åº¦äº¤å‰éªŒè¯ (æ—¶é—´è¯„ä¼°):")
        print(f"  {'æŒ‡æ ‡':<10} {'çº¯XGBoost':<12} {'GTNNW-XGBoost':<12} {'æå‡':<10}")
        print("-" * 50)

        # MAEå¯¹æ¯”
        xgb_mae = xgb_yearly.get('MAE', np.nan)
        gtnnw_mae = gtnnw_yearly.get('MAE', np.nan)
        if not np.isnan(xgb_mae) and not np.isnan(gtnnw_mae):
            mae_improve = (xgb_mae - gtnnw_mae) / xgb_mae * 100
            print(f"  {'MAE (mm)':<10} {xgb_mae:<12.3f} {gtnnw_mae:<12.3f} {mae_improve:>+8.1f}%")

        # Rå¯¹æ¯”
        xgb_r = xgb_yearly.get('R', np.nan)
        gtnnw_r = gtnnw_yearly.get('R', np.nan)
        if not np.isnan(xgb_r) and not np.isnan(gtnnw_r):
            r_improve = (gtnnw_r - xgb_r) / abs(xgb_r) * 100
            print(f"  {'R':<10} {xgb_r:<12.3f} {gtnnw_r:<12.3f} {r_improve:>+8.1f}%")

    print("\nğŸ’¡ ç»“è®º:")
    if 'r_improve' in locals() and not np.isnan(r_improve):
        if r_improve > 0:
            print(f"  âœ… GTNNW-XGBoostç›¸æ¯”çº¯XGBooståœ¨RæŒ‡æ ‡ä¸Šæå‡äº†{r_improve:.1f}%")
            print(f"  âœ… å»ºè®®ä½¿ç”¨GTNNW-XGBoostè¿›è¡ŒSWEé¢„æµ‹")
        else:
            print(f"  âš ï¸  GTNNW-XGBoostç›¸æ¯”çº¯XGBooståœ¨RæŒ‡æ ‡ä¸Šä¸‹é™äº†{abs(r_improve):.1f}%")
            print(f"  âš ï¸  å»ºè®®ç»§ç»­ä½¿ç”¨çº¯XGBoostè¿›è¡ŒSWEé¢„æµ‹")

    print("=" * 80)


def load_data(file_path):
    """åŠ è½½æ•°æ®æ–‡ä»¶

    Args:
        file_path (str): æ–‡ä»¶è·¯å¾„

    Returns:
        pd.DataFrame: åŠ è½½çš„æ•°æ®

    Raises:
        FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨æ—¶æŠ›å‡º
        ValueError: æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒæ—¶æŠ›å‡º
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    file_ext = os.path.splitext(file_path)[1].lower()

    try:
        if file_ext == '.csv':
            df = pd.read_csv(file_path)
            logger.info(f"CSVæ–‡ä»¶åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
            return df

        elif file_ext in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path)
            logger.info(f"Excelæ–‡ä»¶åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
            return df

        elif file_ext == '.parquet':
            df = pd.read_parquet(file_path)
            logger.info(f"Parquetæ–‡ä»¶åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
            return df

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ã€‚æ”¯æŒæ ¼å¼: CSV, Excel, Parquet")

    except Exception as e:
        logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        # å°è¯•å…¶ä»–ç¼–ç æ–¹å¼åŠ è½½CSV
        if file_ext == '.csv':
            try:
                logger.info("å°è¯•ä½¿ç”¨GBKç¼–ç åŠ è½½CSV...")
                df = pd.read_csv(file_path, encoding='gbk')
                logger.info(f"ä½¿ç”¨GBKç¼–ç åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
                return df
            except:
                try:
                    logger.info("å°è¯•ä½¿ç”¨latin1ç¼–ç åŠ è½½CSV...")
                    df = pd.read_csv(file_path, encoding='latin1')
                    logger.info(f"ä½¿ç”¨latin1ç¼–ç åŠ è½½æˆåŠŸ: {len(df)} è¡Œ")
                    return df
                except:
                    raise ValueError(f"CSVæ–‡ä»¶æ— æ³•ç”¨ä»»ä½•ç¼–ç åŠ è½½: {e}")
        else:
            raise


def print_summary(results):
    """æ‰“å°ç»“æœæ‘˜è¦"""
    print("\n" + "=" * 70)
    print("ğŸ‰ SWEæ¨¡å‹è®­ç»ƒå®Œæˆæ‘˜è¦")
    print("=" * 70)

    # æ˜¾ç¤ºæ¨¡å‹ç±»å‹
    if hasattr(results, 'use_gtnnwr'):
        model_type = "GTNNW-XGBoost" if results.use_gtnnwr else "çº¯XGBoost"
        print(f"æ¨¡å‹ç±»å‹: {model_type}")
    elif 'preprocessing' in results and 'use_gtnnwr' in results['preprocessing']:
        model_type = "GTNNW-XGBoost" if results['preprocessing']['use_gtnnwr'] else "çº¯XGBoost"
        print(f"æ¨¡å‹ç±»å‹: {model_type}")

    # ç«™ç‚¹äº¤å‰éªŒè¯ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'station_cv' in results:
        station = results['station_cv']['overall']
        print(f"\nğŸ“ ç«™ç‚¹äº¤å‰éªŒè¯ (ç©ºé—´è¯„ä¼°):")
        print(f"   MAE:  {station['MAE']:8.3f} mm")
        print(f"   RMSE: {station['RMSE']:8.3f} mm")
        print(f"   R:    {station['R']:8.3f}")
        print(f"   æ ·æœ¬æ•°: {station.get('æ ·æœ¬æ•°', station.get('samples', 'N/A')):>6}")
        print(f"   æŠ˜å æ•°: {results['station_cv']['folds']:6d}")

    # å¹´åº¦äº¤å‰éªŒè¯ç»“æœ
    if 'yearly_cv' in results:
        yearly = results['yearly_cv']['overall']
        print(f"\nğŸ“… å¹´åº¦äº¤å‰éªŒè¯ (æ—¶é—´è¯„ä¼°):")
        print(f"   MAE:  {yearly['MAE']:8.3f} mm")
        print(f"   RMSE: {yearly['RMSE']:8.3f} mm")
        print(f"   R:    {yearly['R']:8.3f}")
        print(f"   æ ·æœ¬æ•°: {yearly.get('æ ·æœ¬æ•°', yearly.get('samples', 'N/A')):>6}")
        print(f"   æŠ˜å æ•°: {results['yearly_cv']['folds']:6d}")

    # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'feature_importance' in results:
        top_features = results['feature_importance'].head(3)
        print(f"\nğŸ” é‡è¦ç‰¹å¾ Top 3:")
        for i, (_, row) in enumerate(top_features.iterrows(), 1):
            print(f"   {i}. {row['feature']:20} {row['importance']:.4f}")

    # æ€§èƒ½æ¯”è¾ƒï¼ˆå¦‚æœä¸¤è€…éƒ½å­˜åœ¨ï¼‰
    if 'station_cv' in results and 'yearly_cv' in results:
        station_r = results['station_cv']['overall']['R']
        yearly_r = results['yearly_cv']['overall']['R']

        print(f"\nğŸ’¡ å»ºè®®:")
        if station_r > yearly_r:
            print(f"   ç«™ç‚¹CVæ€§èƒ½æ›´ä¼˜ï¼Œæ¨èç”¨äºç©ºé—´è¯„ä¼°")
        else:
            print(f"   å¹´åº¦CVæ€§èƒ½æ›´ä¼˜ï¼Œæ¨èç”¨äºæ—¶é—´è¯„ä¼°")
    else:
        print(f"\nğŸ’¡ å»ºè®®:")
        print(f"   ä½¿ç”¨å¹´åº¦äº¤å‰éªŒè¯ç»“æœè¿›è¡Œè¯„ä¼°")

    print("=" * 70)
    print("ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°è¾“å‡ºç›®å½•")


def interactive_mode():
    """äº¤äº’å¼æ¨¡å¼"""
    print("\nğŸ” SWEæ¨¡å‹è®­ç»ƒ - äº¤äº’æ¨¡å¼")
    print("-" * 50)

    try:
        # è·å–æ•°æ®æ–‡ä»¶è·¯å¾„
        data_file = input("è¯·è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„: ").strip()
        if not data_file:
            print("âŒ å¿…é¡»æä¾›æ•°æ®æ–‡ä»¶è·¯å¾„")
            return

        if not os.path.exists(data_file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return

        # åŠ è½½æ•°æ®
        print("ğŸ“¥ åŠ è½½æ•°æ®...")
        df = load_data(data_file)
        if df.empty:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
            return

        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(df)} è¡Œ, {len(df.columns)} åˆ—")

        # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        print(f"  ç«™ç‚¹æ•°é‡: {df['station_id'].nunique()}")
        print(f"  æ—¥æœŸèŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
        print(f"  SWEç»Ÿè®¡: å‡å€¼={df['swe'].mean():.2f}mm, æ ‡å‡†å·®={df['swe'].std():.2f}mm")

        # è¯¢é—®æ˜¯å¦ä½¿ç”¨GTNNWR
        use_gtnnwr = input("\næ˜¯å¦ä½¿ç”¨GTNNWRæƒé‡å¢å¼ºï¼Ÿ(y/n): ").strip().lower()
        use_gtnnwr = use_gtnnwr == 'y'

        if use_gtnnwr:
            print("ğŸ”§ å°†ä½¿ç”¨GTNNW-XGBoostèåˆæ¨¡å‹")
            # æ£€æŸ¥å¿…è¦åˆ—
            required_cols = ['X', 'Y', 'year', 'month', 'doy']
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                print(f"âš ï¸  ç¼ºå°‘GTNNWRéœ€è¦çš„åˆ—: {missing_cols}")
                print("GTNNWRå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
                proceed = input("ç»§ç»­å—ï¼Ÿ(y/n): ").strip().lower()
                if proceed != 'y':
                    use_gtnnwr = False
                    print("åˆ‡æ¢ä¸ºçº¯XGBoostæ¨¡å¼")
        else:
            print("ğŸ”§ å°†ä½¿ç”¨çº¯XGBoostæ¨¡å‹")

        # é€‰æ‹©è¾“å‡ºç›®å½•
        output_dir = input("\nè¯·è¾“å…¥è¾“å‡ºç›®å½• (å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
        if not output_dir:
            output_dir = None
            print("ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•")

        # å¯é€‰ï¼šè‡ªå®šä¹‰å‚æ•°
        print(f"\nâš™ï¸ æ¨¡å‹å‚æ•° (ä½¿ç”¨é»˜è®¤å€¼è¯·ç›´æ¥å›è½¦):")

        trees = input(f"æ ‘çš„æ•°é‡ [é»˜è®¤: 60]: ").strip()
        lr = input(f"å­¦ä¹ ç‡ [é»˜è®¤: 0.17]: ").strip()
        depth = input(f"æœ€å¤§æ·±åº¦ [é»˜è®¤: 5]: ").strip()

        # å¦‚æœæ˜¯GTNNWRæ¨¡å¼ï¼Œè¯¢é—®è®­ç»ƒè½®æ•°å’Œå…¶ä»–å‚æ•°
        if use_gtnnwr:
            gtnnwr_epochs = input(f"GTNNWRè®­ç»ƒè½®æ•° [é»˜è®¤: 5]: ").strip()
            gtnnwr_epochs = int(gtnnwr_epochs) if gtnnwr_epochs else 5

            graph_layers = input(f"GTNNWRå›¾å·ç§¯å±‚ [é»˜è®¤: [[3], [512,256,64]]]: ").strip()
            graph_layers = graph_layers if graph_layers else "[[3], [512,256,64]]"

            drop_out = input(f"GTNNWR dropoutæ¯”ä¾‹ [é»˜è®¤: 0.4]: ").strip()
            drop_out = float(drop_out) if drop_out else 0.4

        # æ„å»ºå‚æ•°å­—å…¸
        params = {}
        if trees:
            params['n_estimators'] = int(trees)
        if lr:
            params['learning_rate'] = float(lr)
        if depth:
            params['max_depth'] = int(depth)

        # ç¡®è®¤å¼€å§‹è®­ç»ƒ
        print(f"\nğŸ” è®­ç»ƒé…ç½®:")
        print(f"  æ•°æ®æ–‡ä»¶: {data_file}")
        print(f"  è¾“å‡ºç›®å½•: {output_dir or 'è‡ªåŠ¨ç”Ÿæˆ'}")
        print(f"  æ¨¡å‹ç±»å‹: {'GTNNW-XGBoost' if use_gtnnwr else 'çº¯XGBoost'}")
        if use_gtnnwr:
            print(f"  GTNNWRè®­ç»ƒè½®æ•°: {gtnnwr_epochs}")
            print(f"  GTNNWRå›¾å·ç§¯å±‚: {graph_layers}")
            print(f"  GTNNWR dropoutæ¯”ä¾‹: {drop_out}")
        print(f"  æ ‘çš„æ•°é‡: {params.get('n_estimators', 60)}")
        print(f"  å­¦ä¹ ç‡: {params.get('learning_rate', 0.17)}")
        print(f"  æœ€å¤§æ·±åº¦: {params.get('max_depth', 5)}")

        confirm = input("\nå¼€å§‹è®­ç»ƒæ¨¡å‹? (y/n): ").strip().lower()
        if confirm != 'y':
            print("âŒ å–æ¶ˆè®­ç»ƒ")
            return

        # è®­ç»ƒæ¨¡å‹
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")

        if use_gtnnwr:
            # GTNNW-XGBoostè®­ç»ƒ
            try:
                graph_layers_eval = eval(graph_layers)
            except:
                graph_layers_eval = [[3], [512, 256, 64]]

            gtnnwr_params = {
                'max_epoch': gtnnwr_epochs,
                'graph_layers': graph_layers_eval,
                'drop_out': drop_out,
                'optimizer_params': {
                    "scheduler": "MultiStepLR",
                    "scheduler_milestones": [1000, 2000, 3000, 4000],
                    "scheduler_gamma": 0.8,
                }
            }
            trainer = GTNNW_XGBoostTrainer(
                params=params,
                gtnnwr_params=gtnnwr_params,
                use_gtnnwr=True
            )
            results = trainer.run_complete_analysis(df, output_dir)
        else:
            # çº¯XGBoostè®­ç»ƒ
            results = train_swe_model(df, output_dir, params)

        # æ˜¾ç¤ºç»“æœ
        print_summary(results)

        print(f"\nğŸ¯ è®­ç»ƒå®Œæˆï¼")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åº“æ˜¯å¦å®‰è£…"""
    required_packages = {
        'pandas': 'pd',
        'numpy': 'np',
        'xgboost': 'xgb',
        'scikit-learn': 'sklearn',
        'scipy': 'scipy',
        'torch': 'torch'
    }

    missing_packages = []

    for package, short_name in required_packages.items():
        try:
            if package == 'scikit-learn':
                import sklearn
            else:
                __import__(package)
        except ImportError:
            missing_packages.append(package)

    if missing_packages:
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–åº“:")
        for package in missing_packages:
            print(f"  - {package}")
        print(f"\nè¯·å®‰è£…: pip install {' '.join(missing_packages)}")
        return False

    return True


def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    print("""
SWE XGBoost/GTNNW-XGBoostæ¨¡å‹è®­ç»ƒå·¥å…·

ä½¿ç”¨æ–¹æ³•:

1. å‘½ä»¤è¡Œæ¨¡å¼:
   python main.py --data <æ•°æ®æ–‡ä»¶> [é€‰é¡¹]

2. äº¤äº’æ¨¡å¼:
   python main.py

ä¸»è¦é€‰é¡¹:
   --use-gtnnwr: ä½¿ç”¨GTNNWRæƒé‡å¢å¼ºXGBoost (GTNNW-XGBoostèåˆ)
   --compare-models: å¯¹æ¯”çº¯XGBoostå’ŒGTNNW-XGBoostæ€§èƒ½
   --gtnnwr-epochs: GTNNWRè®­ç»ƒè½®æ•° (é»˜è®¤: 5)
   --graph-layers: GTNNWRå›¾å·ç§¯å±‚ç»“æ„ (é»˜è®¤: [[3], [512,256,64]])
   --drop-out: GTNNWR dropoutæ¯”ä¾‹ (é»˜è®¤: 0.4)
   --no-gtnnwr: å¼ºåˆ¶ç¦ç”¨GTNNWRï¼Œä½¿ç”¨çº¯XGBoost

æ”¯æŒçš„æ•°æ®æ ¼å¼:
   â€¢ CSVæ–‡ä»¶ (.csv)
   â€¢ Excelæ–‡ä»¶ (.xlsx, .xls)  
   â€¢ Parquetæ–‡ä»¶ (.parquet)

å¿…è¦æ•°æ®åˆ—:
   â€¢ station_id: ç«™ç‚¹ID
   â€¢ date: æ—¥æœŸ
   â€¢ swe: é›ªæ°´å½“é‡å€¼

GTNNWRé¢å¤–éœ€è¦:
   â€¢ X, Y: ç©ºé—´åæ ‡
   â€¢ year, month, doy: æ—¶é—´ä¿¡æ¯

è¾“å‡ºç»“æœ:
   â€¢ è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ (.pkl)
   â€¢ äº¤å‰éªŒè¯é¢„æµ‹ç»“æœ (.csv)
   â€¢ ç‰¹å¾é‡è¦æ€§æ’åº (.csv)
   â€¢ è¯¦ç»†è¯„ä¼°æŠ¥å‘Š (.json, .txt)

ç¤ºä¾‹:
   python main.py -d data.csv                       # çº¯XGBoost
   python main.py -d data.csv --use-gtnnwr           # GTNNW-XGBoost
   python main.py -d data.csv --compare-models      # å¯¹æ¯”å®éªŒ
   python main.py -d data.csv -o ./results --trees 100 --lr 0.1
    """)


if __name__ == "__main__":
    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    print("=" * 70)
    print("â„ï¸  SWE XGBoost/GTNNW-XGBoostæ¨¡å‹è®­ç»ƒå·¥å…·")
    print("=" * 70)

    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        sys.exit(1)

    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œè¿›å…¥äº¤äº’æ¨¡å¼
    if len(sys.argv) == 1:
        interactive_mode()
    else:
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¸®åŠ©è¯·æ±‚
        if any(arg in sys.argv for arg in ['-h', '--help', 'help']):
            show_help()
            sys.exit(0)

        # è¿è¡Œå‘½ä»¤è¡Œæ¨¡å¼
        exit_code = main()
        sys.exit(exit_code)