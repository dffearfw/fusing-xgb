import os
import sys
import warnings
from sklearn.cluster import AgglomerativeClustering
import numpy as np
import pandas as pd
import torch
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
from torch import nn
from scipy.spatial import distance
from sklearn.model_selection import GroupKFold
import optuna  # ğŸ”¥ã€æ–°å¢ã€‘å¯¼å…¥ Optuna

sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))
from gnnwr.datasets import init_dataset_split
from gnnwr.models import GTNNWR
from visualizer import plot_gtnnwr_results, plot_multiple_models_results


# ----------------------------------------------------------------------
# --- ğŸ”¥ã€å°è£…çš„ä¿®å¤è¡¥ä¸ã€‘ä¿®å¤ gnnwr åº“çš„å†…éƒ¨ bug ---
# ----------------------------------------------------------------------
# (è¿™é‡Œçš„ patched_reg_result å‡½æ•°ä¿æŒä¸å˜)
def patched_reg_result(self, filename=None, model_path=None, use_dict=False, only_return=False, map_location=None):
    if model_path is None:
        model_path = self._modelSavePath + "/" + self._modelName + ".pkl"
    if use_dict:
        data = torch.load(model_path, map_location=map_location, weights_only=False)
        self._model.load_state_dict(data)
    else:
        self._model = torch.load(model_path, map_location=map_location, weights_only=False)
    if self._use_gpu:
        self._model = nn.DataParallel(module=self._model)
        self._model, self._out = self._model.cuda(), self._out.cuda()
    else:
        self._model, self._out = self._model.cpu(), self._out.cpu()
    device = torch.device('cuda') if self._use_gpu else torch.device('cpu')
    result = torch.tensor([]).to(torch.float32).to(device)
    train_data_size = valid_data_size = 0
    with torch.no_grad():
        for data, coef, label, data_index in self._train_dataset.dataloader:
            data, coef, label, data_index = data.to(device), coef.to(device), label.to(device), data_index.to(device)
            output = self._out(self._model(data).mul(coef.to(torch.float32)))
            coefficient = self._model(data).mul(torch.tensor(self._coefficient).to(torch.float32).to(device))
            output = torch.cat((coefficient, output, data_index), dim=1)
            result = torch.cat((result, output), 0)
        train_data_size = len(result)
        for data, coef, label, data_index in self._valid_dataset.dataloader:
            data, coef, label, data_index = data.to(device), coef.to(device), label.to(device), data_index.to(device)
            output = self._out(self._model(data).mul(coef.to(torch.float32)))
            coefficient = self._model(data).mul(torch.tensor(self._coefficient).to(torch.float32).to(device))
            output = torch.cat((coefficient, output, data_index), dim=1)
            result = torch.cat((result, output), 0)
        valid_data_size = len(result) - train_data_size
        for data, coef, label, data_index in self._test_dataset.dataloader:
            data, coef, label, data_index = data.to(device), coef.to(device), label.to(device), data_index.to(device)
            output = self._out(self._model(data).mul(coef.to(torch.float32)))
            coefficient = self._model(data).mul(torch.tensor(self._coefficient).to(torch.float32).to(device))
            output = torch.cat((coefficient, output, data_index), dim=1)
            result = torch.cat((result, output), 0)
    result = result.cpu().detach().numpy()
    columns = list(self._train_dataset.x)
    for i in range(len(columns)):
        columns[i] = "coef_" + columns[i]
    columns.append("bias")
    columns = columns + ["Pred_" + self._train_dataset.y[0]] + self._train_dataset.id
    result = pd.DataFrame(result, columns=columns)
    result[self._train_dataset.id] = result[self._train_dataset.id].astype(np.int32)
    result["Pred_" + self._train_dataset.y[0]] = result["Pred_" + self._train_dataset.y[0]].astype(np.float32)
    result['dataset_belong'] = np.concatenate([
        np.full(train_data_size, 'train'),
        np.full(valid_data_size, 'valid'),
        np.full(len(result) - train_data_size - valid_data_size, 'test')
    ])
    pred_col_name = "Pred_" + self._train_dataset.y[0]

    if self._train_dataset.y[0] == 'swe_log':
        print("æ£€æµ‹åˆ°å¯¹æ•°å˜æ¢ç›®æ ‡ï¼Œæ­£åœ¨å¯¹é¢„æµ‹ç»“æœè¿›è¡Œåå‘å˜æ¢...")
        result['denormalized_pred_result'] = np.expm1(result[pred_col_name])
    elif self._train_dataset.y_scale_info:
        _, denormalized_pred = self._train_dataset.rescale(None, result[pred_col_name].to_frame())
        result['denormalized_pred_result'] = denormalized_pred.iloc[:, 0]
    else:
        result['denormalized_pred_result'] = result[pred_col_name]

    if only_return:
        return result
    if filename is not None:
        result.to_csv(filename, index=False)
    else:
        warnings.warn(
            "Warning! The input write file path is not set. Result is returned by function but not saved as file.",
            RuntimeWarning)
    return result


# ----------------------------------------------------------------------
# --- ä¸»æµç¨‹ ---
# ----------------------------------------------------------------------
# ğŸ”¥ã€å…³é”®ã€‘åœ¨åˆ›å»ºä»»ä½•æ¨¡å‹ä¹‹å‰ï¼Œåº”ç”¨ä¿®å¤è¡¥ä¸
GTNNWR.reg_result = patched_reg_result

# --- ã€å‰ç½®æ­¥éª¤ï¼šæ•°æ®å‡†å¤‡ã€åŸºäº zone_id çš„ç©ºé—´åˆ†åŒºå’Œç«™ç‚¹åˆ’åˆ†ã€‘---
# ----------------------------------------------------------------------
print("=== 1. åŠ è½½å¹¶é¢„å¤„ç†æ•°æ® ===")
data = pd.read_excel('lu_onehot.xlsx')
# ç¡®ä¿æ•°æ®ä¸­åŒ…å« zone_id åˆ—
if 'zone_id' not in data.columns:
    raise ValueError("æ•°æ®æ–‡ä»¶ 'lu_onehot.xlsx' ä¸­æœªæ‰¾åˆ° 'zone_id' åˆ—ã€‚")

data["id"] = np.arange(len(data))
data['station_id'] = data['X'].astype(str) + '_' + data['Y'].astype(str)
data['swe_log'] = np.log1p(data['swe'])

# ğŸ”¥ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç›´æ¥ä½¿ç”¨ zone_id ä½œä¸ºç©ºé—´åˆ†åŒºï¼ˆclusterï¼‰çš„ä¾æ®
# å°† zone_id è½¬æ¢ä¸ºä»0å¼€å§‹çš„æ•´æ•°ç±»åˆ«ï¼Œæ–¹ä¾¿åç»­å¤„ç†
data['cluster'] = data['zone_id'].astype('category').cat.codes
print(f"æ•°æ®å·²æ ¹æ® 'zone_id' åˆ’åˆ†ä¸º {data['cluster'].nunique()} ä¸ªç©ºé—´åˆ†åŒºã€‚")

# å®šä¹‰ç‰¹å¾å’Œç›®æ ‡åˆ—
x_columns = ['aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation', 'std_slope', 'std_eastness',
             'std_tpi', 'std_curvature1', 'std_curvature2', 'std_high', 'std_aspect', 'glsnow', 'cswe',
             'snow_depth_snow_depth', 'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'gldas', 'scp_start', 'scp_end', 'd1', 'd2',
             'da', 'db', 'dc', 'dd', 'landuse_11', 'landuse_12', 'landuse_21', 'landuse_22', 'landuse_23', 'landuse_24',
             'landuse_31', 'landuse_32', 'landuse_33', 'landuse_41', 'landuse_43', 'landuse_46', 'landuse_51',
             'landuse_52', 'landuse_53', 'landuse_62', 'landuse_64']
y_column_transformed = ['swe_log']

print("\n=== 2. åŸºäº zone_id (cluster) åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†å’Œæµ‹è¯•é›† ===")
# ğŸ”¥ã€æ ¸å¿ƒä¿®æ”¹ã€‘æŒ‰ç©ºé—´åˆ†åŒºï¼ˆclusterï¼‰æ¥åˆ’åˆ†æµ‹è¯•é›†ï¼Œä»¥è¯„ä¼°ç©ºé—´å¤–æ¨èƒ½åŠ›
all_clusters = sorted(data['cluster'].unique())
np.random.seed(42) # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°
np.random.shuffle(all_clusters)

# é€‰æ‹© 20% çš„åˆ†åŒºä½œä¸ºæµ‹è¯•åŒºåŸŸ
test_clusters = all_clusters[:int(len(all_clusters) * 0.2)]
train_val_clusters = all_clusters[int(len(all_clusters) * 0.2):]

print(f"æµ‹è¯•åˆ†åŒº: {sorted(test_clusters)}")
print(f"è®­ç»ƒ/éªŒè¯åˆ†åŒº: {sorted(train_val_clusters)}")

train_val_data_full = data[data['cluster'].isin(train_val_clusters)].copy()
test_data_full = data[data['cluster'].isin(test_clusters)].copy()

print(f"è®­ç»ƒ/éªŒè¯é›†æ ·æœ¬æ•°: {len(train_val_data_full)}")
print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_data_full)}")

print("\n=== 3. åœ¨è®­ç»ƒ/éªŒè¯é›†å’Œæµ‹è¯•é›†å†…éƒ¨æŒ‰æ—¶é—´åˆ’åˆ† ===")
# è®­ç»ƒ/éªŒè¯é›†çš„æ—¶é—´åˆ’åˆ†ï¼šå–æœ€å20%çš„æ—¶é—´ä½œä¸ºéªŒè¯é›†
train_val_df_sorted = train_val_data_full.sort_values(by=['year', 'month', 'doy'])
val_sample_count = int(len(train_val_df_sorted) * 0.2)
val_data_full = train_val_df_sorted.iloc[-val_sample_count:].copy() # ä½¿ç”¨æœ€åä¸€æ®µæ—¶é—´ä½œä¸ºéªŒè¯é›†
train_data_full = train_val_df_sorted.iloc[:-val_sample_count].copy()

# æµ‹è¯•é›†çš„æ—¶é—´åˆ’åˆ†ï¼šç¡®ä¿æµ‹è¯•é›†çš„æ—¶é—´èŒƒå›´ä¸éªŒè¯é›†æœ‰é‡å ï¼Œä»¥è¿›è¡Œå…¬å¹³æ¯”è¾ƒ
test_df_sorted = test_data_full.sort_values(by=['year', 'month', 'doy'])
# ç­›é€‰å‡ºä¸éªŒè¯é›†å¹´ä»½ç›¸åŒçš„æµ‹è¯•æ•°æ®
min_year, max_year = val_data_full['year'].min(), val_data_full['year'].max()
test_data = test_df_sorted[(test_df_sorted['year'] >= min_year) & (test_df_sorted['year'] <= max_year)].copy()

print(f"æœ€ç»ˆè®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_data_full)}")
print(f"æœ€ç»ˆéªŒè¯é›†æ ·æœ¬æ•°: {len(val_data_full)}")
print(f"æœ€ç»ˆæµ‹è¯•é›†æ ·æœ¬æ•° (æ—¶é—´ç­›é€‰å): {len(test_data)}")

# åˆå¹¶è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç«™ç‚¹ä¿¡æ¯ï¼Œä¸º GroupKFold äº¤å‰éªŒè¯åšå‡†å¤‡
train_val_stations = list(set(train_data_full['station_id'].unique()) | set(val_data_full['station_id'].unique()))
train_val_data_full = data[data['station_id'].isin(train_val_stations)].copy()
# --- å‰ç½®æ­¥éª¤ç»“æŸ ---


# ----------------------------------------------------------------------
# --- ğŸ”¥ã€ä¼˜åŒ–ç‰ˆã€‘ä½¿ç”¨ Optuna è¿›è¡Œè¶…å‚æ•°æœç´¢ ---
# ----------------------------------------------------------------------
# ----------------------------------------------------------------------
# --- ğŸ”¥ã€ä¼˜åŒ–ç‰ˆã€‘ä½¿ç”¨ Optuna è¿›è¡Œè¶…å‚æ•°æœç´¢ ---
# ----------------------------------------------------------------------
def objective(trial):
    """
    ä¸€ä¸ªæ›´å¥å£®çš„ Optuna ç›®æ ‡å‡½æ•°ï¼ŒåŒ…å«æ›´å¹¿æ³›çš„æœç´¢ç©ºé—´ã€å‰ªæå’Œé”™è¯¯å¤„ç†ã€‚
    """
    # 1. å®šä¹‰æ›´å¹¿æ³›çš„è¶…å‚æ•°æœç´¢ç©ºé—´
    # ğŸ”¥ã€ä¼˜åŒ–å™¨é€‰æ‹©ã€‘è®© Optuna é€‰æ‹©ä¼˜åŒ–å™¨
    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'Adadelta', 'AdamW'])

    # ğŸ”¥ã€å­¦ä¹ ç‡é€‰æ‹©ã€‘ä¸ºä¸åŒä¼˜åŒ–å™¨è®¾ç½®ä¸åŒçš„åˆç†èŒƒå›´
    if optimizer_name == 'Adam':
        lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)
    elif optimizer_name == 'AdamW':
        lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)
    else:  # Adadelta
        lr = trial.suggest_float('lr', 1e-3, 1e-1, log=True)

    dropout = trial.suggest_float('dropout', 0.1, 0.5)
    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)

    # ğŸ”¥ã€ç½‘ç»œç»“æ„ã€‘åŠ¨æ€å®šä¹‰ç½‘ç»œå±‚æ•°å’Œå•å…ƒæ•°
    n_layers = trial.suggest_int('n_layers', 1, 4)
    layers = []
    for i in range(n_layers):
        num_units = trial.suggest_int(f'n_units_l{i}', 32, 512, step=32)
        layers.append(num_units)
    hidden_dims = [[3], layers]  # STPNN å’Œ SWNN çš„ç»“æ„

    # ğŸ”¥ã€è°ƒåº¦å™¨é€‰æ‹©ã€‘è®© Optuna é€‰æ‹©å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler_name = trial.suggest_categorical('scheduler', ['MultiStepLR', 'CosineAnnealingLR'])

    # ğŸ”¥ã€è°ƒåº¦å™¨å‚æ•°ã€‘æ ¹æ®é€‰æ‹©çš„è°ƒåº¦å™¨åŠ¨æ€é…ç½®å‚æ•°
    if scheduler_name == 'MultiStepLR':
        # åŠ¨æ€å»ºè®® 2 åˆ° 4 ä¸ªé‡Œç¨‹ç¢‘ç‚¹
        n_milestones = trial.suggest_int('n_milestones', 2, 4)

        # ğŸ”¥ã€ä¿®å¤ã€‘åˆ†ä¸¤æ­¥ç”Ÿæˆ milestones
        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰å»ºè®®çš„æµ®ç‚¹æ•°
        milestone_floats = [trial.suggest_float(f'milestone_{i}', 0.2, 0.8, step=0.2) for i in range(n_milestones)]
        # ç¬¬äºŒæ­¥ï¼šå°†æµ®ç‚¹æ•°è½¬æ¢ä¸ºæ•´æ•°å¹¶æ’åº
        milestones = sorted([int(m * 200) for m in milestone_floats])

        scheduler_gamma = trial.suggest_float('scheduler_gamma', 0.5, 0.9)
    elif scheduler_name == 'CosineAnnealingLR':
        scheduler_T_max = trial.suggest_int('scheduler_T_max', 100, 500)
        scheduler_eta_min = trial.suggest_float('scheduler_eta_min', 1e-4, 1e-2, log=True)

    print(f"\n--- Trial {trial.number}: Testing params ---")
    print(f"  Optimizer: {optimizer_name}, LR: {lr:.5f}, Dropout: {dropout:.2f}, Weight Decay: {weight_decay:.2e}")
    print(f"  Layers: {hidden_dims}, Scheduler: {scheduler_name}")
    if scheduler_name == 'MultiStepLR':
        print(f"  Milestones: {milestones}, Gamma: {scheduler_gamma:.2f}")
    else:
        print(f"  T_max: {scheduler_T_max}, Eta_min: {scheduler_eta_min:.5f}")

    # 2. è®¾ç½®äº¤å‰éªŒè¯
    N_SPLITS = 5  # å¢åŠ æŠ˜æ•°ï¼Œä½¿è¯„ä¼°æ›´ç¨³å®š

    # ğŸ”¥ã€ä¿®å¤ã€‘GroupKFold ä¸æ”¯æŒ shuffle å’Œ random_stateï¼Œç›´æ¥ç§»é™¤
    gkf = GroupKFold(n_splits=N_SPLITS)

    fold_scores = []

    # 3. éå†æ¯ä¸€æŠ˜
    for fold, (train_idx, val_idx) in enumerate(
            gkf.split(train_val_data_full, groups=train_val_data_full['station_id'])):
        print(f"  Fold {fold + 1}/{N_SPLITS}...")

        fold_train_data = train_val_data_full.iloc[train_idx]
        fold_val_data = train_val_data_full.iloc[val_idx]

        # ğŸ”¥ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿æ¯æ¬¡è¯•éªŒçš„æ•°æ®åˆ’åˆ†éƒ½æ˜¯åŸºäºè¯¥è¯•éªŒçš„éšæœºç§å­
        fold_train_dataset, fold_val_dataset, _ = init_dataset_split(
            train_data=fold_train_data, val_data=fold_val_data, test_data=fold_val_data,
            x_column=x_columns, y_column=y_column_transformed,
            spatial_column=['X', 'Y', 'Z'], temp_column=['doy', 'year', 'month'],
            id_column=['id'], use_model="gtnnwr", batch_size=128, dropna=True
        )

        # ğŸ”¥ã€å…³é”®ä¿®å¤ã€‘æ„å»º optimizer_params å­—å…¸
        optimizer_params_cv = {
            "weight_decay": weight_decay,
            "scheduler": scheduler_name,
        }
        if scheduler_name == 'MultiStepLR':
            optimizer_params_cv["scheduler_milestones"] = milestones
            optimizer_params_cv["scheduler_gamma"] = scheduler_gamma
        elif scheduler_name == 'CosineAnnealingLR':
            optimizer_params_cv["scheduler_T_max"] = scheduler_T_max
            optimizer_params_cv["scheduler_eta_min"] = scheduler_eta_min

        # ğŸ”¥ã€å…³é”®ä¿®å¤ã€‘é€šè¿‡ start_lr=lr ä¼ é€’å­¦ä¹ ç‡
        model_cv = GTNNWR(
            fold_train_dataset, fold_val_dataset, fold_val_dataset,
            hidden_dims, drop_out=dropout,
            optimizer=optimizer_name, start_lr=lr, optimizer_params=optimizer_params_cv,
            write_path=f"../demo_result/optuna_runs/trial_{trial.number}",
            model_name=f"fold_{fold + 1}"
        )

        try:
            # è¿è¡Œæ¨¡å‹ï¼Œè®¾ç½®æ—©åœ
            model_cv.run(max_epoch=200, early_stop=30)  # å¢åŠ æ—©åœçš„è€å¿ƒå€¼

            # ğŸ”¥ã€å‰ªææ ¸å¿ƒã€‘å‘ Optuna æŠ¥å‘Šä¸­é—´ç»“æœ
            # æˆ‘ä»¬ä½¿ç”¨éªŒè¯æŸå¤±ä½œä¸ºå‰ªæçš„ä¾æ®
            current_val_loss = model_cv._validLossList[-1]
            trial.report(current_val_loss, fold + 1)

            # ğŸ”¥ã€å‰ªæåˆ¤æ–­ã€‘æ£€æŸ¥æ˜¯å¦åº”è¯¥å‰ªæ
            if trial.should_prune():
                print(f"    !!! Fold {fold + 1} was pruned.")
                raise optuna.exceptions.TrialPruned()

        except torch._C._LinAlgError:
            print(f"    !!! Fold {fold + 1} failed due to a singular matrix. Pruning this trial.")
            raise optuna.exceptions.TrialPruned()
        except RuntimeError as e:
            # æ•è·å¦‚ CUDA OOM ç­‰è¿è¡Œæ—¶é”™è¯¯
            print(f"    !!! Fold {fold + 1} failed with a RuntimeError: {e}. Pruning this trial.")
            raise optuna.exceptions.TrialPruned()
        except Exception as e:
            print(f"    !!! Fold {fold + 1} failed with an unexpected error: {e}. Pruning this trial.")
            raise optuna.exceptions.TrialPruned()

        # è®°å½•å½“å‰æŠ˜çš„æœ€ç»ˆéªŒè¯æŸå¤±
        score = model_cv._validLossList[-1]
        fold_scores.append(score)
        print(f"    >> Fold {fold + 1} finished with validation loss: {score:.6f}")

        # ğŸ”¥ã€èµ„æºæ¸…ç†ã€‘å½»åº•æ¸…ç†æ¨¡å‹å’Œç¼“å­˜
        del model_cv
        torch.cuda.empty_cache()

    # 4. è¿”å›å½“å‰è¶…å‚æ•°ç»„åˆçš„å¹³å‡éªŒè¯æŸå¤±
    # å¦‚æœæœ‰æŠ˜è¢«å‰ªæï¼Œfold_scores å¯èƒ½ä¸å®Œæ•´ï¼Œéœ€è¦å¤„ç†
    if len(fold_scores) < N_SPLITS:
        print(f"  >> Trial {trial.number} was pruned early.")
        raise optuna.exceptions.TrialPruned()

    mean_score = np.mean(fold_scores)
    std_score = np.std(fold_scores)
    print(f"  >> Trial {trial.number} finished. Mean Loss: {mean_score:.6f}, Std: {std_score:.6f}")

    return mean_score


# ----------------------------------------------------------------------
# --- è¿è¡Œ Optuna ç ”ç©¶ ---
# ----------------------------------------------------------------------
print("\n=== å¼€å§‹ Optuna è¶…å‚æ•°æœç´¢ ===")
# åˆ›å»ºä¸€ä¸ª study å¯¹è±¡ï¼Œç›®æ ‡æ˜¯ 'minimize' (æœ€å°åŒ–éªŒè¯æŸå¤±)
study = optuna.create_study(direction='minimize')
# è¿è¡Œä¼˜åŒ–ï¼Œä¾‹å¦‚å°è¯• 50 æ¬¡ä¸åŒçš„è¶…å‚æ•°ç»„åˆ
study.optimize(objective, n_trials=50, timeout=3600) # å¢åŠ ä¸€ä¸ªè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

print("\n=== Optuna æœç´¢å®Œæˆ ===")
print(f"æœ€ä¼˜å‚æ•°: {study.best_params}")
print(f"æœ€ä¼˜éªŒè¯æŸå¤±: {study.best_value:.6f}")

# å¯ä»¥å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹
# try:
#     import optuna.visualization as vis
#     fig = vis.plot_optimization_history(study)
#     fig.show()
# except ImportError:
#     print("è¯·å®‰è£… plotly ä»¥å¯è§†åŒ– Optuna ç»“æœ: pip install plotly")


# ----------------------------------------------------------------------
# --- ğŸ”¥ã€æœ€ç»ˆè®­ç»ƒã€‘ä½¿ç”¨ Optuna æ‰¾åˆ°çš„æœ€ä¼˜è¶…å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹ ---
# ----------------------------------------------------------------------
print("\n=== ä½¿ç”¨ Optuna æœ€ä¼˜è¶…å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹ ===")

best_params = study.best_params

# ğŸ”¥ã€å…³é”®ã€‘å°† Optuna çš„å‚æ•°è½¬æ¢ä¸º GTNNWR éœ€è¦çš„æ ¼å¼
final_hidden_dims = [[3], []]
for i in range(best_params['n_layers']):
    final_hidden_dims[1].append(best_params[f'n_units_l{i}'])

# ä¸ºäº†åœ¨æœ€ç»ˆè®­ç»ƒæ—¶ç›‘æ§æ€§èƒ½ï¼Œæˆ‘ä»¬ä» train_val_data_full ä¸­å†åˆ’åˆ†ä¸€ä¸ªå°çš„éªŒè¯é›†
# æ³¨æ„ï¼šè¿™é‡Œçš„åˆ’åˆ†æ–¹å¼éœ€è¦ä¸äº¤å‰éªŒè¯æ—¶ä¸€è‡´ï¼Œå³æŒ‰ç«™ç‚¹åˆ†ç»„
# ä½†ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨ä¹‹å‰åˆ’åˆ†å¥½çš„ train_data_full å’Œ val_data_full
final_train_dataset, final_val_dataset, final_test_dataset = init_dataset_split(
    train_data=train_data_full, val_data=val_data_full, test_data=test_data,
    x_column=x_columns, y_column=y_column_transformed,
    spatial_column=['X', 'Y', 'Z'], temp_column=['doy', 'year', 'month'],
    id_column=['id'], use_model="gtnnwr", batch_size=128, dropna=True
)

# ğŸ”¥ã€å…³é”®ã€‘æ ¹æ® Optuna æ‰¾åˆ°çš„æœ€ä½³è°ƒåº¦å™¨ç±»å‹æ¥æ„å»ºå‚æ•°
optimizer_params_final = {
    "weight_decay": best_params['weight_decay'],
    "scheduler": best_params['scheduler'],
}
if best_params['scheduler'] == 'MultiStepLR':
    # åœ¨æœ€ç»ˆè®­ç»ƒä¸­ï¼Œå¯ä»¥ä½¿ç”¨æ›´é€šç”¨çš„milestones
    optimizer_params_final["scheduler_milestones"] = [200, 400, 600, 800]
    optimizer_params_final["scheduler_gamma"] = 0.7
elif best_params['scheduler'] == 'CosineAnnealingLR':
    optimizer_params_final["scheduler_T_max"] = 500
    optimizer_params_final["scheduler_eta_min"] = 1e-5


final_model = GTNNWR(
    final_train_dataset, final_val_dataset, final_test_dataset,
    final_hidden_dims, drop_out=best_params['dropout'],
    optimizer=best_params['optimizer'], optimizer_params=optimizer_params_final,
    # ğŸ”¥ã€å…³é”®ä¿®å¤ã€‘é€šè¿‡ start_lr=best_params['lr'] ä¼ é€’å­¦ä¹ ç‡
    start_lr=best_params['lr'],
    write_path="../demo_result/final_model_optuna",
    model_name="GTNNWR_Optuna_Best"
)
final_model.add_graph()
final_model.run(max_epoch=1000, early_stop=100) # ä½¿ç”¨æ›´å¤šçš„epochè¿›è¡Œå……åˆ†è®­ç»ƒ

# ----------------------------------------------------------------------
print("\n=== åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹ ===")

# ğŸ”¥ã€å…³é”®ä¿®å¤4ã€‘ä½¿ç”¨ reg_result æ¥è·å–æ‰€æœ‰é¢„æµ‹ç»“æœï¼Œè€Œä¸æ˜¯è®¿é—®ä¸å­˜åœ¨çš„ _test_dataset.pred
final_results_df = final_model.reg_result(only_return=True)

# ä»ç»“æœDataFrameä¸­ç­›é€‰å‡ºæµ‹è¯•é›†çš„é¢„æµ‹å’ŒçœŸå®å€¼
test_results_df = final_results_df[final_results_df['dataset_belong'] == 'test'].copy()

# ğŸ”¥ã€å…³é”®ä¿®å¤5ã€‘ä½¿ç”¨æ­£ç¡®çš„åˆ—å 'Pred_swe_log' æ¥è·å–é¢„æµ‹å€¼
# ä½ çš„ y_column_transformed æ˜¯ ['swe_log']ï¼Œæ‰€ä»¥é¢„æµ‹åˆ—åæ˜¯ 'Pred_' + 'swe_log'
pred_log = test_results_df['Pred_swe_log'].values
# ğŸ”¥ã€å…³é”®ä¿®å¤6ã€‘ä»åŸå§‹æ•°æ®ä¸­è·å–çœŸå®å€¼ï¼Œè€Œä¸æ˜¯ DataFrame
# å‡è®¾ä½ çš„åŸå§‹æ•°æ® test_data ä¸­æœ‰ 'swe' åˆ—
true_original_scale = test_data['swe'].values

# å°†å¯¹æ•°é¢„æµ‹ç»“æœè¿˜åŸä¸ºåŸå§‹å°ºåº¦
pred_original_scale = np.expm1(pred_log)

print("å·²è·å–æµ‹è¯•é›†çš„çœŸå®å€¼å’Œé¢„æµ‹å€¼ï¼Œå¹¶è¿˜åŸä¸ºåŸå§‹å°ºåº¦ï¼Œå‡†å¤‡è¿›è¡Œæœ€ç»ˆè¯„ä¼°ã€‚")

# ä½¿ç”¨ visualizer è¿›è¡Œè¯„ä¼°å’Œç»˜å›¾
# æ³¨æ„ï¼šplot_gtnnwr_results å‡½æ•°å¯èƒ½éœ€è¦è°ƒæ•´ï¼Œå› ä¸ºå®ƒæœŸæœ›çš„æ˜¯ä¸€ä¸ªæ¨¡å‹å¯¹è±¡
# å¦‚æœä¸èƒ½ç›´æ¥ä¿®æ”¹ï¼Œæˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨è®¡ç®—æŒ‡æ ‡
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

r2 = r2_score(true_original_scale, pred_original_scale)
rmse = np.sqrt(mean_squared_error(true_original_scale, pred_original_scale))
mae = mean_absolute_error(true_original_scale, pred_original_scale)

print("\n=== æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡ ===")
print(f"R2: {r2:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")

# å¦‚æœä½ ä»ç„¶æƒ³ç”¨ plot_gtnnwr_resultsï¼Œå¯èƒ½éœ€è¦åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„æ¨¡å‹å¯¹è±¡æ¥ä¼ é€’æ•°æ®
# æˆ–è€…ä¿®æ”¹è¯¥å‡½æ•°ä»¥æ¥å— DataFrame ä½œä¸ºè¾“å…¥ã€‚è¿™é‡Œæˆ‘ä»¬å…ˆæ‰‹åŠ¨è®¡ç®—å¹¶ç»˜å›¾ã€‚
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
plt.scatter(true_original_scale, pred_original_scale, alpha=0.5, label='Data Points')
plt.plot([min(true_original_scale), max(true_original_scale)], [min(true_original_scale), max(true_original_scale)], '--', color='red', label='Ideal Fit')
plt.xlabel('True Values (Original Scale)')
plt.ylabel('Predictions (Original Scale)')
plt.title('True vs. Predicted Values on Test Set')
plt.legend()
plt.grid(True)
save_path = "../demo_result/final_model_optuna/GTNNWR_Optuna_Best_Results.png"
os.makedirs(os.path.dirname(save_path), exist_ok=True)
plt.savefig(save_path)
plt.show()
