import os
import sys
import warnings
from sklearn.cluster import AgglomerativeClustering
import numpy as np  # ğŸ”¥ã€å¯¹æ•°å˜æ¢ã€‘ç¡®ä¿ numpy å·²å¯¼å…¥
import pandas as pd
import torch
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
from torch import nn
from scipy.spatial import distance

sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))
from gnnwr.datasets import init_dataset_split
from gnnwr.models import GTNNWR
from visualizer import plot_gtnnwr_results, plot_multiple_models_results


# ----------------------------------------------------------------------
# --- ğŸ”¥ã€å°è£…çš„ä¿®å¤è¡¥ä¸ã€‘ä¿®å¤ gnnwr åº“çš„å†…éƒ¨ bug ---
# ----------------------------------------------------------------------
def patched_reg_result(self, filename=None, model_path=None, use_dict=False, only_return=False, map_location=None):
    if model_path is None:
        model_path = self._modelSavePath + "/" + self._modelName + ".pkl"
    if use_dict:
        data = torch.load(model_path, map_location=map_location, weights_only=False)
        self._model.load_state_dict(data)
    else:
        self._model = torch.load(model_path, map_location=map_location, weights_only=False)
    if self._use_gpu:
        self._model = nn.DataParallel(module=self._model)
        self._model, self._out = self._model.cuda(), self._out.cuda()
    else:
        self._model, self._out = self._model.cpu(), self._out.cpu()
    device = torch.device('cuda') if self._use_gpu else torch.device('cpu')
    result = torch.tensor([]).to(torch.float32).to(device)
    train_data_size = valid_data_size = 0
    with torch.no_grad():
        for data, coef, label, data_index in self._train_dataset.dataloader:
            data, coef, label, data_index = data.to(device), coef.to(device), label.to(device), data_index.to(device)
            output = self._out(self._model(data).mul(coef.to(torch.float32)))
            coefficient = self._model(data).mul(torch.tensor(self._coefficient).to(torch.float32).to(device))
            output = torch.cat((coefficient, output, data_index), dim=1)
            result = torch.cat((result, output), 0)
        train_data_size = len(result)
        for data, coef, label, data_index in self._valid_dataset.dataloader:
            data, coef, label, data_index = data.to(device), coef.to(device), label.to(device), data_index.to(device)
            output = self._out(self._model(data).mul(coef.to(torch.float32)))
            coefficient = self._model(data).mul(torch.tensor(self._coefficient).to(torch.float32).to(device))
            output = torch.cat((coefficient, output, data_index), dim=1)
            result = torch.cat((result, output), 0)
        valid_data_size = len(result) - train_data_size
        for data, coef, label, data_index in self._test_dataset.dataloader:
            data, coef, label, data_index = data.to(device), coef.to(device), label.to(device), data_index.to(device)
            output = self._out(self._model(data).mul(coef.to(torch.float32)))
            coefficient = self._model(data).mul(torch.tensor(self._coefficient).to(torch.float32).to(device))
            output = torch.cat((coefficient, output, data_index), dim=1)
            result = torch.cat((result, output), 0)
    result = result.cpu().detach().numpy()
    columns = list(self._train_dataset.x)
    for i in range(len(columns)):
        columns[i] = "coef_" + columns[i]
    columns.append("bias")
    columns = columns + ["Pred_" + self._train_dataset.y[0]] + self._train_dataset.id
    result = pd.DataFrame(result, columns=columns)
    result[self._train_dataset.id] = result[self._train_dataset.id].astype(np.int32)
    result["Pred_" + self._train_dataset.y[0]] = result["Pred_" + self._train_dataset.y[0]].astype(np.float32)
    result['dataset_belong'] = np.concatenate([
        np.full(train_data_size, 'train'),
        np.full(valid_data_size, 'valid'),
        np.full(len(result) - train_data_size - valid_data_size, 'test')
    ])
    pred_col_name = "Pred_" + self._train_dataset.y[0]

    # ğŸ”¥ã€å¯¹æ•°å˜æ¢ã€‘ä¿®æ”¹ï¼šåœ¨ä¿å­˜ç»“æœæ—¶ï¼Œå¦‚æœç›®æ ‡æ˜¯swe_logï¼Œåˆ™å°†é¢„æµ‹ç»“æœåå‘å˜æ¢å›åŸå§‹å°ºåº¦
    if self._train_dataset.y[0] == 'swe_log':
        print("æ£€æµ‹åˆ°å¯¹æ•°å˜æ¢ç›®æ ‡ï¼Œæ­£åœ¨å¯¹é¢„æµ‹ç»“æœè¿›è¡Œåå‘å˜æ¢...")
        result['denormalized_pred_result'] = np.expm1(result[pred_col_name])
    elif self._train_dataset.y_scale_info:
        _, denormalized_pred = self._train_dataset.rescale(None, result[pred_col_name].to_frame())
        result['denormalized_pred_result'] = denormalized_pred.iloc[:, 0]
    else:
        result['denormalized_pred_result'] = result[pred_col_name]

    if only_return:
        return result
    if filename is not None:
        result.to_csv(filename, index=False)
    else:
        warnings.warn(
            "Warning! The input write file path is not set. Result is returned by function but not saved as file.",
            RuntimeWarning)
    return result


# ----------------------------------------------------------------------
# --- ä¸»æµç¨‹ ---
# ----------------------------------------------------------------------
# ğŸ”¥ã€å…³é”®ã€‘åœ¨åˆ›å»ºä»»ä½•æ¨¡å‹ä¹‹å‰ï¼Œåº”ç”¨ä¿®å¤è¡¥ä¸
GTNNWR.reg_result = patched_reg_result

data = pd.read_excel('lu_onehot.xlsx')
data["id"] = np.arange(len(data))
data['station_id'] = data['X'].astype(str) + '_' + data['Y'].astype(str)

# ğŸ”¥ã€å¯¹æ•°å˜æ¢ã€‘ä¿®æ”¹1ï¼šåˆ›å»ºå¯¹æ•°å˜æ¢åçš„ç›®æ ‡å˜é‡åˆ—
data['swe_log'] = np.log1p(data['swe'])

# å®šä¹‰ç‰¹å¾åˆ—ï¼Œä»¥ä¾¿æ•™å¸ˆæ¨¡å‹å’Œæœ€ç»ˆæ¨¡å‹å…±ç”¨
x_columns = [
    'aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation', 'std_slope',
    'std_eastness', 'std_tpi', 'std_curvature1', 'std_curvature2', 'std_high', 'std_aspect', 'glsnow',
    'cswe', 'snow_depth_snow_depth', 'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'gldas',
    'scp_start', 'scp_end', 'd1', 'd2', 'da', 'db', 'dc', 'dd',
    'landuse_11', 'landuse_12', 'landuse_21', 'landuse_22', 'landuse_23', 'landuse_24',
    'landuse_31', 'landuse_32', 'landuse_33', 'landuse_41', 'landuse_43',
    'landuse_46', 'landuse_51', 'landuse_52', 'landuse_53', 'landuse_62', 'landuse_64'
]

# ğŸ”¥ã€å¯¹æ•°å˜æ¢ã€‘ä¿®æ”¹2ï¼šå®šä¹‰å˜æ¢åçš„yåˆ—å
y_column_transformed = ['swe_log']

# --- ã€æ–°å¢ã€‘ç¬¬0.5æ­¥ï¼šè®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œä¸ºèšç±»æä¾›é«˜è´¨é‡ç‰¹å¾ ---
print("=== æ­¥éª¤0.5: è®­ç»ƒæ•™å¸ˆæ¨¡å‹ä»¥ç”Ÿæˆèšç±»ç‰¹å¾ ===")
# 0.5.1 ğŸ”¥ã€ä¿®æ”¹ã€‘ä½¿ç”¨å…¨ä½“æ•°æ®ä½œä¸ºæ•™å¸ˆæ¨¡å‹çš„æ•°æ®æº
teacher_data = data.copy()

# 0.5.2 ğŸ”¥ã€ä¿®æ”¹ã€‘æ•™å¸ˆæ¨¡å‹æ•°æ®é›†åˆå§‹åŒ– (ä»…æŒ‰æ—¶é—´åˆ’åˆ†ï¼Œä¸è¿›è¡Œç©ºé—´åˆ’åˆ†)
# å¯¹å…¨ä½“æ•°æ®æŒ‰æ—¶é—´æ’åº
teacher_data_sorted = teacher_data.sort_values(by=['year', 'month', 'doy'])

# æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆä¾‹å¦‚ï¼Œç”¨æœ€å20%çš„æ—¶é—´æ®µä½œä¸ºéªŒè¯é›†ï¼‰
val_size = int(len(teacher_data_sorted) * 0.2)
teacher_train = teacher_data_sorted.iloc[:-val_size].copy()
teacher_val = teacher_data_sorted.iloc[-val_size:].copy()

teacher_train_dataset, teacher_val_dataset, _ = init_dataset_split(
    train_data=teacher_train, val_data=teacher_val, test_data=teacher_val,
    x_column=x_columns, y_column=y_column_transformed,  # ğŸ”¥ã€å¯¹æ•°å˜æ¢ã€‘ä¿®æ”¹3ï¼šä½¿ç”¨å˜æ¢åçš„yåˆ—
    spatial_column=['X', 'Y', 'Z'],
    temp_column=['doy', 'year', 'month'], id_column=['id'], use_model="gtnnwr",
    batch_size=128, process_fn="minmax_scale", process_var=["x", "y"], dropna=True
)

# 0.5.3 è®­ç»ƒæ•™å¸ˆæ¨¡å‹
print("å¼€å§‹è®­ç»ƒæ•™å¸ˆæ¨¡å‹...")
optimizer_params_teacher = {"scheduler": "MultiStepLR", "scheduler_milestones": [200, 400, 600, 800],
                            "scheduler_gamma": 0.8}
teacher_model = GTNNWR(
    teacher_train_dataset, teacher_val_dataset, teacher_val_dataset,
    [[3], [128, 64]], drop_out=0.3, optimizer='Adadelta',
    optimizer_params=optimizer_params_teacher,
    write_path="../demo_result/teacher_model",
    model_name="Teacher_Model"
)
teacher_model.run(100, 500)

# 0.5.4 æå–æ¨¡å‹ç³»æ•°ä½œä¸ºèšç±»ç‰¹å¾
print("æå–æ¨¡å‹å­¦ä¹ åˆ°çš„ç©ºé—´ç³»æ•°ä½œä¸ºèšç±»ç‰¹å¾...")
# ä½¿ç”¨ä¿®å¤åçš„ result æ–¹æ³•ï¼Œå¹¶ç›´æ¥è·å–è¿”å›å€¼
teacher_results = teacher_model.reg_result(only_return=True)

coef_columns = [col for col in teacher_results.columns if col.startswith('coef_')]

# --- ğŸ”¥ã€å…³é”®ä¿®å¤1ã€‘è·å–å®Œæ•´çš„ id -> station_id æ˜ å°„å¹¶èšåˆåˆ°ç«™ç‚¹çº§åˆ« ---
# 1. ä»æ•™å¸ˆæ¨¡å‹ä½¿ç”¨çš„å®Œæ•´æ•°æ®ä¸­è·å–æ˜ å°„
id_to_station_full = teacher_data[['id', 'station_id']].drop_duplicates()

# 2. å°† station_id åˆå¹¶åˆ°æ¨¡å‹ç»“æœä¸­
results_with_station_id = pd.merge(teacher_results, id_to_station_full, on='id', how='left')

# 3. æŒ‰ station_id èšåˆç³»æ•°ï¼Œå¾—åˆ°æ¯ä¸ªç«™ç‚¹çš„ä»£è¡¨æ€§ç³»æ•°
station_level_coefs = results_with_station_id.groupby('station_id')[coef_columns].mean().reset_index()

# 4. æ¸…ç†å¯èƒ½å› åˆå¹¶äº§ç”Ÿçš„ NaN è¡Œï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
station_level_coefs.dropna(inplace=True)

print(f"æˆåŠŸä¸º {len(station_level_coefs)} ä¸ªç«™ç‚¹èšåˆäº†ç³»æ•°ç‰¹å¾ã€‚")

# --- ç¬¬1æ­¥ï¼šåŸºäºã€æ•™å¸ˆæ¨¡å‹ç‰¹å¾ã€‘å¯¹ç«™ç‚¹è¿›è¡Œèšç±» ---
# 1.1 ğŸ”¥ã€ä¿®æ”¹ã€‘ä½¿ç”¨èšåˆåçš„ç«™ç‚¹ç³»æ•°ä½œä¸ºèšç±»ç‰¹å¾
clustering_features = coef_columns
station_features_for_clustering = station_level_coefs[['station_id'] + clustering_features].copy()

# 1.2 ğŸ”¥ã€å…³é”®ã€‘å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
scaler = StandardScaler()
features_scaled = scaler.fit_transform(station_features_for_clustering[clustering_features])

# --- ã€ç»•è¿‡æ–¹æ¡ˆã€‘ä½¿ç”¨ AgglomerativeClustering æ›¿ä»£ DBSCAN ---
from sklearn.cluster import AgglomerativeClustering

# ğŸ”¥ã€å…³é”®ã€‘æ‚¨éœ€è¦é¢„å…ˆæŒ‡å®šç°‡çš„æ•°é‡
N_CLUSTERS = 5

print(f"\nä½¿ç”¨ AgglomerativeClustering è¿›è¡Œèšç±»ï¼Œé¢„è®¾ç°‡æ•°é‡ä¸º: {N_CLUSTERS}")

# 1.3 ä½¿ç”¨å±‚æ¬¡èšç±»
agglo = AgglomerativeClustering(n_clusters=N_CLUSTERS)
station_features_for_clustering['cluster'] = agglo.fit_predict(features_scaled)

# 1.4 ç»Ÿè®¡ç»“æœ
n_clusters = station_features_for_clustering['cluster'].nunique()
print(f"\nç«™ç‚¹å·²èšç±»ä¸º {n_clusters} ç±»ã€‚")
print("å„ç°‡ç«™ç‚¹æ•°é‡ï¼š")
print(station_features_for_clustering['cluster'].value_counts().sort_index())

# 1.5 ğŸ”¥ã€å…³é”®ä¿®å¤2ã€‘å®‰å…¨åœ°å°†èšç±»æ ‡ç­¾åˆå¹¶å›åŸå§‹æ•°æ®
data = pd.merge(data, station_features_for_clustering[['station_id', 'cluster']], on='station_id', how='left')
if data['cluster'].isnull().any():
    max_cluster_id = data['cluster'].max()
    data['cluster'].fillna(max_cluster_id + 1, inplace=True)
    print(f"å°† {data['cluster'].isnull().sum()} ä¸ªæœªèšç±»ç«™ç‚¹å½’å…¥æ–°ç°‡ {int(max_cluster_id + 1)}ã€‚")

# --- ğŸ”¥ã€ä¿®æ”¹ã€‘å°†é‡‡æ ·å’Œè®­ç»ƒæ”¾å…¥å¾ªç¯ï¼Œè¿›è¡Œ10æ¬¡å®éªŒ ---
all_predictions = []
all_true_values_list = []  # æ¯æ¬¡æµ‹è¯•é›†çš„çœŸå®å€¼å¯èƒ½ä¸åŒï¼Œæ‰€ä»¥ä¹Ÿä¿å­˜èµ·æ¥
successful_runs = 0
failed_runs = 0
total_attempts = 0
max_attempts = 50  # è®¾ç½®æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯

optimizer_params = {
    "scheduler": "MultiStepLR",
    "scheduler_milestones": [1000, 2000, 3000, 4000],
    "scheduler_gamma": 0.8,
}

print("\n=== å¼€å§‹10æ¬¡ç‹¬ç«‹çš„é‡‡æ ·å’Œè®­ç»ƒå®éªŒï¼ˆå¸¦é”™è¯¯æ¢å¤ï¼‰ ===")
while successful_runs < 10 and total_attempts < max_attempts:
    total_attempts += 1
    print(f"\n--- å°è¯•ç¬¬ {total_attempts} æ¬¡å®éªŒ (å·²æˆåŠŸ {successful_runs}/10) ---")

    try:
        # --- ç¬¬2æ­¥ï¼šåœ¨æ¯ä¸ªç°‡å†…è¿›è¡Œåˆ†å±‚ç©ºé—´é‡‡æ · (æ¯æ¬¡å¾ªç¯éƒ½é‡æ–°é‡‡æ ·) ---
        train_stations, val_stations, test_stations = [], [], []
        clustered_stations_df = station_features_for_clustering.copy()

        for cluster_id in clustered_stations_df['cluster'].unique():
            cluster_stations = clustered_stations_df[clustered_stations_df['cluster'] == cluster_id][
                'station_id'].unique()
            np.random.shuffle(cluster_stations)  # ğŸ”¥ã€å…³é”®ã€‘æ¯æ¬¡å¾ªç¯éƒ½é‡æ–°æ‰“ä¹±

            n = len(cluster_stations)
            if n < 10:
                train_stations.extend(cluster_stations)
                continue

            test_set = cluster_stations[:int(n * 0.1)]
            val_set = cluster_stations[int(n * 0.1):int(n * 0.2)]
            train_set = cluster_stations[int(n * 0.2):]

            train_stations.extend(train_set)
            val_stations.extend(val_set)
            test_stations.extend(test_set)

        # --- ç¬¬3æ­¥ï¼šæ ¹æ®åˆ’åˆ†å¥½çš„ç«™ç‚¹åˆ›å»ºæ•°æ®é›† ---
        train_data_full = data[data['station_id'].isin(train_stations)].copy()
        val_data_full = data[data['station_id'].isin(val_stations)].copy()
        test_data_full = data[data['station_id'].isin(test_stations)].copy()

        # --- ç¬¬4æ­¥ï¼šåœ¨ç©ºé—´åˆ’åˆ†çš„åŸºç¡€ä¸Šï¼Œè¿›è¡Œæ—¶é—´åˆ’åˆ† ---
        train_val_df_sorted = train_data_full.sort_values(by=['year', 'month', 'doy'])
        val_sample_count = int(len(val_data_full))
        val_data = train_val_df_sorted.iloc[:val_sample_count].copy()
        train_data = train_val_df_sorted.iloc[val_sample_count:].copy()

        val_start_time = val_data['year'].min()
        val_end_time = val_data['year'].max()
        test_df_sorted = test_data_full.sort_values(by=['year', 'month', 'doy'])
        test_data = test_df_sorted[
            (test_df_sorted['year'] >= val_start_time) & (test_df_sorted['year'] <= val_end_time)
            ].copy()

        # åˆå§‹åŒ–æ•°æ®é›†
        train_dataset, val_dataset, test_dataset = init_dataset_split(train_data=train_data,
                                                                      val_data=val_data,
                                                                      test_data=test_data,
                                                                      x_column=x_columns,
                                                                      y_column=y_column_transformed,
                                                                      spatial_column=['X', 'Y', 'Z'],
                                                                      temp_column=['doy', 'year', 'month'],
                                                                      id_column=['id'],
                                                                      use_model="gtnnwr",
                                                                      batch_size=128)

        # è®­ç»ƒå­¦ç”Ÿæ¨¡å‹
        gtnnwr = GTNNWR(train_dataset, val_dataset, test_dataset, [[3], [256, 128, 64]],
                        drop_out=0.4, optimizer='Adadelta', optimizer_params=optimizer_params,
                        write_path=f"../demo_result/gtnnwr_runs/run_{successful_runs + 1}",
                        model_name=f"GTNNWR_Run_{successful_runs + 1}")

        gtnnwr.add_graph()
        gtnnwr.run(100, 1000)

        # è·å–é¢„æµ‹ç»“æœ
        results_df = gtnnwr.reg_result(only_return=True)
        test_results = results_df[results_df['dataset_belong'] == 'test']

        # è¿˜åŸåˆ°åŸå§‹å°ºåº¦
        pred_log = test_results['Pred_swe_log'].values
        pred_original_scale = np.expm1(pred_log)

        # ä¿å­˜æœ¬æ¬¡å®éªŒçš„é¢„æµ‹å’ŒçœŸå®å€¼
        all_predictions.append(pred_original_scale)
        all_true_values_list.append(test_data['swe'].values)

        successful_runs += 1
        print(f"âœ… ç¬¬ {successful_runs} æ¬¡å®éªŒæˆåŠŸå®Œæˆï¼Œæµ‹è¯•é›†æ ·æœ¬æ•°: {len(pred_original_scale)}")

    except Exception as e:
        failed_runs += 1
        print(f"âŒ ç¬¬ {total_attempts} æ¬¡å®éªŒå¤±è´¥: {str(e)}")
        print("   å°†è·³è¿‡æ­¤æ¬¡å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•...")
        # æ¸…ç†GPUå†…å­˜ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        continue

# æ£€æŸ¥æ˜¯å¦æˆåŠŸå®Œæˆäº†10æ¬¡å®éªŒ
if successful_runs < 10:
    print(f"\nâš ï¸ è­¦å‘Šï¼šä»…æˆåŠŸå®Œæˆäº† {successful_runs}/10 æ¬¡å®éªŒï¼ˆå°è¯•äº† {total_attempts} æ¬¡ï¼‰")
else:
    print(f"\nâœ… æˆåŠŸå®Œæˆäº† {successful_runs} æ¬¡å®éªŒï¼ˆå…±å°è¯•äº† {total_attempts} æ¬¡ï¼Œå¤±è´¥ {failed_runs} æ¬¡ï¼‰")

# åªæœ‰å½“è‡³å°‘æœ‰ä¸€æ¬¡æˆåŠŸæ—¶æ‰ç»§ç»­
if successful_runs > 0:
    # --- ğŸ”¥ã€æ–°å¢ã€‘ç»˜åˆ¶æˆåŠŸå®éªŒçš„æ•£ç‚¹å›¾ ---
    plt.figure(figsize=(12, 10))
    colors = plt.cm.tab10(np.linspace(0, 1, successful_runs))

    # ç»˜åˆ¶æ¯æ¬¡è¿è¡Œçš„ç»“æœ
    for i, (preds, trues) in enumerate(zip(all_predictions, all_true_values_list)):
        plt.scatter(trues, preds, alpha=0.6, color=colors[i], s=20, label=f'Run {i + 1}')

    # ç»˜åˆ¶ç†æƒ³çº¿
    all_trues_flat = np.concatenate(all_true_values_list)
    all_preds_flat = np.concatenate(all_predictions)
    max_val = max(all_trues_flat.max(), all_preds_flat.max())
    min_val = min(all_trues_flat.min(), all_preds_flat.min())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Ideal (y=x)')

    plt.xlabel('True SWE', fontsize=12)
    plt.ylabel('Predicted SWE', fontsize=12)
    plt.title(f'{successful_runs} GTNNWR Experiments (Different Splits): True vs Predicted SWE', fontsize=14)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    # ä¿å­˜å›¾åƒ
    save_path = f"../demo_result/gtnnwr_runs/{successful_runs}_experiments_comparison.png"
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\næ•£ç‚¹å›¾å·²ä¿å­˜è‡³: {save_path}")

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\n=== å®éªŒç»Ÿè®¡ä¿¡æ¯ ===")
    r2_scores = []
    mae_scores = []
    rmse_scores = []

    for i, (preds, trues) in enumerate(zip(all_predictions, all_true_values_list)):
        r2 = np.corrcoef(trues, preds)[0, 1] ** 2
        mae = np.mean(np.abs(trues - preds))
        rmse = np.sqrt(np.mean((trues - preds) ** 2))

        r2_scores.append(r2)
        mae_scores.append(mae)
        rmse_scores.append(rmse)

        print(f"Run {i + 1}: RÂ²={r2:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}")

    print(f"\nå¹³å‡æ€§èƒ½: RÂ²={np.mean(r2_scores):.4f}Â±{np.std(r2_scores):.4f}, "
          f"MAE={np.mean(mae_scores):.4f}Â±{np.std(mae_scores):.4f}, "
          f"RMSE={np.mean(rmse_scores):.4f}Â±{np.std(rmse_scores):.4f}")

    plt.show()
else:
    print("\nâŒ æ²¡æœ‰æˆåŠŸçš„å®éªŒï¼Œæ— æ³•ç”Ÿæˆç»“æœã€‚")
