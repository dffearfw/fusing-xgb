import os
import sys
import warnings
from sklearn.cluster import AgglomerativeClustering
import numpy as np
import pandas as pd
import torch
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
from torch import nn
from scipy.spatial import distance

sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))
from gnnwr.datasets import init_dataset_split
from gnnwr.models import GTNNWR
from visualizer import plot_gtnnwr_results, plot_multiple_models_results

# ----------------------------------------------------------------------
# --- ğŸ”¥ã€å°è£…çš„ä¿®å¤è¡¥ä¸ã€‘ä¿®å¤ gnnwr åº“çš„å†…éƒ¨ bug ---
# ----------------------------------------------------------------------
def patched_reg_result(self, filename=None, model_path=None, use_dict=False, only_return=False, map_location=None):
    if model_path is None:
        model_path = self._modelSavePath + "/" + self._modelName + ".pkl"
    if use_dict:
        data = torch.load(model_path, map_location=map_location, weights_only=False)
        self._model.load_state_dict(data)
    else:
        self._model = torch.load(model_path, map_location=map_location, weights_only=False)
    if self._use_gpu:
        self._model = nn.DataParallel(module=self._model)
        self._model, self._out = self._model.cuda(), self._out.cuda()
    else:
        self._model, self._out = self._model.cpu(), self._out.cpu()
    device = torch.device('cuda') if self._use_gpu else torch.device('cpu')
    result = torch.tensor([]).to(torch.float32).to(device)
    train_data_size = valid_data_size = 0
    with torch.no_grad():
        for data, coef, label, data_index in self._train_dataset.dataloader:
            data, coef, label, data_index = data.to(device), coef.to(device), label.to(device), data_index.to(device)
            output = self._out(self._model(data).mul(coef.to(torch.float32)))
            coefficient = self._model(data).mul(torch.tensor(self._coefficient).to(torch.float32).to(device))
            output = torch.cat((coefficient, output, data_index), dim=1)
            result = torch.cat((result, output), 0)
        train_data_size = len(result)
        for data, coef, label, data_index in self._valid_dataset.dataloader:
            data, coef, label, data_index = data.to(device), coef.to(device), label.to(device), data_index.to(device)
            output = self._out(self._model(data).mul(coef.to(torch.float32)))
            coefficient = self._model(data).mul(torch.tensor(self._coefficient).to(torch.float32).to(device))
            output = torch.cat((coefficient, output, data_index), dim=1)
            result = torch.cat((result, output), 0)
        valid_data_size = len(result) - train_data_size
        for data, coef, label, data_index in self._test_dataset.dataloader:
            data, coef, label, data_index = data.to(device), coef.to(device), label.to(device), data_index.to(device)
            output = self._out(self._model(data).mul(coef.to(torch.float32)))
            coefficient = self._model(data).mul(torch.tensor(self._coefficient).to(torch.float32).to(device))
            output = torch.cat((coefficient, output, data_index), dim=1)
            result = torch.cat((result, output), 0)
    result = result.cpu().detach().numpy()
    columns = list(self._train_dataset.x)
    for i in range(len(columns)):
        columns[i] = "coef_" + columns[i]
    columns.append("bias")
    columns = columns + ["Pred_" + self._train_dataset.y[0]] + self._train_dataset.id
    result = pd.DataFrame(result, columns=columns)
    result[self._train_dataset.id] = result[self._train_dataset.id].astype(np.int32)
    result["Pred_" + self._train_dataset.y[0]] = result["Pred_" + self._train_dataset.y[0]].astype(np.float32)
    result['dataset_belong'] = np.concatenate([
        np.full(train_data_size, 'train'),
        np.full(valid_data_size, 'valid'),
        np.full(len(result) - train_data_size - valid_data_size, 'test')
    ])
    pred_col_name = "Pred_" + self._train_dataset.y[0]
    if self._train_dataset.y_scale_info:
        _, denormalized_pred = self._train_dataset.rescale(None, result[pred_col_name].to_frame())
        result['denormalized_pred_result'] = denormalized_pred.iloc[:, 0]
    else:
        result['denormalized_pred_result'] = result[pred_col_name]
    if only_return:
        return result
    if filename is not None:
        result.to_csv(filename, index=False)
    else:
        warnings.warn("Warning! The input write file path is not set. Result is returned by function but not saved as file.", RuntimeWarning)
    return result

# ----------------------------------------------------------------------
# --- ä¸»æµç¨‹ ---
# ----------------------------------------------------------------------
# ğŸ”¥ã€å…³é”®ã€‘åœ¨åˆ›å»ºä»»ä½•æ¨¡å‹ä¹‹å‰ï¼Œåº”ç”¨ä¿®å¤è¡¥ä¸
GTNNWR.reg_result = patched_reg_result

data = pd.read_excel('lu_onehot.xlsx')
data["id"] = np.arange(len(data))
data['station_id'] = data['X'].astype(str) + '_' + data['Y'].astype(str)

# å®šä¹‰ç‰¹å¾åˆ—ï¼Œä»¥ä¾¿æ•™å¸ˆæ¨¡å‹å’Œæœ€ç»ˆæ¨¡å‹å…±ç”¨
x_columns = [
    'aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation', 'std_slope',
    'std_eastness', 'std_tpi', 'std_curvature1', 'std_curvature2', 'std_high', 'std_aspect', 'glsnow',
    'cswe', 'snow_depth_snow_depth', 'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'gldas',
    'scp_start', 'scp_end', 'd1', 'd2','da', 'db', 'dc', 'dd',
    'landuse_11', 'landuse_12', 'landuse_21', 'landuse_22', 'landuse_23', 'landuse_24',
    'landuse_31', 'landuse_32', 'landuse_33', 'landuse_41',  'landuse_43',
    'landuse_46', 'landuse_51', 'landuse_52', 'landuse_53', 'landuse_62', 'landuse_64'
]

# --- ã€æ–°å¢ã€‘ç¬¬0.5æ­¥ï¼šè®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œä¸ºèšç±»æä¾›é«˜è´¨é‡ç‰¹å¾ ---
print("=== æ­¥éª¤0.5: è®­ç»ƒæ•™å¸ˆæ¨¡å‹ä»¥ç”Ÿæˆèšç±»ç‰¹å¾ ===")
# 0.5.1 åˆ’åˆ†æ•™å¸ˆæ¨¡å‹æ•°æ® (éšæœºæŠ½å–80%çš„ç«™ç‚¹)
unique_stations = data['station_id'].unique()
np.random.shuffle(unique_stations)
teacher_stations = unique_stations[:int(0.8 * len(unique_stations))]
teacher_data = data[data['station_id'].isin(teacher_stations)].copy()

# 0.5.2 æ•™å¸ˆæ¨¡å‹æ•°æ®é›†åˆå§‹åŒ– (ç®€å•æŒ‰æ—¶é—´åˆ’åˆ†)
teacher_data_sorted = teacher_data.sort_values(by=['year', 'month', 'doy'])
val_size = int(len(teacher_data) * 0.2)
teacher_train = teacher_data_sorted.iloc[:-val_size]
teacher_val = teacher_data_sorted.iloc[-val_size:]

teacher_train_dataset, teacher_val_dataset, _ = init_dataset_split(
    train_data=teacher_train, val_data=teacher_val, test_data=teacher_val,
    x_column=x_columns, y_column=['swe'], spatial_column=['X', 'Y', 'Z'],
    temp_column=['doy', 'year', 'month'], id_column=['id'], use_model="gtnnwr",
    batch_size=128, process_fn="minmax_scale", process_var=["x", "y"], dropna=True
)

# 0.5.3 è®­ç»ƒæ•™å¸ˆæ¨¡å‹
print("å¼€å§‹è®­ç»ƒæ•™å¸ˆæ¨¡å‹...")
optimizer_params_teacher = {"scheduler": "MultiStepLR", "scheduler_milestones": [200, 400, 600, 800], "scheduler_gamma": 0.8}
teacher_model = GTNNWR(
    teacher_train_dataset, teacher_val_dataset, teacher_val_dataset,
    [[3], [128, 64]], drop_out=0.3, optimizer='Adadelta',
    optimizer_params=optimizer_params_teacher,
    write_path="../demo_result/teacher_model",
    model_name="Teacher_Model"
)
teacher_model.run(5, 500)

# 0.5.4 æå–æ¨¡å‹ç³»æ•°ä½œä¸ºèšç±»ç‰¹å¾
print("æå–æ¨¡å‹å­¦ä¹ åˆ°çš„ç©ºé—´ç³»æ•°ä½œä¸ºèšç±»ç‰¹å¾...")
# ä½¿ç”¨ä¿®å¤åçš„ result æ–¹æ³•ï¼Œå¹¶ç›´æ¥è·å–è¿”å›å€¼
teacher_results = teacher_model.reg_result(only_return=True)

coef_columns = [col for col in teacher_results.columns if col.startswith('coef_')]

# --- ğŸ”¥ã€å…³é”®ä¿®å¤1ã€‘è·å–å®Œæ•´çš„ id -> station_id æ˜ å°„å¹¶èšåˆåˆ°ç«™ç‚¹çº§åˆ« ---
# 1. ä»æ•™å¸ˆæ¨¡å‹ä½¿ç”¨çš„å®Œæ•´æ•°æ®ä¸­è·å–æ˜ å°„
id_to_station_full = teacher_data[['id', 'station_id']].drop_duplicates()

# 2. å°† station_id åˆå¹¶åˆ°æ¨¡å‹ç»“æœä¸­
results_with_station_id = pd.merge(teacher_results, id_to_station_full, on='id', how='left')

# 3. æŒ‰ station_id èšåˆç³»æ•°ï¼Œå¾—åˆ°æ¯ä¸ªç«™ç‚¹çš„ä»£è¡¨æ€§ç³»æ•°
station_level_coefs = results_with_station_id.groupby('station_id')[coef_columns].mean().reset_index()

# 4. æ¸…ç†å¯èƒ½å› åˆå¹¶äº§ç”Ÿçš„ NaN è¡Œï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
station_level_coefs.dropna(inplace=True)

print(f"æˆåŠŸä¸º {len(station_level_coefs)} ä¸ªç«™ç‚¹èšåˆäº†ç³»æ•°ç‰¹å¾ã€‚")


# --- ç¬¬1æ­¥ï¼šåŸºäºã€æ•™å¸ˆæ¨¡å‹ç‰¹å¾ã€‘å¯¹ç«™ç‚¹è¿›è¡Œèšç±» ---
# 1.1 ğŸ”¥ã€ä¿®æ”¹ã€‘ä½¿ç”¨èšåˆåçš„ç«™ç‚¹ç³»æ•°ä½œä¸ºèšç±»ç‰¹å¾
clustering_features = coef_columns
station_features_for_clustering = station_level_coefs[['station_id'] + clustering_features].copy()

# 1.2 ğŸ”¥ã€å…³é”®ã€‘å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
scaler = StandardScaler()
features_scaled = scaler.fit_transform(station_features_for_clustering[clustering_features])

# 1.3 ä½¿ç”¨DBSCANè¿›è¡Œèšç±»
dbscan = DBSCAN(eps=0.5, min_samples=5)
station_features_for_clustering['cluster'] = dbscan.fit_predict(features_scaled)

# 1.4 å¤„ç†å™ªå£°ç‚¹å¹¶ç»Ÿè®¡ç»“æœ
station_features_for_clustering['cluster'] = station_features_for_clustering['cluster'].apply(
    lambda x: x if x != -1 else station_features_for_clustering['cluster'].max() + 1)
n_clusters = station_features_for_clustering['cluster'].nunique()

print(f"\nç«™ç‚¹å·²èšç±»ä¸º {n_clusters} ç±»ã€‚")
print("å„ç°‡ç«™ç‚¹æ•°é‡ï¼š")
print(station_features_for_clustering['cluster'].value_counts().sort_index())

# 1.5 ğŸ”¥ã€å…³é”®ä¿®å¤2ã€‘å®‰å…¨åœ°å°†èšç±»æ ‡ç­¾åˆå¹¶å›åŸå§‹æ•°æ®ï¼Œé¿å…ç¬›å¡å°”ç§¯
# station_features_for_clustering çš„ 'station_id' ç°åœ¨æ˜¯å”¯ä¸€çš„ï¼Œå¯ä»¥å®‰å…¨ merge
data = pd.merge(data, station_features_for_clustering[['station_id', 'cluster']], on='station_id', how='left')

# å¤„ç†æœªè¢«èšç±»çš„ç«™ç‚¹ï¼ˆä¾‹å¦‚ï¼Œåœ¨æ•™å¸ˆæ¨¡å‹ä¸­æœªå‡ºç°çš„20%çš„ç«™ç‚¹ï¼‰
# è¿™é‡Œæˆ‘ä»¬å°†å®ƒä»¬å½’ä¸ºä¸€ä¸ªæ–°çš„ç°‡
if data['cluster'].isnull().any():
    max_cluster_id = data['cluster'].max()
    data['cluster'].fillna(max_cluster_id + 1, inplace=True)
    print(f"å°† {data['cluster'].isnull().sum()} ä¸ªæœªèšç±»ç«™ç‚¹å½’å…¥æ–°ç°‡ {int(max_cluster_id + 1)}ã€‚")


# --- ç¬¬2æ­¥ï¼šåœ¨æ¯ä¸ªç°‡å†…è¿›è¡Œåˆ†å±‚ç©ºé—´é‡‡æ · ---
train_stations, val_stations, test_stations = [], [], []

# ğŸ”¥ã€ä¿®æ”¹ã€‘ä»æ­£ç¡®çš„èšç±»ç»“æœä¸­è·å–å”¯ä¸€çš„ç«™ç‚¹å’Œç°‡
clustered_stations_df = station_features_for_clustering

for cluster_id in clustered_stations_df['cluster'].unique():
    cluster_stations = clustered_stations_df[clustered_stations_df['cluster'] == cluster_id]['station_id'].unique()
    np.random.shuffle(cluster_stations)

    n = len(cluster_stations)
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®ç‚¹è¿›è¡Œåˆ’åˆ†
    if n < 10: # å¦‚æœç°‡å¤ªå°ï¼Œå¯ä»¥å…¨éƒ¨æ”¾å…¥è®­ç»ƒé›†
        train_stations.extend(cluster_stations)
        print(f"ç°‡ {cluster_id} å¤ªå° ({n}ä¸ªç«™ç‚¹)ï¼Œå·²å…¨éƒ¨æ”¾å…¥è®­ç»ƒé›†ã€‚")
        continue

    test_set = cluster_stations[:int(n * 0.1)]
    val_set = cluster_stations[int(n * 0.1):int(n * 0.2)]
    train_set = cluster_stations[int(n * 0.2):]

    train_stations.extend(train_set)
    val_stations.extend(val_set)
    test_stations.extend(test_set)

print(f"\nåˆ†å±‚é‡‡æ ·åï¼š")
print(f"è®­ç»ƒé›†ç«™ç‚¹æ•°: {len(train_stations)}")
print(f"éªŒè¯é›†ç«™ç‚¹æ•°: {len(val_stations)}")
print(f"æµ‹è¯•é›†ç«™ç‚¹æ•°: {len(test_stations)}")

# --- ç¬¬3æ­¥ï¼šæ ¹æ®åˆ’åˆ†å¥½çš„ç«™ç‚¹åˆ›å»ºæ•°æ®é›† ---
# (è¿™é‡Œçš„ä»£ç ä¿æŒä¸å˜ï¼Œå› ä¸ºå®ƒç°åœ¨åŸºäºæ­£ç¡®çš„ç«™ç‚¹åˆ—è¡¨)
train_data_full = data[data['station_id'].isin(train_stations)].copy()
val_data_full = data[data['station_id'].isin(val_stations)].copy()
test_data_full = data[data['station_id'].isin(test_stations)].copy()

# --- ç¬¬4æ­¥ï¼šåœ¨ç©ºé—´åˆ’åˆ†çš„åŸºç¡€ä¸Šï¼Œè¿›è¡Œæ—¶é—´åˆ’åˆ† ---
# è®­ç»ƒ/éªŒè¯é›†ï¼šæ—¶é—´åˆ’åˆ†
train_val_df_sorted = train_data_full.sort_values(by=['year', 'month', 'doy'])
val_sample_count = int(len(val_data_full))  # ä½¿ç”¨éªŒè¯é›†çš„æ ·æœ¬æ•°ä½œä¸ºåˆ’åˆ†æ ‡å‡†ï¼Œä¿æŒæ¯”ä¾‹
val_data = train_val_df_sorted.iloc[:val_sample_count].copy()
train_data = train_val_df_sorted.iloc[val_sample_count:].copy()

# æµ‹è¯•é›†ï¼šä½¿ç”¨ä¸éªŒè¯é›†ç›¸åŒçš„æ—¶é—´çª—å£ï¼Œé˜²æ­¢æ³„éœ²
val_start_time = val_data['year'].min()
val_end_time = val_data['year'].max()
test_df_sorted = test_data_full.sort_values(by=['year', 'month', 'doy'])
test_data = test_df_sorted[
    (test_df_sorted['year'] >= val_start_time) & (test_df_sorted['year'] <= val_end_time)
    ].copy()

print(f"\næœ€ç»ˆæ•°æ®é›†æ ·æœ¬æ•°ï¼š")
print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_data)}")
print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_data)}")
print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_data)}")

train_dataset, val_dataset, test_dataset = init_dataset_split(train_data=train_data,
                                                              val_data=val_data,
                                                              test_data=test_data,
                                                              x_column=x_columns,
                                                              y_column=['swe'],
                                                              spatial_column=['X', 'Y','Z'],
                                                              temp_column=['doy','year','month'],
                                                              id_column=['id'],
                                                              use_model="gtnnwr",
                                                              batch_size=128)


optimizer_params = {
    "scheduler":"MultiStepLR",
    "scheduler_milestones":[1000, 2000, 3000, 4000],
    "scheduler_gamma":0.8,
}
gtnnwr = GTNNWR(train_dataset, val_dataset, test_dataset, [[3], [256,128,64]],drop_out=0.4,optimizer='Adadelta',optimizer_params=optimizer_params,
                write_path = "../demo_result/gtnnwr_runs",
                model_name="GTNNWR_Final")
gtnnwr.add_graph()

gtnnwr.run(100,1000)

gtnnwr.result()
save_path = "../demo_result/gtnnwr_runs/GTNNWR_Final_results.png"

metrics = plot_gtnnwr_results(gtnnwr, save_path=save_path, show_plot=True)
