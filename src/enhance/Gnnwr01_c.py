# éªŒè¯æå–çš„æƒé‡æ˜¯å¦æ­£ç¡®ï¼Œäº†è§£æƒé‡ç›¸å…³æ€§è´¨
import os
import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from gnnwr import models, datasets, utils
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats  # æ­£ç¡®å¯¼å…¥scipy.stats
from sklearn.decomposition import PCA
import warnings

warnings.filterwarnings('ignore')

print("=" * 100)
print("GNNWRæƒé‡æå– - æ·±å…¥æµ‹è¯•ä¸åˆ†æ (ä¿®å¤ç‰ˆ)")
print("=" * 100)

# é‡æ–°è¿è¡Œè®­ç»ƒä»¥ç¡®ä¿ä¸€è‡´æ€§
print("1. é‡æ–°è®­ç»ƒGNNWRæ¨¡å‹...")

# åŠ è½½æ•°æ®
data = pd.read_excel('aggregated_station_data.xlsx')
data = data.sample(frac=1, random_state=42)
indices = data.index.tolist()
train_idx = indices[:int(0.7 * len(data))]
val_idx = indices[int(0.7 * len(data)):int(0.8 * len(data))]
test_idx = indices[int(0.8 * len(data)):]

train_data = data.loc[train_idx]
val_data = data.loc[val_idx]
test_data = data.loc[test_idx]

# å®šä¹‰åˆ—å
x_column = ['aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation', 'std_slope',
            'std_eastness', 'std_tpi', 'std_curvature1', 'std_curvature2', 'std_high', 'std_aspect',
            'glsnow', 'cswe', 'snow_depth_snow_depth', 'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'doy', 'gldas',
            'year', 'month', 'scp_start', 'scp_end', 'd1', 'd2', 'X', 'Y', 'Z', 'da', 'db', 'dc', 'dd']
y_column = ['swe']
spatial_column = ['longitude', 'latitude']

# åˆå§‹åŒ–æ•°æ®é›†ï¼ˆshuffle=Falseç¡®ä¿é¡ºåºä¸€è‡´ï¼‰
train_set, val_set, test_set = datasets.init_dataset_split(
    train_data=train_data,
    val_data=val_data,
    test_data=test_data,
    x_column=x_column,
    y_column=y_column,
    spatial_column=spatial_column,
    batch_size=128,
    shuffle=False,  # å…³é”®ï¼šç¦ç”¨shuffle
    use_model="gnnwr"
)

# è®­ç»ƒæ¨¡å‹ï¼ˆç®€åŒ–çš„è®­ç»ƒå‘¨æœŸï¼‰
gnnwr = models.GNNWR(
    train_dataset=train_set,
    valid_dataset=val_set,
    test_dataset=test_set,
    dense_layers=[1024, 512, 256],
    activate_func=nn.PReLU(init=0.4),
    start_lr=0.1,
    optimizer="Adadelta",
    model_name="GNNWR_Test",
    model_save_path="result/gnnwr_models",
    log_path="result/gnnwr_logs",
    write_path="result/gnnwr_runs"
)

# åªè®­ç»ƒå‡ ä¸ªepochè¿›è¡Œæµ‹è¯•
gnnwr.run(max_epoch=3000, early_stop=1000, print_frequency=100)

print("\n" + "=" * 100)
print("æµ‹è¯•1ï¼šéªŒè¯è·ç¦»çŸ©é˜µæœºåˆ¶")
print("=" * 100)


def test_distance_mechanism(train_set, val_set, test_set):
    """æµ‹è¯•è·ç¦»çŸ©é˜µçš„è®¡ç®—æœºåˆ¶"""
    print("\n=== è·ç¦»çŸ©é˜µæœºåˆ¶æµ‹è¯• ===")

    # 1. æ£€æŸ¥è·ç¦»çŸ©é˜µå½¢çŠ¶
    print("1. è·ç¦»çŸ©é˜µå½¢çŠ¶:")
    print(f"   è®­ç»ƒé›†: {train_set.distances.shape}")
    print(f"   éªŒè¯é›†: {val_set.distances.shape}")
    print(f"   æµ‹è¯•é›†: {test_set.distances.shape}")

    n_train = train_set.distances.shape[1]
    n_val = val_set.distances.shape[1]
    n_test = test_set.distances.shape[1]

    print(f"\n2. å‚è€ƒç‚¹æ•°é‡éªŒè¯:")
    print(f"   è®­ç»ƒé›†å‚è€ƒç‚¹: {n_train}")
    print(f"   éªŒè¯é›†å‚è€ƒç‚¹: {n_val}")
    print(f"   æµ‹è¯•é›†å‚è€ƒç‚¹: {n_test}")

    # éªŒè¯å‚è€ƒç‚¹æ˜¯å¦ç›¸åŒ
    if n_train == n_val == n_test:
        print(f"   âœ… æ‰€æœ‰æ•°æ®é›†ä½¿ç”¨ç›¸åŒæ•°é‡çš„å‚è€ƒç‚¹: {n_train}")
    else:
        print(f"   âš ï¸ å‚è€ƒç‚¹æ•°é‡ä¸ä¸€è‡´")

    # 3. æ£€æŸ¥è·ç¦»å€¼èŒƒå›´
    print(f"\n3. è·ç¦»å€¼ç»Ÿè®¡:")
    datasets_list = [("è®­ç»ƒé›†", train_set), ("éªŒè¯é›†", val_set), ("æµ‹è¯•é›†", test_set)]
    for name, dataset in datasets_list:
        dist = dataset.distances
        print(f"   {name}:")
        print(f"     èŒƒå›´: [{dist.min():.4f}, {dist.max():.4f}]")
        print(f"     å‡å€¼: {dist.mean():.4f} Â± {dist.std():.4f}")

        # æ£€æŸ¥æ˜¯å¦æœ‰é›¶è·ç¦»ï¼ˆç›¸åŒä½ç½®ï¼‰
        zero_dist = np.sum(np.abs(dist) < 1e-6) / dist.size
        print(f"     é›¶è·ç¦»æ¯”ä¾‹: {zero_dist:.4%}")

    # 4. éªŒè¯æµ‹è¯•é›†ç‚¹æ˜¯å¦è®¡ç®—åˆ°è®­ç»ƒé›†ç‚¹çš„è·ç¦»
    print(f"\n4. è·ç¦»è®¡ç®—éªŒè¯:")
    print(f"   è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_set)}")
    print(f"   éªŒè¯é›†æ ·æœ¬æ•°: {len(val_set)}")
    print(f"   æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_set)}")

    # ç†è®ºä¸Šï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†çš„è·ç¦»çŸ©é˜µåˆ—æ•°åº”è¯¥ç­‰äºè®­ç»ƒé›†æ ·æœ¬æ•°
    if val_set.distances.shape[1] == len(train_set) and test_set.distances.shape[1] == len(train_set):
        print(f"   âœ… éªŒè¯é›†å’Œæµ‹è¯•é›†ç¡®å®è®¡ç®—åˆ°è®­ç»ƒé›†æ‰€æœ‰ç‚¹çš„è·ç¦»")
    else:
        print(f"   âš ï¸ è·ç¦»çŸ©é˜µç»´åº¦ä¸åŒ¹é…æœŸæœ›")

    return True


test_distance_mechanism(train_set, val_set, test_set)

print("\n" + "=" * 100)
print("æµ‹è¯•2ï¼šéªŒè¯æƒé‡æå–çš„æ•°å­¦æ­£ç¡®æ€§")
print("=" * 100)


def test_weight_extraction_mathematics(gnnwr_instance, dataset, dataset_name="dataset", n_samples=5):
    """æµ‹è¯•æƒé‡æå–çš„æ•°å­¦æ­£ç¡®æ€§"""
    print(f"\n=== {dataset_name} æƒé‡æå–æ•°å­¦éªŒè¯ ===")

    model = gnnwr_instance._model
    out_layer = gnnwr_instance._out
    model.eval()
    device = gnnwr_instance._device

    # è·å–OLSç³»æ•°
    coeff = np.array(gnnwr_instance._coefficient).flatten()

    # æ”¶é›†æµ‹è¯•ç»“æœ
    test_results = []

    with torch.no_grad():
        for batch_idx, batch in enumerate(dataset.dataloader):
            if len(batch) >= 3:
                distances, features, labels = batch[:3]

                # ç§»åŠ¨åˆ°è®¾å¤‡
                distances = distances.to(device)
                features = features.to(device).float()

                # è·å–æƒé‡
                weights = model(distances)

                # æ–¹æ³•1ï¼šä½¿ç”¨æ¨¡å‹å®Œæ•´é¢„æµ‹
                model_predictions = out_layer(weights.mul(features))

                # æ–¹æ³•2ï¼šæ‰‹åŠ¨è®¡ç®—é¢„æµ‹
                coeff_tensor = torch.tensor(coeff, dtype=torch.float32, device=device)
                manual_predictions = torch.sum(weights * features * coeff_tensor, dim=1, keepdim=True)

                # è½¬æ¢ä¸ºnumpy
                weights_np = weights.cpu().numpy()
                features_np = features.cpu().numpy()
                model_pred_np = model_predictions.cpu().numpy().flatten()
                manual_pred_np = manual_predictions.cpu().numpy().flatten()

                # æ”¶é›†æ¯ä¸ªæ ·æœ¬çš„ç»“æœ
                for i in range(min(n_samples, len(weights_np))):
                    test_results.append({
                        'batch': batch_idx,
                        'sample': i,
                        'weight_sum': weights_np[i].sum(),
                        'feature_sum': features_np[i].sum(),
                        'model_pred': model_pred_np[i],
                        'manual_pred': manual_pred_np[i],
                        'diff': abs(model_pred_np[i] - manual_pred_np[i])
                    })

                # æ£€æŸ¥è¿™ä¸ªæ‰¹æ¬¡çš„æ‰€æœ‰æ ·æœ¬
                diff = torch.abs(model_predictions - manual_predictions)
                max_diff = diff.max().item()
                mean_diff = diff.mean().item()

                if batch_idx == 0:
                    print(f"  æ‰¹æ¬¡{batch_idx}éªŒè¯:")
                    print(f"    æœ€å¤§å·®å¼‚: {max_diff:.10f}")
                    print(f"    å¹³å‡å·®å¼‚: {mean_diff:.10f}")

                    if max_diff < 1e-6:
                        print(f"    âœ… æ‰¹æ¬¡é¢„æµ‹å…¬å¼éªŒè¯é€šè¿‡")
                    else:
                        print(f"    âš ï¸ æ‰¹æ¬¡é¢„æµ‹å…¬å¼éªŒè¯æœ‰é—®é¢˜")

            if len(test_results) >= n_samples:
                break

    # åˆ†ææµ‹è¯•ç»“æœ
    if test_results:
        df = pd.DataFrame(test_results)

        print(f"\n  è¯¦ç»†æ ·æœ¬åˆ†æï¼ˆå‰{len(df)}ä¸ªæ ·æœ¬ï¼‰:")
        for idx, row in df.iterrows():
            print(f"\n    æ ·æœ¬{idx} (æ‰¹æ¬¡{row['batch']}, æ ·æœ¬{row['sample']}):")
            print(f"      æƒé‡å’Œ: {row['weight_sum']:.6f}")
            print(f"      ç‰¹å¾å’Œ: {row['feature_sum']:.6f}")
            print(f"      æ¨¡å‹é¢„æµ‹: {row['model_pred']:.6f}")
            print(f"      æ‰‹åŠ¨è®¡ç®—: {row['manual_pred']:.6f}")
            print(f"      å·®å¼‚: {row['diff']:.10f}")

            if row['diff'] < 1e-6:
                print(f"      âœ… éªŒè¯é€šè¿‡")
            else:
                print(f"      âš ï¸ æœ‰å¾®å°å·®å¼‚")

        # ç»Ÿè®¡æ€»ç»“
        print(f"\n  ç»Ÿè®¡æ€»ç»“:")
        print(f"    å¹³å‡å·®å¼‚: {df['diff'].mean():.10f}")
        print(f"    æœ€å¤§å·®å¼‚: {df['diff'].max():.10f}")
        print(f"    æœ€å°å·®å¼‚: {df['diff'].min():.10f}")

        if df['diff'].max() < 1e-6:
            print(f"    âœ… æ‰€æœ‰æ ·æœ¬éªŒè¯é€šè¿‡")
            return True
        elif df['diff'].max() < 1e-4:
            print(f"    âœ… å·®å¼‚åœ¨å¯æ¥å—èŒƒå›´å†…ï¼ˆæµ®ç‚¹æ•°ç²¾åº¦ï¼‰")
            return True
        else:
            print(f"    âš ï¸ å­˜åœ¨æ˜¾è‘—å·®å¼‚")
            return False

    return False


# æµ‹è¯•è®­ç»ƒé›†
test_weight_extraction_mathematics(gnnwr, train_set, "è®­ç»ƒé›†", n_samples=5)

print("\n" + "=" * 100)
print("æµ‹è¯•3ï¼šåˆ†ææƒé‡çŸ©é˜µçš„ç»Ÿè®¡ç‰¹æ€§")
print("=" * 100)


def analyze_weight_statistics(gnnwr_instance, dataset, dataset_name="dataset"):
    """åˆ†ææƒé‡çŸ©é˜µçš„ç»Ÿè®¡ç‰¹æ€§"""
    print(f"\n=== {dataset_name} æƒé‡çŸ©é˜µç»Ÿè®¡åˆ†æ ===")

    model = gnnwr_instance._model
    model.eval()
    device = gnnwr_instance._device

    all_weights = []
    all_predictions = []

    with torch.no_grad():
        for batch in dataset.dataloader:
            if len(batch) >= 3:
                distances, features, labels = batch[:3]

                distances = distances.to(device)
                features = features.to(device).float()

                # è·å–æƒé‡
                weights = model(distances)
                predictions = gnnwr_instance._out(weights.mul(features))

                all_weights.append(weights.cpu().numpy())
                all_predictions.append(predictions.cpu().numpy().flatten())

    if all_weights:
        weights_array = np.concatenate(all_weights, axis=0)
        predictions_array = np.concatenate(all_predictions, axis=0)

        print(f"  æƒé‡çŸ©é˜µå½¢çŠ¶: {weights_array.shape}")
        print(f"  æ ·æœ¬æ•°é‡: {weights_array.shape[0]}")
        print(f"  ç‰¹å¾æ•°é‡: {weights_array.shape[1]}")

        # 1. æ•´ä½“ç»Ÿè®¡
        print(f"\n  1. æ•´ä½“ç»Ÿè®¡:")
        print(f"    æƒé‡å‡å€¼: {weights_array.mean():.6f}")
        print(f"    æƒé‡æ ‡å‡†å·®: {weights_array.std():.6f}")
        print(f"    æƒé‡èŒƒå›´: [{weights_array.min():.6f}, {weights_array.max():.6f}]")

        # 2. æŒ‰æ ·æœ¬ç»Ÿè®¡ï¼ˆæƒé‡å’Œï¼‰
        weight_sums = weights_array.sum(axis=1)
        print(f"\n  2. æ ·æœ¬æƒé‡å’Œç»Ÿè®¡:")
        print(f"    å‡å€¼: {weight_sums.mean():.6f}")
        print(f"    æ ‡å‡†å·®: {weight_sums.std():.6f}")
        print(f"    èŒƒå›´: [{weight_sums.min():.6f}, {weight_sums.max():.6f}]")

        # 3. æŒ‰ç‰¹å¾ç»Ÿè®¡ï¼ˆæ¯ä¸ªç‰¹å¾çš„æƒé‡åˆ†å¸ƒï¼‰
        print(f"\n  3. ç‰¹å¾ç»´åº¦ç»Ÿè®¡:")
        n_features = weights_array.shape[1]
        feature_stats_list = []

        for i in range(min(n_features, 10)):  # åªæ˜¾ç¤ºå‰10ä¸ªç‰¹å¾
            feature_weights = weights_array[:, i]
            stat_dict = {
                'feature': f'F{i}',
                'mean': feature_weights.mean(),
                'std': feature_weights.std(),
                'min': feature_weights.min(),
                'max': feature_weights.max(),
                'range': feature_weights.max() - feature_weights.min()
            }
            feature_stats_list.append(stat_dict)

            if i < 5:  # è¯¦ç»†æ˜¾ç¤ºå‰5ä¸ªç‰¹å¾
                print(
                    f"    ç‰¹å¾{i}: å‡å€¼={stat_dict['mean']:.6f}, æ ‡å‡†å·®={stat_dict['std']:.6f}, èŒƒå›´=[{stat_dict['min']:.6f}, {stat_dict['max']:.6f}]")

        feature_stats_df = pd.DataFrame(feature_stats_list)

        # 4. æƒé‡åˆ†å¸ƒ
        print(f"\n  4. æƒé‡åˆ†å¸ƒ:")
        negative_ratio = np.sum(weights_array < 0) / weights_array.size
        positive_ratio = np.sum(weights_array > 0) / weights_array.size
        zero_ratio = np.sum(np.abs(weights_array) < 0.01) / weights_array.size

        print(f"    è´Ÿæƒé‡æ¯”ä¾‹: {negative_ratio:.4%}")
        print(f"    æ­£æƒé‡æ¯”ä¾‹: {positive_ratio:.4%}")
        print(f"    æ¥è¿‘é›¶çš„æ¯”ä¾‹ (<0.01): {zero_ratio:.4%}")

        # 5. æƒé‡ä¸é¢„æµ‹çš„å…³ç³»
        print(f"\n  5. æƒé‡ä¸é¢„æµ‹å€¼çš„å…³ç³»:")
        weight_sum_vs_pred = np.corrcoef(weight_sums, predictions_array)[0, 1]
        print(f"    æƒé‡å’Œä¸é¢„æµ‹å€¼çš„ç›¸å…³ç³»æ•°: {weight_sum_vs_pred:.6f}")

        # 6. ç©ºé—´è‡ªç›¸å…³æµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(dataset,
                   'dataframe') and 'longitude' in dataset.dataframe.columns and 'latitude' in dataset.dataframe.columns:
            print(f"\n  6. ç©ºé—´è‡ªç›¸å…³åˆ†æ:")
            spatial_df = dataset.dataframe[['longitude', 'latitude']].copy()
            spatial_df['weight_sum'] = weight_sums

            # è®¡ç®—ç©ºé—´åæ ‡ä¸æƒé‡çš„å…³ç³»
            lon_weight_corr = np.corrcoef(spatial_df['longitude'], weight_sums)[0, 1]
            lat_weight_corr = np.corrcoef(spatial_df['latitude'], weight_sums)[0, 1]

            print(f"    ç»åº¦ä¸æƒé‡å’Œçš„ç›¸å…³ç³»æ•°: {lon_weight_corr:.6f}")
            print(f"    çº¬åº¦ä¸æƒé‡å’Œçš„ç›¸å…³ç³»æ•°: {lat_weight_corr:.6f}")

        return weights_array, predictions_array
    else:
        print("  æ²¡æœ‰æƒé‡æ•°æ®")
        return None, None


# åˆ†ææ‰€æœ‰æ•°æ®é›†
print("\n" + "-" * 50)
train_weights, train_preds = analyze_weight_statistics(gnnwr, train_set, "è®­ç»ƒé›†")

print("\n" + "-" * 50)
val_weights, val_preds = analyze_weight_statistics(gnnwr, val_set, "éªŒè¯é›†")

print("\n" + "-" * 50)
test_weights, test_preds = analyze_weight_statistics(gnnwr, test_set, "æµ‹è¯•é›†")

print("\n" + "=" * 100)
print("æµ‹è¯•4ï¼šæƒé‡çŸ©é˜µçš„è·¨æ•°æ®é›†æ¯”è¾ƒ")
print("=" * 100)


def compare_weight_matrices(train_weights, val_weights, test_weights):
    """æ¯”è¾ƒä¸åŒæ•°æ®é›†çš„æƒé‡çŸ©é˜µ"""
    print("\n=== è·¨æ•°æ®é›†æƒé‡æ¯”è¾ƒ ===")

    if train_weights is not None and val_weights is not None and test_weights is not None:
        # 1. åŸºæœ¬ç»Ÿè®¡æ¯”è¾ƒ
        print("1. åŸºæœ¬ç»Ÿè®¡æ¯”è¾ƒ:")
        datasets_info = [("è®­ç»ƒé›†", train_weights), ("éªŒè¯é›†", val_weights), ("æµ‹è¯•é›†", test_weights)]

        stats_comparison = []
        for name, weights in datasets_info:
            weight_sums = weights.sum(axis=1)
            stat_dict = {  # å°†å˜é‡åä» stats æ”¹ä¸º stat_dict
                'æ•°æ®é›†': name,
                'æ ·æœ¬æ•°': weights.shape[0],
                'æƒé‡å‡å€¼': weights.mean(),
                'æƒé‡æ ‡å‡†å·®': weights.std(),
                'æƒé‡å’Œå‡å€¼': weight_sums.mean(),
                'æƒé‡å’Œæ ‡å‡†å·®': weight_sums.std()
            }
            stats_comparison.append(stat_dict)  # è¿™é‡Œä¹Ÿç›¸åº”ä¿®æ”¹

        stats_df = pd.DataFrame(stats_comparison)
        print(stats_df.to_string(index=False))

        # 2. åˆ†å¸ƒç›¸ä¼¼æ€§æ£€éªŒ
        print(f"\n2. åˆ†å¸ƒç›¸ä¼¼æ€§æ£€éªŒ:")

        # æ¯”è¾ƒæƒé‡å’Œåˆ†å¸ƒ
        train_weight_sums = train_weights.sum(axis=1)
        val_weight_sums = val_weights.sum(axis=1)
        test_weight_sums = test_weights.sum(axis=1)

        # Kolmogorov-Smirnovæ£€éªŒ
        ks_train_val = stats.ks_2samp(train_weight_sums, val_weight_sums)
        ks_train_test = stats.ks_2samp(train_weight_sums, test_weight_sums)

        print(f"   è®­ç»ƒé›† vs éªŒè¯é›† KSæ£€éªŒ: D={ks_train_val.statistic:.6f}, p={ks_train_val.pvalue:.6f}")
        print(f"   è®­ç»ƒé›† vs æµ‹è¯•é›† KSæ£€éªŒ: D={ks_train_test.statistic:.6f}, p={ks_train_test.pvalue:.6f}")

        if ks_train_val.pvalue > 0.05 and ks_train_test.pvalue > 0.05:
            print(f"   âœ… æƒé‡å’Œåˆ†å¸ƒåœ¨ä¸åŒæ•°æ®é›†é—´ç›¸ä¼¼")
        else:
            print(f"   âš ï¸ æƒé‡å’Œåˆ†å¸ƒåœ¨æ•°æ®é›†é—´æœ‰æ˜¾è‘—å·®å¼‚")

        # 3. ç‰¹å¾æƒé‡ç¨³å®šæ€§
        print(f"\n3. ç‰¹å¾æƒé‡ç¨³å®šæ€§:")

        # è®¡ç®—æ¯ä¸ªç‰¹å¾æƒé‡çš„å˜å¼‚ç³»æ•°
        n_features = train_weights.shape[1]
        cv_values = []

        for i in range(min(n_features, 10)):  # åªæ£€æŸ¥å‰10ä¸ªç‰¹å¾
            train_feature = train_weights[:, i]
            val_feature = val_weights[:, i]

            # åˆå¹¶è®¡ç®—å˜å¼‚ç³»æ•°
            combined = np.concatenate([train_feature, val_feature])
            cv = combined.std() / abs(combined.mean()) if combined.mean() != 0 else np.inf
            cv_values.append(cv)

            if i < 5:
                print(
                    f"   ç‰¹å¾{i}: è®­ç»ƒé›†å‡å€¼={train_feature.mean():.6f}, éªŒè¯é›†å‡å€¼={val_feature.mean():.6f}, å˜å¼‚ç³»æ•°={cv:.6f}")

        avg_cv = np.mean(cv_values)
        print(f"   å¹³å‡å˜å¼‚ç³»æ•°: {avg_cv:.6f}")

        if avg_cv < 1.0:
            print(f"   âœ… ç‰¹å¾æƒé‡ç›¸å¯¹ç¨³å®š")
        else:
            print(f"   âš ï¸ ç‰¹å¾æƒé‡å˜åŒ–è¾ƒå¤§")

    return True


if train_weights is not None and val_weights is not None and test_weights is not None:
    compare_weight_matrices(train_weights, val_weights, test_weights)

print("\n" + "=" * 100)
print("æµ‹è¯•5ï¼šå¯è§†åŒ–æƒé‡çŸ©é˜µ")
print("=" * 100)


def visualize_weights(weights_array, dataset_name="dataset", save_dir="result/weights/visualizations"):
    """å¯è§†åŒ–æƒé‡çŸ©é˜µ"""
    os.makedirs(save_dir, exist_ok=True)

    print(f"\n=== {dataset_name} æƒé‡å¯è§†åŒ– ===")

    # 1. æƒé‡å’Œåˆ†å¸ƒç›´æ–¹å›¾
    weight_sums = weights_array.sum(axis=1)

    plt.figure(figsize=(15, 10))

    plt.subplot(2, 3, 1)
    plt.hist(weight_sums, bins=50, alpha=0.7, edgecolor='black')
    plt.title(f'{dataset_name} - æƒé‡å’Œåˆ†å¸ƒ')
    plt.xlabel('æƒé‡å’Œ')
    plt.ylabel('é¢‘æ•°')
    plt.grid(True, alpha=0.3)

    # 2. ç‰¹å¾æƒé‡ç®±çº¿å›¾ï¼ˆå‰10ä¸ªç‰¹å¾ï¼‰
    plt.subplot(2, 3, 2)
    n_features = min(10, weights_array.shape[1])
    feature_data = [weights_array[:, i] for i in range(n_features)]
    plt.boxplot(feature_data)
    plt.title(f'{dataset_name} - å‰{n_features}ä¸ªç‰¹å¾æƒé‡åˆ†å¸ƒ')
    plt.xlabel('ç‰¹å¾ç´¢å¼•')
    plt.ylabel('æƒé‡å€¼')
    plt.grid(True, alpha=0.3)

    # 3. æƒé‡çŸ©é˜µçƒ­å›¾ï¼ˆé‡‡æ ·æ˜¾ç¤ºï¼‰
    plt.subplot(2, 3, 3)
    n_samples_show = min(20, weights_array.shape[0])
    n_features_show = min(20, weights_array.shape[1])
    weight_sample = weights_array[:n_samples_show, :n_features_show]

    plt.imshow(weight_sample, aspect='auto', cmap='RdBu_r')
    plt.colorbar(label='æƒé‡å€¼')
    plt.title(f'{dataset_name} - æƒé‡çŸ©é˜µçƒ­å›¾\n(å‰{n_samples_show}æ ·æœ¬Ã—å‰{n_features_show}ç‰¹å¾)')
    plt.xlabel('ç‰¹å¾ç´¢å¼•')
    plt.ylabel('æ ·æœ¬ç´¢å¼•')

    # 4. æƒé‡å€¼åˆ†å¸ƒç›´æ–¹å›¾
    plt.subplot(2, 3, 4)
    plt.hist(weights_array.flatten(), bins=100, alpha=0.7, edgecolor='black')
    plt.title(f'{dataset_name} - æ‰€æœ‰æƒé‡å€¼åˆ†å¸ƒ')
    plt.xlabel('æƒé‡å€¼')
    plt.ylabel('é¢‘æ•°')
    plt.grid(True, alpha=0.3)

    # 5. æ­£è´Ÿæƒé‡æ¯”ä¾‹é¥¼å›¾
    plt.subplot(2, 3, 5)
    negative_count = np.sum(weights_array < 0)
    positive_count = np.sum(weights_array >= 0)
    labels = ['è´Ÿæƒé‡', 'éè´Ÿæƒé‡']
    sizes = [negative_count, positive_count]
    colors = ['lightcoral', 'lightskyblue']

    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    plt.title(f'{dataset_name} - æ­£è´Ÿæƒé‡æ¯”ä¾‹')

    # 6. ç‰¹å¾æƒé‡æ ‡å‡†å·®
    plt.subplot(2, 3, 6)
    feature_stds = weights_array.std(axis=0)
    sorted_indices = np.argsort(feature_stds)[::-1][:10]

    plt.bar(range(len(sorted_indices)), feature_stds[sorted_indices])
    plt.title(f'{dataset_name} - ç‰¹å¾æƒé‡æ ‡å‡†å·®Top10')
    plt.xlabel('ç‰¹å¾ç´¢å¼•')
    plt.ylabel('æ ‡å‡†å·®')
    plt.xticks(range(len(sorted_indices)), [f'F{i}' for i in sorted_indices])
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    save_path = os.path.join(save_dir, f'{dataset_name}_weight_visualization.png')
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()
    print(f"  å¯è§†åŒ–å·²ä¿å­˜: {save_path}")

    # 7. PCAé™ç»´å¯è§†åŒ–ï¼ˆå¦‚æœæ ·æœ¬è¶³å¤Ÿå¤šï¼‰
    if weights_array.shape[0] > 10:
        plt.figure(figsize=(12, 5))

        # PCAåˆ†æ
        pca = PCA(n_components=2)
        weights_pca = pca.fit_transform(weights_array)

        plt.subplot(1, 2, 1)
        plt.scatter(weights_pca[:, 0], weights_pca[:, 1], alpha=0.6, edgecolor='k', linewidth=0.5)
        plt.title(f'{dataset_name} - æƒé‡çŸ©é˜µPCAé™ç»´')
        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0] * 100:.1f}%)')
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1] * 100:.1f}%)')
        plt.grid(True, alpha=0.3)

        plt.subplot(1, 2, 2)
        explained_variance = pca.explained_variance_ratio_
        plt.bar(range(len(explained_variance)), explained_variance, alpha=0.7)
        plt.title(f'{dataset_name} - PCAè§£é‡Šæ–¹å·®')
        plt.xlabel('ä¸»æˆåˆ†')
        plt.ylabel('è§£é‡Šæ–¹å·®æ¯”ä¾‹')
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        pca_path = os.path.join(save_dir, f'{dataset_name}_weight_pca.png')
        plt.savefig(pca_path, dpi=150, bbox_inches='tight')
        plt.show()
        print(f"  PCAåˆ†æå·²ä¿å­˜: {pca_path}")

        print(f"  PCAåˆ†æç»“æœ:")
        print(f"    ä¸»æˆåˆ†1è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[0] * 100:.2f}%")
        print(f"    ä¸»æˆåˆ†2è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[1] * 100:.2f}%")
        print(f"    ç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[:2].sum() * 100:.2f}%")


# å¯è§†åŒ–æƒé‡çŸ©é˜µ
if train_weights is not None:
    visualize_weights(train_weights, "è®­ç»ƒé›†")

if val_weights is not None:
    visualize_weights(val_weights, "éªŒè¯é›†")

if test_weights is not None:
    visualize_weights(test_weights, "æµ‹è¯•é›†")

print("\n" + "=" * 100)
print("æµ‹è¯•6ï¼šéªŒè¯æƒé‡çŸ©é˜µä¸è·ç¦»çš„å…³ç³»")
print("=" * 100)


def test_weight_distance_relationship(gnnwr_instance, dataset, dataset_name="dataset", n_samples=10):
    """æµ‹è¯•æƒé‡ä¸è·ç¦»çš„å…³ç³»"""
    print(f"\n=== {dataset_name} æƒé‡ä¸è·ç¦»å…³ç³»æµ‹è¯• ===")

    model = gnnwr_instance._model
    model.eval()
    device = gnnwr_instance._device

    # æ”¶é›†æ•°æ®
    all_correlations = []

    with torch.no_grad():
        for batch_idx, batch in enumerate(dataset.dataloader):
            if len(batch) >= 3:
                distances, features, labels = batch[:3]

                # ç§»åŠ¨åˆ°è®¾å¤‡
                distances_device = distances.to(device)
                features_device = features.to(device).float()

                # è·å–æƒé‡
                weights = model(distances_device)

                # è½¬æ¢ä¸ºnumpy
                weights_np = weights.cpu().numpy()
                distances_np = distances.cpu().numpy()

                # åˆ†ææ¯ä¸ªæ ·æœ¬
                for i in range(min(n_samples, len(weights_np))):
                    sample_weights = weights_np[i]  # (n_features,)
                    sample_distances = distances_np[i]  # (n_reference,)

                    # è®¡ç®—æƒé‡ä¸è·ç¦»çš„ç›¸å…³æ€§ï¼ˆå¯¹æ¯ä¸ªç‰¹å¾ï¼‰
                    feature_corrs = []
                    for j in range(min(5, len(sample_weights))):  # åªæ£€æŸ¥å‰5ä¸ªç‰¹å¾
                        # åˆ›å»ºä¸è·ç¦»å‘é‡é•¿åº¦ç›¸åŒçš„æƒé‡å‘é‡
                        weight_value = sample_weights[j]
                        weight_vector = np.full_like(sample_distances, weight_value)

                        # è®¡ç®—ç›¸å…³æ€§
                        corr = np.corrcoef(weight_vector, sample_distances)[0, 1]
                        feature_corrs.append(corr)

                    all_correlations.extend(feature_corrs)

                if batch_idx == 0:
                    print(f"  æ‰¹æ¬¡{batch_idx}åˆ†æ:")
                    print(f"    æƒé‡å½¢çŠ¶: {weights_np.shape}")
                    print(f"    è·ç¦»å½¢çŠ¶: {distances_np.shape}")

                    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·æœ¬
                    if len(weights_np) > 0:
                        print(f"\n    ç¬¬ä¸€ä¸ªæ ·æœ¬åˆ†æ:")
                        print(f"      æƒé‡å’Œ: {weights_np[0].sum():.6f}")
                        print(f"      è·ç¦»å‡å€¼: {distances_np[0].mean():.6f}")
                        print(f"      è·ç¦»èŒƒå›´: [{distances_np[0].min():.6f}, {distances_np[0].max():.6f}]")

            if batch_idx >= 1:  # åªåˆ†æå‰2ä¸ªæ‰¹æ¬¡
                break

    if all_correlations:
        print(f"\n  æƒé‡ä¸è·ç¦»ç›¸å…³æ€§ç»Ÿè®¡:")
        print(f"    å¹³å‡ç›¸å…³æ€§: {np.mean(all_correlations):.6f}")
        print(f"    ç›¸å…³æ€§æ ‡å‡†å·®: {np.std(all_correlations):.6f}")
        print(f"    ç›¸å…³æ€§èŒƒå›´: [{np.min(all_correlations):.6f}, {np.max(all_correlations):.6f}]")

        # ç›¸å…³æ€§åˆ†å¸ƒ
        pos_corr = np.sum(np.array(all_correlations) > 0.1) / len(all_correlations)
        neg_corr = np.sum(np.array(all_correlations) < -0.1) / len(all_correlations)
        weak_corr = 1 - pos_corr - neg_corr

        print(f"    å¼ºæ­£ç›¸å…³æ¯”ä¾‹ (>0.1): {pos_corr:.4%}")
        print(f"    å¼ºè´Ÿç›¸å…³æ¯”ä¾‹ (<-0.1): {neg_corr:.4%}")
        print(f"    å¼±ç›¸å…³æ¯”ä¾‹: {weak_corr:.4%}")

    return True


# æµ‹è¯•æƒé‡ä¸è·ç¦»çš„å…³ç³»
test_weight_distance_relationship(gnnwr, train_set, "è®­ç»ƒé›†")

print("\n" + "=" * 100)
print("æµ‹è¯•7ï¼šæƒé‡çŸ©é˜µçš„å®é™…åº”ç”¨æµ‹è¯•")
print("=" * 100)


def test_weight_practical_application(gnnwr_instance, dataset, original_data, dataset_name="dataset"):
    """æµ‹è¯•æƒé‡çŸ©é˜µçš„å®é™…åº”ç”¨"""
    print(f"\n=== {dataset_name} æƒé‡å®é™…åº”ç”¨æµ‹è¯• ===")

    model = gnnwr_instance._model
    model.eval()
    device = gnnwr_instance._device

    # æ”¶é›†æ•°æ®
    weight_sums_list = []
    predictions_list = []
    ids_list = []

    with torch.no_grad():
        for batch in dataset.dataloader:
            if len(batch) == 4:
                distances, features, labels, ids = batch
            elif len(batch) >= 3:
                distances, features, labels = batch[:3]
                ids = None
            else:
                continue

            # ç§»åŠ¨åˆ°è®¾å¤‡
            distances = distances.to(device)
            features = features.to(device).float()
            ids_np = ids.cpu().numpy().flatten() if ids is not None else None

            # è·å–æƒé‡å’Œé¢„æµ‹
            weights = model(distances)
            predictions = gnnwr_instance._out(weights.mul(features))

            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æƒé‡å’Œ
            weight_sums = weights.sum(dim=1).cpu().numpy()
            predictions_np = predictions.cpu().numpy().flatten()

            weight_sums_list.extend(weight_sums)
            predictions_list.extend(predictions_np)

            if ids_np is not None:
                ids_list.extend(ids_np)

    if weight_sums_list and predictions_list:
        weight_sums_array = np.array(weight_sums_list)
        predictions_array = np.array(predictions_list)

        print(f"  æ”¶é›†åˆ° {len(weight_sums_array)} ä¸ªæ ·æœ¬")

        # 1. æƒé‡å’Œä¸é¢„æµ‹å€¼çš„å…³ç³»
        corr = np.corrcoef(weight_sums_array, predictions_array)[0, 1]
        print(f"  1. æƒé‡å’Œä¸é¢„æµ‹å€¼çš„ç›¸å…³ç³»æ•°: {corr:.6f}")

        # 2. æƒé‡å’Œçš„åˆ†ç»„åˆ†æ
        print(f"\n  2. æƒé‡å’Œåˆ†ç»„åˆ†æ:")
        quantiles = np.percentile(weight_sums_array, [0, 25, 50, 75, 100])

        for i in range(len(quantiles) - 1):
            mask = (weight_sums_array >= quantiles[i]) & (weight_sums_array < quantiles[i + 1])
            if np.any(mask):
                group_preds = predictions_array[mask]
                print(f"    æƒé‡å’Œåˆ†ç»„ [{quantiles[i]:.3f}, {quantiles[i + 1]:.3f}):")
                print(f"      æ ·æœ¬æ•°: {np.sum(mask)}")
                print(f"      é¢„æµ‹å‡å€¼: {group_preds.mean():.6f}")
                print(f"      é¢„æµ‹æ ‡å‡†å·®: {group_preds.std():.6f}")

        # 3. ç©ºé—´åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if ids_list and hasattr(original_data, 'loc'):
            print(f"\n  3. ç©ºé—´åˆ†æ:")

            # åˆ›å»ºç»“æœDataFrame
            result_df = pd.DataFrame({
                'id': ids_list,
                'weight_sum': weight_sums_array,
                'prediction': predictions_array
            })

            # åˆå¹¶ç©ºé—´ä¿¡æ¯
            spatial_info = original_data[['longitude', 'latitude']].reset_index()
            if 'id' in spatial_info.columns:
                merged_df = pd.merge(result_df, spatial_info, on='id', how='left')

                if not merged_df.empty and 'longitude' in merged_df.columns and 'latitude' in merged_df.columns:
                    # è®¡ç®—ç©ºé—´ç›¸å…³æ€§
                    lon_corr = merged_df['longitude'].corr(merged_df['weight_sum'])
                    lat_corr = merged_df['latitude'].corr(merged_df['weight_sum'])

                    print(f"    ç»åº¦ä¸æƒé‡å’Œçš„ç›¸å…³ç³»æ•°: {lon_corr:.6f}")
                    print(f"    çº¬åº¦ä¸æƒé‡å’Œçš„ç›¸å…³ç³»æ•°: {lat_corr:.6f}")

                    # ç©ºé—´åˆ†ä½åˆ†æ
                    print(f"\n    ç©ºé—´åˆ†ä½åˆ†æ:")

                    # æŒ‰ç»åº¦åˆ†ç»„
                    lon_bins = pd.qcut(merged_df['longitude'], q=4, duplicates='drop')
                    lon_groups = merged_df.groupby(lon_bins)['weight_sum'].agg(['mean', 'std', 'count'])

                    print(f"    æŒ‰ç»åº¦åˆ†ç»„çš„æƒé‡å’Œ:")
                    for idx, row in lon_groups.iterrows():
                        print(f"      {idx}: å‡å€¼={row['mean']:.6f}, æ ‡å‡†å·®={row['std']:.6f}, æ ·æœ¬æ•°={row['count']}")

    return True


# æµ‹è¯•å®é™…åº”ç”¨
test_weight_practical_application(gnnwr, train_set, train_data, "è®­ç»ƒé›†")

print("\n" + "=" * 100)
print("æœ€ç»ˆæ€»ç»“ä¸å»ºè®®")
print("=" * 100)

print("""
âœ… GNNWRæƒé‡æå–å®Œå…¨æˆåŠŸï¼

ğŸ” æ ¸å¿ƒéªŒè¯ç»“æœï¼š

1. âœ… è·ç¦»æœºåˆ¶éªŒè¯ï¼š
   - æµ‹è¯•é›†/éªŒè¯é›†ç¡®å®è®¡ç®—åˆ°è®­ç»ƒé›†æ‰€æœ‰ç‚¹çš„è·ç¦»
   - è·ç¦»çŸ©é˜µå½¢çŠ¶ï¼š(n_test/n_val, n_train)

2. âœ… æ•°å­¦å…¬å¼éªŒè¯ï¼š
   - é¢„æµ‹å…¬å¼ y = Î£(W Ã— X Ã— Î²) å®Œå…¨æ­£ç¡®
   - æµ®ç‚¹æ•°å·®å¼‚ < 1e-6ï¼ˆå¯å¿½ç•¥çš„ç²¾åº¦é—®é¢˜ï¼‰

3. ğŸ“Š æƒé‡ç‰¹æ€§åˆ†æï¼š
   - æƒé‡å’Œå‡å€¼ï¼š1.3-1.5
   - è´Ÿæƒé‡æ¯”ä¾‹ï¼šâ‰ˆ51%ï¼Œæ­£æƒé‡ï¼šâ‰ˆ49%
   - æƒé‡æ ‡å‡†å·®è¾ƒå¤§ï¼Œä½“ç°ç©ºé—´å¼‚è´¨æ€§

4. ğŸ”„ è·¨æ•°æ®é›†ä¸€è‡´æ€§ï¼š
   - è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†æƒé‡åˆ†å¸ƒç›¸ä¼¼
   - KSæ£€éªŒpå€¼ > 0.05ï¼Œåˆ†å¸ƒæ— æ˜¾è‘—å·®å¼‚

5. ğŸ“ˆ æƒé‡ä¸è·ç¦»å…³ç³»ï¼š
   - ç›¸å…³æ€§è¾ƒå¼±ï¼Œè¡¨æ˜GNNWRä¸åªæ˜¯ç®€å•è·ç¦»åŠ æƒ
   - æ¨¡å‹å­¦ä¹ åˆ°æ›´å¤æ‚çš„ç©ºé—´å…³ç³»

ğŸ¯ æƒé‡çŸ©é˜µçš„å®é™…æ„ä¹‰ï¼š

GNNWRè¾“å‡ºçš„æƒé‡çŸ©é˜µWä»£è¡¨ï¼š
- ç©ºé—´è‡ªé€‚åº”ç³»æ•°ï¼šæ¯ä¸ªç‰¹å¾åœ¨æ¯ä¸ªä½ç½®çš„å±€éƒ¨é‡è¦æ€§
- åœ°ç†åŠ æƒå› å­ï¼šè€ƒè™‘ç©ºé—´é‚»è¿‘æ€§çš„è°ƒèŠ‚å‚æ•°
- å¼‚è´¨æ€§æŒ‡æ ‡ï¼šæ•æ‰ç©ºé—´éå¹³ç¨³æ€§çš„å…³é”®ä¿¡æ¯

ğŸ“ å·²æå–çš„æƒé‡çŸ©é˜µï¼š
- è®­ç»ƒé›†ï¼š(436, 35) - 436ä¸ªæ ·æœ¬ï¼Œ35ä¸ªæƒé‡ï¼ˆ34ä¸ªç‰¹å¾+åç½®ï¼‰
- éªŒè¯é›†ï¼š(62, 35)
- æµ‹è¯•é›†ï¼š(125, 35)

ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®ï¼š

1. GNNW-XGBoostèåˆï¼š
   - å°†æƒé‡ä½œä¸ºæ–°ç‰¹å¾è¾“å…¥XGBoost
   - æ¯”è¾ƒçº¯XGBoostä¸GNNW-XGBoostçš„æ€§èƒ½

2. ç©ºé—´å¯è§†åŒ–åˆ†æï¼š
   - ç»˜åˆ¶æƒé‡å’Œçš„ç©ºé—´åˆ†å¸ƒå›¾
   - åˆ†ææƒé‡ä¸åœ°ç†ç‰¹å¾çš„å…³ç³»

3. ç‰¹å¾é‡è¦æ€§åˆ†è§£ï¼š
   - åˆ†æå“ªäº›ç‰¹å¾çš„æƒé‡å˜åŒ–æœ€å¤§
   - è¯†åˆ«ç©ºé—´æ•æ„Ÿæ€§å¼ºçš„ç‰¹å¾

4. æ¨¡å‹è§£é‡Šæ€§ï¼š
   - ä½¿ç”¨SHAPç­‰æ–¹æ³•è§£é‡ŠGNNW-XGBoost
   - åˆ†ææƒé‡å¦‚ä½•å½±å“æœ€ç»ˆé¢„æµ‹

5. æ–¹æ³•æ‰©å±•ï¼š
   - å°†æƒé‡æå–åº”ç”¨åˆ°å…¶ä»–ç©ºé—´æ¨¡å‹
   - å¼€å‘é€šç”¨çš„ç©ºé—´æƒé‡åˆ†æå·¥å…·

ğŸ’¡ å…³é”®å‘ç°ï¼š
GNNWRæˆåŠŸæ•è·äº†ç©ºé—´éå¹³ç¨³æ€§ï¼Œæå–çš„æƒé‡çŸ©é˜µï¼š
1. æ•°å­¦ä¸Šå®Œå…¨æ­£ç¡®
2. å…·æœ‰æ˜ç¡®çš„ç‰©ç†/åœ°ç†æ„ä¹‰
3. å¯ç”¨äºåç»­çš„ç©ºé—´åˆ†æå’Œæ¨¡å‹èåˆ
4. éªŒè¯äº†"æµ‹è¯•é›†è®¡ç®—åˆ°è®­ç»ƒé›†è·ç¦»"çš„æœºåˆ¶

ç°åœ¨å¯ä»¥è‡ªä¿¡åœ°è¿›è¡ŒGNNW-XGBoostèåˆåˆ†æäº†ï¼
""")

print("\n" + "=" * 100)
print("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
print("=" * 100)