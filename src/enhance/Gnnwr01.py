import gc
import os
import sys
import traceback

import torch
from gnnwr import models, datasets, utils
import pandas as pd
import torch.nn as nn
import numpy as np
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import warnings

warnings.filterwarnings('ignore')

# ä¿å­˜åŸå§‹æ–¹æ³•
original_randperm = torch.randperm


def patched_randperm(n, generator=None, out=None, dtype=torch.int64, layout=torch.strided, device=None,
                     requires_grad=False):
    """ä¿®å¤è®¾å¤‡ä¸åŒ¹é…çš„randperm"""
    if generator is not None:
        current_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        if hasattr(generator, 'device') and generator.device != current_device:
            # åˆ›å»ºæ–°çš„ç”Ÿæˆå™¨
            generator = torch.Generator(device=current_device)
            generator.manual_seed(torch.randint(0, 1000000, (1,)).item())

    return original_randperm(n, generator=generator, out=out, dtype=dtype, layout=layout, device=device,
                             requires_grad=requires_grad)


# åº”ç”¨è¡¥ä¸
torch.randperm = patched_randperm

# ç«‹å³ä¼˜åŒ–è®¾ç½®
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
torch.cuda.empty_cache()


def quick_gpu_fix():
    """å¿«é€ŸGPUä¿®å¤"""
    # è®¾ç½®GPUè®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # æ¸…ç†å†…å­˜
    if torch.cuda.is_available():
        torch.cuda.synchronize()
        torch.cuda.empty_cache()

    return device


def setup_device(device_id=0):
    """è®¾ç½®GPUè®¾å¤‡"""
    if torch.cuda.is_available():
        # æ£€æŸ¥å¯ç”¨çš„GPUæ•°é‡
        gpu_count = torch.cuda.device_count()
        print(f"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")

        # ç¡®ä¿è®¾å¤‡IDæœ‰æ•ˆ
        if device_id < gpu_count:
            device = torch.device(f'cuda:{device_id}')
            torch.cuda.set_device(device_id)  # ä½¿ç”¨æ•´æ•°ç´¢å¼•
            print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(device_id)}")
        else:
            device = torch.device('cpu')
            print(f"è®¾å¤‡ID {device_id} æ— æ•ˆï¼Œä½¿ç”¨CPU")
    else:
        device = torch.device('cpu')
        print("CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    return device


def station_based_kfold_cross_validation():
    """åŸºäºç«™ç‚¹çš„10æŠ˜äº¤å‰éªŒè¯ - GPUä¼˜åŒ–ç‰ˆæœ¬"""
    device = quick_gpu_fix()

    print("=== åŸºäºç«™ç‚¹çš„10æŠ˜äº¤å‰éªŒè¯ (GPUä¼˜åŒ–ç‰ˆ) ===")

    # è¯»å–æ•°æ®
    data = pd.read_excel('lu_onehot.xlsx')
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {data.shape}")

    # å®šä¹‰ç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—
    x_column = ['aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation', 'std_slope',
                'std_eastness', 'std_tpi', 'std_curvature1',
                'std_curvature2', 'std_high', 'std_aspect', 'glsnow', 'cswe', 'snow_depth_snow_depth',
                'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'doy', 'gldas',
                'year', 'month', 'scp_start', 'scp_end', 'd1', 'd2', 'X', 'Y', 'Z', 'da', 'db', 'dc', 'dd',
                'landuse_11', 'landuse_12', 'landuse_21', 'landuse_22',
                'landuse_23', 'landuse_24', 'landuse_31', 'landuse_32', 'landuse_33', 'landuse_41', 'landuse_42',
                'landuse_43', 'landuse_46',
                'landuse_51', 'landuse_52', 'landuse_53', 'landuse_62', 'landuse_63', 'landuse_64']
    y_column = ['swe']
    spatial_column = ['longitude', 'latitude']

    # ç§»é™¤æ ‡å‡†å·®ä¸ºé›¶çš„ç‰¹å¾
    safe_x_columns = []
    for col in x_column:
        if col in data.columns and data[col].std() > 0:
            safe_x_columns.append(col)
        else:
            print(f"è·³è¿‡ç‰¹å¾ {col}")

    print(f"ä½¿ç”¨ {len(safe_x_columns)} ä¸ªæœ‰æ•ˆç‰¹å¾")

    # è¯†åˆ«å”¯ä¸€ç«™ç‚¹ï¼ˆåŸºäºç»çº¬åº¦ï¼‰
    print("\nè¯†åˆ«ç«™ç‚¹ä¸­...")
    unique_stations = data[spatial_column].drop_duplicates()
    print(f"è¯†åˆ«åˆ° {len(unique_stations)} ä¸ªå”¯ä¸€ç«™ç‚¹")

    # ä¸ºæ¯ä¸ªç«™ç‚¹åˆ†é…ID
    station_ids = {}
    for idx, (_, row) in enumerate(unique_stations.iterrows()):
        station_ids[(row[spatial_column[0]], row[spatial_column[1]])] = idx

    # ä¸ºæ•°æ®æ·»åŠ ç«™ç‚¹ID
    data_with_station = data.copy()
    data_with_station['station_id'] = data_with_station.apply(
        lambda row: station_ids.get((row[spatial_column[0]], row[spatial_column[1]]), -1), axis=1
    )

    # æ£€æŸ¥ç«™ç‚¹æ•°æ®åˆ†å¸ƒ
    station_counts = data_with_station['station_id'].value_counts()
    print(f"\nç«™ç‚¹æ•°æ®åˆ†å¸ƒç»Ÿè®¡:")
    print(f"å¹³å‡æ¯ä¸ªç«™ç‚¹çš„æ ·æœ¬æ•°: {station_counts.mean():.2f}")
    print(f"æœ€å°‘æ ·æœ¬çš„ç«™ç‚¹: {station_counts.min()}")
    print(f"æœ€å¤šæ ·æœ¬çš„ç«™ç‚¹: {station_counts.max()}")
    print(f"æ ·æœ¬æ•°å°‘äº10çš„ç«™ç‚¹æ•°: {len(station_counts[station_counts < 10])}")

    # 10æŠ˜äº¤å‰éªŒè¯ - åŸºäºç«™ç‚¹åˆ’åˆ†
    kf = KFold(n_splits=10, shuffle=True, random_state=42)
    unique_station_ids = sorted(data_with_station['station_id'].unique())

    print(f"\nå¼€å§‹10æŠ˜äº¤å‰éªŒè¯ï¼Œå…± {len(unique_station_ids)} ä¸ªç«™ç‚¹...")

    fold_results = []
    detailed_predictions = []

    for fold, (train_val_station_idx, test_station_idx) in enumerate(kf.split(unique_station_ids)):
        print(f"\n{'=' * 60}")
        print(f"ç¬¬ {fold + 1}/10 æŠ˜")
        print(f"{'=' * 60}")

        # è·å–å½“å‰æŠ˜çš„ç«™ç‚¹ID
        train_val_stations = [unique_station_ids[i] for i in train_val_station_idx]
        test_stations = [unique_station_ids[i] for i in test_station_idx]

        # ä»è®­ç»ƒéªŒè¯é›†ä¸­åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ç«™ç‚¹
        train_stations, val_stations = train_test_split(
            train_val_stations, test_size=0.2, random_state=42
        )

        # æ ¹æ®ç«™ç‚¹IDè·å–å¯¹åº”çš„æ•°æ®
        train_data = data_with_station[data_with_station['station_id'].isin(train_stations)].drop('station_id', axis=1)
        val_data = data_with_station[data_with_station['station_id'].isin(val_stations)].drop('station_id', axis=1)
        test_data = data_with_station[data_with_station['station_id'].isin(test_stations)].drop('station_id', axis=1)

        print(f"è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬, {len(train_stations)} ç«™ç‚¹")
        print(f"éªŒè¯é›†: {len(val_data)} æ ·æœ¬, {len(val_stations)} ç«™ç‚¹")
        print(f"æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬, {len(test_stations)} ç«™ç‚¹")

        # æ£€æŸ¥æ•°æ®å¹³è¡¡æ€§
        if len(train_data) == 0 or len(val_data) == 0 or len(test_data) == 0:
            print("âš ï¸ è­¦å‘Š: æŸä¸ªé›†åˆä¸ºç©ºï¼Œè·³è¿‡è¯¥æŠ˜")
            continue

        try:
            # åˆ›å»ºæ•°æ®é›†ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
            print("åˆ›å»ºæ•°æ®é›†ä¸­...")
            train_set, val_set, test_set = create_memory_efficient_dataset(
                train_data=train_data,
                val_data=val_data,
                test_data=test_data,
                safe_x_columns=safe_x_columns,
                y_column=y_column,
                spatial_column=spatial_column
            )

            # åˆå§‹åŒ–æ¨¡å‹
            print("åˆå§‹åŒ–æ¨¡å‹ä¸­...")
            gnnwr = models.GNNWR(
                train_dataset=train_set,
                valid_dataset=val_set,
                test_dataset=test_set,
                dense_layers=[256, 128, 64],
                activate_func=nn.ReLU(),
                start_lr=0.001,
                optimizer="Adam",
                model_name=f"GNNWR_Station_Fold_{fold + 1}",
                model_save_path=f"result/station_kfold/fold_{fold + 1}",
                log_path=f"result/station_kfold/logs_fold_{fold + 1}",
                write_path=f"result/station_kfold/runs_fold_{fold + 1}",
                optimizer_params={}
            )

            # è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦å†…å­˜ç®¡ç†ï¼‰
            print("å¼€å§‹è®­ç»ƒ...")
            # gnnwr.add_graph()

            # ä½¿ç”¨å®‰å…¨çš„GPUè®­ç»ƒ
            training_success = safe_gnnwr_training(
                gnnwr,
                max_epoch=300,
                early_stop=50,
                print_frequency=50
            )

            if training_success:
                # è¯„ä¼°æ¨¡å‹
                model_path = f'result/station_kfold/fold_{fold + 1}/GNNWR_Station_Fold_{fold + 1}.pkl'
                if os.path.exists(model_path):
                    print("åŠ è½½æ¨¡å‹è¿›è¡Œè¯„ä¼°...")
                    gnnwr.load_model(model_path)
                    results = gnnwr.result(return_metrics=True)

                    # è·å–è¯¦ç»†é¢„æµ‹ç»“æœ
                    predictions = gnnwr.predict(return_result=True)
                    if predictions is not None:
                        test_with_pred = test_data.copy()
                        if hasattr(predictions, 'shape') and len(predictions) == len(test_data):
                            test_with_pred['predicted_swe'] = predictions
                            test_with_pred['fold'] = fold + 1
                            detailed_predictions.append(test_with_pred)

                    fold_result = {
                        'fold': fold + 1,
                        'train_stations': len(train_stations),
                        'val_stations': len(val_stations),
                        'test_stations': len(test_stations),
                        'train_samples': len(train_data),
                        'val_samples': len(val_data),
                        'test_samples': len(test_data),
                        'metrics': results
                    }
                    fold_results.append(fold_result)

                    print(f"âœ… ç¬¬ {fold + 1} æŠ˜å®Œæˆ")
                    print(f"   æµ‹è¯•é›†æŒ‡æ ‡: {results}")

                else:
                    print(f"âŒ ç¬¬ {fold + 1} æŠ˜æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°")
            else:
                print(f"âŒ ç¬¬ {fold + 1} æŠ˜è®­ç»ƒå¤±è´¥")

        except Exception as e:
            print(f"âŒ ç¬¬ {fold + 1} æŠ˜è®­ç»ƒå¤±è´¥: {e}")
            traceback.print_exc()
            continue

        finally:
            # æ¯ä¸ªæŠ˜ç»“æŸåæ¸…ç†å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()

    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 80)
    print("åŸºäºç«™ç‚¹çš„10æŠ˜äº¤å‰éªŒè¯æ±‡æ€»ç»“æœ")
    print("=" * 80)

    if fold_results:
        # è®¡ç®—å„æŒ‡æ ‡çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
        metrics_summary = {}
        for result in fold_results:
            print(f"\nç¬¬ {result['fold']} æŠ˜:")
            print(
                f"  ç«™ç‚¹æ•° - è®­ç»ƒ: {result['train_stations']}, éªŒè¯: {result['val_stations']}, æµ‹è¯•: {result['test_stations']}")
            print(
                f"  æ ·æœ¬æ•° - è®­ç»ƒ: {result['train_samples']}, éªŒè¯: {result['val_samples']}, æµ‹è¯•: {result['test_samples']}")
            print(f"  æŒ‡æ ‡: {result['metrics']}")

            for metric, value in result['metrics'].items():
                if metric not in metrics_summary:
                    metrics_summary[metric] = []
                metrics_summary[metric].append(value)

        print(f"\n{'=' * 50}")
        print("å¹³å‡ç»“æœ (Â± æ ‡å‡†å·®):")
        print(f"{'=' * 50}")
        for metric, values in metrics_summary.items():
            mean_val = np.mean(values)
            std_val = np.std(values)
            print(f"{metric.upper():<8}: {mean_val:.4f} Â± {std_val:.4f}")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        os.makedirs('result/station_kfold', exist_ok=True)

        # ä¿å­˜æŒ‡æ ‡ç»“æœ
        summary_df = pd.DataFrame(fold_results)

        # å±•å¼€metricsåˆ—
        metrics_expanded = pd.json_normalize(summary_df['metrics'])
        summary_expanded = pd.concat([summary_df.drop('metrics', axis=1), metrics_expanded], axis=1)
        summary_expanded.to_csv('result/station_kfold/cross_validation_summary.csv', index=False)

        # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
        if detailed_predictions:
            all_predictions = pd.concat(detailed_predictions, ignore_index=True)
            all_predictions.to_csv('result/station_kfold/detailed_predictions.csv', index=False)
            print(f"\nè¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: result/station_kfold/detailed_predictions.csv")

        print(f"\næ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: result/station_kfold/cross_validation_summary.csv")
        print(f"æˆåŠŸå®Œæˆçš„æŠ˜æ•°: {len(fold_results)}/10")

        return summary_expanded, all_predictions if detailed_predictions else None

    else:
        print("æ‰€æœ‰æŠ˜çš„è®­ç»ƒéƒ½å¤±è´¥äº†")
        return None, None


def create_memory_efficient_dataset(train_data, val_data, test_data, safe_x_columns, y_column, spatial_column):
    """åˆ›å»ºå†…å­˜é«˜æ•ˆçš„æ•°æ®é›†"""
    try:
        # ä½¿ç”¨è¾ƒå°çš„æ‰¹é‡å¤§å°
        batch_size = 16

        train_set, val_set, test_set = datasets.init_dataset_split(
            train_data=train_data,
            val_data=val_data,
            test_data=test_data,
            x_column=safe_x_columns,
            y_column=y_column,
            spatial_column=spatial_column,
            batch_size=batch_size,  # å‡å°‘æ‰¹é‡å¤§å°
            use_model="gnnwr"
        )

        print(f"ğŸ“Š æ•°æ®é›†åˆ›å»ºå®Œæˆ - æ‰¹é‡å¤§å°: {batch_size}")
        return train_set, val_set, test_set

    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        raise

class GPUMemoryManager:
    """GPUå†…å­˜ç®¡ç†å™¨"""

    def __init__(self, safety_margin_gb=1.0):
        self.safety_margin_gb = safety_margin_gb

    def get_available_memory(self):
        """è·å–å¯ç”¨GPUå†…å­˜"""
        if not torch.cuda.is_available():
            return 0
        allocated = torch.cuda.memory_allocated() / 1024 ** 3
        total = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
        return total - allocated - self.safety_margin_gb

    def can_allocate(self, estimated_size_gb):
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆ†é…æŒ‡å®šå¤§å°çš„å†…å­˜"""
        return self.get_available_memory() >= estimated_size_gb

    def optimize_memory(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()

    def handle_oom_error(self, gnnwr_instance, current_batch_size):
        """å¤„ç†OOMé”™è¯¯"""
        print("ğŸ”„ å¤„ç†OOMé”™è¯¯...")
        self.optimize_memory()

        # å‡å°‘æ‰¹é‡å¤§å°
        new_batch_size = max(1, current_batch_size // 2)
        if hasattr(gnnwr_instance, 'batch_size'):
            gnnwr_instance.batch_size = new_batch_size
            print(f"ğŸ”½ æ‰¹é‡å¤§å°è°ƒæ•´ä¸º: {new_batch_size}")

        # ç­‰å¾…å†…å­˜ç¨³å®š
        import time
        time.sleep(2)

        return new_batch_size

def safe_gnnwr_training(gnnwr_instance, max_epoch=300, early_stop=50, print_frequency=50):
    """å®‰å…¨çš„GNNWRè®­ç»ƒï¼Œå¸¦å†…å­˜ç®¡ç†"""
    memory_manager = GPUMemoryManager(safety_margin_gb=1.0)

    # åˆå§‹å†…å­˜ä¼˜åŒ–
    memory_manager.optimize_memory()

    # è®¾ç½®è¾ƒå°çš„æ‰¹é‡å¤§å°
    current_batch_size = getattr(gnnwr_instance, 'batch_size', 64)
    if current_batch_size > 16:
        gnnwr_instance.batch_size = 16
        print(f"ğŸ“Š åˆå§‹æ‰¹é‡å¤§å°è®¾ç½®ä¸º: 16")

    max_retries = 3
    retry_count = 0

    while retry_count < max_retries:
        try:
            # è®­ç»ƒå‰æ£€æŸ¥å†…å­˜
            if not memory_manager.can_allocate(2.0):  # é¢„ä¼°éœ€è¦2GB
                print("âš ï¸ å†…å­˜ç´§å¼ ï¼Œæ¸…ç†ç¼“å­˜...")
                memory_manager.optimize_memory()

            # è®­ç»ƒæ¨¡å‹
            gnnwr_instance.run(max_epoch=max_epoch, early_stop=early_stop, print_frequency=print_frequency)
            return True

        except RuntimeError as e:
            error_msg = str(e)
            if "out of memory" in error_msg.lower():
                retry_count += 1
                print(f"ğŸ’¥ OOMé”™è¯¯ (å°è¯• {retry_count}/{max_retries})")

                if retry_count >= max_retries:
                    print("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œåˆ‡æ¢åˆ°CPUæ¨¡å¼")
                    # return force_cpu_training(gnnwr_instance, max_epoch, early_stop, print_frequency)

                # å¤„ç†OOMé”™è¯¯
                current_batch_size = memory_manager.handle_oom_error(gnnwr_instance, current_batch_size)

            else:
                print(f"âŒ è®­ç»ƒé”™è¯¯: {e}")
                return False

    return False

def analyze_station_distribution():
    """åˆ†æç«™ç‚¹æ•°æ®åˆ†å¸ƒ"""
    print("=== ç«™ç‚¹æ•°æ®åˆ†å¸ƒåˆ†æ ===")

    data = pd.read_excel('lu_onehot.xlsx')
    spatial_column = ['longitude', 'latitude']

    # è¯†åˆ«ç«™ç‚¹
    unique_stations = data[spatial_column].drop_duplicates()
    print(f"æ€»ç«™ç‚¹æ•°: {len(unique_stations)}")

    # ä¸ºæ¯ä¸ªç«™ç‚¹åˆ†é…IDå¹¶ç»Ÿè®¡æ ·æœ¬æ•°
    station_stats = []
    for idx, (_, row) in enumerate(unique_stations.iterrows()):
        lon, lat = row[spatial_column[0]], row[spatial_column[1]]
        station_data = data[(data[spatial_column[0]] == lon) & (data[spatial_column[1]] == lat)]
        station_stats.append({
            'station_id': idx,
            'longitude': lon,
            'latitude': lat,
            'sample_count': len(station_data),
            'mean_swe': station_data['swe'].mean() if 'swe' in station_data.columns else 0
        })

    station_df = pd.DataFrame(station_stats)

    print(f"\nç«™ç‚¹æ•°æ®åˆ†å¸ƒ:")
    print(f"å¹³å‡æ¯ä¸ªç«™ç‚¹æ ·æœ¬æ•°: {station_df['sample_count'].mean():.2f}")
    print(f"æ ·æœ¬æ•°ç»Ÿè®¡:")
    print(station_df['sample_count'].describe())

    # ä¿å­˜ç«™ç‚¹åˆ†å¸ƒä¿¡æ¯
    os.makedirs('result', exist_ok=True)
    station_df.to_csv('result/station_distribution.csv', index=False)
    print(f"\nç«™ç‚¹åˆ†å¸ƒä¿¡æ¯å·²ä¿å­˜åˆ°: result/station_distribution.csv")

    return station_df


if __name__ == "__main__":
    # é¦–å…ˆåˆ†æç«™ç‚¹åˆ†å¸ƒ
    device = setup_device(0)

    station_info = analyze_station_distribution()

    # æ‰§è¡ŒåŸºäºç«™ç‚¹çš„10æŠ˜äº¤å‰éªŒè¯
    print("\nå¼€å§‹åŸºäºç«™ç‚¹çš„10æŠ˜äº¤å‰éªŒè¯...")
    results, predictions = station_based_kfold_cross_validation()

    if results is not None:
        print("\nğŸ‰ åŸºäºç«™ç‚¹çš„10æŠ˜äº¤å‰éªŒè¯å®Œæˆï¼")
        print("ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: result/station_kfold/")
    else:
        print("\nâŒ äº¤å‰éªŒè¯å¤±è´¥")