import os
import sys
from gnnwr import models, datasets, utils
import pandas as pd
import torch.nn as nn
import numpy as np
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import warnings

warnings.filterwarnings('ignore')


def station_based_kfold_cross_validation():
    """åŸºäºç«™ç‚¹çš„10æŠ˜äº¤å‰éªŒè¯"""
    print("=== åŸºäºç«™ç‚¹çš„10æŠ˜äº¤å‰éªŒè¯ ===")

    # è¯»å–æ•°æ®
    data = pd.read_excel('lu_onehot.xlsx')
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {data.shape}")

    # å®šä¹‰ç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—
    x_column = ['aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation', 'std_slope',
                'std_eastness', 'std_tpi', 'std_curvature1',
                'std_curvature2', 'std_high', 'std_aspect', 'glsnow', 'cswe', 'snow_depth_snow_depth',
                'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'doy', 'gldas',
                'year', 'month', 'scp_start', 'scp_end', 'd1', 'd2', 'X', 'Y', 'Z', 'da', 'db', 'dc', 'dd',
                'landuse_11', 'landuse_12', 'landuse_21', 'landuse_22',
                'landuse_23', 'landuse_24', 'landuse_31', 'landuse_32', 'landuse_33', 'landuse_41', 'landuse_42',
                'landuse_43', 'landuse_46',
                'landuse_51', 'landuse_52', 'landuse_53', 'landuse_62', 'landuse_63', 'landuse_64']
    y_column = ['swe']
    spatial_column = ['longitude', 'latitude']

    # ç§»é™¤æ ‡å‡†å·®ä¸ºé›¶çš„ç‰¹å¾
    safe_x_columns = []
    for col in x_column:
        if col in data.columns and data[col].std() > 0:
            safe_x_columns.append(col)
        else:
            print(f"è·³è¿‡ç‰¹å¾ {col}")

    print(f"ä½¿ç”¨ {len(safe_x_columns)} ä¸ªæœ‰æ•ˆç‰¹å¾")

    # è¯†åˆ«å”¯ä¸€ç«™ç‚¹ï¼ˆåŸºäºç»çº¬åº¦ï¼‰
    print("\nè¯†åˆ«ç«™ç‚¹ä¸­...")
    unique_stations = data[spatial_column].drop_duplicates()
    print(f"è¯†åˆ«åˆ° {len(unique_stations)} ä¸ªå”¯ä¸€ç«™ç‚¹")

    # ä¸ºæ¯ä¸ªç«™ç‚¹åˆ†é…ID
    station_ids = {}
    for idx, (_, row) in enumerate(unique_stations.iterrows()):
        station_ids[(row[spatial_column[0]], row[spatial_column[1]])] = idx

    # ä¸ºæ•°æ®æ·»åŠ ç«™ç‚¹ID
    data_with_station = data.copy()
    data_with_station['station_id'] = data_with_station.apply(
        lambda row: station_ids.get((row[spatial_column[0]], row[spatial_column[1]]), -1), axis=1
    )

    # æ£€æŸ¥ç«™ç‚¹æ•°æ®åˆ†å¸ƒ
    station_counts = data_with_station['station_id'].value_counts()
    print(f"\nç«™ç‚¹æ•°æ®åˆ†å¸ƒç»Ÿè®¡:")
    print(f"å¹³å‡æ¯ä¸ªç«™ç‚¹çš„æ ·æœ¬æ•°: {station_counts.mean():.2f}")
    print(f"æœ€å°‘æ ·æœ¬çš„ç«™ç‚¹: {station_counts.min()}")
    print(f"æœ€å¤šæ ·æœ¬çš„ç«™ç‚¹: {station_counts.max()}")
    print(f"æ ·æœ¬æ•°å°‘äº10çš„ç«™ç‚¹æ•°: {len(station_counts[station_counts < 10])}")

    # 10æŠ˜äº¤å‰éªŒè¯ - åŸºäºç«™ç‚¹åˆ’åˆ†
    kf = KFold(n_splits=10, shuffle=True, random_state=42)
    unique_station_ids = sorted(data_with_station['station_id'].unique())

    print(f"\nå¼€å§‹10æŠ˜äº¤å‰éªŒè¯ï¼Œå…± {len(unique_station_ids)} ä¸ªç«™ç‚¹...")

    fold_results = []
    detailed_predictions = []

    for fold, (train_val_station_idx, test_station_idx) in enumerate(kf.split(unique_station_ids)):
        print(f"\n{'=' * 60}")
        print(f"ç¬¬ {fold + 1}/10 æŠ˜")
        print(f"{'=' * 60}")

        # è·å–å½“å‰æŠ˜çš„ç«™ç‚¹ID
        train_val_stations = [unique_station_ids[i] for i in train_val_station_idx]
        test_stations = [unique_station_ids[i] for i in test_station_idx]

        # ä»è®­ç»ƒéªŒè¯é›†ä¸­åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ç«™ç‚¹
        train_stations, val_stations = train_test_split(
            train_val_stations, test_size=0.2, random_state=42
        )

        # æ ¹æ®ç«™ç‚¹IDè·å–å¯¹åº”çš„æ•°æ®
        train_data = data_with_station[data_with_station['station_id'].isin(train_stations)].drop('station_id', axis=1)
        val_data = data_with_station[data_with_station['station_id'].isin(val_stations)].drop('station_id', axis=1)
        test_data = data_with_station[data_with_station['station_id'].isin(test_stations)].drop('station_id', axis=1)

        print(f"è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬, {len(train_stations)} ç«™ç‚¹")
        print(f"éªŒè¯é›†: {len(val_data)} æ ·æœ¬, {len(val_stations)} ç«™ç‚¹")
        print(f"æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬, {len(test_stations)} ç«™ç‚¹")

        # æ£€æŸ¥æ•°æ®å¹³è¡¡æ€§
        if len(train_data) == 0 or len(val_data) == 0 or len(test_data) == 0:
            print("âš ï¸ è­¦å‘Š: æŸä¸ªé›†åˆä¸ºç©ºï¼Œè·³è¿‡è¯¥æŠ˜")
            continue

        try:
            # åˆ›å»ºæ•°æ®é›†
            print("åˆ›å»ºæ•°æ®é›†ä¸­...")
            train_set, val_set, test_set = datasets.init_dataset_split(
                train_data=train_data,
                val_data=val_data,
                test_data=test_data,
                x_column=safe_x_columns,
                y_column=y_column,
                spatial_column=spatial_column,
                batch_size=256,
                use_model="gnnwr"
            )

            # åˆå§‹åŒ–æ¨¡å‹
            print("åˆå§‹åŒ–æ¨¡å‹ä¸­...")
            gnnwr = models.GNNWR(
                train_dataset=train_set,
                valid_dataset=val_set,
                test_dataset=test_set,
                dense_layers=[256, 128, 64],
                activate_func=nn.ReLU(),
                start_lr=0.001,
                optimizer="Adam",
                model_name=f"GNNWR_Station_Fold_{fold + 1}",
                model_save_path=f"result/station_kfold/fold_{fold + 1}",
                log_path=f"result/station_kfold/logs_fold_{fold + 1}",
                write_path=f"result/station_kfold/runs_fold_{fold + 1}",
                optimizer_params={}
            )

            # è®­ç»ƒæ¨¡å‹
            print("å¼€å§‹è®­ç»ƒ...")
            gnnwr.add_graph()
            gnnwr.run(max_epoch=300, early_stop=50, print_frequency=50)

            # è¯„ä¼°æ¨¡å‹
            model_path = f'result/station_kfold/fold_{fold + 1}/GNNWR_Station_Fold_{fold + 1}.pkl'
            if os.path.exists(model_path):
                print("åŠ è½½æ¨¡å‹è¿›è¡Œè¯„ä¼°...")
                gnnwr.load_model(model_path)
                results = gnnwr.result(return_metrics=True)

                # è·å–è¯¦ç»†é¢„æµ‹ç»“æœ
                predictions = gnnwr.predict(return_result=True)
                if predictions is not None:
                    test_with_pred = test_data.copy()
                    if hasattr(predictions, 'shape') and len(predictions) == len(test_data):
                        test_with_pred['predicted_swe'] = predictions
                        test_with_pred['fold'] = fold + 1
                        detailed_predictions.append(test_with_pred)

                fold_result = {
                    'fold': fold + 1,
                    'train_stations': len(train_stations),
                    'val_stations': len(val_stations),
                    'test_stations': len(test_stations),
                    'train_samples': len(train_data),
                    'val_samples': len(val_data),
                    'test_samples': len(test_data),
                    'metrics': results
                }
                fold_results.append(fold_result)

                print(f"âœ… ç¬¬ {fold + 1} æŠ˜å®Œæˆ")
                print(f"   æµ‹è¯•é›†æŒ‡æ ‡: {results}")

            else:
                print(f"âŒ ç¬¬ {fold + 1} æŠ˜æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°")

        except Exception as e:
            print(f"âŒ ç¬¬ {fold + 1} æŠ˜è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue

    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 80)
    print("åŸºäºç«™ç‚¹çš„10æŠ˜äº¤å‰éªŒè¯æ±‡æ€»ç»“æœ")
    print("=" * 80)

    if fold_results:
        # è®¡ç®—å„æŒ‡æ ‡çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
        metrics_summary = {}
        for result in fold_results:
            print(f"\nç¬¬ {result['fold']} æŠ˜:")
            print(
                f"  ç«™ç‚¹æ•° - è®­ç»ƒ: {result['train_stations']}, éªŒè¯: {result['val_stations']}, æµ‹è¯•: {result['test_stations']}")
            print(
                f"  æ ·æœ¬æ•° - è®­ç»ƒ: {result['train_samples']}, éªŒè¯: {result['val_samples']}, æµ‹è¯•: {result['test_samples']}")
            print(f"  æŒ‡æ ‡: {result['metrics']}")

            for metric, value in result['metrics'].items():
                if metric not in metrics_summary:
                    metrics_summary[metric] = []
                metrics_summary[metric].append(value)

        print(f"\n{'=' * 50}")
        print("å¹³å‡ç»“æœ (Â± æ ‡å‡†å·®):")
        print(f"{'=' * 50}")
        for metric, values in metrics_summary.items():
            mean_val = np.mean(values)
            std_val = np.std(values)
            print(f"{metric.upper():<8}: {mean_val:.4f} Â± {std_val:.4f}")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        os.makedirs('result/station_kfold', exist_ok=True)

        # ä¿å­˜æŒ‡æ ‡ç»“æœ
        summary_df = pd.DataFrame(fold_results)

        # å±•å¼€metricsåˆ—
        metrics_expanded = pd.json_normalize(summary_df['metrics'])
        summary_expanded = pd.concat([summary_df.drop('metrics', axis=1), metrics_expanded], axis=1)
        summary_expanded.to_csv('result/station_kfold/cross_validation_summary.csv', index=False)

        # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
        if detailed_predictions:
            all_predictions = pd.concat(detailed_predictions, ignore_index=True)
            all_predictions.to_csv('result/station_kfold/detailed_predictions.csv', index=False)
            print(f"\nè¯¦ç»†é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: result/station_kfold/detailed_predictions.csv")

        print(f"\næ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: result/station_kfold/cross_validation_summary.csv")
        print(f"æˆåŠŸå®Œæˆçš„æŠ˜æ•°: {len(fold_results)}/10")

        return summary_expanded, all_predictions if detailed_predictions else None

    else:
        print("æ‰€æœ‰æŠ˜çš„è®­ç»ƒéƒ½å¤±è´¥äº†")
        return None, None


def analyze_station_distribution():
    """åˆ†æç«™ç‚¹æ•°æ®åˆ†å¸ƒ"""
    print("=== ç«™ç‚¹æ•°æ®åˆ†å¸ƒåˆ†æ ===")

    data = pd.read_excel('lu_onehot.xlsx')
    spatial_column = ['longitude', 'latitude']

    # è¯†åˆ«ç«™ç‚¹
    unique_stations = data[spatial_column].drop_duplicates()
    print(f"æ€»ç«™ç‚¹æ•°: {len(unique_stations)}")

    # ä¸ºæ¯ä¸ªç«™ç‚¹åˆ†é…IDå¹¶ç»Ÿè®¡æ ·æœ¬æ•°
    station_stats = []
    for idx, (_, row) in enumerate(unique_stations.iterrows()):
        lon, lat = row[spatial_column[0]], row[spatial_column[1]]
        station_data = data[(data[spatial_column[0]] == lon) & (data[spatial_column[1]] == lat)]
        station_stats.append({
            'station_id': idx,
            'longitude': lon,
            'latitude': lat,
            'sample_count': len(station_data),
            'mean_swe': station_data['swe'].mean() if 'swe' in station_data.columns else 0
        })

    station_df = pd.DataFrame(station_stats)

    print(f"\nç«™ç‚¹æ•°æ®åˆ†å¸ƒ:")
    print(f"å¹³å‡æ¯ä¸ªç«™ç‚¹æ ·æœ¬æ•°: {station_df['sample_count'].mean():.2f}")
    print(f"æ ·æœ¬æ•°ç»Ÿè®¡:")
    print(station_df['sample_count'].describe())

    # ä¿å­˜ç«™ç‚¹åˆ†å¸ƒä¿¡æ¯
    os.makedirs('result', exist_ok=True)
    station_df.to_csv('result/station_distribution.csv', index=False)
    print(f"\nç«™ç‚¹åˆ†å¸ƒä¿¡æ¯å·²ä¿å­˜åˆ°: result/station_distribution.csv")

    return station_df


if __name__ == "__main__":
    # é¦–å…ˆåˆ†æç«™ç‚¹åˆ†å¸ƒ
    station_info = analyze_station_distribution()

    # æ‰§è¡ŒåŸºäºç«™ç‚¹çš„10æŠ˜äº¤å‰éªŒè¯
    print("\nå¼€å§‹åŸºäºç«™ç‚¹çš„10æŠ˜äº¤å‰éªŒè¯...")
    results, predictions = station_based_kfold_cross_validation()

    if results is not None:
        print("\nğŸ‰ åŸºäºç«™ç‚¹çš„10æŠ˜äº¤å‰éªŒè¯å®Œæˆï¼")
        print("ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: result/station_kfold/")
    else:
        print("\nâŒ äº¤å‰éªŒè¯å¤±è´¥")