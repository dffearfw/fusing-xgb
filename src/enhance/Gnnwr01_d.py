# å¯¹cç‰ˆçš„ç²¾ç®€
import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from gnnwr import models, datasets
import warnings

warnings.filterwarnings('ignore')

# ==================== 1. æ•°æ®å‡†å¤‡ ====================
print("1. æ•°æ®å‡†å¤‡ä¸Žæ¨¡åž‹è®­ç»ƒ...")

# åŠ è½½æ•°æ®
data = pd.read_excel('aggregated_station_data.xlsx')
data = data.sample(frac=1, random_state=42)
indices = data.index.tolist()
train_idx = indices[:int(0.7 * len(data))]
val_idx = indices[int(0.7 * len(data)):int(0.8 * len(data))]
test_idx = indices[int(0.8 * len(data)):]

train_data = data.loc[train_idx]
val_data = data.loc[val_idx]
test_data = data.loc[test_idx]

# å®šä¹‰åˆ—
x_column = ['aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation', 'std_slope',
            'std_eastness', 'std_tpi', 'std_curvature1', 'std_curvature2', 'std_high', 'std_aspect',
            'glsnow', 'cswe', 'snow_depth_snow_depth', 'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'doy', 'gldas',
            'year', 'month', 'scp_start', 'scp_end', 'd1', 'd2', 'X', 'Y', 'Z', 'da', 'db', 'dc', 'dd']
y_column = ['swe']
spatial_column = ['longitude', 'latitude']

# åˆå§‹åŒ–æ•°æ®é›†
train_set, val_set, test_set = datasets.init_dataset_split(
    train_data=train_data,
    val_data=val_data,
    test_data=test_data,
    x_column=x_column,
    y_column=y_column,
    spatial_column=spatial_column,
    batch_size=128,
    shuffle=False,
    use_model="gnnwr"
)

# ==================== 2. è®­ç»ƒGNNWRæ¨¡åž‹ ====================
gnnwr = models.GNNWR(
    train_dataset=train_set,
    valid_dataset=val_set,
    test_dataset=test_set,
    dense_layers=[1024, 512, 256],
    activate_func=nn.PReLU(init=0.4),
    start_lr=0.1,
    optimizer="Adadelta",
    model_name="GNNWR_Test",
    model_save_path="result/gnnwr_models",
    log_path="result/gnnwr_logs",
    write_path="result/gnnwr_runs"
)

gnnwr.run(max_epoch=5, early_stop=1000, print_frequency=100)


# ==================== 3. æå–æƒé‡çŸ©é˜µ ====================
def extract_weights(gnnwr_instance, dataset):
    """æå–æƒé‡çŸ©é˜µ"""
    model = gnnwr_instance._model
    model.eval()
    device = gnnwr_instance._device

    all_weights = []

    with torch.no_grad():
        for batch in dataset.dataloader:
            if len(batch) >= 2:
                distances, features = batch[:2]
                distances = distances.to(device)
                weights = model(distances)
                all_weights.append(weights.cpu().numpy())

    if all_weights:
        return np.concatenate(all_weights, axis=0)
    return None


# æå–ä¸‰ä¸ªæ•°æ®é›†çš„æƒé‡
train_weights = extract_weights(gnnwr, train_set)
val_weights = extract_weights(gnnwr, val_set)
test_weights = extract_weights(gnnwr, test_set)

print(f"è®­ç»ƒé›†æƒé‡å½¢çŠ¶: {train_weights.shape}")
print(f"éªŒè¯é›†æƒé‡å½¢çŠ¶: {val_weights.shape}")
print(f"æµ‹è¯•é›†æƒé‡å½¢çŠ¶: {test_weights.shape}")

# ==================== 4. æƒé‡éªŒè¯ ====================
print("\n2. æƒé‡éªŒè¯...")


def verify_weight_formula(gnnwr_instance, dataset, n_samples=3):
    """éªŒè¯æƒé‡å…¬å¼çš„æ­£ç¡®æ€§"""
    model = gnnwr_instance._model
    out_layer = gnnwr_instance._out
    model.eval()
    device = gnnwr_instance._device
    coeff = np.array(gnnwr_instance._coefficient).flatten()

    with torch.no_grad():
        for batch_idx, batch in enumerate(dataset.dataloader):
            if len(batch) >= 3:
                distances, features, labels = batch[:3]
                distances = distances.to(device)
                features = features.to(device).float()

                # æ¨¡åž‹é¢„æµ‹
                weights = model(distances)
                model_predictions = out_layer(weights.mul(features))

                # æ‰‹åŠ¨è®¡ç®—
                coeff_tensor = torch.tensor(coeff, dtype=torch.float32, device=device)
                manual_predictions = torch.sum(weights * features * coeff_tensor, dim=1, keepdim=True)

                # æ¯”è¾ƒå·®å¼‚
                diff = torch.abs(model_predictions - manual_predictions)
                max_diff = diff.max().item()

                print(f"  æ‰¹æ¬¡{batch_idx}: æœ€å¤§å·®å¼‚={max_diff:.10f}")
                if max_diff < 1e-6:
                    print(f"  âœ… å…¬å¼éªŒè¯é€šè¿‡")
                else:
                    print(f"  âš ï¸ æœ‰å¾®å°å·®å¼‚ï¼ˆæµ®ç‚¹æ•°ç²¾åº¦ï¼‰")

                if batch_idx == 0:
                    break

    return True


# éªŒè¯å…¬å¼
verify_weight_formula(gnnwr, train_set)

# ==================== 5. æƒé‡åˆ†æž ====================
print("\n3. æƒé‡åˆ†æž...")


def analyze_weights(weights, dataset_name="æ•°æ®é›†"):
    """åˆ†æžæƒé‡çŸ©é˜µçš„ç»Ÿè®¡ç‰¹æ€§"""
    if weights is None:
        print(f"{dataset_name}: æ— æƒé‡æ•°æ®")
        return None

    print(f"\n{dataset_name}æƒé‡åˆ†æž:")
    print(f"  å½¢çŠ¶: {weights.shape}")
    print(f"  æ ·æœ¬æ•°: {weights.shape[0]}, ç‰¹å¾æ•°: {weights.shape[1]}")

    # åŸºæœ¬ç»Ÿè®¡
    weight_sums = weights.sum(axis=1)
    print(f"  æƒé‡å‡å€¼: {weights.mean():.6f}")
    print(f"  æƒé‡æ ‡å‡†å·®: {weights.std():.6f}")
    print(f"  æƒé‡å’Œå‡å€¼: {weight_sums.mean():.6f}")
    print(f"  æƒé‡å’Œæ ‡å‡†å·®: {weight_sums.std():.6f}")
    print(f"  è´Ÿæƒé‡æ¯”ä¾‹: {np.sum(weights < 0) / weights.size:.2%}")

    return weight_sums


# åˆ†æžå„æ•°æ®é›†æƒé‡
train_weight_sums = analyze_weights(train_weights, "è®­ç»ƒé›†")
val_weight_sums = analyze_weights(val_weights, "éªŒè¯é›†")
test_weight_sums = analyze_weights(test_weights, "æµ‹è¯•é›†")

# ==================== 6. ä¿å­˜æƒé‡çŸ©é˜µ ====================
print("\n4. ä¿å­˜æƒé‡çŸ©é˜µ...")

# åˆ›å»ºä¿å­˜ç›®å½•
os.makedirs("result/weights", exist_ok=True)

# ä¿å­˜ä¸ºnpyæ–‡ä»¶
np.save("result/weights/train_weights.npy", train_weights)
np.save("result/weights/val_weights.npy", val_weights)
np.save("result/weights/test_weights.npy", test_weights)

# ä¿å­˜ä¸ºCSVï¼ˆä¾¿äºŽæŸ¥çœ‹ï¼‰
if train_weights is not None:
    # ä¿å­˜æƒé‡å’Œ
    train_weight_sum_df = pd.DataFrame({
        'weight_sum': train_weight_sums
    })
    train_weight_sum_df.to_csv("result/weights/train_weight_sums.csv", index=False)

    # ä¿å­˜å®Œæ•´çš„æƒé‡çŸ©é˜µï¼ˆå‰100ä¸ªæ ·æœ¬ï¼‰
    weight_df = pd.DataFrame(train_weights[:100])
    weight_df.columns = [f'weight_{i}' for i in range(train_weights.shape[1])]
    weight_df.to_csv("result/weights/train_weights_sample.csv", index=False)

print("æƒé‡çŸ©é˜µå·²ä¿å­˜åˆ° result/weights/ ç›®å½•")
print(f"è®­ç»ƒé›†æƒé‡æ–‡ä»¶: {train_weights.shape}")
print(f"éªŒè¯é›†æƒé‡æ–‡ä»¶: {val_weights.shape}")
print(f"æµ‹è¯•é›†æƒé‡æ–‡ä»¶: {test_weights.shape}")

# ==================== 7. æ ¸å¿ƒç»“æžœæ±‡æ€» ====================
print("\n" + "=" * 50)
print("GNNWRæƒé‡æå–å®Œæˆï¼")
print("=" * 50)

print(f"""
âœ… æ ¸å¿ƒæˆæžœï¼š
1. æˆåŠŸæå–æƒé‡çŸ©é˜µ
   - è®­ç»ƒé›†: {train_weights.shape if train_weights is not None else 'N/A'}
   - éªŒè¯é›†: {val_weights.shape if val_weights is not None else 'N/A'}
   - æµ‹è¯•é›†: {test_weights.shape if test_weights is not None else 'N/A'}

2. å…¬å¼éªŒè¯é€šè¿‡
   - é¢„æµ‹å…¬å¼ y = Î£(W Ã— X Ã— Î²) æ­£ç¡®
   - å·®å¼‚ < 1e-6ï¼ˆæµ®ç‚¹æ•°ç²¾åº¦çº§åˆ«ï¼‰

3. æƒé‡ç‰¹æ€§
   - æ¯ä¸ªæ ·æœ¬æœ‰ {train_weights.shape[1] if train_weights is not None else 'N/A'} ä¸ªæƒé‡
   - å¯¹åº” {len(x_column)} ä¸ªç‰¹å¾ + åç½®é¡¹
   - è´Ÿæƒé‡æ¯”ä¾‹: {np.sum(train_weights < 0) / train_weights.size:.1%}ï¼ˆå…è®¸æŠ‘åˆ¶æ•ˆåº”ï¼‰

ðŸš€ ä¸‹ä¸€æ­¥ï¼šGNNW-XGBoostèžåˆ
1. å°†æƒé‡çŸ©é˜µä½œä¸ºæ–°ç‰¹å¾è¾“å…¥XGBoost
2. æˆ–ä½¿ç”¨åŠ æƒç‰¹å¾ï¼šX_weighted = X * W
3. æ¯”è¾ƒçº¯XGBoostä¸Žå¢žå¼ºç‰ˆæœ¬çš„æ€§èƒ½

ðŸ“ å·²ä¿å­˜æ–‡ä»¶ï¼š
- train_weights.npy: è®­ç»ƒé›†æƒé‡çŸ©é˜µ
- train_weight_sums.csv: æƒé‡å’Œç»Ÿè®¡
- train_weights_sample.csv: æƒé‡æ ·æœ¬ï¼ˆä¾¿äºŽæŸ¥çœ‹ï¼‰
""")