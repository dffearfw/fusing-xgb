import os
import sys
import pandas as pd
import numpy as np
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from gnnwr import models, datasets, utils
import time
import psutil
import gc
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns


def monitor_performance(step_name):
    """ç®€å•çš„æ€§èƒ½ç›‘æ§"""
    memory = psutil.virtual_memory()
    process = psutil.Process()
    memory_usage = process.memory_info().rss / 1024 / 1024  # MB
    print(f"[æ€§èƒ½ç›‘æ§] {step_name} - å†…å­˜ä½¿ç”¨: {memory_usage:.1f}MB, ç³»ç»Ÿå†…å­˜: {memory.percent}%")


def fix_timestamp_issues(data, x_columns, y_columns):
    """ä¿®å¤æ—¶é—´æˆ³æ•°æ®ç±»å‹é—®é¢˜"""
    print("ğŸ”§ ä¿®å¤æ—¶é—´æˆ³é—®é¢˜...")

    data_fixed = data.copy()
    updated_x_columns = x_columns.copy()

    for col in x_columns + y_columns:
        if col in data_fixed.columns:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¶é—´æˆ³ç±»å‹æˆ–å­—ç¬¦ä¸²ç±»å‹
            if data_fixed[col].dtype == 'object' or 'time' in str(col).lower() or 'date' in str(col).lower():
                try:
                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
                    data_fixed[col] = pd.to_numeric(data_fixed[col], errors='coerce')
                    print(f"   - å·²è½¬æ¢åˆ— {col} ä¸ºæ•°å€¼ç±»å‹")
                except:
                    # å¦‚æœæ˜¯æ—¶é—´å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºæ—¶é—´æˆ³å†æå–ç‰¹å¾
                    try:
                        timestamp_col = pd.to_datetime(data_fixed[col], errors='coerce')
                        if not timestamp_col.isna().all():
                            # æˆåŠŸè½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼Œæå–æ•°å€¼ç‰¹å¾
                            data_fixed[f'{col}_year'] = timestamp_col.dt.year
                            data_fixed[f'{col}_month'] = timestamp_col.dt.month
                            data_fixed[f'{col}_day'] = timestamp_col.dt.day
                            data_fixed[f'{col}_dayofweek'] = timestamp_col.dt.dayofweek
                            data_fixed[f'{col}_hour'] = timestamp_col.dt.hour

                            # æ›´æ–°ç‰¹å¾åˆ—
                            updated_x_columns.extend([f'{col}_year', f'{col}_month', f'{col}_day',
                                                      f'{col}_dayofweek', f'{col}_hour'])
                            print(f"   - å·²ä»æ—¶é—´åˆ— {col} æå–ç‰¹å¾")

                            # ç§»é™¤åŸå§‹åˆ—
                            if col in updated_x_columns:
                                updated_x_columns.remove(col)
                            data_fixed = data_fixed.drop(columns=[col])
                    except:
                        print(f"   âš ï¸ æ— æ³•å¤„ç†åˆ— {col}ï¼Œå°†åˆ é™¤")
                        if col in updated_x_columns:
                            updated_x_columns.remove(col)
                        data_fixed = data_fixed.drop(columns=[col])

    # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
    for col in updated_x_columns + y_columns:
        if col in data_fixed.columns:
            data_fixed[col] = pd.to_numeric(data_fixed[col], errors='coerce')

    print(f"   - ä¿®å¤åç‰¹å¾åˆ—æ•°: {len(updated_x_columns)}")
    return data_fixed, updated_x_columns


def create_10_fold_by_station(data, station_column='station_id', random_state=42):
    """å°†ç«™ç‚¹åˆ†æˆ10æŠ˜"""
    print("ğŸ¯ åˆ›å»º10æŠ˜äº¤å‰éªŒè¯ï¼ˆæŒ‰ç«™ç‚¹åˆ’åˆ†ï¼‰...")

    # è·å–æ‰€æœ‰å”¯ä¸€ç«™ç‚¹
    stations = data[station_column].unique()
    print(f"   - æ€»ç«™ç‚¹æ•°: {len(stations)}")

    # éšæœºæ‰“ä¹±ç«™ç‚¹
    np.random.seed(random_state)
    np.random.shuffle(stations)

    # åˆ†æˆ10æŠ˜
    n_folds = 10
    fold_size = len(stations) // n_folds
    station_folds = []

    for i in range(n_folds):
        if i < n_folds - 1:
            fold_stations = stations[i * fold_size:(i + 1) * fold_size]
        else:
            fold_stations = stations[i * fold_size:]  # æœ€åä¸€æŠ˜åŒ…å«å‰©ä½™ç«™ç‚¹
        station_folds.append(fold_stations)
        print(f"   - æŠ˜ {i + 1}: {len(fold_stations)} ä¸ªç«™ç‚¹")

    return station_folds


def run_10_fold_cross_validation(data, x_columns, y_columns, spatial_columns, station_column='station_id'):
    """è¿è¡Œ10æŠ˜äº¤å‰éªŒè¯"""
    print("ğŸš€ å¼€å§‹10æŠ˜äº¤å‰éªŒè¯...")

    # ä¿®å¤æ—¶é—´æˆ³é—®é¢˜
    data_fixed, x_columns_fixed = fix_timestamp_issues(data, x_columns, y_columns)
    print(f"   - ä¿®å¤åç‰¹å¾åˆ—: {x_columns_fixed}")
    print(f"   - ç›®æ ‡åˆ—: {y_columns}")

    # æ•°æ®æ¸…æ´—
    clean_data = robust_data_cleaning(data_fixed, x_columns_fixed, y_columns, spatial_columns, station_column)

    # åˆ›å»º10æŠ˜
    station_folds = create_10_fold_by_station(clean_data, station_column)

    all_true = []
    all_pred = []
    all_results = []
    fold_metrics_list = []

    total_start_time = time.time()

    for fold_idx, val_stations in enumerate(station_folds):
        print(f"\n=== æŠ˜ {fold_idx + 1}/10 ===")

        try:
            # è®­ç»ƒé›†ï¼šå…¶ä»–9æŠ˜çš„ç«™ç‚¹
            train_stations = []
            for i, stations in enumerate(station_folds):
                if i != fold_idx:
                    train_stations.extend(stations)

            # åˆ†å‰²æ•°æ®
            train_data = clean_data[clean_data[station_column].isin(train_stations)]
            val_data = clean_data[clean_data[station_column].isin(val_stations)]

            print(f"   - è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬, {len(train_stations)} ç«™ç‚¹")
            print(f"   - éªŒè¯é›†: {len(val_data)} æ ·æœ¬, {len(val_stations)} ç«™ç‚¹")

            if len(train_data) == 0 or len(val_data) == 0:
                print("âš ï¸ è®­ç»ƒé›†æˆ–éªŒè¯é›†ä¸ºç©ºï¼Œè·³è¿‡è¯¥æŠ˜")
                continue

            # æ•°æ®æ ‡å‡†åŒ–
            data_standardized = standardize_data(pd.concat([train_data, val_data]), x_columns_fixed, y_columns)
            train_data_std = data_standardized[data_standardized[station_column].isin(train_stations)]
            val_data_std = data_standardized[data_standardized[station_column].isin(val_stations)]

            # åˆ›å»ºæ•°æ®é›†
            train_set, val_set = safe_dataset_initialization(
                train_data_std, val_data_std, x_columns_fixed, y_columns, spatial_columns
            )

            # é…ç½®æ¨¡å‹
            model_name = f"GNNWR_10fold_{fold_idx + 1}"
            gnnwr = models.GNNWR(
                train_dataset=train_set,
                valid_dataset=val_set,
                test_dataset=val_set,
                dense_layers=[128, 64],
                activate_func=nn.ReLU(),
                start_lr=0.0005,
                optimizer="Adam",
                model_name=model_name,
                model_save_path="result/10fold_cv_models",
                log_path="result/10fold_cv_logs",
                write_path="result/10fold_cv_runs",
                optimizer_params={
                    "scheduler": "MultiStepLR",
                    "scheduler_milestones": [20, 40],
                    "scheduler_gamma": 0.8,
                }
            )

            # åˆ›å»ºç›®å½•
            os.makedirs("result/10fold_cv_models", exist_ok=True)
            os.makedirs("result/10fold_cv_logs", exist_ok=True)
            os.makedirs("result/10fold_cv_runs", exist_ok=True)

            # è®­ç»ƒæ¨¡å‹ï¼ˆå‡å°‘epochæ•°ï¼‰
            gnnwr.add_graph()
            gnnwr.run(max_epoch=20, early_stop=8, print_frequency=5)

            # é¢„æµ‹
            gnnwr.load_model(f'result/10fold_cv_models/{model_name}.pkl')
            val_predictions = gnnwr.predict(val_set)

            if len(val_predictions) == 0:
                print(f"âš ï¸ æ— é¢„æµ‹ç»“æœï¼Œè·³è¿‡è¯¥æŠ˜")
                continue

            val_true = val_data_std[y_columns[0]].values

            # å­˜å‚¨ç»“æœ
            all_true.extend(val_true)
            all_pred.extend(val_predictions)

            # è®¡ç®—å½“å‰æŠ˜çš„æŒ‡æ ‡
            fold_metrics = calculate_metrics(val_true, val_predictions)
            fold_results = {
                'fold': fold_idx + 1,
                'train_stations': len(train_stations),
                'val_stations': len(val_stations),
                'train_samples': len(train_data),
                'val_samples': len(val_data),
                **fold_metrics
            }
            all_results.append(fold_results)
            fold_metrics_list.append(fold_metrics)

            print(f"âœ… æŠ˜ {fold_idx + 1} å®Œæˆ - RMSE: {fold_metrics['RMSE']:.4f}, RÂ²: {fold_metrics['R2']:.4f}")

            # æ¸…ç†å†…å­˜
            del gnnwr, train_set, val_set
            gc.collect()

        except Exception as e:
            print(f"âŒ æŠ˜ {fold_idx + 1} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue

    # æ±‡æ€»ç»“æœ
    total_time = time.time() - total_start_time
    print(f"\n=== 10æŠ˜äº¤å‰éªŒè¯å®Œæˆ ===")
    print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"æˆåŠŸå®Œæˆçš„æŠ˜æ•°: {len(all_results)}/10")

    if len(all_true) > 0:
        overall_metrics = calculate_metrics(all_true, all_pred)

        # è®¡ç®—å„æŠ˜å¹³å‡æŒ‡æ ‡
        if fold_metrics_list:
            avg_r2 = np.mean([m['R2'] for m in fold_metrics_list])
            avg_rmse = np.mean([m['RMSE'] for m in fold_metrics_list])
            std_r2 = np.std([m['R2'] for m in fold_metrics_list])
            std_rmse = np.std([m['RMSE'] for m in fold_metrics_list])

            print(f"\nğŸ“Š 10æŠ˜äº¤å‰éªŒè¯ç»Ÿè®¡:")
            print(f"  å¹³å‡ RÂ²: {avg_r2:.4f} Â± {std_r2:.4f}")
            print(f"  å¹³å‡ RMSE: {avg_rmse:.4f} Â± {std_rmse:.4f}")
            print(f"  æ€»ä½“ RÂ²: {overall_metrics['R2']:.4f}")
            print(f"  æ€»ä½“ RMSE: {overall_metrics['RMSE']:.4f}")

        # ä¿å­˜ç»“æœ
        results_df = pd.DataFrame({
            'True': all_true,
            'Predicted': all_pred
        })
        results_df.to_csv('result/10fold_cv_results.csv', index=False)

        detailed_results = pd.DataFrame(all_results)
        detailed_results.to_csv('result/10fold_cv_detailed.csv', index=False)

        # ç»˜åˆ¶ç»“æœå›¾
        plot_aggregated_scatter(all_true, all_pred, overall_metrics, "result/10fold_cv_results")

        print("\næ€»ä½“è¯„ä¼°æŒ‡æ ‡:")
        for metric, value in overall_metrics.items():
            print(f"{metric}: {value:.4f}")

        return overall_metrics, detailed_results
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„äº¤å‰éªŒè¯æŠ˜")
        return None, None


def quick_2_fold_test(data, x_columns, y_columns, spatial_columns, station_column='station_id'):
    """ä¿®å¤çš„å¿«é€Ÿ2æŠ˜æµ‹è¯•"""
    print("âš¡ æ‰§è¡Œä¿®å¤çš„å¿«é€Ÿ2æŠ˜æµ‹è¯•...")

    # ä½¿ç”¨å·²ç»æ¸…æ´—è¿‡çš„æ•°æ®ï¼Œè·³è¿‡æ—¶é—´æˆ³ä¿®å¤
    data_fixed, x_fixed = fix_timestamp_issues(data, x_columns, y_columns)

    # å†æ¬¡ç¡®ä¿æ•°æ®æ¸…æ´—
    clean_data = enhanced_robust_data_cleaning(data_fixed, x_fixed, y_columns, spatial_columns, station_column)

    # åªå–å‰10ä¸ªç«™ç‚¹æµ‹è¯•ï¼ˆæ›´å°‘çš„æ•°æ®ç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    stations = clean_data[station_column].unique()[:10]
    test_data = clean_data[clean_data[station_column].isin(stations)]

    if len(test_data) < 10:
        print("âš ï¸ æµ‹è¯•æ•°æ®è¿‡å°‘ï¼Œè·³è¿‡å¿«é€Ÿæµ‹è¯•")
        return []

    # åˆ†æˆ2æŠ˜
    np.random.seed(42)
    np.random.shuffle(stations)
    fold1_stations = stations[:5]  # æ¯æŠ˜5ä¸ªç«™ç‚¹
    fold2_stations = stations[5:10]

    results = []
    all_true = []
    all_pred = []

    for fold_idx, (train_stations, val_stations) in enumerate(
            [(fold2_stations, fold1_stations), (fold1_stations, fold2_stations)]):
        print(f"\nå¿«é€Ÿæµ‹è¯•æŠ˜ {fold_idx + 1}:")

        train_data = test_data[test_data[station_column].isin(train_stations)]
        val_data = test_data[test_data[station_column].isin(val_stations)]

        if len(train_data) < 5 or len(val_data) < 2:
            print("âš ï¸ è®­ç»ƒé›†æˆ–éªŒè¯é›†æ ·æœ¬è¿‡å°‘ï¼Œè·³è¿‡")
            continue

        print(f"  è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬, {len(train_stations)} ç«™ç‚¹")
        print(f"  éªŒè¯é›†: {len(val_data)} æ ·æœ¬, {len(val_stations)} ç«™ç‚¹")

        try:
            # æ•°æ®æ ‡å‡†åŒ–
            data_standardized = standardize_data(pd.concat([train_data, val_data]), x_fixed, y_columns)
            train_data_std = data_standardized[data_standardized[station_column].isin(train_stations)]
            val_data_std = data_standardized[data_standardized[station_column].isin(val_stations)]

            # æ•°æ®é›†åˆå§‹åŒ–
            train_set, val_set = enhanced_safe_dataset_initialization(
                train_data_std, val_data_std, x_fixed, y_columns, spatial_columns
            )

            # ç®€åŒ–æ¨¡å‹é…ç½®
            model_name = f"quick_test_{fold_idx}"
            gnnwr = models.GNNWR(
                train_dataset=train_set,
                valid_dataset=val_set,
                test_dataset=val_set,
                dense_layers=[32, 16],  # æ›´å°çš„ç½‘ç»œ
                start_lr=0.001,
                optimizer="Adam",
                model_name=model_name,
                model_save_path="result/quick_test"
            )

            # åˆ›å»ºç›®å½•
            os.makedirs("result/quick_test", exist_ok=True)

            # å¿«é€Ÿè®­ç»ƒ
            gnnwr.add_graph()
            gnnwr.run(max_epoch=3, early_stop=1, print_frequency=1)  # æ›´å°‘çš„epoch

            # é¢„æµ‹
            model_path = f'result/quick_test/{model_name}.pkl'
            if os.path.exists(model_path):
                gnnwr.load_model(model_path)
                val_predictions = gnnwr.predict(val_set)
                val_true = val_data_std[y_columns[0]].values

                if len(val_predictions) > 0:
                    all_true.extend(val_true)
                    all_pred.extend(val_predictions)

                    fold_metrics = calculate_metrics(val_true, val_predictions)
                    results.append({
                        'fold': fold_idx + 1,
                        'r2': fold_metrics['R2'],
                        'rmse': fold_metrics['RMSE']
                    })

                    print(f"âœ… å¿«é€Ÿæµ‹è¯•æŠ˜ {fold_idx + 1} å®Œæˆ: RÂ² = {fold_metrics['R2']:.4f}")
                else:
                    print(f"âš ï¸ å¿«é€Ÿæµ‹è¯•æŠ˜ {fold_idx + 1} æ— é¢„æµ‹ç»“æœ")
            else:
                print(f"âš ï¸ å¿«é€Ÿæµ‹è¯•æŠ˜ {fold_idx + 1} æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")

        except Exception as e:
            print(f"âŒ å¿«é€Ÿæµ‹è¯•æŠ˜ {fold_idx + 1} å¤±è´¥: {e}")
            continue

        # æ¸…ç†å†…å­˜
        try:
            del gnnwr, train_set, val_set
            gc.collect()
        except:
            pass

    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    if len(all_true) > 0:
        overall_metrics = calculate_metrics(all_true, all_pred)
        print(f"\nå¿«é€Ÿæµ‹è¯•æ€»ä½“ç»“æœ: RÂ² = {overall_metrics['R2']:.4f}, RMSE = {overall_metrics['RMSE']:.4f}")

    return results


# ä¿®æ”¹ä¸»å‡½æ•°ä»¥ä½¿ç”¨10æŠ˜äº¤å‰éªŒè¯
def main():
    """ä¸»å‡½æ•° - ä¿®å¤ç‰ˆæœ¬"""
    try:
        # 1. åŠ è½½æ•°æ®
        print("åŠ è½½æ•°æ®...")
        monitor_performance("ç¨‹åºå¼€å§‹")
        if not os.path.exists('lu_onehot.xlsx'):
            raise FileNotFoundError("æ•°æ®æ–‡ä»¶ 'lu_onehot.xlsx' ä¸å­˜åœ¨")

        data = pd.read_excel('lu_onehot.xlsx')
        print(f"åŸå§‹æ•°æ®: {data.shape}")
        monitor_performance("æ•°æ®åŠ è½½å")

        # 2. å®šä¹‰ç‰¹å¾
        x_columns = ['aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation',
                     'std_slope', 'std_eastness', 'std_tpi', 'std_curvature1', 'std_curvature2',
                     'std_high', 'std_aspect', 'glsnow', 'cswe', 'snow_depth_snow_depth',
                     'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'doy', 'gldas', 'year', 'month', 'scp_start',
                     'scp_end', 'd1', 'd2', 'X', 'Y', 'Z', 'da', 'db', 'dc', 'dd', 'landuse_11',
                     'landuse_12', 'landuse_21', 'landuse_22', 'landuse_23', 'landuse_24',
                     'landuse_31', 'landuse_32', 'landuse_33', 'landuse_41', 'landuse_42',
                     'landuse_43', 'landuse_46', 'landuse_51', 'landuse_52', 'landuse_53',
                     'landuse_62', 'landuse_63', 'landuse_64']
        y_columns = ['swe']
        spatial_columns = ['X', 'Y']
        station_column = 'station_id'

        # 3. ä½¿ç”¨å¢å¼ºçš„æ•°æ®è°ƒè¯•
        print("\n=== æ‰§è¡Œå¢å¼ºæ•°æ®è°ƒè¯• ===")
        if not enhanced_debug_data_issues(data, x_columns, y_columns, spatial_columns, station_column):
            print("âŒ æ•°æ®è°ƒè¯•å‘ç°é—®é¢˜ï¼Œå°†å°è¯•ä¿®å¤...")

        # 4. ä½¿ç”¨å¢å¼ºçš„æ•°æ®æ¸…æ´—
        print("\n=== æ‰§è¡Œå¢å¼ºæ•°æ®æ¸…æ´— ===")
        clean_data = enhanced_robust_data_cleaning(
            data, x_columns, y_columns, spatial_columns, station_column
        )

        # 5. é‡æ–°è°ƒè¯•æ¸…æ´—åçš„æ•°æ®
        print("\n=== æ£€æŸ¥æ¸…æ´—åæ•°æ® ===")
        if not enhanced_debug_data_issues(clean_data, x_columns, y_columns, spatial_columns, station_column):
            raise ValueError("æ•°æ®æ¸…æ´—åä»ç„¶å­˜åœ¨é—®é¢˜")

        print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼Œå¼€å§‹æ¨¡å‹è®­ç»ƒ...")

        # 6. å…ˆè¿è¡Œå¿«é€Ÿæµ‹è¯•
        print("\n=== å¼€å§‹å¿«é€Ÿ2æŠ˜æµ‹è¯• ===")
        quick_test_results = quick_2_fold_test(
            clean_data, x_columns, y_columns, spatial_columns, station_column
        )

        if quick_test_results and len(quick_test_results) > 0:
            print("âœ… å¿«é€Ÿæµ‹è¯•é€šè¿‡ï¼Œå¼€å§‹å®Œæ•´10æŠ˜äº¤å‰éªŒè¯...")

            # 7. æ‰§è¡Œ10æŠ˜äº¤å‰éªŒè¯
            overall_metrics, detailed_results = improved_10_fold_cross_validation(
                clean_data, x_columns, y_columns, spatial_columns, station_column
            )

            if overall_metrics is not None:
                print("\nğŸ‰ 10æŠ˜äº¤å‰éªŒè¯æˆåŠŸå®Œæˆ!")
                # ... å…¶ä½™ä»£ç ä¿æŒä¸å˜
            else:
                print("âŒ 10æŠ˜äº¤å‰éªŒè¯å¤±è´¥ï¼Œå›é€€åˆ°ç®€åŒ–ç‰ˆæœ¬...")
                simple_station_cv_version()
        else:
            print("âŒ å¿«é€Ÿæµ‹è¯•å¤±è´¥ï¼è¯·å…ˆä¿®å¤é—®é¢˜å†ç»§ç»­")
            return

        monitor_performance("ç¨‹åºç»“æŸ")

    except Exception as e:
        print(f"âŒ ä¸»ç¨‹åºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


# æ·»åŠ ç¼ºå¤±çš„å‡½æ•°ï¼ˆä¿æŒåŸæœ‰å®ç°ï¼‰
def enhanced_debug_data_issues(data, x_columns, y_columns, spatial_columns, station_column='station_id'):
    """å¢å¼ºçš„æ•°æ®é—®é¢˜è°ƒè¯•"""
    print("=== å¢å¼ºæ•°æ®è°ƒè¯• ===")
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {data.shape}")

    # 1. æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
    all_required_columns = x_columns + y_columns + spatial_columns + [station_column]
    missing_columns = [col for col in all_required_columns if col not in data.columns]
    if missing_columns:
        print(f"âŒ ç¼ºå¤±åˆ—: {missing_columns}")
        print(f"å¯ç”¨åˆ—: {list(data.columns)}")
        return False

    print("âœ… æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨")

    # 2. æ£€æŸ¥æ•°æ®ç±»å‹
    print("æ£€æŸ¥æ•°æ®ç±»å‹...")
    for col in all_required_columns:
        dtype = data[col].dtype
        print(f"   {col}: {dtype}")

    # 3. æ£€æŸ¥ç¼ºå¤±å€¼
    print("æ£€æŸ¥ç¼ºå¤±å€¼...")
    missing_stats = data[all_required_columns].isnull().sum()
    if missing_stats.sum() > 0:
        print("âŒ å‘ç°ç¼ºå¤±å€¼:")
        for col, count in missing_stats.items():
            if count > 0:
                print(f"   {col}: {count} ä¸ªç¼ºå¤±å€¼ ({count / len(data):.2%})")
    else:
        print("âœ… æ— ç¼ºå¤±å€¼")

    # 4. æ£€æŸ¥æ— ç©·å¤§å€¼
    print("æ£€æŸ¥æ— ç©·å¤§å€¼...")
    numeric_columns = data[all_required_columns].select_dtypes(include=[np.number]).columns
    inf_found = False
    for col in numeric_columns:
        inf_count = np.isinf(data[col]).sum()
        if inf_count > 0:
            print(f"âŒ {col}: {inf_count} ä¸ªæ— ç©·å¤§å€¼")
            inf_found = True

    if not inf_found:
        print("âœ… æ— æ— ç©·å¤§å€¼")

    # 5. æ£€æŸ¥é›¶æ–¹å·®ç‰¹å¾
    print("æ£€æŸ¥é›¶æ–¹å·®ç‰¹å¾...")
    for col in x_columns:
        if col in data.columns and data[col].dtype in [np.number]:
            variance = data[col].var()
            if variance == 0:
                print(f"âš ï¸ {col}: é›¶æ–¹å·®ç‰¹å¾")

    # 6. æ£€æŸ¥æ•°æ®èŒƒå›´
    print("æ£€æŸ¥æ•°æ®èŒƒå›´...")
    for col in y_columns:
        if col in data.columns:
            print(f"   {col}: min={data[col].min():.4f}, max={data[col].max():.4f}, mean={data[col].mean():.4f}")

    return not (missing_stats.sum() > 0 or inf_found)


def enhanced_robust_data_cleaning(data, x_columns, y_columns, spatial_columns, station_column='station_id'):
    """å¢å¼ºç‰ˆæœ¬çš„æ•°æ®æ¸…æ´—ï¼Œä¸“é—¨å¤„ç†infå’ŒNaNå€¼"""
    print("å¼€å§‹å¢å¼ºæ•°æ®æ¸…æ´—...")
    clean_data = data.copy()

    # æ£€æŸ¥å¿…éœ€åˆ—
    all_required_columns = x_columns + y_columns + spatial_columns + [station_column]
    missing_columns = [col for col in all_required_columns if col not in clean_data.columns]
    if missing_columns:
        raise ValueError(f"ç¼ºå°‘åˆ—: {missing_columns}")

    # ç¬¬ä¸€æ­¥ï¼šå¤„ç†æ— ç©·å¤§å€¼
    print("å¤„ç†æ— ç©·å¤§å€¼...")
    numeric_columns = clean_data.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        if col in clean_data.columns:
            # æ›¿æ¢infä¸ºNaN
            clean_data[col] = clean_data[col].replace([np.inf, -np.inf], np.nan)
            # ç»Ÿè®¡infæ•°é‡
            inf_count = np.isinf(clean_data[col]).sum()
            if inf_count > 0:
                print(f"   - åˆ— {col}: æ›¿æ¢ {inf_count} ä¸ªæ— ç©·å¤§å€¼ä¸ºNaN")

    # ç¬¬äºŒæ­¥ï¼šå¤„ç†ç¼ºå¤±å€¼
    print("å¤„ç†ç¼ºå¤±å€¼...")
    required_data_columns = x_columns + y_columns + spatial_columns + [station_column]

    # æ£€æŸ¥æ¯åˆ—çš„ç¼ºå¤±ç‡
    missing_stats = clean_data[required_data_columns].isnull().sum()
    high_missing_cols = missing_stats[missing_stats > 0].index.tolist()

    if high_missing_cols:
        print(f"   - æœ‰ç¼ºå¤±å€¼çš„åˆ—: {high_missing_cols}")
        for col in high_missing_cols:
            missing_rate = missing_stats[col] / len(clean_data)
            print(f"     {col}: {missing_stats[col]} ä¸ªç¼ºå¤±å€¼ ({missing_rate:.2%})")

    # åˆ é™¤åœ¨å¿…éœ€åˆ—ä¸­æœ‰ç¼ºå¤±å€¼çš„è¡Œ
    initial_count = len(clean_data)
    clean_data = clean_data.dropna(subset=required_data_columns)
    removed_count = initial_count - len(clean_data)
    print(f"   - åˆ é™¤ {removed_count} ä¸ªæœ‰ç¼ºå¤±å€¼çš„è¡Œ")
    print(f"   - å‰©ä½™æ•°æ®é‡: {len(clean_data)}")

    if len(clean_data) == 0:
        raise ValueError("æ•°æ®æ¸…æ´—åæ— æœ‰æ•ˆæ•°æ®")

    # ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥å¹¶ä¿®å¤é›¶æ–¹å·®ç‰¹å¾
    print("æ£€æŸ¥é›¶æ–¹å·®ç‰¹å¾...")
    zero_variance_cols = []
    for col in x_columns:
        if col in clean_data.columns:
            variance = clean_data[col].var()
            if variance == 0:
                zero_variance_cols.append(col)
                # æ·»åŠ å¾®å°å™ªå£°ä¿®å¤é›¶æ–¹å·®
                noise = np.random.normal(0, 1e-6, len(clean_data))
                clean_data[col] = clean_data[col] + noise
                print(f"   - ä¿®å¤é›¶æ–¹å·®åˆ— {col}")

    if zero_variance_cols:
        print(f"   - ä¿®å¤çš„é›¶æ–¹å·®åˆ—: {zero_variance_cols}")

    # ç¬¬å››æ­¥ï¼šå¤„ç†å¼‚å¸¸å€¼ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
    print("å¤„ç†å¼‚å¸¸å€¼...")
    for col in y_columns:
        if col in clean_data.columns:
            Q1 = clean_data[col].quantile(0.25)
            Q3 = clean_data[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outliers = clean_data[(clean_data[col] < lower_bound) | (clean_data[col] > upper_bound)]
            if len(outliers) > 0:
                print(f"   - {col}: å‘ç° {len(outliers)} ä¸ªå¼‚å¸¸å€¼")
                # å¯ä»¥é€‰æ‹©åˆ é™¤æˆ–ç¼©å°¾å¤„ç†
                clean_data = clean_data[(clean_data[col] >= lower_bound) & (clean_data[col] <= upper_bound)]

    # ç¬¬äº”æ­¥ï¼šç­›é€‰æœ‰æ•ˆç«™ç‚¹
    print("ç­›é€‰æœ‰æ•ˆç«™ç‚¹...")
    station_counts = clean_data[station_column].value_counts()
    valid_stations = station_counts[station_counts >= 3].index
    clean_data = clean_data[clean_data[station_column].isin(valid_stations)]

    print(f"   - æœ‰æ•ˆç«™ç‚¹æ•°: {clean_data[station_column].nunique()}")
    print(f"   - æœ€ç»ˆæ•°æ®é‡: {len(clean_data)}")

    # æœ€ç»ˆæ£€æŸ¥
    final_missing = clean_data[required_data_columns].isnull().sum().sum()
    final_inf = np.isinf(clean_data[required_data_columns].select_dtypes(include=[np.number])).sum().sum()

    print(f"æ•°æ®æ¸…æ´—å®Œæˆ:")
    print(f"   - å‰©ä½™ç¼ºå¤±å€¼: {final_missing}")
    print(f"   - æ— ç©·å¤§å€¼: {final_inf}")

    return clean_data


def standardize_data(data, x_column, y_column):
    """æ•°æ®æ ‡å‡†åŒ–"""
    print("æ ‡å‡†åŒ–æ•°æ®...")
    standardized_data = data.copy()

    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    standardized_data[x_column] = scaler.fit_transform(standardized_data[x_column])

    print("æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")
    return standardized_data


def safe_dataset_initialization(train_data, val_data, x_column, y_column, spatial_column):
    """ä¿®å¤ç‰ˆæœ¬çš„æ•°æ®é›†åˆå§‹åŒ–"""
    print("åˆå§‹åŒ–æ•°æ®é›†...")
    monitor_performance("æ•°æ®é›†åˆå§‹åŒ–å‰")

    try:
        start_time = time.time()

        # å…³é”®ä¿®å¤ï¼šåœ¨åˆå§‹åŒ–å‰æ£€æŸ¥å¹¶ä¿®å¤æ•°æ®
        train_data_fixed = train_data.copy()
        val_data_fixed = val_data.copy()

        # ä¿®å¤é›¶æ–¹å·®é—®é¢˜ - å¯¹æ¯ä¸ªç‰¹å¾åˆ—è¿›è¡Œæ£€æŸ¥
        for col in x_column:
            if col in train_data_fixed.columns:
                # å¦‚æœè®­ç»ƒé›†è¯¥åˆ—æ–¹å·®ä¸ºé›¶ï¼Œæ·»åŠ å¾®å°å™ªå£°
                if train_data_fixed[col].var() == 0:
                    print(f"âš ï¸ ä¿®å¤é›¶æ–¹å·®åˆ—: {col}")
                    noise = np.random.normal(0, 1e-6, len(train_data_fixed))
                    train_data_fixed[col] = train_data_fixed[col] + noise

                # åŒæ ·ä¿®å¤éªŒè¯é›†
                if col in val_data_fixed.columns and val_data_fixed[col].var() == 0:
                    noise = np.random.normal(0, 1e-6, len(val_data_fixed))
                    val_data_fixed[col] = val_data_fixed[col] + noise

        train_set, val_set, _ = datasets.init_dataset_split(
            train_data=train_data_fixed,
            val_data=val_data_fixed,
            test_data=val_data_fixed,
            x_column=x_column,
            y_column=y_column,
            spatial_column=spatial_column,
            batch_size=64,
            use_model="gnnwr"
        )

        init_time = time.time() - start_time
        print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–æˆåŠŸ - è€—æ—¶: {init_time:.2f}ç§’")
        monitor_performance("æ•°æ®é›†åˆå§‹åŒ–å")
        return train_set, val_set

    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆå§‹åŒ–å¤±è´¥: {e}")
        raise


def calculate_metrics(y_true, y_pred):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mse = mean_squared_error(y_true, y_pred)
    r = np.corrcoef(y_true, y_pred)[0, 1]
    r2 = r2_score(y_true, y_pred)

    return {
        'RMSE': rmse,
        'MSE': mse,
        'R': r,
        'R2': r2
    }


def plot_aggregated_scatter(all_true, all_pred, metrics, save_path="result/cross_validation_results"):
    """ç»˜åˆ¶èšåˆæ•£ç‚¹å›¾"""
    os.makedirs(save_path, exist_ok=True)

    plt.figure(figsize=(12, 10))

    # æ•£ç‚¹å›¾
    plt.subplot(2, 2, 1)
    plt.scatter(all_true, all_pred, alpha=0.6, s=10)

    # æ·»åŠ 1:1çº¿
    min_val = min(np.min(all_true), np.min(all_pred))
    max_val = max(np.max(all_true), np.max(all_pred))
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)

    plt.xlabel('çœŸå®å€¼')
    plt.ylabel('é¢„æµ‹å€¼')
    plt.title('10æŠ˜äº¤å‰éªŒè¯ç»“æœ')
    plt.grid(True, alpha=0.3)

    # åœ¨å³ä¸Šè§’æ·»åŠ æŒ‡æ ‡æ–‡æœ¬
    metrics_text = f"RMSE: {metrics['RMSE']:.4f}\nRÂ²: {metrics['R2']:.4f}\nR: {metrics['R']:.4f}\nMSE: {metrics['MSE']:.4f}"
    plt.text(0.95, 0.05, metrics_text, transform=plt.gca().transAxes,
             fontsize=12, verticalalignment='bottom', horizontalalignment='right',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

    # æ®‹å·®å›¾
    plt.subplot(2, 2, 2)
    residuals = all_pred - all_true
    plt.scatter(all_pred, residuals, alpha=0.6, s=10)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('é¢„æµ‹å€¼')
    plt.ylabel('æ®‹å·®')
    plt.title('æ®‹å·®å›¾')
    plt.grid(True, alpha=0.3)

    # åˆ†å¸ƒå›¾
    plt.subplot(2, 2, 3)
    plt.hist(all_true, bins=50, alpha=0.7, label='çœŸå®å€¼', density=True)
    plt.hist(all_pred, bins=50, alpha=0.7, label='é¢„æµ‹å€¼', density=True)
    plt.xlabel('å€¼')
    plt.ylabel('å¯†åº¦')
    plt.title('çœŸå®å€¼ä¸é¢„æµ‹å€¼åˆ†å¸ƒ')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # æ®‹å·®åˆ†å¸ƒå›¾
    plt.subplot(2, 2, 4)
    plt.hist(residuals, bins=50, alpha=0.7, color='green', density=True)
    plt.axvline(x=0, color='r', linestyle='--', linewidth=2)
    plt.xlabel('æ®‹å·®')
    plt.ylabel('å¯†åº¦')
    plt.title('æ®‹å·®åˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)

    # åœ¨æ®‹å·®åˆ†å¸ƒå›¾ä¸­æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    residual_stats = f"æ®‹å·®ç»Ÿè®¡:\nå‡å€¼: {np.mean(residuals):.4f}\næ ‡å‡†å·®: {np.std(residuals):.4f}"
    plt.text(0.95, 0.95, residual_stats, transform=plt.gca().transAxes,
             fontsize=10, verticalalignment='top', horizontalalignment='right',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

    plt.tight_layout()
    plt.savefig(f"{save_path}/aggregated_scatter_plot.png", dpi=300, bbox_inches='tight')
    plt.savefig(f"{save_path}/aggregated_scatter_plot.pdf", bbox_inches='tight')
    plt.close()

    print(f"âœ… æ•£ç‚¹å›¾å·²ä¿å­˜è‡³: {save_path}/aggregated_scatter_plot.png")


def simple_station_cv_version():
    """ç®€åŒ–ç‰ˆæœ¬çš„ç«™ç‚¹çº§äº¤å‰éªŒè¯"""
    try:
        print("å°è¯•ç®€åŒ–ç‰ˆæœ¬ç«™ç‚¹çº§äº¤å‰éªŒè¯...")

        # 1. åŠ è½½æ•°æ®
        data = pd.read_excel('lu_onehot.xlsx')
        print(f"åŸå§‹æ•°æ®: {data.shape}")

        # 2. å®šä¹‰ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
        x_columns = ['elevation', 'slope', 'aspect', 'X', 'Y', 'doy', 'year', 'month']
        y_columns = ['swe']
        spatial_columns = ['X', 'Y']
        station_column = 'station_id'

        # 3. ä¿®å¤æ—¶é—´æˆ³é—®é¢˜
        data_fixed, x_columns_fixed = fix_timestamp_issues(data, x_columns, y_columns)

        # 4. ç®€åŒ–æ•°æ®æ¸…æ´—
        clean_data = data_fixed.copy()
        clean_data = clean_data.dropna(subset=x_columns_fixed + y_columns + [station_column])

        # ç§»é™¤æ•°æ®é‡è¿‡å°‘çš„ç«™ç‚¹
        station_counts = clean_data[station_column].value_counts()
        valid_stations = station_counts[station_counts >= 3].index
        clean_data = clean_data[clean_data[station_column].isin(valid_stations)]

        print(f"ç®€åŒ–æ¸…æ´—åæ•°æ®: {clean_data.shape}")
        print(f"å¯ç”¨ç«™ç‚¹æ•°: {clean_data[station_column].nunique()}")

        # 5. æ‰§è¡Œç®€åŒ–çš„äº¤å‰éªŒè¯ï¼ˆåªè¿è¡Œå‰10ä¸ªç«™ç‚¹ä½œä¸ºæµ‹è¯•ï¼‰
        unique_stations = clean_data[station_column].unique()[:10]
        print(f"æµ‹è¯•è¿è¡Œå‰ {len(unique_stations)} ä¸ªç«™ç‚¹...")

        all_true = []
        all_pred = []

        for i, test_station in enumerate(unique_stations):
            print(f"æŠ˜ {i + 1}/{len(unique_stations)}: ç«™ç‚¹ {test_station}")

            try:
                # åˆ†å‰²æ•°æ®
                train_data = clean_data[clean_data[station_column] != test_station]
                val_data = clean_data[clean_data[station_column] == test_station]

                if len(train_data) == 0 or len(val_data) == 0:
                    continue

                # æ•°æ®æ ‡å‡†åŒ–
                data_standardized = standardize_data(pd.concat([train_data, val_data]), x_columns_fixed, y_columns)
                train_data_std = data_standardized[
                    data_standardized[station_column].isin(train_data[station_column].unique())]
                val_data_std = data_standardized[data_standardized[station_column].isin([test_station])]

                # åˆå§‹åŒ–æ•°æ®é›†
                train_set, val_set, _ = datasets.init_dataset_split(
                    train_data=train_data_std,
                    val_data=val_data_std,
                    test_data=val_data_std,
                    x_column=x_columns_fixed,
                    y_column=y_columns,
                    spatial_column=spatial_columns,
                    batch_size=32,  # æ›´å°çš„batch size
                    use_model="gnnwr"
                )

                # ç®€åŒ–æ¨¡å‹
                model_name = f"GNNWR_Simple_Fold_{i + 1}"
                gnnwr = models.GNNWR(
                    train_dataset=train_set,
                    valid_dataset=val_set,
                    test_dataset=val_set,
                    dense_layers=[128, 64],  # æ›´ç®€å•çš„ç½‘ç»œ
                    activate_func=nn.ReLU(),
                    start_lr=0.001,
                    optimizer="Adam",
                    model_name=model_name,
                    model_save_path="result/simple_cv_models",
                    log_path="result/simple_cv_logs",
                    write_path="result/simple_cv_runs"
                )

                # åˆ›å»ºç›®å½•
                os.makedirs("result/simple_cv_models", exist_ok=True)

                # å¿«é€Ÿè®­ç»ƒ
                gnnwr.add_graph()
                gnnwr.run(max_epoch=30, early_stop=10, print_frequency=5)

                # é¢„æµ‹
                gnnwr.load_model(f'result/simple_cv_models/{model_name}.pkl')
                val_predictions = gnnwr.predict(val_set)
                val_true = val_data_std[y_columns[0]].values

                all_true.extend(val_true)
                all_pred.extend(val_predictions)

                print(f"æŠ˜ {i + 1} å®Œæˆ")

            except Exception as e:
                print(f"æŠ˜ {i + 1} å¤±è´¥: {e}")
                continue

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        if len(all_true) > 0:
            overall_metrics = calculate_metrics(all_true, all_pred)
            print("\nç®€åŒ–ç‰ˆæœ¬ç»“æœ:")
            for metric, value in overall_metrics.items():
                print(f"{metric}: {value:.4f}")

            # ç»˜åˆ¶æ•£ç‚¹å›¾
            plot_aggregated_scatter(np.array(all_true), np.array(all_pred), overall_metrics,
                                    "result/simple_cv_results")

            return overall_metrics
        else:
            print("âŒ ç®€åŒ–ç‰ˆæœ¬æ²¡æœ‰æˆåŠŸé¢„æµ‹")
            return None

    except Exception as e:
        print(f"ç®€åŒ–ç‰ˆæœ¬å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    # å…ˆå°è¯•10æŠ˜äº¤å‰éªŒè¯ç‰ˆæœ¬
    try:
        main()
    except Exception as e:
        print(f"10æŠ˜äº¤å‰éªŒè¯ç‰ˆæœ¬å¤±è´¥: {e}")
        print("\nå°è¯•ç®€åŒ–ç‰ˆæœ¬...")
        # å¦‚æœ10æŠ˜ç‰ˆæœ¬å¤±è´¥ï¼Œå°è¯•ç®€åŒ–ç‰ˆæœ¬
        simple_station_cv_version()