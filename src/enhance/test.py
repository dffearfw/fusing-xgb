import os
import sys
import pandas as pd
import numpy as np
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from gnnwr import models, datasets, utils
import time
import psutil
import gc
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns


def monitor_performance(step_name):
    """ç®€å•çš„æ€§èƒ½ç›‘æ§"""
    memory = psutil.virtual_memory()
    process = psutil.Process()
    memory_usage = process.memory_info().rss / 1024 / 1024  # MB
    print(f"[æ€§èƒ½ç›‘æ§] {step_name} - å†…å­˜ä½¿ç”¨: {memory_usage:.1f}MB, ç³»ç»Ÿå†…å­˜: {memory.percent}%")


def debug_data_issues(data, x_column, y_column, spatial_column, station_column='station_id'):
    """è¯¦ç»†çš„æ•°æ®é—®é¢˜è°ƒè¯•"""
    print("=== æ•°æ®è°ƒè¯•ä¿¡æ¯ ===")
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {data.shape}")

    # 1. æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
    all_required_columns = x_column + y_column + spatial_column + [station_column]
    missing_columns = [col for col in all_required_columns if col not in data.columns]
    if missing_columns:
        print(f"âŒ ç¼ºå¤±åˆ—: {missing_columns}")
        print(f"å¯ç”¨åˆ—: {list(data.columns)}")
        return False

    print("âœ… æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨")

    # 2. æ£€æŸ¥ç«™ç‚¹æ•°é‡
    unique_stations = data[station_column].nunique()
    print(f"ç«™ç‚¹æ•°é‡: {unique_stations}")

    # 3. æ£€æŸ¥ç¼ºå¤±å€¼
    print("\n=== ç¼ºå¤±å€¼åˆ†æ ===")
    missing_info = data[all_required_columns].isnull().sum()
    total_missing = missing_info.sum()
    print(f"æ€»ç¼ºå¤±å€¼: {total_missing}")

    if total_missing > 0:
        print("ç¼ºå¤±å€¼è¯¦æƒ…:")
        for col, missing_count in missing_info[missing_info > 0].items():
            print(f"  {col}: {missing_count} ä¸ªç¼ºå¤±å€¼ ({missing_count / len(data):.1%})")

    # 4. æ£€æŸ¥æ— ç©·å€¼
    print("\n=== æ— ç©·å€¼æ£€æŸ¥ ===")
    numeric_cols = data[all_required_columns].select_dtypes(include=[np.number]).columns
    inf_count = 0
    for col in numeric_cols:
        if np.isinf(data[col]).any():
            inf_count += 1
            print(f"âŒ åˆ— '{col}' åŒ…å«æ— ç©·å€¼")

    if inf_count == 0:
        print("âœ… æ²¡æœ‰æ— ç©·å€¼")

    # 5. æ£€æŸ¥æ¯ä¸ªç«™ç‚¹çš„æ•°æ®é‡
    print("\n=== ç«™ç‚¹æ•°æ®é‡åˆ†å¸ƒ ===")
    station_counts = data[station_column].value_counts()
    print(f"æ¯ä¸ªç«™ç‚¹å¹³å‡æ•°æ®é‡: {station_counts.mean():.1f}")
    print(f"æœ€å°æ•°æ®é‡: {station_counts.min()}")
    print(f"æœ€å¤§æ•°æ®é‡: {station_counts.max()}")
    print(f"æ•°æ®é‡å°‘äº5æ¡çš„ç«™ç‚¹æ•°: {(station_counts < 5).sum()}")

    return True


def robust_data_cleaning(data, x_column, y_column, spatial_column, station_column):
    """é²æ£’çš„æ•°æ®æ¸…æ´—"""
    print("å¼€å§‹æ•°æ®æ¸…æ´—...")
    clean_data = data.copy()

    # 1. æ£€æŸ¥ç¼ºå¤±å€¼
    all_columns = x_column + y_column + spatial_column + [station_column]
    missing_rates = clean_data[all_columns].isnull().mean()

    print("å„åˆ—ç¼ºå¤±ç‡:")
    for col in all_columns:
        # ç¡®ä¿è·å–çš„æ˜¯æ ‡é‡å€¼
        rate = float(missing_rates[col])  # è½¬æ¢ä¸ºfloatç¡®ä¿æ˜¯æ ‡é‡
        print(f"  {col}: {rate:.2%}")

    # ä¿®å¤ç¼ºå¤±å€¼å¤„ç†é€»è¾‘
    for col in all_columns:
        # ç¡®ä¿è·å–æ ‡é‡å€¼
        rate = float(missing_rates[col])

        if rate > 0 and rate < 0.3:  # ç¼ºå¤±ç‡ä½äº30%
            if col in ['elevation', 'slope', 'aspect', 'X', 'Y']:  # æ•°å€¼å‹ç‰¹å¾
                median_val = clean_data[col].median()
                if not pd.isna(median_val):
                    clean_data[col].fillna(median_val, inplace=True)
                else:
                    clean_data[col].fillna(0, inplace=True)
            elif col in ['doy', 'year', 'month']:  # æ—¶é—´ç‰¹å¾
                mode_vals = clean_data[col].mode()
                if len(mode_vals) > 0 and not pd.isna(mode_vals.iloc[0]):
                    clean_data[col].fillna(mode_vals.iloc[0], inplace=True)
                else:
                    clean_data[col].fillna(0, inplace=True)
            else:  # å…¶ä»–ç‰¹å¾
                median_val = clean_data[col].median()
                if not pd.isna(median_val):
                    clean_data[col].fillna(median_val, inplace=True)
                else:
                    clean_data[col].fillna(0, inplace=True)
        elif rate >= 0.3:  # ç¼ºå¤±ç‡è¿‡é«˜
            print(f"âš ï¸ åˆ— {col} ç¼ºå¤±ç‡è¿‡é«˜ ({rate:.2%})ï¼Œè€ƒè™‘åˆ é™¤")

    # 2. ç§»é™¤ä»æœ‰ç¼ºå¤±å€¼çš„è¡Œ
    initial_rows = len(clean_data)
    clean_data = clean_data.dropna(subset=all_columns)
    removed_rows = initial_rows - len(clean_data)
    print(f"ç§»é™¤ {removed_rows} ä¸ªä»æœ‰ç¼ºå¤±å€¼çš„è¡Œ")

    # 3. æ£€æŸ¥å¹¶å¤„ç†æ— ç©·å¤§å€¼
    numeric_columns = clean_data[x_column + y_column].select_dtypes(include=[np.number]).columns
    if len(numeric_columns) > 0:
        inf_mask = np.isinf(clean_data[numeric_columns]).any(axis=1)
        if inf_mask.any():
            print(f"ç§»é™¤ {inf_mask.sum()} ä¸ªåŒ…å«æ— ç©·å¤§å€¼çš„è¡Œ")
            clean_data = clean_data[~inf_mask]

    # 4. æ£€æŸ¥ç«™ç‚¹æ•°æ®é‡
    station_counts = clean_data[station_column].value_counts()
    valid_stations = station_counts[station_counts >= 3].index  # è‡³å°‘3ä¸ªæ ·æœ¬
    clean_data = clean_data[clean_data[station_column].isin(valid_stations)]
    print(f"ç§»é™¤æ•°æ®é‡å°‘äº3çš„ç«™ç‚¹ï¼Œå‰©ä½™ {len(valid_stations)} ä¸ªç«™ç‚¹")

    # 5. æ£€æŸ¥ç‰¹å¾å€¼èŒƒå›´
    print("\nç‰¹å¾å€¼èŒƒå›´:")
    for col in x_column + y_column:
        if col in clean_data.columns:
            min_val = float(clean_data[col].min())  # ç¡®ä¿æ˜¯æ ‡é‡
            max_val = float(clean_data[col].max())  # ç¡®ä¿æ˜¯æ ‡é‡
            print(f"  {col}: [{min_val:.4f}, {max_val:.4f}]")

    print(f"æ¸…æ´—åæ•°æ®: {clean_data.shape}")
    return clean_data


def safe_dataset_initialization(train_data, val_data, x_column, y_column, spatial_column):
    """å®‰å…¨çš„æ•°æ®é›†åˆå§‹åŒ–"""
    print("åˆå§‹åŒ–æ•°æ®é›†...")
    monitor_performance("æ•°æ®é›†åˆå§‹åŒ–å‰")

    # éªŒè¯æ¯ä¸ªæ•°æ®é›†éƒ½ä¸ä¸ºç©º
    for name, dataset in [("è®­ç»ƒé›†", train_data), ("éªŒè¯é›†", val_data)]:
        if len(dataset) == 0:
            raise ValueError(f"{name} ä¸ºç©º")
        print(f"{name}: {len(dataset)} è¡Œ")

    try:
        start_time = time.time()
        train_set, val_set, _ = datasets.init_dataset_split(
            train_data=train_data,
            val_data=val_data,
            test_data=val_data,  # ä½¿ç”¨éªŒè¯é›†ä½œä¸ºæµ‹è¯•é›†å ä½
            x_column=x_column,
            y_column=y_column,
            spatial_column=spatial_column,
            batch_size=64,
            use_model="gnnwr"
        )
        init_time = time.time() - start_time
        print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–æˆåŠŸ - è€—æ—¶: {init_time:.2f}ç§’")
        monitor_performance("æ•°æ®é›†åˆå§‹åŒ–å")
        return train_set, val_set
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆå§‹åŒ–å¤±è´¥: {e}")
        raise


def calculate_metrics(y_true, y_pred):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mse = mean_squared_error(y_true, y_pred)
    r = np.corrcoef(y_true, y_pred)[0, 1]
    r2 = r2_score(y_true, y_pred)

    return {
        'RMSE': rmse,
        'MSE': mse,
        'R': r,
        'R2': r2
    }


def plot_aggregated_scatter(all_true, all_pred, metrics, save_path="result/cross_validation_results"):
    """ç»˜åˆ¶èšåˆæ•£ç‚¹å›¾"""
    os.makedirs(save_path, exist_ok=True)

    plt.figure(figsize=(12, 10))

    # æ•£ç‚¹å›¾
    plt.subplot(2, 2, 1)
    plt.scatter(all_true, all_pred, alpha=0.6, s=10)

    # æ·»åŠ 1:1çº¿
    min_val = min(all_true.min(), all_pred.min())
    max_val = max(all_true.max(), all_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)

    plt.xlabel('çœŸå®å€¼')
    plt.ylabel('é¢„æµ‹å€¼')
    plt.title('ç«™ç‚¹çº§äº¤å‰éªŒè¯ç»“æœ')
    plt.grid(True, alpha=0.3)

    # åœ¨å³ä¸Šè§’æ·»åŠ æŒ‡æ ‡æ–‡æœ¬
    metrics_text = f"RMSE: {metrics['RMSE']:.4f}\nRÂ²: {metrics['R2']:.4f}\nR: {metrics['R']:.4f}\nMSE: {metrics['MSE']:.4f}"
    plt.text(0.95, 0.05, metrics_text, transform=plt.gca().transAxes,
             fontsize=12, verticalalignment='bottom', horizontalalignment='right',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

    # æ®‹å·®å›¾
    plt.subplot(2, 2, 2)
    residuals = all_pred - all_true
    plt.scatter(all_pred, residuals, alpha=0.6, s=10)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('é¢„æµ‹å€¼')
    plt.ylabel('æ®‹å·®')
    plt.title('æ®‹å·®å›¾')
    plt.grid(True, alpha=0.3)

    # åˆ†å¸ƒå›¾
    plt.subplot(2, 2, 3)
    plt.hist(all_true, bins=50, alpha=0.7, label='çœŸå®å€¼', density=True)
    plt.hist(all_pred, bins=50, alpha=0.7, label='é¢„æµ‹å€¼', density=True)
    plt.xlabel('å€¼')
    plt.ylabel('å¯†åº¦')
    plt.title('çœŸå®å€¼ä¸é¢„æµ‹å€¼åˆ†å¸ƒ')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # æ®‹å·®åˆ†å¸ƒå›¾ï¼ˆæ–°å¢ï¼‰
    plt.subplot(2, 2, 4)
    plt.hist(residuals, bins=50, alpha=0.7, color='green', density=True)
    plt.axvline(x=0, color='r', linestyle='--', linewidth=2)
    plt.xlabel('æ®‹å·®')
    plt.ylabel('å¯†åº¦')
    plt.title('æ®‹å·®åˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)

    # åœ¨æ®‹å·®åˆ†å¸ƒå›¾ä¸­æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    residual_stats = f"æ®‹å·®ç»Ÿè®¡:\nå‡å€¼: {residuals.mean():.4f}\næ ‡å‡†å·®: {residuals.std():.4f}"
    plt.text(0.95, 0.95, residual_stats, transform=plt.gca().transAxes,
             fontsize=10, verticalalignment='top', horizontalalignment='right',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

    plt.tight_layout()
    plt.savefig(f"{save_path}/aggregated_scatter_plot.png", dpi=300, bbox_inches='tight')
    plt.savefig(f"{save_path}/aggregated_scatter_plot.pdf", bbox_inches='tight')
    plt.close()

    print(f"âœ… æ•£ç‚¹å›¾å·²ä¿å­˜è‡³: {save_path}/aggregated_scatter_plot.png")


def station_level_cross_validation(data, x_column, y_column, spatial_column, station_column='station_id'):
    """ç«™ç‚¹çº§äº¤å‰éªŒè¯"""
    print("å¼€å§‹ç«™ç‚¹çº§äº¤å‰éªŒè¯...")

    # è·å–æ‰€æœ‰å”¯ä¸€ç«™ç‚¹
    unique_stations = data[station_column].unique()
    n_stations = len(unique_stations)
    print(f"æ€»ç«™ç‚¹æ•°: {n_stations}")

    # å­˜å‚¨æ‰€æœ‰é¢„æµ‹ç»“æœ
    all_true = []
    all_pred = []
    fold_results = []

    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs("result/cross_validation_results", exist_ok=True)

    # æ•°æ®æ ‡å‡†åŒ–ï¼ˆä½¿ç”¨å…¨ä½“æ•°æ®ï¼‰
    print("æ•°æ®æ ‡å‡†åŒ–...")
    scaler = StandardScaler()
    data_standardized = data.copy()
    data_standardized[x_column] = scaler.fit_transform(data_standardized[x_column])

    total_start_time = time.time()

    for i, test_station in enumerate(unique_stations):
        print(f"\n--- æŠ˜ {i + 1}/{n_stations}: éªŒè¯ç«™ç‚¹ {test_station} ---")

        try:
            # åˆ†å‰²æ•°æ®ï¼šä¸€ä¸ªç«™ç‚¹ä½œä¸ºéªŒè¯é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†
            train_data = data_standardized[data_standardized[station_column] != test_station]
            val_data = data_standardized[data_standardized[station_column] == test_station]

            if len(train_data) == 0 or len(val_data) == 0:
                print(f"è·³è¿‡ç«™ç‚¹ {test_station}: è®­ç»ƒé›†æˆ–éªŒè¯é›†ä¸ºç©º")
                continue

            print(f"è®­ç»ƒé›†: {len(train_data)} è¡Œ, éªŒè¯é›†: {len(val_data)} è¡Œ")

            # åˆå§‹åŒ–æ•°æ®é›†
            train_set, val_set = safe_dataset_initialization(
                train_data, val_data, x_column, y_column, spatial_column
            )

            # é…ç½®æ¨¡å‹å‚æ•°
            optimizer_params = {
                "scheduler": "MultiStepLR",
                "scheduler_milestones": [100, 200, 300],
                "scheduler_gamma": 0.75,
            }

            # åˆå§‹åŒ–æ¨¡å‹
            model_name = f"GNNWR_Fold_{i + 1}"
            gnnwr = models.GNNWR(
                train_dataset=train_set,
                valid_dataset=val_set,
                test_dataset=val_set,  # ä½¿ç”¨éªŒè¯é›†ä½œä¸ºæµ‹è¯•é›†
                dense_layers=[256, 128, 64],
                activate_func=nn.PReLU(),
                start_lr=0.001,
                optimizer="Adam",
                model_name=model_name,
                model_save_path="result/cross_validation_models",
                log_path="result/cross_validation_logs",
                write_path="result/cross_validation_runs",
                optimizer_params=optimizer_params
            )

            # åˆ›å»ºç›®å½•
            os.makedirs("result/cross_validation_models", exist_ok=True)
            os.makedirs("result/cross_validation_logs", exist_ok=True)
            os.makedirs("result/cross_validation_runs", exist_ok=True)

            # è®­ç»ƒæ¨¡å‹ï¼ˆè¾ƒå°‘çš„epochï¼Œå› ä¸ºæœ‰å¾ˆå¤šæŠ˜ï¼‰
            gnnwr.add_graph()
            gnnwr.run(max_epoch=50, early_stop=20, print_frequency=10)

            # åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹
            gnnwr.load_model(f'result/cross_validation_models/{model_name}.pkl')

            # è·å–éªŒè¯é›†é¢„æµ‹ç»“æœ
            val_predictions = gnnwr.predict(val_set)
            val_true = val_data[y_column[0]].values

            # å­˜å‚¨ç»“æœ
            all_true.extend(val_true)
            all_pred.extend(val_predictions)

            # è®¡ç®—å½“å‰æŠ˜çš„æŒ‡æ ‡
            fold_metrics = calculate_metrics(val_true, val_predictions)
            fold_results.append({
                'station_id': test_station,
                'fold': i + 1,
                'n_train': len(train_data),
                'n_val': len(val_data),
                **fold_metrics
            })

            print(f"æŠ˜ {i + 1} å®Œæˆ - RMSE: {fold_metrics['RMSE']:.4f}, RÂ²: {fold_metrics['R2']:.4f}")

            # æ¸…ç†å†…å­˜
            del gnnwr, train_set, val_set
            gc.collect()

        except Exception as e:
            print(f"æŠ˜ {i + 1} å¤±è´¥: {e}")
            continue

    total_time = time.time() - total_start_time
    print(f"\n=== äº¤å‰éªŒè¯å®Œæˆ ===")
    print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"å¹³å‡æ¯æŠ˜è€—æ—¶: {total_time / len(fold_results):.2f}ç§’")
    print(f"æˆåŠŸå®Œæˆçš„æŠ˜æ•°: {len(fold_results)}/{n_stations}")

    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    if len(all_true) > 0:
        overall_metrics = calculate_metrics(all_true, all_pred)

        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_df = pd.DataFrame(fold_results)
        results_df.to_csv("result/cross_validation_results/detailed_results.csv", index=False)

        # ä¿å­˜æ€»ä½“ç»“æœ
        overall_results = {
            'total_stations': n_stations,
            'successful_folds': len(fold_results),
            'total_samples': len(all_true),
            **overall_metrics
        }
        pd.DataFrame([overall_results]).to_csv("result/cross_validation_results/overall_results.csv", index=False)

        # ç»˜åˆ¶èšåˆæ•£ç‚¹å›¾
        plot_aggregated_scatter(np.array(all_true), np.array(all_pred), overall_metrics)

        # æ‰“å°ç»“æœ
        print("\n=== æ€»ä½“è¯„ä¼°ç»“æœ ===")
        for metric, value in overall_metrics.items():
            print(f"{metric}: {value:.4f}")

        return overall_metrics, results_df
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„äº¤å‰éªŒè¯æŠ˜")
        return None, None


def main():
    """ä¸»å‡½æ•° - ç«™ç‚¹çº§äº¤å‰éªŒè¯ç‰ˆæœ¬"""
    try:
        # 1. åŠ è½½æ•°æ®
        print("åŠ è½½æ•°æ®...")
        monitor_performance("ç¨‹åºå¼€å§‹")
        if not os.path.exists('lu_onehot.xlsx'):
            raise FileNotFoundError("æ•°æ®æ–‡ä»¶ 'lu_onehot.xlsx' ä¸å­˜åœ¨")

        data = pd.read_excel('lu_onehot.xlsx')
        print(f"åŸå§‹æ•°æ®: {data.shape}")
        monitor_performance("æ•°æ®åŠ è½½å")


        # 2. å®šä¹‰ç‰¹å¾
        x_column = ['aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation',
                    'std_slope', 'std_eastness', 'std_tpi', 'std_curvature1', 'std_curvature2',
                    'std_high', 'std_aspect', 'glsnow', 'cswe', 'snow_depth_snow_depth',
                    'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'doy', 'gldas', 'year', 'month', 'scp_start',
                    'scp_end', 'd1', 'd2', 'X', 'Y', 'Z', 'da', 'db', 'dc', 'dd', 'landuse_11',
                    'landuse_12', 'landuse_21', 'landuse_22', 'landuse_23', 'landuse_24',
                    'landuse_31', 'landuse_32', 'landuse_33', 'landuse_41', 'landuse_42',
                    'landuse_43', 'landuse_46', 'landuse_51', 'landuse_52', 'landuse_53',
                    'landuse_62', 'landuse_63', 'landuse_64']
        y_column = ['swe']
        spatial_column = ['X', 'Y']
        station_column = 'station_id'

        # 3. æ•°æ®è°ƒè¯•
        if not debug_data_issues(data, x_column, y_column, spatial_column, station_column):
            raise ValueError("æ•°æ®è°ƒè¯•å‘ç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ•°æ®")

        # 4. æ•°æ®æ¸…æ´—
        clean_data = robust_data_cleaning(data, x_column, y_column, spatial_column, station_column)

        # 5. æ‰§è¡Œç«™ç‚¹çº§äº¤å‰éªŒè¯
        overall_metrics, detailed_results = station_level_cross_validation(
            clean_data, x_column, y_column, spatial_column, station_column
        )

        if overall_metrics is not None:
            print("\nğŸ‰ ç«™ç‚¹çº§äº¤å‰éªŒè¯æˆåŠŸå®Œæˆ!")
            print(f"æœ€ç»ˆç»“æœä¿å­˜åœ¨: result/cross_validation_results/")

            # æ‰“å°æœ€ä½³å’Œæœ€å·®ç«™ç‚¹
            if detailed_results is not None:
                best_station = detailed_results.loc[detailed_results['R2'].idxmax()]
                worst_station = detailed_results.loc[detailed_results['R2'].idxmin()]

                print(f"\næœ€ä½³é¢„æµ‹ç«™ç‚¹: {best_station['station_id']} (RÂ²: {best_station['R2']:.4f})")
                print(f"æœ€å·®é¢„æµ‹ç«™ç‚¹: {worst_station['station_id']} (RÂ²: {worst_station['R2']:.4f})")

        monitor_performance("ç¨‹åºç»“æŸ")

    except Exception as e:
        print(f"âŒ ä¸»ç¨‹åºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def simple_station_cv_version():
    """ç®€åŒ–ç‰ˆæœ¬çš„ç«™ç‚¹çº§äº¤å‰éªŒè¯"""
    try:
        print("å°è¯•ç®€åŒ–ç‰ˆæœ¬ç«™ç‚¹çº§äº¤å‰éªŒè¯...")

        # 1. åŠ è½½æ•°æ®
        data = pd.read_excel('lu_onehot.xlsx')
        print(f"åŸå§‹æ•°æ®: {data.shape}")

        # 2. å®šä¹‰ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
        x_column = ['elevation', 'slope', 'aspect', 'X', 'Y', 'doy', 'year', 'month']
        y_column = ['swe']
        spatial_column = ['X', 'Y']
        station_column = 'station_id'

        # 3. ç®€åŒ–æ•°æ®æ¸…æ´—
        clean_data = data.copy()
        clean_data = clean_data.dropna(subset=x_column + y_column + [station_column])

        # ç§»é™¤æ•°æ®é‡è¿‡å°‘çš„ç«™ç‚¹
        station_counts = clean_data[station_column].value_counts()
        valid_stations = station_counts[station_counts >= 3].index
        clean_data = clean_data[clean_data[station_column].isin(valid_stations)]

        print(f"ç®€åŒ–æ¸…æ´—åæ•°æ®: {clean_data.shape}")
        print(f"å¯ç”¨ç«™ç‚¹æ•°: {clean_data[station_column].nunique()}")

        # 4. æ‰§è¡Œç®€åŒ–çš„äº¤å‰éªŒè¯ï¼ˆåªè¿è¡Œå‰10ä¸ªç«™ç‚¹ä½œä¸ºæµ‹è¯•ï¼‰
        unique_stations = clean_data[station_column].unique()[:10]
        print(f"æµ‹è¯•è¿è¡Œå‰ {len(unique_stations)} ä¸ªç«™ç‚¹...")

        all_true = []
        all_pred = []

        for i, test_station in enumerate(unique_stations):
            print(f"æŠ˜ {i + 1}/{len(unique_stations)}: ç«™ç‚¹ {test_station}")

            try:
                # åˆ†å‰²æ•°æ®
                train_data = clean_data[clean_data[station_column] != test_station]
                val_data = clean_data[clean_data[station_column] == test_station]

                if len(train_data) == 0 or len(val_data) == 0:
                    continue

                # åˆå§‹åŒ–æ•°æ®é›†
                train_set, val_set, _ = datasets.init_dataset_split(
                    train_data=train_data,
                    val_data=val_data,
                    test_data=val_data,
                    x_column=x_column,
                    y_column=y_column,
                    spatial_column=spatial_column,
                    batch_size=32,  # æ›´å°çš„batch size
                    use_model="gnnwr"
                )

                # ç®€åŒ–æ¨¡å‹
                model_name = f"GNNWR_Simple_Fold_{i + 1}"
                gnnwr = models.GNNWR(
                    train_dataset=train_set,
                    valid_dataset=val_set,
                    test_dataset=val_set,
                    dense_layers=[128, 64],  # æ›´ç®€å•çš„ç½‘ç»œ
                    activate_func=nn.ReLU(),
                    start_lr=0.001,
                    optimizer="Adam",
                    model_name=model_name,
                    model_save_path="result/simple_cv_models",
                    log_path="result/simple_cv_logs",
                    write_path="result/simple_cv_runs"
                )

                # åˆ›å»ºç›®å½•
                os.makedirs("result/simple_cv_models", exist_ok=True)

                # å¿«é€Ÿè®­ç»ƒ
                gnnwr.add_graph()
                gnnwr.run(max_epoch=30, early_stop=10, print_frequency=5)

                # é¢„æµ‹
                gnnwr.load_model(f'result/simple_cv_models/{model_name}.pkl')
                val_predictions = gnnwr.predict(val_set)
                val_true = val_data[y_column[0]].values

                all_true.extend(val_true)
                all_pred.extend(val_predictions)

                print(f"æŠ˜ {i + 1} å®Œæˆ")

            except Exception as e:
                print(f"æŠ˜ {i + 1} å¤±è´¥: {e}")
                continue

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        if len(all_true) > 0:
            overall_metrics = calculate_metrics(all_true, all_pred)
            print("\nç®€åŒ–ç‰ˆæœ¬ç»“æœ:")
            for metric, value in overall_metrics.items():
                print(f"{metric}: {value:.4f}")

            # ç»˜åˆ¶æ•£ç‚¹å›¾
            plot_aggregated_scatter(np.array(all_true), np.array(all_pred), overall_metrics,
                                    "result/simple_cv_results")

            return overall_metrics
        else:
            print("âŒ ç®€åŒ–ç‰ˆæœ¬æ²¡æœ‰æˆåŠŸé¢„æµ‹")
            return None

    except Exception as e:
        print(f"ç®€åŒ–ç‰ˆæœ¬å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    # å…ˆå°è¯•å®Œæ•´ç‰ˆæœ¬
    try:
        main()
    except Exception as e:
        print(f"å®Œæ•´ç‰ˆæœ¬å¤±è´¥: {e}")
        print("\nå°è¯•ç®€åŒ–ç‰ˆæœ¬...")
        # å¦‚æœå®Œæ•´ç‰ˆæœ¬å¤±è´¥ï¼Œå°è¯•ç®€åŒ–ç‰ˆæœ¬
        simple_station_cv_version()

