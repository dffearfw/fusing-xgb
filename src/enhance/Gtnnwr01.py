import os
import sys
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))
from gnnwr.datasets import init_dataset_split
from gnnwr.models import GTNNWR
from visualizer import plot_gtnnwr_results, plot_multiple_models_results


data = pd.read_excel('lu_onehot.xlsx')
data["id"] = np.arange(len(data))
# æ·»åŠ æ··åˆåˆ†å‰²ç­–ç•¥
data['station_id'] = data['X'].astype(str) + '_' + data['Y'].astype(str)

# --- ç¬¬1æ­¥ï¼šåŸºäºåœ°å½¢ç‰¹å¾å¯¹ç«™ç‚¹è¿›è¡Œèšç±» ---
# 1.1 é€‰æ‹©ç”¨äºèšç±»çš„é™æ€ç‰¹å¾
clustering_features = ['longitude','latitude','Altitude','snowDensity','snowDepth']

# 1.2 ä¸ºæ¯ä¸ªç«™ç‚¹è®¡ç®—è¿™äº›ç‰¹å¾çš„å¹³å‡å€¼ï¼ˆæŒ‰ç«™ç‚¹èšåˆï¼‰
station_features = data.groupby('station_id')[clustering_features].mean().reset_index()

# 1.3 ğŸ”¥ã€å…³é”®ã€‘å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
# DBSCANå¯¹ç‰¹å¾çš„å°ºåº¦éå¸¸æ•æ„Ÿï¼Œå¿…é¡»å…ˆæ ‡å‡†åŒ–ï¼Œè®©æ‰€æœ‰ç‰¹å¾åœ¨åŒä¸€é‡çº§ä¸Š
scaler = StandardScaler()
features_scaled = scaler.fit_transform(station_features[clustering_features])

# 1.4 ä½¿ç”¨DBSCANè¿›è¡Œèšç±»
# epså’Œmin_samplesæ˜¯éœ€è¦è°ƒè¯•çš„æ ¸å¿ƒå‚æ•°ï¼Œä¸‹é¢ä¼šè¯¦ç»†è§£é‡Š
# æˆ‘ä»¬å…ˆç»™ä¸€ä¸ªåˆå§‹å€¼
dbscan = DBSCAN(eps=0.5, min_samples=5)
station_features['cluster'] = dbscan.fit_predict(features_scaled)

# 1.5 å¤„ç†å™ªå£°ç‚¹å¹¶ç»Ÿè®¡ç»“æœ
# DBSCANä¼šå°†å™ªå£°ç‚¹æ ‡è®°ä¸º-1ï¼Œæˆ‘ä»¬å°†å®ƒä»¬å½’ä¸ºä¸€ä¸ªå•ç‹¬çš„ç°‡ï¼Œä¿è¯æ‰€æœ‰ç«™ç‚¹éƒ½è¢«ä½¿ç”¨
station_features['cluster'] = station_features['cluster'].apply(
    lambda x: x if x != -1 else station_features['cluster'].max() + 1)
n_clusters = station_features['cluster'].nunique()

print(f"ç«™ç‚¹å·²èšç±»ä¸º {n_clusters} ç±»ã€‚")
print("å„ç°‡ç«™ç‚¹æ•°é‡ï¼š")
print(station_features['cluster'].value_counts().sort_index())

# 1.6 å°†èšç±»æ ‡ç­¾åˆå¹¶å›åŸå§‹æ•°æ®
data = pd.merge(data, station_features[['station_id', 'cluster']], on='station_id', how='left')

# --- ç¬¬2æ­¥ï¼šåœ¨æ¯ä¸ªç°‡å†…è¿›è¡Œåˆ†å±‚ç©ºé—´é‡‡æ · ---
train_stations, val_stations, test_stations = [], [], []

# å¯¹æ¯ä¸ªç°‡è¿›è¡Œç‹¬ç«‹çš„éšæœºé‡‡æ ·
for cluster_id in station_features['cluster'].unique():
    cluster_stations = station_features[station_features['cluster'] == cluster_id]['station_id'].unique()
    np.random.shuffle(cluster_stations)  # æ‰“ä¹±é¡ºåº

    # æŒ‰æ¯”ä¾‹åˆ’åˆ† (å¯ä»¥è°ƒæ•´ä¸º 8:1:1)
    n = len(cluster_stations)
    test_set = cluster_stations[:int(n * 0.1)]
    val_set = cluster_stations[int(n * 0.1):int(n * 0.2)]
    train_set = cluster_stations[int(n * 0.2):]

    train_stations.extend(train_set)
    val_stations.extend(val_set)
    test_stations.extend(test_set)

print(f"\nåˆ†å±‚é‡‡æ ·åï¼š")
print(f"è®­ç»ƒé›†ç«™ç‚¹æ•°: {len(train_stations)}")
print(f"éªŒè¯é›†ç«™ç‚¹æ•°: {len(val_stations)}")
print(f"æµ‹è¯•é›†ç«™ç‚¹æ•°: {len(test_stations)}")

# --- ç¬¬3æ­¥ï¼šæ ¹æ®åˆ’åˆ†å¥½çš„ç«™ç‚¹åˆ›å»ºæ•°æ®é›† ---
train_data_full = data[data['station_id'].isin(train_stations)].copy()
val_data_full = data[data['station_id'].isin(val_stations)].copy()
test_data_full = data[data['station_id'].isin(test_stations)].copy()

# --- ç¬¬4æ­¥ï¼šåœ¨ç©ºé—´åˆ’åˆ†çš„åŸºç¡€ä¸Šï¼Œè¿›è¡Œæ—¶é—´åˆ’åˆ† ---
# è®­ç»ƒ/éªŒè¯é›†ï¼šæ—¶é—´åˆ’åˆ†
train_val_df_sorted = train_data_full.sort_values(by=['year', 'month', 'doy'])
val_sample_count = int(len(val_data_full))  # ä½¿ç”¨éªŒè¯é›†çš„æ ·æœ¬æ•°ä½œä¸ºåˆ’åˆ†æ ‡å‡†ï¼Œä¿æŒæ¯”ä¾‹
val_data = train_val_df_sorted.iloc[:val_sample_count].copy()
train_data = train_val_df_sorted.iloc[val_sample_count:].copy()

# æµ‹è¯•é›†ï¼šä½¿ç”¨ä¸éªŒè¯é›†ç›¸åŒçš„æ—¶é—´çª—å£ï¼Œé˜²æ­¢æ³„éœ²
val_start_time = val_data['year'].min()
val_end_time = val_data['year'].max()
test_df_sorted = test_data_full.sort_values(by=['year', 'month', 'doy'])
test_data = test_df_sorted[
    (test_df_sorted['year'] >= val_start_time) & (test_df_sorted['year'] <= val_end_time)
    ].copy()

print(f"\næœ€ç»ˆæ•°æ®é›†æ ·æœ¬æ•°ï¼š")
print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_data)}")
print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_data)}")
print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_data)}")

train_dataset, val_dataset, test_dataset = init_dataset_split(train_data=train_data,
                                                              val_data=val_data,
                                                              test_data=test_data,
                                                              x_column=[
                                                                  'aspect', 'slope', 'eastness', 'tpi', 'curvature1',
                                                                  'curvature2', 'elevation', 'std_slope',
                                                                  'std_eastness', 'std_tpi', 'std_curvature1',
                                                                  'std_curvature2', 'std_high', 'std_aspect', 'glsnow',
                                                                  'cswe', 'snow_depth_snow_depth',
                                                                  'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'gldas',
                                                                  'scp_start', 'scp_end',
                                                                    'd1', 'd2','da', 'db', 'dc', 'dd',
                                                                  'landuse_11', 'landuse_12', 'landuse_21',
                                                                  'landuse_22', 'landuse_23', 'landuse_24',
                                                                  'landuse_31', 'landuse_32', 'landuse_33',
                                                                  'landuse_41',  'landuse_43',
                                                                  'landuse_46', 'landuse_51', 'landuse_52',
                                                                  'landuse_53', 'landuse_62',
                                                                  'landuse_64'
                                                              ],
                                                              y_column=['swe'],
                                                              spatial_column=['X', 'Y','Z'],
                                                              temp_column=['doy','year','month'],
                                                              id_column=['id'],
                                                              use_model="gtnnwr",
                                                              batch_size=128)


optimizer_params = {
    "scheduler":"MultiStepLR",
    "scheduler_milestones":[1000, 2000, 3000, 4000],
    "scheduler_gamma":0.8,
}
gtnnwr = GTNNWR(train_dataset, val_dataset, test_dataset, [[3], [256,128,64]],drop_out=0.4,optimizer='Adadelta',optimizer_params=optimizer_params,
                write_path = "../demo_result/gtnnwr_runs", # è¿™é‡Œéœ€è¦ä¿®æ”¹
                model_name="GTNNWR_Di")
gtnnwr.add_graph()

gtnnwr.run(100,1000)

gtnnwr.result()
save_path = "../demo_result/gtnnwr_runs/GTNNWR_DSi_results.png"

metrics = plot_gtnnwr_results(gtnnwr, save_path=save_path, show_plot=True)
