import os
import sys
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))
from gnnwr.datasets import init_dataset_split
from gnnwr.models import GTNNWR
from visualizer import plot_gtnnwr_results, plot_multiple_models_results


data = pd.read_excel('lu_onehot.xlsx')
data["id"] = np.arange(len(data))
# æ·»åŠ æ··åˆåˆ†å‰²ç­–ç•¥
data['station_id'] = data['X'].astype(str) + '_' + data['Y'].astype(str)

# å®šä¹‰ç‰¹å¾åˆ—ï¼Œä»¥ä¾¿æ•™å¸ˆæ¨¡å‹å’Œæœ€ç»ˆæ¨¡å‹å…±ç”¨
x_columns = [
    'aspect', 'slope', 'eastness', 'tpi', 'curvature1', 'curvature2', 'elevation', 'std_slope',
    'std_eastness', 'std_tpi', 'std_curvature1', 'std_curvature2', 'std_high', 'std_aspect', 'glsnow',
    'cswe', 'snow_depth_snow_depth', 'ERA5æ¸©åº¦_ERA5æ¸©åº¦', 'era5_swe', 'gldas',
    'scp_start', 'scp_end', 'd1', 'd2','da', 'db', 'dc', 'dd',
    'landuse_11', 'landuse_12', 'landuse_21', 'landuse_22', 'landuse_23', 'landuse_24',
    'landuse_31', 'landuse_32', 'landuse_33', 'landuse_41',  'landuse_43',
    'landuse_46', 'landuse_51', 'landuse_52', 'landuse_53', 'landuse_62', 'landuse_64'
]

# --- ã€æ–°å¢ã€‘ç¬¬0.5æ­¥ï¼šè®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œä¸ºèšç±»æä¾›é«˜è´¨é‡ç‰¹å¾ ---
print("=== æ­¥éª¤0.5: è®­ç»ƒæ•™å¸ˆæ¨¡å‹ä»¥ç”Ÿæˆèšç±»ç‰¹å¾ ===")
# 0.5.1 åˆ’åˆ†æ•™å¸ˆæ¨¡å‹æ•°æ® (éšæœºæŠ½å–80%çš„ç«™ç‚¹)
unique_stations = data['station_id'].unique()
np.random.shuffle(unique_stations)
teacher_stations = unique_stations[:int(0.8 * len(unique_stations))]
teacher_data = data[data['station_id'].isin(teacher_stations)].copy()

# 0.5.2 æ•™å¸ˆæ¨¡å‹æ•°æ®é›†åˆå§‹åŒ– (ç®€å•æŒ‰æ—¶é—´åˆ’åˆ†)
teacher_data_sorted = teacher_data.sort_values(by=['year', 'month', 'doy'])
val_size = int(len(teacher_data) * 0.2)
teacher_train = teacher_data_sorted.iloc[:-val_size]
teacher_val = teacher_data_sorted.iloc[-val_size:]

teacher_train_dataset, teacher_val_dataset, _ = init_dataset_split(
    train_data=teacher_train, val_data=teacher_val, test_data=teacher_val,
    x_column=x_columns, y_column=['swe'], spatial_column=['X', 'Y', 'Z'],
    temp_column=['doy', 'year', 'month'], id_column=['id'], use_model="gtnnwr",
    batch_size=128, process_fn="minmax_scale", process_var=["x", "y"], dropna=True
)

# 0.5.3 è®­ç»ƒæ•™å¸ˆæ¨¡å‹
print("å¼€å§‹è®­ç»ƒæ•™å¸ˆæ¨¡å‹...")
optimizer_params_teacher = {"scheduler": "MultiStepLR", "scheduler_milestones": [200, 400, 600, 800], "scheduler_gamma": 0.8}
teacher_model = GTNNWR(
    teacher_train_dataset, teacher_val_dataset, teacher_val_dataset,
    [[3], [128, 64]], drop_out=0.3, optimizer='Adadelta',
    optimizer_params=optimizer_params_teacher,
    write_path="../demo_result/teacher_model",
    model_name="Teacher_Model"
)
teacher_model.run(50, 500)

# 0.5.4 æå–æ¨¡å‹ç³»æ•°ä½œä¸ºèšç±»ç‰¹å¾
print("æå–æ¨¡å‹å­¦ä¹ åˆ°çš„ç©ºé—´ç³»æ•°ä½œä¸ºèšç±»ç‰¹å¾...")
# å‡è®¾ teacher_model.result() è¿”å›ä¸€ä¸ªåŒ…å«ç³»æ•°çš„ DataFrame
teacher_results = teacher_model.result()
# å¦‚æœ result() æ˜¯ä¿å­˜æ–‡ä»¶ï¼Œä½ éœ€è¦å…ˆè¯»å–å®ƒï¼Œä¾‹å¦‚ï¼š
# teacher_results = pd.read_csv("../demo_result/teacher_model/Teacher_Model_results.csv")

coef_columns = [col for col in teacher_results.columns if col.startswith('coef_')]
station_coefs = teacher_results.groupby('id')[coef_columns].mean().reset_index()
# å°† id æ˜ å°„å› station_id
id_to_station = teacher_train[['id', 'station_id']].drop_duplicates()
station_coefs = pd.merge(station_coefs, id_to_station, on='id', how='left')
print(f"æˆåŠŸä¸º {len(station_coefs)} ä¸ªç«™ç‚¹æå–äº†ç³»æ•°ç‰¹å¾ã€‚")


# --- ç¬¬1æ­¥ï¼šåŸºäºã€æ•™å¸ˆæ¨¡å‹ç‰¹å¾ã€‘å¯¹ç«™ç‚¹è¿›è¡Œèšç±» ---
# 1.1 ğŸ”¥ã€ä¿®æ”¹ã€‘ä½¿ç”¨æ•™å¸ˆæ¨¡å‹çš„ç³»æ•°ä½œä¸ºèšç±»ç‰¹å¾
clustering_features = coef_columns
station_features = station_coefs[['station_id'] + clustering_features].copy()

# 1.2 ğŸ”¥ã€å…³é”®ã€‘å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
scaler = StandardScaler()
features_scaled = scaler.fit_transform(station_features[clustering_features])

# 1.3 ä½¿ç”¨DBSCANè¿›è¡Œèšç±»
dbscan = DBSCAN(eps=0.5, min_samples=5)
station_features['cluster'] = dbscan.fit_predict(features_scaled)

# 1.4 å¤„ç†å™ªå£°ç‚¹å¹¶ç»Ÿè®¡ç»“æœ
station_features['cluster'] = station_features['cluster'].apply(
    lambda x: x if x != -1 else station_features['cluster'].max() + 1)
n_clusters = station_features['cluster'].nunique()

print(f"\nç«™ç‚¹å·²èšç±»ä¸º {n_clusters} ç±»ã€‚")
print("å„ç°‡ç«™ç‚¹æ•°é‡ï¼š")
print(station_features['cluster'].value_counts().sort_index())

# 1.5 å°†èšç±»æ ‡ç­¾åˆå¹¶å›åŸå§‹æ•°æ®
data = pd.merge(data, station_features[['station_id', 'cluster']], on='station_id', how='left')

# --- ç¬¬2æ­¥ï¼šåœ¨æ¯ä¸ªç°‡å†…è¿›è¡Œåˆ†å±‚ç©ºé—´é‡‡æ · ---
train_stations, val_stations, test_stations = [], [], []

# å¯¹æ¯ä¸ªç°‡è¿›è¡Œç‹¬ç«‹çš„éšæœºé‡‡æ ·
for cluster_id in station_features['cluster'].unique():
    cluster_stations = station_features[station_features['cluster'] == cluster_id]['station_id'].unique()
    np.random.shuffle(cluster_stations)  # æ‰“ä¹±é¡ºåº

    # æŒ‰æ¯”ä¾‹åˆ’åˆ† (å¯ä»¥è°ƒæ•´ä¸º 8:1:1)
    n = len(cluster_stations)
    test_set = cluster_stations[:int(n * 0.1)]
    val_set = cluster_stations[int(n * 0.1):int(n * 0.2)]
    train_set = cluster_stations[int(n * 0.2):]

    train_stations.extend(train_set)
    val_stations.extend(val_set)
    test_stations.extend(test_set)

print(f"\nåˆ†å±‚é‡‡æ ·åï¼š")
print(f"è®­ç»ƒé›†ç«™ç‚¹æ•°: {len(train_stations)}")
print(f"éªŒè¯é›†ç«™ç‚¹æ•°: {len(val_stations)}")
print(f"æµ‹è¯•é›†ç«™ç‚¹æ•°: {len(test_stations)}")

# --- ç¬¬3æ­¥ï¼šæ ¹æ®åˆ’åˆ†å¥½çš„ç«™ç‚¹åˆ›å»ºæ•°æ®é›† ---
train_data_full = data[data['station_id'].isin(train_stations)].copy()
val_data_full = data[data['station_id'].isin(val_stations)].copy()
test_data_full = data[data['station_id'].isin(test_stations)].copy()

# --- ç¬¬4æ­¥ï¼šåœ¨ç©ºé—´åˆ’åˆ†çš„åŸºç¡€ä¸Šï¼Œè¿›è¡Œæ—¶é—´åˆ’åˆ† ---
# è®­ç»ƒ/éªŒè¯é›†ï¼šæ—¶é—´åˆ’åˆ†
train_val_df_sorted = train_data_full.sort_values(by=['year', 'month', 'doy'])
val_sample_count = int(len(val_data_full))  # ä½¿ç”¨éªŒè¯é›†çš„æ ·æœ¬æ•°ä½œä¸ºåˆ’åˆ†æ ‡å‡†ï¼Œä¿æŒæ¯”ä¾‹
val_data = train_val_df_sorted.iloc[:val_sample_count].copy()
train_data = train_val_df_sorted.iloc[val_sample_count:].copy()

# æµ‹è¯•é›†ï¼šä½¿ç”¨ä¸éªŒè¯é›†ç›¸åŒçš„æ—¶é—´çª—å£ï¼Œé˜²æ­¢æ³„éœ²
val_start_time = val_data['year'].min()
val_end_time = val_data['year'].max()
test_df_sorted = test_data_full.sort_values(by=['year', 'month', 'doy'])
test_data = test_df_sorted[
    (test_df_sorted['year'] >= val_start_time) & (test_df_sorted['year'] <= val_end_time)
    ].copy()

print(f"\næœ€ç»ˆæ•°æ®é›†æ ·æœ¬æ•°ï¼š")
print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_data)}")
print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_data)}")
print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_data)}")

train_dataset, val_dataset, test_dataset = init_dataset_split(train_data=train_data,
                                                              val_data=val_data,
                                                              test_data=test_data,
                                                              x_column=x_columns,
                                                              y_column=['swe'],
                                                              spatial_column=['X', 'Y','Z'],
                                                              temp_column=['doy','year','month'],
                                                              id_column=['id'],
                                                              use_model="gtnnwr",
                                                              batch_size=128)


optimizer_params = {
    "scheduler":"MultiStepLR",
    "scheduler_milestones":[1000, 2000, 3000, 4000],
    "scheduler_gamma":0.8,
}
gtnnwr = GTNNWR(train_dataset, val_dataset, test_dataset, [[3], [256,128,64]],drop_out=0.4,optimizer='Adadelta',optimizer_params=optimizer_params,
                write_path = "../demo_result/gtnnwr_runs", # è¿™é‡Œéœ€è¦ä¿®æ”¹
                model_name="GTNNWR_Di")
gtnnwr.add_graph()

gtnnwr.run(100,1000)

gtnnwr.result()
save_path = "../demo_result/gtnnwr_runs/GTNNWR_DSi_results.png"

metrics = plot_gtnnwr_results(gtnnwr, save_path=save_path, show_plot=True)
